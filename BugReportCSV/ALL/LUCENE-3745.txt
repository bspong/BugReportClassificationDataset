Need stopwords and stoptags lists for default Japanese configuration
Stopwords and stoptags lists for Japanese needs to be developed tested and integrated into Lucene. I m attaching some lexical assets that are useful for building stopwords and stoptag lists. The frequency lists are made from 1.5 million segmented Japanese Wikipedia documents from after some scrubbing and handling. I d prefer to use a more balanced corpus for this but I believe Wikipedia will be fine for this. The following files are attached in TSV format using UTF-8 encoding top-pos.txt - Part-of-speech tag distribution top-100000.txt - Top 100 000 most frequent surface forms and their frequencies top-1000000-pos.txt - Top 1 000 000 most frequent surface form and part-of-speech tag combinations and their frequencies There s also a tool filter stoptags.py attached that reads a set of stoptags and evaluates it on top-1000000-pos.txt to give us an idea what passes through any given stoptag set. An example with my current stoptag set is given below. filter stoptags.py -s stoptags.txt top-1000000-pos.txt stop freq 14426806 pos - stop freq 14212851 pos - stop freq 10553747 pos - stop freq 8956177 pos - stop freq 8757138 pos - - stop freq 7723958 pos - - stop freq 7417005 pos - stop freq 7366368 pos stop freq 5427730 pos - - stop freq 4874861 pos - pass freq 4312613 pos - stop freq 3702106 pos - - stop freq 3485125 pos - stop freq 3049861 pos - stop freq 3045461 pos - pass freq 2722773 pos - pass freq 2441965 pos - stop freq 2403133 pos stop freq 2250725 pos - stop freq 1962142 pos - pass freq 1959374 pos - pass freq 1937789 pos - stop freq 1927529 pos - - pass freq 1796435 pos - - stop freq 1701848 pos - stop freq 1697926 pos - - stop freq 1672052 pos - stop freq 1414661 pos - - stop freq 1400235 pos stop freq 1319235 pos - pass freq 1272503 pos - - stop freq 1254673 pos stop freq 1110771 pos - pass freq 1037815 pos - - stop freq 1002940 pos - - stop freq 989166 pos - pass freq 923836 pos - ... I ll submit a patch for this tomorrow. Please find a patch attached. I ve made stoptags.txt lighter by not stopping all prefixes and also allowing auxiliary verbs and interjections to pass. I didn t come across any occurrences of unclassified symbols in Wikipedia but it is now stopped as that seem to align better with our overall stop approach for symbols. Many of the most frequent terms that now pass have been re-introduced in stopwords.txt so they are stopped using a StopFilter instead of KuromojiPartOfSpeechStopFilter. I believe this configuration is more balanced. Overall I ve used the term frequencies attached to as a governing guideline for what to introduce into stopwords.txt. It mostly contains hiragana words and expressions and I ve deliberately left out common kanji as I d like to keep the stopping fairly light. I ll create a separate JIRA for introducing stopwords and stoptags to Solr. Thanks for doing this it will be much nicer to have a properly built configuration here I agree with the overall approach of leaning towards the conservative side if someone wants they can always be more aggressive and use the data on this issue as a guide . Thanks a lot for looking at this Robert. This was the thinking. I ve referred to the issue in the stopwords and stoptags files. Lets get my previous ad-hoc lists out of there I ll commit this for now and if there are any concerns we can reopen or refine in further issues. Thanks Christian 
