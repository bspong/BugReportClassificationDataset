make BaseCharFilter more efficient in performance
Performance degradation in Solr 1.4 was reported. See http www.lucidimagination.com search document 43c4bdaf5c9ec98d html stripping slower in solr 1 4 The inefficiency has been pointed out in BaseCharFilter javadoc by Mike NOTE This class is not particularly efficient. For example a new class instance is created for every call to addOffCorrectMap int int which is then appended to a private list. i haven t benchmarked to see if this is any faster maybe even worse. but its no longer a linear algorithm Why did this cause Solr to slowdown... Did Solr previously have a more efficient impl and then they cutover to Lucene s Patch looks like it should be a good net net improvement Ð lookups of the offset correction should now be fast though insertion cost is probably higher Ð we create likely 3 new objects 2 ints one TreeMap Entry per insert but I expect that s a good tradeoff. Why did this cause Solr to slowdown... Did Solr previously have a more efficient impl and then they cutover to Lucene s Solr used another Filter in 1.3. Ahh ok. Probably we should switch to parallel arrays here to make it very fast... yes this will consume RAM 8 bytes per position if we keep all of them . Really most apps do not need all positions stored ie they only need to see typically the current token. So maybe we could make a filter that takes a lookbehind size and it d only keep that number of mappings cached That d have to be the max size of any token you may analyze so hard to bound perfectly but eg setting this to the max allowed token in IndexWriter would guarantee that we d never have a miss For analyzers that buffer tokens... they d have to set this max to infinity or ensure they remap the offsets before capturing the token s state Mark did some quick tests and this patch only seems to make things slower. Really most apps do not need all positions stored ie they only need to see typically the current token. I think this is why it got slower with my patch in practice it didn t matter that this thing did backwards linear lookup due to this reason I think this is why it got slower with my patch in practice it didn t matter that this thing did backwards linear lookup due to this reason Ahh yes since presumably the test was simply looking up the offsets for the current token... I think the best way to proceed would be to make it easy to benchmark CharFilters in contrib benchmark especially this HTML stripping one. Honestly we don t even know for sure any performance degradation reported in the original link is really due to BaseCharFilter yet so I think we need to benchmark and profile. ok i think this one is fixed. i ran a loop with the example doc in the tests and tested both removing the object creation and switching to binary search both help. I d like to commit to trunk and 3x tomorrow. here are the files i tested htmlStripCharFilterTest.html from the test 12kb file and http en.wikipedia.org wiki Benjamin Franklin 360kb file i ran each 3 times file before after htmlStripCharFilterTest.html 9709ms 9560ms 9587ms 8755ms 8697ms 8708ms benFranklin.html 26877ms 26963ms 26495ms 17593ms 17674ms 17694ms here was the code crude but i think it shows the point the larger the files the worse the offset correction was Charset charset Charset.forName UTF-8 WhitespaceTokenizer tokenizer new WhitespaceTokenizer Version.LUCENE CURRENT new StringReader long startMS System.currentTimeMillis for int i 0 i 10000 i InputStream stream HTMLStripCharFilterTest.class.getResourceAsStream htmlStripReaderTest.html HTMLStripCharFilter reader new HTMLStripCharFilter CharReader.get new InputStreamReader stream charset tokenizer.reset reader tokenizer.reset while tokenizer.incrementToken System.out.println time System.currentTimeMillis - startMS Patch looks good Robert I added a check code for currentOff before doing binary search. Committed 990161 trunk 990167 3x reopening for potential backport. I think this one n 2 is really buggish territory the original user reports 2x slowdown with solr 1.4 I think the fix is safe its baked in trunk 3.x a while and if we have perf bugs like this with no api breaks it makes sense... but if someone objects I won t backport. Robert please backport. Committed revision 1029077 to 3.0.x. Committed revision 1029087 to 2.9.x. I also cleared up the svn eol-style problems on these branches if you are merging on windows you shouldn t see any property conflicts anymore.
