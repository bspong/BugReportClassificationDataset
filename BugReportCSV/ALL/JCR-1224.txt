Release references to JCR items in tearDown
On my 64-Bit environment OS JVM I tried a mvn clean install and got an OutOfMemory Exception. On my 32-Bit environment Mac OSX 10.5 Java 1.5 the tests were all fine and the IndexMerger was significant faster. Running org.apache.jackrabbit.test.TestAll 21.11.2007 10 29 51 INFO IndexMerger IndexMerger merged 549 documents in 289 ms into a. IndexMerger.java line 304 21.11.2007 10 29 55 ERROR main ImportHandler fatal error encountered at line 1 column 10 while parsing XML stream org.xml.sax.SAXParseException Attribute name is associated with an element type this must be followed by the character. ImportHandler.java line 116 21.11.2007 10 29 55 ERROR main ImportHandler fatal error encountered at line 1 column 10 while parsing XML stream org.xml.sax.SAXParseException Attribute name is associated with an element type this must be followed by the character. ImportHandler.java line 104 21.11.2007 10 29 59 ERROR main ImportHandler fatal error encountered at line -1 column -1 while parsing XML stream org.xml.sax.SAXParseException Premature end of file. ImportHandler.java line 104 21.11.2007 10 29 59 ERROR main ImportHandler fatal error encountered at line -1 column -1 while parsing XML stream org.xml.sax.SAXParseException Premature end of file. ImportHandler.java line 116 21.11.2007 10 30 45 INFO IndexMerger IndexMerger merged 555 documents in 2015 ms into l. IndexMerger.java line 304 21.11.2007 10 33 13 INFO IndexMerger IndexMerger merged 412 documents in 25587 ms into w. IndexMerger.java line 304 Exception in thread Timer-1 java.lang.OutOfMemoryError Java heap space         at org.apache.lucene.store.BufferedIndexOutput. init BufferedIndexOutput.java 26         at org.apache.lucene.store.FSDirectory FSIndexOutput. init FSDirectory.java 592         at org.apache.lucene.store.FSDirectory.createOutput FSDirectory.java 435         at org.apache.lucene.util.BitVector.write BitVector.java 122         at org.apache.lucene.index.SegmentReader.doCommit SegmentReader.java 236         at org.apache.lucene.index.IndexReader.commit IndexReader.java 794         at org.apache.lucene.index.FilterIndexReader.doCommit FilterIndexReader.java 190         at org.apache.lucene.index.IndexReader.commit IndexReader.java 825         at org.apache.lucene.index.IndexReader.close IndexReader.java 841         at org.apache.jackrabbit.core.query.lucene.AbstractIndex.close AbstractIndex.java 327         at org.apache.jackrabbit.core.query.lucene.MultiIndex DeleteIndex.execute MultiIndex.java 1715         at org.apache.jackrabbit.core.query.lucene.MultiIndex.executeAndLog MultiIndex.java 936         at org.apache.jackrabbit.core.query.lucene.MultiIndex.flush MultiIndex.java 880         at org.apache.jackrabbit.core.query.lucene.MultiIndex.checkFlush MultiIndex.java 1110         at org.apache.jackrabbit.core.query.lucene.MultiIndex.access 100 MultiIndex.java 75         at org.apache.jackrabbit.core.query.lucene.MultiIndex 1.run MultiIndex.java 324         at java.util.TimerThread.mainLoop Timer.java 512         at java.util.TimerThread.run Timer.java 462 21.11.2007 10 34 37 ERROR main DatabasePersistenceManager failed to write node state cfbffd6d-114d-4738-9383-48da2b5dbc1d DatabasePersistenceManager.java line 441 java.lang.OutOfMemoryError Java heap space         at java.util.Properties LineReader. init Properties.java 346         at java.util.Properties.load Properties.java 284 Hi I also reproduced an OutOfMemory error with a test class similar to FirstHop.java. Here are some findings. Jackrabbit version 1.3.3 Environment Windows XP Pro v.2002 SP2 AMD Athlon 64 1GB RAM JRE Sun JRE 1.5.0 11 Error message java.lang.OutOfMemoryError Java heap space Stack from Eclipse debugger Thread main Suspended breakpoint at line 140 in IndexMerger IndexMerger.indexAdded String int line 140 MultiIndex AddIndex.execute MultiIndex line 1362 MultiIndex.executeAndLog MultiIndex Action line 872 MultiIndex.commitVolatileIndex line 926 MultiIndex.flush line 810 Recovery.run line 172 Recovery.run MultiIndex RedoLog line 85 MultiIndex. init File SearchIndex ItemStateManager NodeId Set NamespaceMappings line 297 SearchIndex.doInit line 295 SearchIndex AbstractQueryHandler .init QueryHandlerContext line 44 SearchManager.initializeQueryHandler line 478 SearchManager. init SearchConfig NamespaceRegistryImpl NodeTypeRegistry ItemStateManager NodeId SearchManager NodeId line 231 RepositoryImpl WorkspaceInfo.getSearchManager line 1580 RepositoryImpl.initWorkspace RepositoryImpl WorkspaceInfo line 570 RepositoryImpl.initStartupWorkspaces line 379 RepositoryImpl. init RepositoryConfig line 286 RepositoryImpl.create RepositoryConfig line 521 TransientRepository 2.getRepository line 245 TransientRepository.startRepository line 265 TransientRepository.login Credentials String line 333 TransientRepository.login line 388 JcrTest.main String line 65 My test class Findings Drilled down to an infinite loop in org.apache.jackrabbit.core.query.lucene.IndexMerger line 140 class IndexMerger extends Thread implements IndexListener ...     void indexAdded String name int numDocs ...                 while upper maxMergeDocs                     indexBuckets.add new IndexBucket lower upper true                     lower upper 1                     upper mergeFactor ...                  .. From repository.xml SearchIndex            param name minMergeDocs value 100            param name maxMergeDocs value 2147483647            param name mergeFactor value 10 The upper variable init at 100 which grows by a factor of 10 constantly skips into negative territory e.g. 1215752192 -727379968 and would probably never equal to 2147483647 before running out of heap space. SearchIndex from my repository.xml copied from website sample and modified the persistence manager to SimpleDbPersistenceManager for MySQL ...          SearchIndex class org.apache.jackrabbit.core.query.lucene.SearchIndex              param name path value default index              param name useCompoundFile value true              param name minMergeDocs value 100              param name volatileIdleTime value 3              param name maxMergeDocs value 2147483647              param name mergeFactor value 10              param name maxFieldLength value 10000              param name bufferSize value 10              param name cacheSize value 1000              param name forceConsistencyCheck value false              param name enableConsistencyCheck value false              param name autoRepair value true              param name analyzer value org.apache.lucene.analysis.standard.StandardAnalyzer              param name queryClass value org.apache.jackrabbit.core.query.QueryImpl              param name respectDocumentOrder value true              param name resultFetchSize value 2147483647              param name extractorPoolSize value 0              param name extractorTimeout value 100              param name extractorBackLogSize value 100          SearchIndex      Workspace Changing the value of maxMergeDocs didn t help as in this particular stack trace the value of IndexMerger.maxMergeDocs was reset to 2147483647. Regards Al I just found out that there is another configuration called workspace.xml under the workspace folder copied from repository.xml. After changing the maxMergeDocs values from 2147483647 to 1000000 in that file I finally got around this OutOfMemory error. I just found out that there is another configuration called workspace.xml under the workspace folder copied from repository.xml. After changing the maxMergeDocs values from 2147483647 to 1000000 in that file I finally got around this OutOfMemory error. Yes this is correct. The repository.xml is only used as a template when there is not yet a workspace.xml. I am afraid this bug is now the default setting since with JCR-1238 maxMergeDocs default is set to Interger.MAX VALUE.Checked with trunk. On 23 oct 2008 the int upper has been changed to long upper . The bug does not occur anymore with 1.4 not yet released and higher. Using 1.3 and older you should not set maxMergeDocs to Interger.MAX VALUE which could lead to large timeouts in 1.3 anyway also solved in trunk I will close this issue since it is outdated 23 oct 2008 must be 23 oct 2007 - Already fixed by changing the IndexMerger int upper to long upper by Marcel on 23 oct 2007Sorry but this effect occurs with the actual trunk. I classified this as a bug for version 1.4 and not 1.3. svn info Path . URL http svn.apache.org repos asf jackrabbit trunk Repository Root http svn.apache.org repos asf Repository UUID 13f79535-47bb-0310-9956-ffa450edef68 Revision 601305 Node Kind directory Schedule normal Last Changed Author dpfister Last Changed Rev 600980 Last Changed Date 2007-12-04 16 41 25 0100 Tue 04 Dec 2007 Do you need more infos about my environment This is the output from mvn clean install from today ...... 05.12.2007 13 04 11 ERROR main ImportHandler fatal error encountered at line 1 column 10 while parsing XML stream org.xml.sax.SAXParseException Attribute name is associated with an element type this must be followed by the character. ImportHandler.java line 122 05.12.2007 13 04 14 ERROR main ImportHandler fatal error encountered at line -1 column -1 while parsing XML stream org.xml.sax.SAXParseException Premature end of file. ImportHandler.java line 122 05.12.2007 13 04 49 INFO IndexMerger IndexMerger merged 558 documents in 788 ms into l. IndexMerger.java line 304 05.12.2007 13 06 02 INFO IndexMerger IndexMerger merged 536 documents in 3512 ms into w. IndexMerger.java line 304 Exception in thread Timer-1 java.lang.OutOfMemoryError Java heap space         at java.lang.StringCoding CharsetSD.decode StringCoding.java 183         at java.lang.StringCoding.decode StringCoding.java 228         at java.lang.String. init String.java 405         at java.lang.String. init String.java 433         at java.io.UnixFileSystem.list Native Method         at java.io.File.list File.java 937         at java.io.File.list File.java 968         at org.apache.lucene.store.FSDirectory.list FSDirectory.java 320         at org.apache.lucene.index.IndexFileDeleter. init IndexFileDeleter.java 131         at org.apache.lucene.index.IndexReader.commit IndexReader.java 784         at org.apache.lucene.index.FilterIndexReader.doCommit FilterIndexReader.java 190         at org.apache.lucene.index.IndexReader.commit IndexReader.java 825         at org.apache.jackrabbit.core.query.lucene.CommittableIndexReader.commitDeleted CommittableIndexReader.java 68         at org.apache.jackrabbit.core.query.lucene.AbstractIndex.commit AbstractIndex.java 322         at org.apache.jackrabbit.core.query.lucene.AbstractIndex.commit AbstractIndex.java 310         at org.apache.jackrabbit.core.query.lucene.MultiIndex.flush MultiIndex.java 877         at org.apache.jackrabbit.core.query.lucene.MultiIndex.checkFlush MultiIndex.java 1110         at org.apache.jackrabbit.core.query.lucene.MultiIndex.access 100 MultiIndex.java 75         at org.apache.jackrabbit.core.query.lucene.MultiIndex 1.run MultiIndex.java 324         at java.util.TimerThread.mainLoop Timer.java 512         at java.util.TimerThread.run Timer.java 462 Reopened Apparently my conclusion was premature regarding the issue. As Al Capone wrote Findings Drilled down to an infinite loop in org.apache.jackrabbit.core.query.lucene.IndexMerger line 140 class IndexMerger extends Thread implements IndexListener ...     void indexAdded String name int numDocs ...                 while upper maxMergeDocs                     indexBuckets.add new IndexBucket lower upper true                     lower upper 1                     upper mergeFactor ...                  .. and AFAIU this is solved in 1.4. apparently the problem still occurs in 1.4 but AFAICS it must be a different reasonAndreas can you please run the build again with the JVM option -XX HeapDumpOnOutOfMemoryError If you can make the dump available for download somewhere that would be great.sorry Marcel but my JVM doesn t know this option. Please tell me what jvm this supports. java -XX HeapDumpOnOutOfMemoryError Unrecognized VM option HeapDumpOnOutOfMemoryError Could not create the Java virtual machine. jul lysiosdev jackrabbit java -version java version 1.5.0 12 Java TM 2 Runtime Environment Standard Edition build 1.5.0 12-b04 Java HotSpot TM 64-Bit Server VM build 1.5.0 12-b04 mixed mode oops the plus sign is missing. The option should be -XX HeapDumpOnOutOfMemoryErrorpart 1 of heappart 2 of heap pls use cat bunzip2Changing component. The dump shows that the error occurs while building testing the jcr2spi module.Andreas can you build the modules if you set the change the jackrabbit-jcr2spi pom.xml and give the test some more memory Index pom.xml pom.xml revision 597629 pom.xml working copy -76 7 76 7               include TestAll.java include             includes             forkMode once forkMode - argLine -Xmx128m -enableassertions argLine argLine -Xmx256m -enableassertions argLine             systemProperties               property                 name derby.system.durability name thanx Marcle. That helps I made a svn up on the trunk and run the build with 128m OutOfHeapMemory. Than run again with 256m o.k. Can you tell me witch action uses the mem Each JUnit test keeps references to JCR items which are not set to null when the test is teared down. We already had a similar issue JCR-1224. Back then we reduced the memory which is kept if the session is logged out but the session instance is still referenced. It seems this not sufficient. I think it s time to change the test cases.Renamed this issue instead of creating a new one to adapt the test cases.Test cases in jackrabbit-jcr-tests now set references to items and sessions to null in tearDown . svn revision 602129Fixed test cases in jackrabbit-core. svn revision 602131Test cases in jackrabbit-jcr2spi are now also fixed. svn revision 602133 128 mb heap size should be more than enough now...
