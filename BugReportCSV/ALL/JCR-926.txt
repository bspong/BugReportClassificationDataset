Global data store for binaries
There are three main problems with the way Jackrabbit currently handles large binary values 1 Persisting a large binary value blocks access to the persistence layer for extended amounts of time see JCR-314 2 At least two copies of binary streams are made when saving them through the JCR API one in the transient space and one when persisting the value 3 Versioining and copy operations on nodes or subtrees that contain large binary values can quickly end up consuming excessive amounts of storage space. To solve these issues and to get other nice benefits I propose that we implement a global data store concept in the repository. A data store is an append-only set of binary values that uses short identifiers to identify and access the stored binary values. The data store would trivially fit the requirements of transient space and transaction handling due to the append-only nature. An explicit mark-and-sweep garbage collection process could be added to avoid concerns about storing garbage values. See the recent NGP value record discussion especially 1 for more background on this idea. 1 http mail-archives.apache.org mod mbox jackrabbit-dev 200705.mbox 3c510143ac0705120919k37d48dc1jc7474b23c9f02cbd mail.gmail.com 3e Attached DataStore.patch is a first draft of what such a data store could look like. The main interface is simply     public interface DataStore         DataRecord getRecord DataIdentifier identifier throws IOException         DataRecord addRecord InputStream stream throws IOException      The patch contains a simple file-based implementation of this interface. See the javadocs in the patch for more details. I will proceed to propose a way to integrate this concept in the existing Jackrabbit core if there s consensus that this approach is worth pursuing.Attached a prototype patch that integrates the data store concept in Jackrabbit. The integration is very ugly at this stage and the attached code is definitely not meant for inclusion as-is. For very rough performance testing I created a simple test application that creates a versionable folder with 100 files in it each containing about 270kB of application octet-stream data. Once populated the entire folder was checked into the version store. The numbers averaged over a couple of test runs are Current svn trunk     100 added files 5625 milliseconds     checkin everything 11094 milliseconds With this patch     100 added files 2750 milliseconds     checkin everything 1906 milliseconds Note that the checkin performance boost will likely be even more impressive with bundle persistence. For this test I used the old style database persistence that is still the default configuration.Hi I wrote a small benchmark application to better understand the problem Upload of a large file will block other concurrent actions as described here http issues.apache.org jira browse JCR-314 http www.mail-archive.com users jackrabbit.apache.org msg02503.html However in the current Jackrabbit it looks like this problem is solved. Here is what I tried - For 10 seconds a new thread is added each second - 2 threads write large files the others just write simple nodes - I made tests with small 8 KB up to large 16 MB files To compare the results my application has a mode where the file is not sent to Jackrabbit instead it is written to disk RandomAccessFile . I wanted to find out how long the simple threads are blocked by one thread writing a large files. The results are - Storing the file outside the repository is about 30 faster    but the reason might be the write buffer size or so . - When a thread writes a large object the other threads are not blocked badly.   At least not more than if the file is stored on the same disk.    If you want to not block others when writing large objects the only solution I found is to store large objects on another hard drive. I have tested this as well and it completely solves the problem. Thomas This may be due to caching on the LocalItemStateManager level. If you create a new session each time you read from the workspace or do random reads on a larger workspace the reading sessions will be blocked while the binary is written. I ve committed a test case that illustrates the problem http svn.apache.org repos asf jackrabbit trunk jackrabbit-core src test java org apache jackrabbit core ReadWhileSaveTest.java Using the default locking strategy in SharedItemState manager the read thread is able to read 127 times on my laptop. When using the fine-grained locking the number goes up to 372.I haven t used the test case with the DataStore patch but I expect that it will be similar to the number of reads for the fine-grained locking.I made a few modifications to ReadWhileSaveTest to better illustrate the problem. See the attached patch that instead of saving a number of 10MB files saves a single 300MB file. It also keeps track of how many times the root node is traversed while the 300MB file is being persisted. The raw output of a test run is below     Wed Jun 20 23 41 17 EEST 2007 - setProperty - 0     Wed Jun 20 23 41 39 EEST 2007 - begin save - 195     Wed Jun 20 23 42 05 EEST 2007 - end save - 197     numReads 198 Essentially      setProperty 22 seconds during which 195 root node traversals happened      save 26 seconds during which 2 root node traversals happened The two traversals reported for save most likely happened between the println and save statements. Observations 1 Currently we create one extra copy of the binary stream the write performance would essentially be doubled simply by removing that extra copy. The stream passed to setProperty should be given directly to the DataStore implementation so no extra copies are needed. 2 More alarmingly this seems to indicate that the fine grained locking from JCR-314 does not work as well as it should i.e. a save still blocks readers. Note that I explicitly added a save call after the stuff node is added to make sure that the write should not affect nodes that are being read. I ran the test against latest svn trunk revision 549230. I ran ReadWhileSaveTest test with the patch applied using the FineGrainedISMLocking many times and the results are completely different from the former one showing in each case that this locking strategy clearly improves concurrency. These are the output of two test runs Wed Jun 20 19 03 54 PDT 2007 - setProperty - 1 Wed Jun 20 19 04 14 PDT 2007 - begin save - 186 Wed Jun 20 19 04 36 PDT 2007 - end save - 402 numReads 403 Wed Jun 20 19 18 31 PDT 2007 - setProperty - 1 Wed Jun 20 19 18 49 PDT 2007 - begin save - 175 Wed Jun 20 19 19 09 PDT 2007 - end save - 373 numReads 373 In all the runs I got similar results. PabloHmm you re correct. I assumed that FineGrainedISMLocking was already the default in latest svn trunk but it isn t. After enabling it I see similar results as you do     Thu Jun 21 11 44 29 EEST 2007 - setProperty - 0     Thu Jun 21 11 44 50 EEST 2007 - begin save - 196     Thu Jun 21 11 45 13 EEST 2007 - end save - 405     numReads 406 For the record the same test run with DataStore2.patch applied     Thu Jun 21 13 41 43 EEST 2007 - setProperty - 0     Thu Jun 21 13 41 54 EEST 2007 - begin save - 81     Thu Jun 21 13 41 57 EEST 2007 - end save - 113     numReads 113 The total time is down from 44 to 14 seconds. - The above comment is probably not comparable with previous numbers the setProperty time should not change considerably change with the DataStore patch in fact it should take a bit longer due to the SHA-1 calculation . To avoid things like disk caches to interfere with the test I increased the size of the test file to 3GB I only have 1GB RAM . With DataStore2.patch and FineGrainedISMLocking the result is     Thu Jun 21 13 51 09 EEST 2007 - setProperty - 1     Thu Jun 21 13 55 17 EEST 2007 - begin save - 2338     Thu Jun 21 13 55 18 EEST 2007 - end save - 2352     numReads 2353 setProperty 248 seconds save 1 second Without DataStore2.patch but with FineGrainedISMLocking the result is     Thu Jun 21 14 08 33 EEST 2007 - setProperty - 0     Thu Jun 21 14 12 58 EEST 2007 - begin save - 2419     Thu Jun 21 14 17 03 EEST 2007 - end save - 4766     numReads 4816 setProperty 265 seconds save 245 seconds I guess the stream copy algorithm in FileDataStore is slightly faster than the one in BLOBFileValue otherwise the numbers are pretty much as expected.This is a patch to clean up InternalValue.internalValue Before replacing the BLOBFileValue class with the new Global Data Store implementation I wanted to clean up a few things in InternalValue. The method InternalValue.internalValue was called a lot in the Jackrabbit code. It returns a java.lang.Object which was then cast to the required class. This has a few disadvantages - Unnecessary casts in many places - Hard to change the internal working of InternalValue specially replace BLOBFileValue - A few times instanceof was used making it hard to change BLOBFileValue - For developers new to the Jackrabbit code like me it s not always easy to understand what is going on in the code - NodeIndexer used the java.lang.Object directly assuming the implementation will always use Boolean Long Double BLOBFileValue and so on objects. In this patch I added specific getter methods to InternalValue like done in the Value interface. Additionally there are getPath for PropertyType.PATH getQName for PropertyType.NAME and getUUID for PropertyType.REFERENCE . I had to make a few assertions some of them were not 100 clear from the code so could you please review them - The value of InternalValue is never null .     ValueConstraint was checking for null but as far as I see   it is never really possible to have a null value. - The type of QName.JCR FROZENUUID is STRING Object.toString was used before . - The type of QName.JCR MIMETYPE is STRING - The type of QName.JCR ENCODING is STRING - Currently for types PropertyType.BINARY the object is always   a BLOBFileValue there was no other constructor for PropertyType.BINARY NodeIndexer still has a few unnecessary type casts addBinaryValue addBinaryValue ... but the methods are protected and I was afraid to change them right now. I hope those can be changed soon to avoid a few unnecessary casts and conversions Long Double . There are no functional changes yet in this patch as far as I see . But I think this patch is required otherwise subsequent changes will be much harder. I didn t remove InternalValue.internalValue so far but set it to deprecated . I hope it can be removed in the near future.   Thomas I had to make a few assertions some of them were not 100 clear from the code so could you please review them - The value of InternalValue is never null . ValueConstraint was checking for null but as far as I see it is never really possible to have a null value. - The type of QName.JCR FROZENUUID is STRING Object.toString was used before . - The type of QName.JCR MIMETYPE is STRING - The type of QName.JCR ENCODING is STRING - Currently for types PropertyType.BINARY the object is always a BLOBFileValue there was no other constructor for PropertyType.BINARY all the above assumptions are correct.Looks good thanks Committed internalValue.patch as-is in revision 550493. This change also makes it easier to switch to an InternalValue class hierarchy as in InternalLongValue etc. later on if we want. Hi This is a refactoring patch for GlobalDataStore. The patch introduces DataStore almost wherever it is required but the behavior is not yet changed the data store is disabled . This patch may break backwards compatibility. NodeImpl.internalCopyPropertyFrom Never used removed. ItemStateBinding.readState and writeState Never used removed. Deprecated class org.apache.jackrabbit.core.state.PMContext and org.apache.jackrabbit.core.state.util.Serializer Removed. Adding a parameter would break backwards compatibility anyway. The parameter DataStore store was added to many constructors and methods. I don t like it. Would there be a better way to do it Idea create a new class RepositoryContext with getNodeTypeRegistry maybe getNamespaceResolver getNamespaceRegistry and getDataStore . Pass this object where appropriate. Sometimes BLOBs are used only for a short time. I renamed the method create InputStream in to createTemporary. BLOBFileValue is now an abstract class. The original implementation was renamed to BLOBFileValueOld . This is only a temporary class until DataStore is done . There is also BLOBFileValueMemory for very small binary properties a few hundres bytes but currently not used. The DataStore parameter is still missing in InternalValue.valueOf this method is never called for BINARY types this will be changed. InternalValue BOOLEAN TRUE and BOOLEAN FALSE is fixed now. A few notes about the FileDataStore implementation I didn t change Jukka s implementation so far but I have a few ideas Currently all files are stored in the same directory. However this is a problem for Windows XP and may be other file systems . I would limit the number of files in the data store root directory to 1024. Afterwards create subdirectories data1024-2047 data2048-3071 ... with 1024 files each. When required FileDataStore reads the directory list. If faster one index file per directory could be created. The file name is currently the SHA-1 digest. I suggest to use SHA-256 unless it is a lot slower or not available on some systems . Yes you can call me paranoid. SHA-1 could be broken in a few years. As the file name I would use id - digest .data. As the DataIdentifier use id - digest . This would speed up finding files when reading as id 1024 is the directory direct lookup . Also this would allow to bundle data files in tar files. Tar file support would be priority 2. I would only bundle very small 4 KB files in tar files anyway. Priority 3 would be compression for text data mainly . There is no garbage collection at this time. This still needs to be implemented. Thomas I forgot to click on Grant license to ASF for inclusion in ASF works .With this patch both the old style BLOBStore and the new style DataStore implementations co-exist. The BLOBStore is used by default. To use the DataStore set the System Property org.jackrabbit.useDataStore to true as in java -Dorg.jackrabbit.useDataStore true ... So this patch can be used to test the GlobalDataStore feature. All unit tests pass. There are still a few things missing There is no garbage collection yet. Almost each blob creates two new subdirectories this works but is a bit slower and means lots of directories can be avoided maybe . The abstract class BLOBFileValue is now called BLOBValue because it is now not always a file . The old BLOBFileValue is now again named BLOBFileValue. New class Base64ReaderInputStream for BufferedStringValue to avoid creating a file when converting long Base64 strings to BINARY data. Actually the higher performance is just a side effect the main reason to implement this was becuase the old constructor is based on a file resource and can t be used with the DataStore. Which is the revision the last version of the binary data store patch should be applied to I would like to have both the old style BLOBStore and the new style DataStore implementations co-exist and the clean up of the InternalValue.internalValue method The simplest steps that I found to apply the dataStore3.patch was - checkout revision 552445 revision of the files modified by this patch - delete org.apache.jackrabbit.core.data package already exists in revision 552445 - make a copy of the file BLOBFileValue.java as BLOBValue.java can t find the file to patch at line 2659 - apply dataStore3.patch Both with the data store feature disabled org.jackrabbit.useDataStore false and with this feature enabled org.jackrabbit.useDataStore true all TCK tests passed using maven mvn test . For some reasons I don t know yet running these tests with the data store enabled within Eclipse IDE I occasionally got 46 errors and 10 failures. I would like to start contributing testing this feature. I have a couple of questions about the data store. Given the append-only nature of this feature when a node with a binary property is removed binary content remains in the data store. When do you expect to have a garbage collection process of binary content files in the file system Are you planning to provide a database-backed implementation of the data store Regards PabloThis patch contains a background garbage collection implementation. The algorithm is - Remember when the scan started - Start an EventListener - Tell the DataStore to update the last modification date of files that are read    usually only storing files or adding a link to an existing file   updates the modification date but now during the GC also reading does - Recursively iterate through all nodes - If the node contains binary properties start reading them   but close the input stream immediately.   This updates the modification date - If new nodes are added the EventListener does the same    recurse through all added nodes .   Actually it would only be required to scan the moved nodes   but not sure how to do that. - The application needs to call scan for each workspace    this is not done yet not sure how to get the list of workspaces . - When the scan is done wait one second. This is for the EventListener   to catch up. How long do we have to wait for the observation listeners   Is there a way to force Jackrabbit to call the observation listeners - Then delete all data records that where not modified since GC scan started. To test the garbage collection there is also a simple application BlobGCTest.java . This is not yet a unit test it is a standalone application. It creates a few nodes node1 node2 node2 nodeWithBlob node2 nodeWithTemporaryBlob Then it deletes nodeWithTemporaryBlob. The file is still in the data store afterwards. Then the garbage collection is started. While the scan is running after node1 was scanned but before node2 the node2 nodeWithBlob is moved to node1 nodeWithBlob. Usually the garbage collection wouldn t notice this as the scan was past node1 already . But because of the EventListener it scans the moved node as well at the very end usually . The output is scanning... scanned node1 moved node2 nodeWithBlob to node1 scanned node2 identifiers   17ec4a160f44f9467b4204aa20e5981d9508c4df   74b5b1b26f806661292b9add2e78f671cf06f432 stop scanning... scanned node1 nodeWithBlob deleting... identifiers   17ec4a160f44f9467b4204aa20e5981d9508c4df This is a patch for revision 553213 actually the revision number is in the patch as well . To delete files early in the garbage collection scan we could do this          A If garbage collection was run before see if there a file with the list of UUIDs uuids.txt .          B If yes and if the checksum is ok read all those nodes first if not so many .     This updates the modified date of all old files that are still in use.     Afterwards delete all files with an older modified date than the last scan     Newer files and files that are read have a newer modification date. C Delete the uuids.txt file in any case .          D Iterate recurse through all nodes and properties like now.     If a node has a binary property store the UUID of the node in the file uuids.txt .     Also store the time when the scan started.          E Checksum and close the file.          F Like now delete files with an older modification date than this scan. We can t use node path for this UUIDs are required as nodes could be moved around. Thomas your patch references org.apache.jackrabbit.benchmark.RandomInputStream wich is not included in the patch and I couldn t find it anywhere. RegardsThe garbage collection implementation has a few disadvantages for example you can t stop and restart it. I suggest to get the nodes directly from the persistence manager. To do this I suggest to add a new method to AbstractBundlePersistenceManager protected synchronized NodeIdIterator getAllNodeIds NodeId startWith int maxCount startWith can be null in which case there is no lower limit. If maxCount is 0 then all node ids are returned. Like this you can stop and restart it Day 1 getAllNodeIds null 1000  ... iterate though all node ids  ... remember the last node id of this batch for example 0x12345678 Day 2 getAllNodeIds 0x12345678 1000  ... iterate though all node ids  ... remember the last node id of this batch ... New nodes are not a problem as the modified date of a node is updated in this case. AbstractBundlePersistenceManager.getAllNodeIds could be used for other features later on for example fast repository cloning backup . ThomasWhat needs to be done to commit this to Jackrabbit My patches get bigger and more complicated...I m sorry I haven t had the cycles lately to properly review the patches. Would it be possible to split them to smaller semi-independent pieces For example we could commit the data store implementation independent of the integration to rest of the Jackrabbit core.Hi To help integrate the changes more quickly I suggest to split the big patches into multiple smaller ones. Here is the addition of the garbage collector. This patch does not affect currently used code. ThomasHi Is there anything I can do to help speed up integrating the global data store changes It s a bit frustrating for me if I have to wait and can t do anything. ThomasRevision 570033 garbage collection implementation for the global data storeRevision 570040 garbage collection implementation for the global data storeRevision 570336 BLOBFileValue and InternalValue refactoring improved GarbageCollectorDoes the GlobalDataStore also prevent the BundleDBPersistenceManager to load the binary property automatically when you get a node I have posted this a few weeks ago in the mailinglist an jukka told me that this problem will hopefully be fixed with the GlobalDataStore. Does the GlobalDataStore also prevent the BundleDBPersistenceManager to load the binary property automatically when you get a node Yes. If Global Data Store is enabled larger binary properties are be stored there and loaded from there. Only the DataIdentifier a String and small binaries up to 1 KB or so needs to be tested will be stored in the persistence manager. thanks for the quick answer thomas. ok and where will the datastore be stored on the filesystem. can i configure it because i think for data backup and recory process it will be very interesting to save the files on a huge fast filesystem. SAN where will the datastore be stored on the filesystem. This will be a configuration option for the FileDataStore Revision 570407 add DataStore to constructorsRevision 570439 re-added useful methodsRevision 570439 renamed BLOBFileValue to BLOBValue new abstract class BLOBFileValueRevision 571094 global data store new in-memory data store and temp file BLOB The data store can now be tested however it is disabled by default. To enable set the system property org.jackrabbit.useDataStore to true before starting the application java -Dorg.jackrabbit.useDataStore true ... this does not work not sure why mvn -Dorg.jackrabbit.useDataStore true ... hi thomas first ... great work can you explain how to configure the datastore or is this feature not yet implemented i mean will the datastore be configureable in the workspace.xml .. i think so each workspace can have its own datastore to define different backup solutions .. thanks claus Nice work this does not work not sure why mvn -Dorg.jackrabbit.useDataStore true ... Maven probably forks a separate JVM instance for running the test suite.Hi great work Most of it was Jukkas work. configure the datastore That s the next priority. Currently you need to use system properties. each workspace can have its own datastore No there is only one data store per repository. Technically it would be possible to store the files in the different workspaces but in my view a lot of the benefit would be lost. different backup solutions Backup online and incremental is important. It is very easy to backup the data store just copy all files. They are never modified and only renamed from temp file to live file. Deleted only when no longer used by the garbage collector . But I know only a few use cases backup everything backup incrementally backup with data compression backup with encryption and restore . What other backup use cases solutions do you have in mind ok great work to both of you - I think only one datastore is not a good way. we have jackrabbit running in a model 3 architecture with one repository and for each application one workspace. the problem is not the backup solution we would prefere backup incremential by third party solutions. at the moment we define for each workspace different persistencemanagers to different db servers. we want for each workspace application define different SAN storage places because we must discount the storage volume for each application. if we have only one datastore it is not possible to know how much space each application consumes. hope for a feature to define that. one thing at the end ... what do you mean with Deleted only when no longer used by the garbage collector . the files in the datastore are the permanent files or not thanks clausHi one repository and for each application one workspace. Why not one repository for each application Like this you can limit the heap memory as well. the files in the datastore are the permanent files If things get deleted the space must eventually be reclaimed unless you work for a hard drive company . Thomasthe heap is not the problem ... we have a lot - i think to use different workspaces is for us the better way .. what are the benefits you would lost with a datastore per workspace i see the datastore as enhancement for the persistencemanager because the other properties will be stored through the persistencemanager per workspace and now we will put everything from all workspaces into one datastore greets clausA central idea of the Global Data Store is that its global to the repository especially to drive down the costs of versioning and other cross-workspace operations. It would in principle be feasible to allow a workspace-specific data store to be configured but that would make handling of cross-workspace operations considerably more complex. IMHO the benefits of workspace-local data stores wouldn t be worth the added complexity. On a longer timescale I also believe Jackrabbit should be moving even more to centralized repository-global resource handling as that would for example help a lot in making things like versioning operations transactional. As for features like per-workspace quota or backups I think those would be best achieved by implementing the features in Jackrabbit instead of relying on the underlying storage mechanism.As far as I understand one important use case is to use one workspace for authoring and another for production . The workspaces contain mostly the same data maybe 90 is the same . Having a data store for each workspace would mean having to copy all large files. Having one data store saves you 50 of the space for large objects . Also you can move data from one workspace to the other very quickly because the files don t have to be copied only the identifiers . Also cloning of a workspace is very fast for the same reasons. i think to use different workspaces is for us the better way .. Do you know about blob store If not you should try it out because it sounds like this would be exactly what you need. The blob store already available. ok if you have a use case as you described i think a global datastore is the best way to make cross-workspace operations more easy. you have only one file and no copies af same files. if the road goes to a centralized repository a global datastore makes of course sense. in my case and i think other has also a similarly use case a per workspace datastore makes things easier i am working for a government and the office employee get a lot of paper every day. they scan it and put it into jackrabbit. now we must keep the documents based on the law up to 5-7 years with fast read access in jackrabbit. after that time we can archive it slow access and therefore we want to store this documents not on a SAN storage because its expensive rather save it to a cheaper storage system tape drive system we have planed to make this with moving the data from one workspace SAN to a other one tape drive system with the global datastore is this not possible i think how would you solve such scenarios greets claus Hi Theoretically the data store could be split to different directories hard drives. Content that is accessed more often could be moved to a faster disk and less used data could eventually be moved to slower cheaper disk. That would be an extension of the memory hierarchy see also http en.wikipedia.org wiki Memory hierarchy . Of course this wouldn t limit the space used per workspace but would improve system performance if done right. Maybe we need to do that anyway in the near future to better support solid state disk. Workspace usage I m not sure what solution would be the best for your use case. Maybe this would better be discussed in a separate thread. See also http wiki.apache.org jackrabbit DavidsModel head-ca639e0ee110b80e8277a50f9b9de092b5d86427Revision 571399 Follow Jackrabbit code style. Better do that now before it is too late before re-formatting makes comparing version difficultRevision 573209 Configuration is now supported. Still the system property org.jackrabbit.useDataStore is required to enable this feature but now the data store class and for the FileDataStore the path can be configured Repository      DataStore class org.apache.jackrabbit.core.data.FileDataStore          param name path value rep.home repository      DataStore     .... Repository The DataStore API was changed a bit to support this. The DataStore configuration is optional if missing the system almost works as now. Almost because the BLOBValue class is no longer used. The system property org.jackrabbit.useDataStore will be removed when this is tested. Also the system property org.jackrabbit.minBlobFileSize will be integrated into DataStore. My idea is that each data store implementation file system database S3 can have a different minimum size depending on the overhead to store load a value. By the way the FileDataStore overhead mainly calculating the SHA-1 digest is quite low smaller than 10 Writing and reading 5 files 100 KB each average over 5 runs FileDataStore 1390 ms FileOutputStream 1287 ms Revision 574543 The FileDataStore now supports the configuration option minRecordLength default 100 . Missing unit tests have been added the test code coverage is now OK. A bug has been fixed when importing a temp file was created but never stored in the data store . The setting org.jackrabbit.useDataStore can now be enabled in my view I will do this in a day or two unless there is a problem. Then the new classes BLOBInResource BLOBInTempFile BLOBInMemory are used instead of BLOBValue. The file data store is still only used when configured in repository.xml. Temp files are still created in some cases for example importing .Revision 15482 Remove unused methods hi thomas what do you think about migration steps for existing workspaces greets claus Compatibility the plan is to make this feature fully backward compatible. When the system property org.jackrabbit.useDataStore is enabled and when FileDataStore is configured in repository.xml old repositories can still be used. Only new binary objects are stored in the data store and only when this is configured in repository.xml. Migration to data store binary data that is currently in the blob store or in the persistence manager can be moved to the data store by cloning a workspace or export import. Migration away from the data store currently you need to export the data with the data store enabled configured and then import into a new repository without data store.Migration from one data store type to another Currently not required as there is only the file data store. A tool will be added when needed. The API required is already there DataStore.getAllIdentifiers .Currently clustering requires all blobs to be stored in the database to ensure transaction semantics. But in our application we would prefer to store blobs externally for performance reasons. I am assuming that with the global data store we would be able to that since blobs in the transient space would be stored separately. Please let me know if this is correct as this will resolve a big issue that we are currently facing with Jackrabbit. Thanks Pankaj clustering - store blobs externally for performance reasons with the global data store we would be able to that Yes clustering is supported. Entries are added as early as possible and deleted only when they are not reachable garbage collection . There is no update operation only add new entry . Data is added before the transaction is committed. Additions are globally atomic cluster nodes can share the same data store. Even different repositories can share the same store as long as garbage collection is done correctly. Advantages no duplicate entries saves space speeds up versioning workspace cloning node copy operations . Improved concurrency. Disadvantage garbage collection is required. There are ideas how to efficiently do that without having to scan through the whole repository .Revision 575304 enabling the data store To use the FileDataStore add this to your repository.xml after the Repository start tag      DataStore class org.apache.jackrabbit.core.data.FileDataStore The files will be stored under repository datastore by default. For more information about the configuration see FileDataStore.javaNice Time to resolve this issue and file new issues for future fixes enhancements 
