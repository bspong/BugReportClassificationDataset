Merge error during add to index IndexOutOfBoundsException 
I ve been batch-building indexes and I ve build a couple hundred indexes with a total of around 150 million records. This only happened once so it s probably impossible to reproduce but anyway... I was building an index with around 9.6 million records and towards the end I got this java.lang.IndexOutOfBoundsException Index 54 Size 24 at java.util.ArrayList.RangeCheck ArrayList.java 547 at java.util.ArrayList.get ArrayList.java 322 at org.apache.lucene.index.FieldInfos.fieldInfo FieldInfos.java 155 at org.apache.lucene.index.FieldInfos.fieldName FieldInfos.java 151 at org.apache.lucene.index.SegmentTermEnum.readTerm SegmentTermEnum.java 149 at org.apache.lucene.index.SegmentTermEnum.next SegmentTermEnum.java 115 at org.apache.lucene.index.SegmentMergeInfo.next SegmentMergeInfo.java 52 at org.apache.lucene.index.SegmentMerger.mergeTermInfos SegmentMerger.java 294 at org.apache.lucene.index.SegmentMerger.mergeTerms SegmentMerger.java 254 at org.apache.lucene.index.SegmentMerger.merge SegmentMerger.java 93 at org.apache.lucene.index.IndexWriter.mergeSegments IndexWriter.java 487 at org.apache.lucene.index.IndexWriter.maybeMergeSegments IndexWriter.java 458 at org.apache.lucene.index.IndexWriter.addDocument IndexWriter.java 310 at org.apache.lucene.index.IndexWriter.addDocument IndexWriter.java 294 Hi Daniel. There is probably not much we can do without more information. If you can create a small index that causes this error and put together some code that we can run and see the error then we can step through Lucene and find the source of the problem. Are you using Lucene 1.4.3 I m using 1.4.3. I ll give a try at re-indexing the data that caused the problem. I figured there wouldn t be much you can do - as I said I ve indexed about 100 million records in different indexes several times and this is the first time I ve seen this error. It was the first time dealing with the particular data that the error occurred on too. Anyway I ll give a try at reproducing it and if I can I ll let you know. I have now seen a related issue twice java.lang.IndexOutOfBoundsException Index 125 Size 31 at java.util.ArrayList.RangeCheck ArrayList.java 547 at java.util.ArrayList.get ArrayList.java 322 at org.apache.lucene.index.FieldInfos.fieldInfo FieldInfos.java 155 at org.apache.lucene.index.FieldsReader.doc FieldsReader.java 66 at org.apache.lucene.index.SegmentReader.document SegmentReader.java 237 at org.apache.lucene.index.SegmentMerger.mergeFields SegmentMerger.java 185 at org.apache.lucene.index.SegmentMerger.merge SegmentMerger.java 92 And I can reproduce it .....on 1.4.3 When FSDirectory.createFile creates a FSOutputStream the random access file may already exist and contain data. The content is not cleaned out. So if segment merging is taking place to a new segment and the merge has written data to this file ....and the machine crashes app is terminated .... you can end up with a partial or full segment file that the segment infos knows nothing about. If you restart then any merge will try to reuse the same file name...and the content it contains..... To reproduce the issue I created the next segment file by copying one that already exists .... and bang....on the next merge I suggest that in FSOutputStream sets the file length to 0 on initialisation as well as opening the channel to the file which can aslo produce some nasty deferred IO erorrs in windows XP a least I am not sure of any side effect of this but will test it. We are seeing this 2-3 times a day if under heavy load or single thread and killing the app at random which may be in the procedss of a segment write... We have tested the above solution pretty heavily since 18 11 2005 and would regard it as stable in 1.4.3. Looking at the 1.9 code stream the issue is likely to be present unless there is some other code that checks if an index segment file already exists when not expected or the next segement is generated based in the segments that actually exist in the directory. In 1.4.3 ..... in FSDirectory ...... final class FSOutputStream extends OutputStream RandomAccessFile file null public FSOutputStream File path throws IOException file new RandomAccessFile path rw file.setLength 0 file.getChannel ...... will sort this issue and some other file handle issues I have seen under XP Something similar is likely to be required in FSIndexOutput in the 1.9 code line I m not sure I understand... the new file will be the output or result of a merge not the input right Is the problem that the output is appended to an existing file In that case I can see the logic of the file.setLength 0 in your fix. Could you elaborate more on the effects of file.getChannel Is it really needed The problem is that the output is going into a file that already exists. I assume it leaves and then finds old bits during random access and gets confused. If a merge fails while it is writing its output segment file you have a segment file that contains rubbish. This can occur if you are unlucky when you kill the JVM and to repeat the problem set a break point and kill the JVM just before the segment write completes . The next time a merge takes place it writes to the segment file that already exists - as the same file name is generated for the new segment file. It always blows with an error similar to that reported for this bug. The file.getChannel solved some fairly odd but repeatable issues with stale invalid file handles under windows XP. the fact that getChannel has side effects in Windows makes me a little uncomfortable about what other side effects it has on other platforms. Is it only needed when truncating a previously existing file or is it always needed to solve fairly odd but repeatable issues with stale invalid file handles under windows XP If the former we could do the following public FSOutputStream File path throws IOException file new RandomAccessFile path rw if file.length 0 file.setLength 0 file.getChannel .close solved some fairly odd but repeatable issues with stale invalid file handles under windows XP. file.getChannel was added on windows. It was before the truncating file issue was found and resolved. It is possible the two are related. I have not verified and tested the same issue on linux. We had just not seen it on other platforms. It is possible file.setLength 0 also resolves the above issue. It certainly solves some JVM crash recovery issues. Andy how easy is it for you to replicate the problems that getChannel solved Is it possible to check if the addition of setLength alone solves the problem. getChannel really shouldn t be needed. note the close on the fileChannel in my version isn t right since it will cause everything to be closed. Thanks Andy I ve committed a patch to set the length to zero if it wasn t already. If this doesn t also fix the some fairly odd but repeatable issues with stale invalid file handles under windows XP. issue please open a new bug. This bug was imported from Bugzilla doesn t have a status and hence I can t resolve it. Anyone else with higher JIRA perms Yonik is this right code if file.length 0 This can happen if there was a previous crash unclean shutdown that left files around then we end up re-using a segment name. If we have a logging framework in the future a warning here might be a good idea. file.setLength 0 maybe it should be if file.length 0 moreover FSDirectory.createDirectory which calls FSIndexOutput constructor already has a check for file existance. Thanks for catching that Volodymyr. I did a further search of the code and FSIndexOutput is only instantiated in one place createOutput which already does a check if the file exists and if so deletes it. If it can t delete it an exception is thrown. So while this patch may have been valid for 1.4 it is no longer needed for Lucene 2.0 Does that look right If so I ll revert it. Closing a jira bug prevented me from closing this earlier .
