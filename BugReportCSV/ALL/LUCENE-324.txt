org.apache.lucene.analysis.cn.ChineseTokenizer missing offset decrement
Apparently in ChineseTokenizer offset should be decremented like bufferIndex when Character is OTHER LETTER. This directly affects startOffset and endOffset values. This is critical to have Highlighter working correctly because Highlighter marks matching text based on these offset values. Created an attachment id 13749 Patch for ChineseTokenizer to correctly count offsets Ray is there a simple way you can show that this is indeed a needed fix Maybe a short class that shows that offsets are wrong. Lucene developers can anyone confirm whether this is really needed it I don t use ChineseTokenizer enough to know for sure if this is a good fix or something that will break the code. Created an attachment id 13758 Testcase that tests ChineseTokenizer and OTHER LETTER offsets The problem arises when OTHER LETTER characters and the rest of the characters are mixed together. When given a string a b tokens and corresponding offsets should be the following a 0 1 1 2 b 2 3 I haven t done a formal trace of the code yet but I think it would make sense that the offset should only be incremented if the character is pushed into the buffer. Current code howerver increments offset by default regardless whether the character is pushed into the buffer. If that s the case then there are more places that needs to be fixed. Ray - let s see if JIRA can handle Chinese Sorry for the delay in applying this patch.
