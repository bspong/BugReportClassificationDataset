benchmark for collation
Steven Rowe attached a contrib benchmark-based benchmark for collation both jdk and icu under LUCENE-2084 along with some instructions to run it... I think it would be a nice if we could turn this into a committable patch and add it to benchmark. Attached .zip d patch over 10MB because of the 4 languages LineDocs integrated into the Ant build for the ICU contrib rather than integrated into the Benchmark build. Invoke using ant benchmark from the contrib icu directory. Hi Steven one idea we can do here is to put the large files online zipped and have ant download them when we run the benchmark... I can put them on people.apache.org for this purpose we already do the same with the huge wikipedia file... what do you think Works for me. I do have one concern though the LineDocSource parser doesn t know how to handle comments so these four files don t have Apache2 license declarations in them. We should put a README or something like it with these files to indicate the license. Different subject I m not sure where it would go but the code I used to produce these top-TF wikipedia files may be useful to other people - where do you think it could live An example maybe I do have one concern though the LineDocSource parser doesn t know how to handle comments so these four files don t have Apache2 license declarations in them. We should put a README or something like it with these files to indicate the license. Are they really apache license or derived from wikipedia content if these files are only being downloaded when you run ant benchmark for collation then it is just like the enwiki task in benchmark it downloads some huge wikipedia data and runs it. So someone please correct me if I am wrong but I don t think we should be putting apache license headers in these files anyway its just like the benchmark enwiki task we are not shipping it with our source distribution. Different subject I m not sure where it would go but the code I used to produce these top-TF wikipedia files may be useful to other people - where do you think it could live An example maybe hmm I will have to think about this... anyone got ideas I think this would be useful too I admit to not having yet looked at the implementation here are two examples Karl could use this to evaluate his swedish stemming improvements taking frequency into account. the obvious use of when you need to build a stopword list these top terms are where you want to start. ... these four files don t have Apache2 license declarations in them. We should put a README or something like it with these files to indicate the license. Are they really apache license or derived from wikipedia content ... I don t think we should be putting apache license headers in these files Hmm I just assumed that since these files were not anything even close to verbatim copies that they were independently licensable new works but it s definitely more complicated than that... This looks like the place to start where licensing is concerned http en.wikipedia.org wiki Wikipedia Copyright My way non-expert reading of this is that Wikipedia-derived works and I m pretty sure these frequency lists qualify as such must be licensed under the Creative Commons Attribution-Share Alike 3.0 Unported license which does not appear to me to be entirely compatible with the Apache2 license. So I agree with you - with the caveat that some form of attribution and a pointer to licensing info should be included with these files. Hi Robert In the new version of the patch ant benchmark from the contrib icu directory attempts to download the attached tar.bz2 file from http people.apache.org rmuir wikipedia please change this to the location where you end up putting the file then unpacks the archive to the contrib icu src benchmark work directory then compiles and runs the benchmark. In addition to the top 100K word lists the tar.bz2 file contains LICENSE.txt which contains links to the Wikipedia dumps from which the lists were extracted along with a link to the license that Wikipedia uses. Hi Steve thanks a lot for your work here this is nice. I put the files in my apache directory but modified your patch somewhat I moved it to the benchmark package proper I created two tasks NewLocale and NewCollationAnalyzer The NewLocale task sets a Locale in the PerfRunData in my opinion this is reusable beyond collation since it could be used for testing Localized sorting and range searching as well it supports all Locale params lang country variant The NewCollationAnalyzer creates a ICU CollationKeyAnalyzer with the collator defaults depending upon this Locale right now i only added options to specify impl impl jdk or impl icu but in the future we can add strength decomposition etc. Take a look and let me know what you think surely I made some mistakes but it appears to work fine. Looks good. I like the way you ve integrated it into the benchmark suite and as you say the NewLocaleTask should prove useful elsewhere. I put the files in my apache directory but modified your patch somewhat One major thing you changed but didn t mention above is that rather than applying the collation key transform only to the LineDoc body field it s now applied also to the title and date fields. Given the nature of the top 100k words files Ð the title is an integer representing term frequency and the date is essentially meaningless the date on which I created the file Ð I don t think this makes sense and that s why I made analyzers that only applied collation to the body field . Steve ahh i was wondering about the per-field analyzer wrapper sorry i neglected to mention this i just forgot about it ... there are likely other problems too and the new stuff needs tests. What about this per-field thing what if in the data files title and date were simply blank Or should we worry I agree its stupid does it skew the results though One way to look at it is that its also fairly realistic even though its meaningless you see numbers and dates everywhere . The downside to doing per-analyzer wrapper is that it introduces some complexity in all honesty this is not really specific to this collation task right i.e. the existing analysis tokenization benchmarks have this same problem Steven another idea what if we simply added the options to DocMaker so we could turn off the tokenization of title and date fields right now you can only control this stuff for the body fields. imo this would be the best solution. edit ok this is already there just a little strange. we can use doc.tokenized to turn it off for these other fields and doc.body.tokenized turned on so that we are only analyzing the field we want. i ll update the alg file and produce a new patch What about this per-field thing what if in the data files title and date were simply blank Hmm although the date field value is meaningless I like the TF-in-title-field thing. Or should we worry I agree its stupid does it skew the results though One way to look at it is that its also fairly realistic even though its meaningless you see numbers and dates everywhere . I was thinking that it would and that it s not really a meaningful test of collation - who s going to bother running collation over integers and dates - but since the comparison here is between two implementations of collation I think you re right that there is no skew in doing this comparison icu kiwi icu apple icu orange jdk kiwi jdk apple jdk orange instead of this one keyword kiwi keyword apple icu orange keyword kiwi keyword apple jdk orange where the icu X transform keyword X icu-collation X and similarly for the jdk X transform The downside to doing per-analyzer wrapper is that it introduces some complexity in all honesty this is not really specific to this collation task right i.e. the existing analysis tokenization benchmarks have this same problem Yup you re right. A general facility to do this will end up looking modulo syntax like Solr s per-field analysis specification. Steven another idea what if we simply added the options to DocMaker so we could turn off the tokenization of title and date fields Good idea i ll update the alg file and produce a new patch Excellent thanks Steven I also havent forgotten about your other contribution the thing that creates the benchmark corpus in the first place from wikipedia. One idea I had would be that such a thing wouldn t be too out of place in the open relevance project... munging corpora etc ok i think we might be close to something committable now wrote tests for NewLocaleTask and NewCollationAnalyzerTask set doc.stored false doc.tokenized false doc.body.tokenized true in the collation.alg file i moved the two scripts into a scripts directory i thought this made more sense I also renamed the bm2jira.pl script to collation.bm2jira.pl here is the output from ant collation from the benchmark package Language java.text ICU4J KeywordAnalyzer ICU4J Improvement English 10.78s 7.32s 1.58s 60 French 11.48s 7.52s 1.59s 67 German 11.19s 7.52s 1.61s 62 Ukrainian 13.03s 8.68s 1.66s 62 i think its more accurate relative to KeywordAnalyzer now that we aren t storing the body text in a stored field and things like that but of course you can change the .alg file to see if the differences matter in the context of overall indexing by turning these back on. fix a bug in testCollator assertEqualCollation so its actually testing correct behavior rather than being a no-op ok somehow it completely bypassed my brain you are using ReadTokens task so this is a problem because ReadTokens does not respect the DocMaker configuration. In my opinion it should not tokenize fields unless they are configured to be tokenized. So I added the following in this patch to fix this for final Fieldable field fields if field.isTokenized continue now we get the results we expect Language java.text ICU4J KeywordAnalyzer ICU4J Improvement English 3.43s 2.21s 1.15s 115 French 3.78s 2.37s 1.17s 117 German 3.84s 2.42s 1.18s 115 Ukrainian 5.81s 3.67s 1.24s 88 if you comment out the doc.tokenized false then you get the other results i just posted instead as it will analyze the other fields too. Works for me JAVA java version 1.5.0 15 Java TM 2 Runtime Environment Standard Edition build 1.5.0 15-b04 Java HotSpot TM 64-Bit Server VM build 1.5.0 15-b04 mixed mode OS cygwin WinVistaService Pack 2 Service Pack 26060022202561 Language java.text ICU4J KeywordAnalyzer ICU4J Improvement English 5.53s 2.03s 1.20s 422 French 6.41s 2.13s 1.19s 455 German 6.36s 2.19s 1.22s 430 Ukrainian 8.92s 3.62s 1.21s 220 I just ran the contrib benchmark tests and I got one test failure junit Testcase testReadTokens org.apache.lucene.benchmark.byTask.TestPerfTasksLogic FAILED junit expected 3108 but was 3128 junit junit.framework.AssertionFailedError expected 3108 but was 3128 junit at org.apache.lucene.benchmark.byTask.TestPerfTasksLogic.testReadTokens TestPerfTasksLogic.java 480 junit at org.apache.lucene.util.LuceneTestCase.runBare LuceneTestCase.java 212 junit junit junit Test org.apache.lucene.benchmark.byTask.TestPerfTasksLogic FAILED I think NewCollationAnalyzerTask should be a little more careful about parsing its parameters - here s a slightly modified version of your setParams that understands impl jdk and complains about unrecognized params Override public void setParams String params super.setParams params StringTokenizer st new StringTokenizer params while st.hasMoreTokens String param st.nextToken StringTokenizer expr new StringTokenizer param String key expr.nextToken String value expr.nextToken for now we only support the impl parameter. TODO add strength decomposition etc if key.equals impl if value.equalsIgnoreCase icu impl Implementation.ICU else if value.equalsIgnoreCase jdk impl Implementation.JDK else throw new RuntimeException Unknown parameter param else throw new RuntimeException Unknown parameter param Steven I also havent forgotten about your other contribution the thing that creates the benchmark corpus in the first place from wikipedia. One idea I had would be that such a thing wouldn t be too out of place in the open relevance project... munging corpora etc Interesting idea thanks - I ll take a look at what s there now and see how my stuff would fit in. I just ran the contrib benchmark tests and I got one test failure OK I think this is from adjusting the ReadTokensTask... I looked at the test and i think it .... should be improved.... corrected this testReadTokens it tests by adding up token freq across the index and comparing it to the number of tokens read but there is a non-tokenized but indexed field DocMaker.ID FIELD the keywords from this should not add to the expected count. added your addt l parameter checking for NewCollationAnalyzerTask 1 tests all pass and ant collation produced expected output. One minor detail though - shouldn t the output files be renamed to identify their purpose similarly to how you renamed bm2jira.pl Here s the relevant section in contrib benchmark build.txt property name collation.output.file value working.dir benchmark.output.txt property name collation.jira.output.file value working.dir bm2jira.output.txt Steven thanks in addition to your comments I also changed the config to download the top100k files to the temp directory and expand to work top100k-out consistent with the other benchmark datasets. I think this one is ready. If there is no objection I will commit in a day or two. 1 once again tests all pass and ant collation produced expected output. Committed revision 898491. Thanks Steven 
