Token of returns in CJKTokenizer new TestCJKTokenizer
The string returns as Token in the boundary of two byte character and one byte character. There is no problem in CJKAnalyzer. When CJKTokenizer is used with the unit it becomes a problem. Use it with Solr etc. patch attached. I don t fully understand the problem report. What problem are you seeing and does the unit test you included in the patch thank you cover that case The current CJKTokenizer returns a redundant empty string at the end of token stream when it tokenizes CJK characters. String str C1C2C3 Tokenizer tokenizer new CJKTokenizer new StringReader str for Token token tokenizer.next token null token tokenizer.next System.out.println token token.termText This should be token C1C2 token C2C3 but the current CJKTokenizer outputs token C1C2 token C2C3 token The attached test case reproduce this problem and the patch solves it. I attached Solr analysis screen to show the problem and the result of the patch. Hi Koji The test class in your patch is a nice addition. There is no problem in CJKAnalyzer. The only reason that CJKAnalyzer doesn t have this problem is that the empty string is one of the stopwords it filters out from CJKTokenizer s output The following part of your patch appears to address a problem that you haven t covered in your comments - is this so If it is a problem separate from the empty-string issue can you describe the effects of this change -175 8 175 9 length 0 preIsTokened false - break continue else tokenType double break The other part of your patch reads -236 8 237 13 - return new Token new String buffer 0 length start start length String tokenString new String buffer 0 length if dataLen -1 .equals tokenString return null else return new Token tokenString start start length tokenType Wouldn t it be simpler clearer to test length for zero instead of constructing a String and testing it for equality with the empty string if length 0 String tokenString new String buffer 0 length return new Token tokenString start start length tokenType else return null Hi Steven The test class in your patch is a nice addition. Thanks but the attached patch was written by Toru. Matsuzawa-san can you follow up the comments Sorry Toru I saw Koji s two most recent comments on the issue and assumed that he was the reporter and didn t scroll up and check . Thank you for Sekiguchi-san and Steven comment. I am sorry for slow comment . The following part of your patch appears to address a problem that you haven t covered in your comments - is this so If it is a problem separate from the empty-string issue can you describe the effects of this change In current CJKTokenizer C3 becomes Single of non-ascii as shown by the following examples. C1C2C3 is non-ascii String str C1C2abcC3def Tokenizer tokenizer new CJKTokenizer new StringReader str for Token token tokenizer.next token null token tokenizer.next System.out.println token token.termText type token.type current CJKTokenizer outputs token C1C2 type double token type single token abc type single token C3 type single token def type single applying patch token C1C2 type double token C2 type double token abc type single token C3 type double token def type single Wouldn t it be simpler clearer to test length for zero instead of constructing a String and testing it for equality with the empty string I think that your correction is better. I think that your correction is better. Cool the LUCENE-973.patch file incorporates this change - I had to add a recursion in order to handle this properly. Also Added a test for a single CJK character stream Sync d with the current trunk mostly reusable Token changes Cleaned up formatting a little Switched token types to be constant ints instead of hard-coded strings Note that this patch is nice not just because of the bug it fixes but also mainly because of the unit tests it adds Ð currently CJKTokenizer has no tests. A patch attached which uses unicode character representations uNNNN style to avoid Japanese characters in the test code. You guys looking for this for 2.9 If so any volunteers If I assign myself any more I won t likely get to them all. 1 from me for inclusion in 2.9. Mark as you wrote a couple of hours ago on java-dev in response to Robert Muir s complaint about the lack of tests in contrib we should probably push for tests or write them before committing more often. Here s a chance to improve the situation this issue adds a test to a contrib module where there currently are none very nice. although it might be a tad trickier to convert to the new API anything with tests is easier in other words i have the existing cjktokenizer converted but whose to say I did it right So the latest patch is ready to go in I guess I could take this one then. I ll take it Mark Fixes a bug and adds tests for CJKAnalyzer Tokenizer where there were none before.... big step forward Does anyone know if the added recursive call to next is bounded Ie is there any way that the next method could hit length 0 an unbounded number of times in a row for some unlucky input text I don t want to run out of stack... sounds like another good test case add a few thousand 0-length tokens to the stream and see what happens... Well my question is is there any input text that would cause an arbitrary number of such 0-length tokens in a row Eg the original cause of that was just at the boundary of two byte character and one byte character... so if that s the only case that hits 0-length token then we are OK. But if there are other cases such that one could chain any number of such tokens in sequence we re not and we have to translate recursion into iteration. Michael i don t see anything obvious but a test case for two byte char one byte char tons of english text and or whitespace and or punctuation would make me feel better Or... how about we just switch to iteration not recursion I attached a patch w that change... Or... how about we just switch to iteration not recursion I attached a patch w that change... 1 from me - thanks Mike michael thanks. OK I will commit shortly Another one down Thanks for persisting on this Toru Koji Steven Mark Robert... this one was open for far toooo long.
