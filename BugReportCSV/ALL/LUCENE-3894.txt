Make BaseTokenStreamTestCase a bit more evil
Throw an exception from the Reader while tokenizing stop after not consuming all tokens sometimes spoon-feed chars from the reader... Patch tests pass. I had to fix up Edge NGramTokenizers to work w spoon feeding but otherwise no analyzers seem to be failing at least on one run... I had to do some sneaky things with MockTokenizer to work around its state machine... Fixed a few things... 1 Mike here s an updated patch... the random test for icutokenizer now passes spoonfeeding caught a bug . But now testHugeDoc fails... not a random test . I think that new read method needs to use the incoming offset ie pass location offset not location as 2nd arg to input.read Does testHugeDoc then pass Thats it But this new read method is not really new its from commons-io we should open a bug over there... I opened IO-311 for the missing offset bug. Thanks Rob I think we have bugs in some tokenizers. Its currently very hard to reproduce and we get no random seed I think the issue is the maxWordLength 20. This is not long enough to catch bugs in tokenizers I think we should exceed whatever buffersize they use for example. So I think we need to refactor this logic so that the multithreaded tests take maxWordLength and ensure this parameter is always respected. This way tests for things like tokenizers can bump this up to things like CharTokenizer.IO BUFFER SIZE 2 or whatever makes sense to them to ensure we really test them well. I don t like the fact that only my stupid trivial test testHugeDoc found the IO-311 bug what if we didn t have that silly test I ll add a patch. patch for the maxWordLength issue. This also makes the single-threaded version that the multi-threaded versions call private so that its not accidentally used losing test coverage . Now we can beef up tokenizer tests to test longer strings for stemmers and filters i think 20 is probably fine though.
