Change all contrib TokenStreams Filters to use the new TokenStream API
Now that we have the new TokenStream API LUCENE-1422 we should change all contrib modules to use it. is anyone working on this I have some functionality that needs some of these to be new-api so i have at least half of them done. wanna post what you have Robert perhaps then someone will finish off the other half. only partial solution... some of the analyzers don t have any tests so I think thats a bit more work the AsciiFoldingFilter fix is in here too i know its not in contrib but it doesnt support new API either. Robert would you divide your patch into contrib core. That would make it easier to help out and eventually commit it. Thanks split patch core contrib I added the ASCIIFoldingFilter patch into LUCENE-1693 patch comes shortly . So this part is finished. Now that LUCENE-1693 is almost committed we can tackle this one. We need to keep these points in mind that Uwe posted on 1693 rewrite and replace next Token next implementations by new API if the class is final no next Token next methods needed must be removed if the class is non-final add the following methods to the class deprecated Will be removed in Lucene 3.0. This method is final as it should not be overridden. Delegates to the backwards compatibility layer. public final Token next final Token reusableToken throws java.io.IOException return super.next reusableToken deprecated Will be removed in Lucene 3.0. This method is final as it should not be overridden. Delegates to the backwards compatibility layer. public final Token next throws java.io.IOException return super.next Also the incrementToken method must be final in this case. Michael I think we should try to coordinate timing on this with LUCENE-1728 where we are trying to reorganize contrib analyzers... Michael I think we should try to coordinate timing on this with LUCENE-1728 where we are trying to reorganize contrib analyzers... It seems like 1728 is ready to commit Simon said on java-dev he will try to finish it by end of this week In that case we should commit 1728 first. But we can finish the patch here I think... after 1728 is committed we just do a find replace on the patch and replace contrib analyzers with contrib analyzers common It seems like 1728 is ready to commit Simon said on java-dev he will try to finish it by end of this week That is correct. I can commit it today I think. Will make this issue dependent on 1728 and finish it by the end of today. simon Cool Thanks Simon. Michael after 1728 I can take another look at this. the reason is that I added some tests to these analyzers and found a bug in the Thai offsets. When i submitted this i only duplicated the existing behavior but I don t want to reintroduce the bug into incrementToken LUCENE-1728 is committed. You can go ahead on this. Thanks I ll work on updating the patch. Question instead of converting the tests to new api would it be more beneficial to test both testNewAPI testOldAPI Question instead of converting the tests to new api would it be more beneficial to test both testNewAPI testOldAPI No need to do this. The correct backwards-compatibility of the new API is checked separately by the extra test in LUCENE-1693. It is enough if you remove all next methods add incrementToken and make the above mentioned changes to make the token streams final where possible see the comment of Michael Busch above Ð which is the howto for the conversion . The testy should only test the new API but until they are also converted the old tests should also still work because of the backwards-compatibility layer . Be sure to apply LUCENE-1693 which does not change anything in contrib before starting to create a patch to have the newest API. Hopefully Michael has committed 1693 tomorrow. LUCENE-1460 core.txt is already in LUCENE-1693 as it is part of core token streams. Uwe thanks Yes I will follow your guidelines that Michael re-posted here. additionally some tokenstreams are not final in contrib but have not been put into a release example arabic my fault Any objection to making these final so I can just remove the old api instead of declaring final deprecated next methods no make them final. the same happened with asciifoldingfilter was never released so can be changed. In general all tokenstreams should be final or have final impls see LUCENE-1753 for explanation . The only real example of a non-final one is CharTokenizer which is abstract but the tokenization impls are final Make next final is only the last chance to prevent users from overriding never-called next-methods if the class was not final before. This makes the backwards-break as small as possible by only making these methods final . See the discussion in 1693. i updated the previous patch to the current reality still incomplete. some more done the rest of the various language analyzers at least . i have visitors coming in town so if you feel like punishing yourself just let me know so we don t duplicate efforts. I promise I left all the fun ones still to be converted. i have visitors coming in town so if you feel like punishing yourself just let me know so we don t duplicate efforts. I promise I left all the fun ones still to be converted. Yay I m excited I think when these patches are committed I don t want to hear the word TokenStream for a while... I can work on this Friday and during the weekend. Let me know which ones you re gonna work on and which ones I should take. Btw Thanks for all your work here Robert Michael I m not gonna be able to work on this at all until monday at least that s what I am saying. So I will check with you Monday and see how things are. Michael oh also as Uwe mentioned I applied LUCENE-1693 before creating this patch. If you do this all the tests will pass otherwise I am not so sure Michael anyone else if you apply the patch it should go cleanly now that 1693 is committed then the easiest way to see what remains to be done is to declare TokenStream.incrementToken abstract. This still causes my eclipse to light up like a christmas tree so there is a lot to do Thanks Robert I m currently working on LUCENE-1448 will finish that tonight. Tomorrow and or Sunday I ll work on this one. I converted some more contribs... More progress... ngram was a bit tricky. But I think it is much more efficiently implemented now. It used to clone every token it returns. Now it only clones the term that it receives from the input stream. Would be good if someone could take a look at the ngram changes... well the testcases pass. Btw I m taking a tokenstream break today... so if anyone feels the sudden urge to convert some of the remaining streams don t hesitate - it won t conflict with my work the patch I posted late last night is still current. I ll try to continue tomorrow. Michael looks like you got a ton done. I ll take a look and see late sunday monday at what you did with ngram for curiousity at least. if you get a moment maybe someone could review what I did with Thai I didn t look to hard to see if save restore state was worse than the previous cloning... thanks for tackling the tougher ones here Some more progress - mostly in contrib memory. Michael I looked at your patch. What do you think about the remaining ones should they be left as is for now or do you think some of these should still expose Token i.e. in their public protected methods but just as back compat convenience and work w the new api behind the scenes with analyzers compound with analyzers compound Cool. Thanks for all your work here if you get a moment maybe someone could review what I did with Thai I ll review it before we commit this. Michael sorry to leave it incomplete I think I am not the best for the remaining ones. For example I am a little intimidated by things such as this note in ShingleMatrix This method exists in order to avoid reursive calls to the method as the complexity of a fairlt small matrix then easily would require a gigabyte sized stack per thread. I wonder if we should just deprecate PrefixAwareTokenFilter and PrefixAndSuffixAwareTokenFilter. I don t really understand what they re supposed to be used for and I find the implementation a bit strange. I don t think it s possible to convert them in a backwards-compatible way anyways and writing a replacement seems not really worthwhile - or does someone use these For example I am a little intimidated by things such as this note in ShingleMatrix Same here. The shingle ones are the only remaining ones... I ll try but I don t know the code at all. They do a lot of caching and stuff - so probably the hardest to convert. so probably the hardest to convert. I was thinking there are varying degrees of conversion. I think w the new api you could reduce cloning and really optimize these or the other extreme you are almost merely pushing the back compat logic up to that level... kinda like my compound hack. i tried the latter and was unsuccessful even doing that for the shingles . I ll commit this current patch soon and then make a separate patch for the shingle filters. Latest patch that converts all streams except the single ones. I ll commit this soon. Committed. Robert thank you for all your help here You two guys are the best great work I had no time to completely review it but I am happy that the new TokenStream API seems so successful
