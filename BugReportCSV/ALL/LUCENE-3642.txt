EdgeNgrams creates invalid offsets
A user reported this because it was causing his highlighting to throw an error. screenshot from the user I thought up a hackish way we can test for these invalid offsets for all filters... I ll see if it works. here s a test. the problem is a previous filter lengthens this term by folding ¾ - ae but EdgeNGramFilter computes the offsets additively offsetAtt.setOffset tokStart start tokStart end Because of this if a word has been lengthened by a previous filter edgengram will produce offsets that are longer than the original text. and probably bogus ones if its been shortened . I think we should what WDF does here if the original offsets have already been changed startOffset termLength endOffset then we should simply preserve them for the new subwords. I added a check for this to basetokenstreamtestcase... now to see if anything else fails... so my assert trips for shit like whitespacespacetokenizer lowercase... how horrible is that There must be offset bugs in CharTokenizer... i ll dig into it. Here s a patch fixing the edge ngrams filters using the same logic as wdf its well-defined i think its the only thing we can do here . Still need to fix the chartokenizer bug and also add some tests for any other filters that are actually tokenizers we might have. Robert that patch for the EdgeNGramTokenFilter worked. If there occur any problems I let you know. Thanks Thanks Max I am currently adding more tests fixes for other broken tokenizers filters with offset bugs. I ll update the patch when these are passing but i think the ngrams stuff is ok. updated patch with a test fix for smartchinese and with a test for CharTokenizer... it currently fails with an off by one incorrect startOffset which is in turn jacking up the endOffsets too. here s the fix for CharTokenizer. Tests are passing I will commit soon. Just looking i see another bug in CharTOkenizer... i ll add another test. patch with tests and fix for the additional bug in CharTokenizer.
