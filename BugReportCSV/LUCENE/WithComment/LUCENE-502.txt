TermScorer caches values unnecessarily
TermScorer aggressively caches the doc and freq of 32 documents at a time for each term scored. When querying for a lot of terms this causes a lot of garbage to be created that s unnecessary. The SegmentTermDocs from which it retrieves its information doesn t have any optimizations for bulk loading and it s unnecessary. In addition it has a SCORE CACHE that s of limited benefit. It s caching the result of a sqrt that should be placed in DefaultSimilarity and if you re only scoring a few documents that contain those terms there s no need to precalculate the SQRT especially on modern VMs. Enclosed is a patch that replaces TermScorer with a version that does not cache the docs or feqs. In the case of a lot of queries that saves 196 bytes term the unnecessary disk IO and extra SQRTs which adds up. Here s the patch Sorry about my lack of proofreading I saved right as I was leaving work. The main point is that the look ahead caching done by TermScorer is unnecessary. It is only of benefit if you are scoring in a given locality i.e. query doc 0 then 30 then 10 then 3 etc . Nearly all use cases are sequential the use of seek vs. next is fine because the underlying BufferedIndexInput has an efficient seek function for sequential access. Here s an HPROF run from a set of sequential wildcard searches with many terms per search . Since this never performs sequential access on documents the cache is completely unnecessary. percent live alloc ed stack class rank self accum bytes objs bytes objs trace name 29 0.79 58.64 1029312 7148 1801296 12509 387945 float 30 0.79 59.43 1029312 7148 1801296 12509 387944 int 31 0.79 60.23 1029312 7148 1801296 12509 387943 int TRACE 387943 org.apache.lucene.search.TermScorer. init TermScorer.java 30 org.apache.lucene.search.TermQuery TermWeight.scorer TermQuery.java 64 org.apache.lucene.search.BooleanQuery BooleanWeight.scorer BooleanQuery.java 165 org.apache.lucene.search.IndexSearcher.search IndexSearcher.java 158 TRACE 387944 org.apache.lucene.search.TermScorer. init TermScorer.java 31 org.apache.lucene.search.TermQuery TermWeight.scorer TermQuery.java 64 org.apache.lucene.search.BooleanQuery BooleanWeight.scorer BooleanQuery.java 165 org.apache.lucene.search.IndexSearcher.search IndexSearcher.java 158 TRACE 387945 org.apache.lucene.search.TermScorer. init TermScorer.java 36 org.apache.lucene.search.TermQuery TermWeight.scorer TermQuery.java 64 org.apache.lucene.search.BooleanQuery BooleanWeight.scorer BooleanQuery.java 165 org.apache.lucene.search.IndexSearcher.search IndexSearcher.java 158 If you re using a WildcardTermEnum this optimization saves a ton. We usually do wildcard searches which retrieve 50-5000 terms. Since each one of these corresponds to a new TermScorer removing the caching saves a lot. For a query that has 1800 terms it saves 800K query plus it s also quicker by about 15 . Don t double buffer. It is not clear to me that your uses are typical uses. These optimizations were added because they made big improvements. They were not premature. In some cases JVM s may have evolved so that some of them are no longer required. But some of them may still make significant improvements for lots of users. We really need a benchmark suite to better understand the effects of things like this... The main point is this When you are using TermScorer to score one document it is doing a lot of extra work. It s reading 31 extra documents from the disk and calculating the weight factors for 31 documents. The question is how does the caching help when you have multiple documents. My analysis is that with a modern VM it helps you only if the docFreq of a term is 16-31 and you are using a ConjunctiveScorer i.e. not Wildcard searches . I would imagine this is a use case that is not uncommon. Anyone using Wildcard searches will have immediate benefit from installing this patch. So I m going to analyze this from the amount of work to do perspective. TermScorer.next If you are calling TermScorer.next there is no real difference. SegmentTermDocs.read int float is no different from calling SegmentTermDocs.next 32 times. The change in the patch switches TermScorer.next to always calling next on the underlying SegmentTermDocs. The only cost I m removing is the caching and I m not adding any new ones. Therefore there s no change with the exception of adding the cache for use in skipTo . TermScorer.skipTo The only case where my patch is worse is if the frequency of the term is greater than the skip interval i.e 16 documents per term . In this case if you are retrieving more than 16 documents but less than 32 you can avoid accessing the skipStream entirely. If you are retrieving more than 32 documents then you need to access the skipStream anyway and since both of the underlying IndexInput s are cached repositioning the freqStream will be only pointer manipulation. TermScorer.score In some cases JVM s may have evolved so that some of them are no longer required. I can imagine that the scoreCache made a lot of sense in JDK 1.1 when the cost of Math.sqrt would be high. However if the TermScorer is only going to be used for a single document this is obviously wrong. Like I said before caching DefaultSimilarity.tf int inside DefaultSimilarity would end up inlined by the HotSpot compiler but Math.sqrt is inlined into a processor trap so it s not a big deal. I want other people to test this and tell me any problems with it. Whether or not you accept the patches into are less important to me than providing them to other people that have similar performance problems. Perhaps I should have created a parallel structure to TermScorer that you can use when you have a low hit term ratio. The question is how does the caching help when you have multiple documents. My analysis is that with a modern VM it helps you only if the docFreq of a term is 16-31 and you are using a ConjunctiveScorer i.e. not Wildcard searches . The conjunctive scorer does not call score HitCollector int . This is only called in a few cases anymore. It can help a lot with a single-term query for a very common term or for disjunctive queries involving very common terms although BooleanScorer2 no longer uses it in this case. That s too bad. If all clauses to a query are optional then the old BooleanScorer was faster. But it didn t always return documents in order... So it indeed may be time to retire this method. SegmentTermDocs.read int int is no different from calling SegmentTermDocs.next 32 times. If that were the case then then termDocs int int method would never have been added Benchmarking showed this to be much faster. There s also optimized C code that implements this method in src gcj. In C with a memory-mapped index the i o completely inlines. When I last benchmarked this in GCJ it was twice as fast as anything HotSpot could do. But without score HitCollector int TermDocs.read int int will never be called. Sigh. As for the scoreCache this is certainly useful for terms that occur in thousands of documents and useless for terms that occur only once. Perhaps we should have two TermScorer implementations one for common terms and one for rare terms and have TermWeight select which to use. The conjunctive scorer does not call score HitCollector int . This is only called in a few cases anymore. However in your comments to LUCENE-505 you said this For example in TermScorer.score HitCollector int Lucene s innermost loop you change two array accesses into a call to an interface. That could make a substantial difference. Which is true Or as it seems likely TermScorer was optimized for a case that is no longer valid i.e. ConjunctiveScorer . If that were the case then then termDocs int int method would never have been added This hasn t been true for at least 3 years. Inlining by hand is not necessary anymore with hotspot I don t know about gcj . Run a benchmark on JDK 1.5 to prove this to yourself. In short we should have two TermScorer implementations. One for low documents term and one for high documents term. The question is how does the caching help when you have multiple documents. My analysis is that with a modern VM it helps you only if the docFreq of a term is 16-31 and you are using a ConjunctiveScorer i.e. not Wildcard searches . The conjunctive scorer does not call score HitCollector int . This is only called in a few cases anymore. It can help a lot with a single-term query for a very common term or for disjunctive queries involving very common terms although BooleanScorer2 no longer uses it in this case. That s too bad. If all clauses to a query are optional then the old BooleanScorer was faster. But it didn t always return documents in order... So it indeed may be time to retire this method. With BooleanScorer2 It is quite possible to use different versions of DisjunctionScorer one for query top level that does not need skipTo and one for lower level that allows skipTo . The top level one can be implemented just like the old BooleanScorer. Iirc the method to implement such different behaviour are already in place for scoring a range of documents it only needs to be implemented for DisjunctionScorer and the top level BooleanScorer2 should then use it when appropriate. Regards Paul Elschot Which is true Or as it seems likely TermScorer was optimized for a case that is no longer valid i.e. ConjunctiveScorer . No it was optimized for BooleanScorer s disjunctive scoring algorithm which is no longer used by default but is faster than BooleanScorer2 s disjunctive scoring algorithm. This applies to a very common type of query classic vector-space searches. So this optimization may not be leveraged much in the current codebase but that does not mean that it is no longer valid. But it may slow other sorts of searches like your wildcards. The challenge is not just how to figure out how to make your application as fast as possible but how to do this without making other s and future applications slower. In short we should have two TermScorer implementations. One for low documents term and one for high documents term. Yes I think that would be useful. Classically total query processing time is dominated by common terms so that s an important case to optimize. But It seems that with wildcard queries over smaller collections that these optimizations become costly. So two implementations seems like it would make everyone happy. Are we interested in this optimization Here is an attempted patch. Two issues 1. Seems it might be better to try and use IDF to determine which scorer to use TermScorer or LowFreqTermScorer rather than doc freq so that doc freq doesn t need to be accessed twice. 2. I don t know at what level the LowFreqTermScorer should be cut out for the TermScorer. Some benching may help. Can we close this one It seems it was geared towards multitermqueries but these are using different codepaths these days e.g. Filter rewrite . I think as this issue stands though I would be against changing the defaults... separately in LUCENE-2392 you can control this in your Similarity yourself In trunk there is no longer a score cache in TermScorer because this is just an optimization for the TF IDF scoring formula. Instead this optimization is in TFIDFSimilarity if you want or don t want similar pre-computations in your scoring you can adjust this by plugging in your own Similarity.
