LowerCaseFilter for Turkish language
java.lang.Character.toLowerCase converts I to i however in Turkish alphabet lowercase of I is not i . It is LATIN SMALL LETTER DOTLESS I. TurkishLowerCaseFilter that lowercases character I correctly is added. Hi Ahmet this patch is looking very nice thank you I have some minor suggestions can we use hex notation maybe also constants too for the special case you can use assertTokenStreamContents here it is in the base test case to simplify your test it works like assertAnalyzesTo but on tokenstream I will let others comment on where this belongs maybe contrib Wherever it is I would like to use it in snowball contrib also. Looks cool it even uses the new CharUtils API. 1 for using assertTokenStreamContents. I have one comment that this will not work correctly on text that is not NFC. This is because uppercase I with dot can be represented as u0130 as you handle it but also decomposed as u0049 u0307. There can also be stuff in between technically... after finding a regular I u0049 we could search ahead for COMBINING DOT ABOVE ignoring any nonspacing marks and format and such along the way and handle this differently. but non-NFC text doesn t work correctly throughout most of lucene s analysis components as it is now anyway so I don t think we should worry about it right now. Maybe we could add a comment for the future though. As this is a new lowercasefilter shouldn t be the default matchVersion completely removed Other filters have deprecated the no-matchVersion filter and this one also. A new class should not have deprecated parts. - remove but non-NFC text doesn t work correctly throughout most of lucene s analysis components as it is now anyway so I don t think we should worry about it right now. Maybe we could add a comment for the future though. It might be good to note the NFC NFKC requirement in the JavaDoc. Maybe its just me but I think it is critical to normalize the input to Lucene for both indexing and searching. Unless a NFCNormalizingFilter is added to Lucene I think it is the responsibility of the caller. For new classes would it be helpful to add since to the class JavaDoc Maybe its just me but I think it is critical to normalize the input to Lucene for both indexing and searching. Unless a NFCNormalizingFilter is added to Lucene I think it is the responsibility of the caller. yeah I think its critical too. It might be good to note the NFC NFKC requirement in the JavaDoc. yeah or maybe just a hint in the comments because this is an exceptionally tricky case . this same problem also applies to ASCIIFoldingFilter pretty much all of the analyzers etc too... Unless a NFCNormalizingFilter is added to Lucene I think it is the responsibility of the caller. btw DM if you are interested I inserted a long discussion about unicode normalization and how it interacts with Lucene tokenstreams in general in the javadoc header of ICUNormalizationFilter for LUCENE-1488. please comment over there if you have suggestions or thoughts on it There is no need to use CharacterUtils in here. You can use Character.codePointAt directly. This is a new class and does not need to preserve any bw. compatibility. I agree with uwe the Version should go away in this patch. Once more thing this patch seems to be in core. I do not see any reason why this should be in core though. We should move it to contrib though as it serves such a specific usecase. Simon I would rather see this in contrib also. Would there be opposition to making contrib snowball depend upon contrib analyzers so the SnowballAnalyzer can use this filter instead of lowercase filter for the Turkish case based upon Version of course Would there be opposition to making contrib snowball depend upon contrib analyzers so the SnowballAnalyzer can use this filter instead of lowercase filter for the Turkish case based upon Version of course i think we can arrange something like that. Since we factored out Smart-cn the jar has reasonable size so this won t be an issue. maybe we should think about moving snowball into analyzers snowball - just an idea. Anyway this is somewhat unrelated to this particular patch but still considerable. I don t think its really unrelated I think its a consideration towards where we put this. The turkish analyzer happens to be in contrib snowball and thats what really needs this for turkish search. Although I agree this filter could be useful on its own assertTokenStreamContents since and hex constants are added. deprecated constructor is removed. Ahmet hi I think you might have accidentally left the old duplicate test in there that does not use assertTokenStreamContents I kept the old test method and added a new one. Should i remove old one Ahmet I think so. they both test the same functionality but the second test is less code and in my opinion better. assertTokenStreamContents does some additional checks it clears attributes in between it calls .end things like that. One othe possibility to resolve the problem in a completely different way You could wrap a MappingCharFilter on top of the input reader in Analyzer and just add a replacement for this one char http lucene.apache.org java 3 0 0 api all org apache lucene analysis MappingCharFilter.html This would be a very easy fix without code duplication. You just change the input before tokenization. And its already in Lucene core just plug it into the analyzer s tokenStream or reusableTokenStream method as a wrapper around the Reader param. This would be very easy also for the other analyzers having problem with seldom chars. It can also be used to remove chars at all or replace them by longer sequences like Š - ae for german . test that does not use assertTokenStreamContents is removed. One othe possibility to resolve the problem in a completely different way You could wrap a MappingCharFilter on top of the input reader in Analyzer and just add a replacement for this one char Uwe but this is inflexible. If we want to make this filter support turkish lowercasing in the future for all of unicode not just NFC composed form we cannot do it with MappingCharFilter. Again I don t think we should fix this now but in the future I think we might want to. The patch s TurkishLowerCaseFilter is as unflexible as that. The idea is just a replacement for the current patch and it is even a little bit more universal because you can change the chars to map . test that does not use assertTokenStreamContents is removed. Thanks Ahmet in my opinion this is good we just have to figure out where to place it. My vote is for contrib analyzers common tr for now. The patch s TurkishLowerCaseFilter is as unflexible as that. The idea is just a replacement for the current patch and it is even a little bit more universal because you can change the chars to map . Uwe this is not true. With a tokenfilter I can use Version that will apply the logic i mentioned above after finding a regular I u0049 we could search ahead for COMBINING DOT ABOVE ignoring any nonspacing marks and format and such along the way and handle this differently. you cannot do this with mappingchar filter or rather you could but there would be millions of mappings for this one character. I could later patch this filter with Version and some lookahead based on unicode properties if i wanted to improve it. if I replace this code from Ahmet s test public class TestTurkishLowerCaseFilter extends BaseTokenStreamTestCase public void testTurkishLowerCaseFilter throws Exception TokenStream stream new WhitespaceTokenizer new StringReader u0130STANBUL u0130ZM u0130R ISPARTA TokenStream filter new TurkishLowerCaseFilter Version.LUCENE 30 stream assertTokenStreamContents filter new String istanbul izmir u0131sparta by that there is not even a new class or anything needed public class TestTurkishLowerCaseFilter extends BaseTokenStreamTestCase static final NormalizeCharMap map new NormalizeCharMap static map.add u0049 0x0131 public void testTurkishLowerCaseFilter throws Exception TokenStream stream new WhitespaceTokenizer new MappingCharFilter map new StringReader u0130STANBUL u0130ZM u0130R ISPARTA TokenStream filter new LowerCaseFilter Version.LUCENE 30 stream assertTokenStreamContents filter new String istanbul izmir u0131sparta It just works. Uwe I don t think you understand what I am saying. if my text is instead I STANBUL versus your STANBUL it will not work. Uwe this is not true. With a tokenfilter I can use Version that will apply the logic i mentioned above I am talking about this patch. Not any later version I suggest to not apply this patch at all and for now tell the user to use above helper construct until we have ICU in core or whatever sorry for the missing u I do not want to edit again... Uwe I am talking about this patch too. it is simple and can be extended to the future to handle such things. your mappingcharfilter approach cannot and I don t see us having ICU in core ever even though I would love such a thing. Additionally it will make it easier to fix SnowballAnalyzer which is currently broken for turkish language because it uses the wrong lowercase. Robert I understand your problem but it affects LowerCaseFilter at all and is not special to the Turkish lower filter. If you have decomposed characters even LowerCaseFilter would fail for all languages even German if you compose Š out of a and two dots . In germany really nobody uses de-composed chars I do not know how this is in Turkey but the last time I was there they just used the simpliest composed chars like germans they even have the umlauts which they use from the basic latin1 range. And for that this filter works and is a quick fix. But I give up now. EDIT Good night I cannot hack my keys anymore Sorry for heavy issue editing. Uwe it is specific to the turkish case. because for german whether you have A umlaut or A umlaut as one character it works regardless. turkish is the only case where its more complex because the casing of the character actually depends upon a diacritic that may not be composed and may have other diacritics in between. this is what makes it such a bear to support in case folding Note that the Turkic mappings do not maintain canonical equivalence without additional processing. See the discussions of case mapping in the Unicode Standard for more information. The problem is that context is required and sometimes marks must actually be deleted for proper casing. When lowercasing remove dot above in the sequence I dot above which will turn into i. This matches the behavior of the canonically equivalent I-dot above 0307 0307 0307 tr After I COMBINING DOT ABOVE 0307 0307 0307 az After I COMBINING DOT ABOVE When lowercasing unless an I is before a dot above it turns into a dotless i. 0049 0131 0049 0049 tr Not Before Dot LATIN CAPITAL LETTER I 0049 0131 0049 0049 az Not Before Dot LATIN CAPITAL LETTER I but the last time I was there they just used the simpliest composed chars like germans . This is why i recommended we not go crazy and only work on the composed form. But in the future we might want to correct this. this is impossible to do with mappingcharfilter that is my only point. attached is a slight modification to support proper lowercasing for decomposed forms as well. when an uppercase I is encountered some lookahead is necessary as long as characters are nonspacing marks they can be in between to see if there is later a COMBINING DOT ABOVE. in this case the uppercase I must be lowercased to a regular i and this COMBINING DOT ABOVE later removed. Hi Uwe now are you ok with this being a TokenFilter Can we discuss where to put it maybe contrib analyzers under tr Robert Looks crazy but seems to work great work Small comments about the patch The int codepoint constants should be static final not just final termAtt should also be final and optionally can be initialized directly in the declaration which I prefer but that s your turn . In the test the Version.LUCENE CURRENT should be used which has no effect on the tests but maybe in future . Uwe it is kinda crazy. but I think worth the effort not to have such wierd behavior. I didn t optimize it yet in general either. In reality on composed text this should not slow you down. it should be one additional getType check after any uppercase I . its 2 right now I will fix I ll also add in your suggestions and move the patch to contrib analyzers tr for now and upload a new patch. updated patch for contrib with optimization i mentioned earlier some cleanups from Uwe and some more docs. Sorry new patch forgot to change tests to LUCENE CURRENT as Uwe mentioned. Maybe I miss something but what is the reason to use Version in here. This is new code and it does not have to maintain any bw compatibility. There is also no need to use CharacterUtils you can use Character directly can t you We should use version from the beginning of new tokenfilters to be prepared for the future. In this case the version could be ignored until we have a new version of this filter. But to be consistent with LowerCaseFilter I would prefer to also use the matchVersion to select the right utils class. But to be consistent with LowerCaseFilter I would prefer to also use the matchVersion to select the right utils class. I agree with the consistency ctor but I don t with the CharacterUtils. I do not see why a new class should yield buggy behavior in any case. If somebody uses Version 3.1 this filter will not work correctly. We should prevent this in any case another possibility would be enforcing a version 3.1 Then I would be ok with it. I think we should have a consistent behaviour for new classes taking version. It would make sense to have a method in Version like public enum Version ... public void ensureOnOrAfter Version other if onOrAfter other throw new IllegalArgumentException Version this.name are not supported That enforces the version for new classes. This would also help to get rid of old behaviour in 4.0 We should open another issue for this I guess. I see many usecases where this could be very useful. If somebody uses Version 3.1 this filter will not work correctly. Simon all the tests pass with Version 3.1 I tried LUCENE 20 . So its just consistency with the other code. I think we should do the same in general just my opinion. For example if I write a new analyzer in 3.1 I m just gonna take Version and pass it to my components. Its true I do not even need Version. Maybe by 3.2 comes out there is only 1 thing affected by Version. And yes if its based on StandardTokenizer someone can set version LUCENE 24 and it will parse acronyms in an invalid way even though the analyzer wasn t around at the time of Lucene 2.4 But this is easy and consistent to just pass along version we already do this in other places I think its not worth changing. Robert I see your point. The root cause why this bugs me is that this TokenFilter changes his behavior at least if you index deseret with this analyzer depending on the passed version. I don t think that new code should try do anything based on version. The Version ctor it totally ok for me in this case but we should really use Integer.CodePointAt instead of CharacterUtil. Once I think about the mess ensureOnOrAfter would create throughout all the code I doubt it would to any good in the end. I don t think that new code should try do anything based on version. ok I will change this. You are right I would look at this problem differently if we didnt have CharacterUtil which makes it just so easy to support the old and new behavior. ok I will change this. You are right I would look at this problem differently if we didnt have CharacterUtil which makes it just so easy to support the old and new behavior. Actually a unused Version argument is silly. If we have to add it in the future because of some change you WANT to deprecate the ctor to make users aware of it. that is what deprecations are made for. I would not argue about consistency as not every TokenFilter has a Version ctor. EdgeNGramTokenFilter for instance - this is just first coming to my mind . I would remove it completely Use Character.codePointAt and you are good to go. patch with Simon s suggestions. Robert this patch looks good Much cleaner now - version is not appropriate here Thanks for updating. trivial update with comment for the test and better example explaining what this is all about. I will take over for Robert - go ahead and get automation done robert I plan to commit the latest patch until 12 06 09 if nobody objects. Hello Simon if this issue is resolved so we do not forget can we open a separate issue to fix the SnowballAnalyzer when using Turkish language I also think we should add some javadocs to the snowball stem filter that explain you need to use this filter beforehand for it to work. I already have some unit tests produced showing it doesn t work correctly with LowerCaseFilter and that it also does not handle uppercase. I will commit this tomorrow if nobody objects. Committed in revision 887535 Thanks Ahmet Robert 
