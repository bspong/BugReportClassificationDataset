Contrib analyzers need tests
The analyzers in contrib need tests preferably ones that test the behavior of all the Token attributes involved offsets type etc and not just what they do with token text. This way they can be converted to the new api without breakage. first I looked at BrazilianAnalyzer... out of curiousity can someone explain to me how the behavior of BrazilianStemmer differs from the Portuguese snowball analyzer... because it looks to be the same algorithm to me answered my own question here s tests for brazilian as a start. add tests for dutchanalyzer. this analyzer claims to implement snowball although tests reveal some differences. it also has about 1MB of text files that don t appear to be in use at all... These are much needed... thanks Robert. Let me know when you re done iterating and or when we need to wrap up 2.9 and we ll get these in. Michael LUCENE-973 would save me from having to create tests for the CJKAnalyzer. It would also fix a bug. thanks i ll upload some more tests hopefully soon. I think most have rudimentary tests. but some are not sufficient to ensure any api conversion is really working. for example ThaiAnalyzer does not have any offset tests but if that broke then highlighting would break. Robert you should probably also hold up on API conversion since the API itself is now changing LUCENE-1693 . michael ok. I know additional tests here against the old api might be more code to convert but I think it will actually make the process easier whenever that is or whatever is involved. i have some time this evening to try to improve the coverage here against the old api . adds tests for thaianalyzer token offsets and types both of which have bugs tests for correct behavior are included but commented out. added tests for czech. added additional tests for smartchineseanalyzer there is a bug very similar to the recent CJK one here... generating empty tokens. michael I m think I m done here. if you consider any of the bugs important just let me know can try to help get them fixed. michael I m think I m done here. OK I ll review. Thanks if you consider any of the bugs important just let me know can try to help get them fixed. Likely I won t be able to judge the severity of these bugs... so please chime in if you think they should be fixed... Michael I think it would be nice to fix the Thai offset bug so highlighter will work. this is a safe one-line fix and its an obvious error. The SmartChineseAnalyzer empty token bug is pretty serious i think indexing empty tokens for every piece of punctuation could really hurt similarity computation am i wrong never tried The Thai .type bug is something that could be fixed later i don t think the token type being ALPHANUM versus NUM is really hurting anyone. The issue where DutchAnalyzer doesnt do what it claims i think thats not really hurting anyone and they can use the snowball version if they want accurate snowball behavior. I do think the huge files in DutchAnalyzer that aren t being used can be removed if you want to save 1MB but I m not sure how important that is. Let me know your thoughts. I m seeing this test failure junit Testcase testBuggyPunctuation org.apache.lucene.analysis.cn.TestSmartChineseAnalyzer Caused an ERROR junit null junit java.lang.AssertionError junit at org.apache.lucene.analysis.StopFilter.next StopFilter.java 240 junit at org.apache.lucene.analysis.cn.TestSmartChineseAnalyzer.testBuggyPunctuation TestSmartChineseAnalyzer.java 51 It s because null is being passed to ts.next in the final assertTrue line nt ts.next nt while nt null assertEquals result i nt.term i nt ts.next nt assertTrue ts.next nt null heh - 1 on fixing them all. Including reclaiming that 1 mb of space if we can ... Me too Robert can you cons up a patch Which files can be safely removed from the DutchAnalyzer stems words.txt michael i guess junit from my eclipse junit from ant because it passes in eclipse...annoying I will fix the test so it runs correctly from ant. michael yes the stems words.txt for stems.txt words.txt I am scratching my head trying to figure out what they were originally intended to do. If its to support dictionary stemming with wordlistloader then it really needs to be one tab-separated file not two files. Probably eclipse isn t running with asserts probably fixed it and testing with ant now. ill upload it at least so you can verify the behavior i ve discovered. do you want me to include patch with the two bugfixes chinese empty token and thai offsets or give you something separate for those for the other 2 bugs fixing the Thai tokentype bug well its really a bug in the standardtokenizer grammar. i wasn t sure you wanted to change that at this moment but if you want it fixed let me know in my opinion fix for DutchAnalyzer is to deprecate remove the contrib completely since it claims to do snowball stemming why shouldnt someone just use the Dutch snowball stemmer from the contrib snowball package Having trouble figuring this one out michael here is an updated patch. i removed that chinese test there s something strange going on here see my screenshot but i can t seem to create a test case to show it ok got it the IDEOGRAPHIC FULL STOP is being converted into a comma token by the tokenizer. if you use the default constructor SmartChineseAnalyzer it won t load the default stopwords list such as from my Luke screenshot. if you instead instantiate it like this SmartChineseAnalyzer true then it loads the default stopwords list. the default stopwords list includes things like comma so it ends out getting removed. maybe its not a bug but this is really non-obvious behavior... patch with new testcase demonstrating the chinese behavior. later tonight i can workup a patch to address the thai offset issue and at least javadoc ing the chinese behavior. if you think the addt l 2 issues thai tokentype dutchanalyzer behavior huge files should be fixed or documented in some way please let me know. patch with the two one-line fixes 1. fix offsets for thai analyzer so highlighting etc will work. 2. use stopwords list by default for smartchineseanalyzer so punctuation isn t indexed in a strange way. i updated the testcases to reflect these. Latest patch looks good Robert thanks Deprecating DutchAnalyzer in favor of Snowball makes sense to me Ð any objections out there And I ll svn rm the two large unused files . Robert could you open a new issue for the Thai token type bug that requires a change to StandardTokenizer s grammar We seem to be accumulating a number of these fix StandardTokeninizer s grammar but we don t have a good way to do this back-compatibly... matchVersion is a good way for the user to express compatibility requirement but we don t know how to cleanly switch on that to different grammar variants. Is that the only issue not addressed by the latest patch michael yes the only issue... i ll open another issue for the thai token type. OK I will commit this soon. Thanks Robert Thanks Robert michael I updated my svn and I think you might have missed some of the tests. there are tests in the patch for BrazilianAnalyzer CzechAnalyzer and DutchAnalyzer... these are new directories maybe that is why Duh I forgot to svn add them Sorry. I m glad you caught that. I m really wanting svn patch .... OK I committed them. Thanks for catching this Robert patch with a couple addtl tests for contrib analysis with some javadocs cleanup and wording. there is also fix to the synonyms test to actually test its reset ... no code changes though. if possible i think these might be good to add for the release. correct missing cjk test. if no one objects i would like to commit these javadocs and tests tomorrow. Committed revision 805400.
