add one setter for start and end offset to OffsetAttribute
add OffsetAttribute. setOffset startOffset endOffset trivial change no JUnit needed Changed CharTokenizer to use it Should we deprecate the separate setters with this addition Not really the attributes API was added for 2.9 so it did not appear until now in official releases it could be just removed. Oh yeah Good I m losing track of what s not yet released... Eks can you update the patch with that Thanks. Separate setters might have their own use I believe I had a pair of filters that set begin and end offset in different parts of the code. But surely that s a very rare case the exception not the rule . Ie nearly always one sets start end offset together I have two cases. In one case I can t access the start offset by the time I set end offset and therefore have to introduce a field on the filter for keeping track of it or use the next case s solution twice if separate setters are removed. In other case I only need to adjust end offset so I ll have to do attr.setOffset attr.getStartOffset newEndOffset . Nothing deadly but I don t see the point of removing methods that might be useful and don t interfere with anything. I am ok with both options removing separate looks a bit better for me as it forces users to think attomic about offset start end . If you separate start and end offset too far in your code probability that you do not see mistake somewhere is higher compared to the case where you manage start and end on your own in these cases this is then rather explicit in you code ... But that is all really something we should not think too much about it We make no mistakes eather way I can provide new patch if needed. removing separate looks a bit better for me as it forces users to think attomic about offset start end . This is my thinking as well. And in general I prefer one clear way to do something the Python way instead providing various different ways to do the same thing the Perl way . removing separate looks a bit better for me as it forces users to think attomic about offset start end . And if it s not atomic by design If you separate start and end offset too far in your code probability that you do not see mistake somewhere is higher compared to the case where you manage start and end on your own in these cases this is then rather explicit in you code ... Instead of having one field for Term which you build incrementally you now have to keep another field for startOffset. Imho that s starting to cross into another meaning of explicit And while you re trying to prevent bugs of using setStartOffset and forgetting about its End counterpart you introduce another set of bugs - overwriting one end of interval when you only need to update another. And in general I prefer one clear way to do something And force everyone who has slightly different use-case to jump through the hoops. Span Query api is a perfect example. Well whatever. And force everyone who has slightly different use-case to jump through the hoops. Simple things should be simple and complex things should be possible is a strong guide when I m thinking about APIs configuration etc. My feeling here is for the vast majority of the cases people set start end offset together so we should shift to the API that makes that easy. This is the simple case. For the remaining minority your interesting use case you can still do what you need but yes there are some hoops to go through. This is the complex case. Span Query api is a perfect example. Can you describe the limitations here in more detail the same as the first patch just with removed setStart EndOffset int Thanks Eks. You also need to fix all the places that call the old methods things don t compile w the new patch . whoops this time it compiles I still get compilation errors mkdir Created dir lucene src lucene.offsets build classes java javac Compiling 372 source files to lucene src lucene.offsets build classes java javac lucene src lucene.offsets src java org apache lucene analysis KeywordTokenizer.java 62 cannot find symbol javac symbol method setStartOffset int javac location class org.apache.lucene.analysis.tokenattributes.OffsetAttribute javac offsetAtt.setStartOffset 0 javac javac lucene src lucene.offsets src java org apache lucene analysis KeywordTokenizer.java 63 cannot find symbol javac symbol method setEndOffset int javac location class org.apache.lucene.analysis.tokenattributes.OffsetAttribute javac offsetAtt.setEndOffset upto javac javac lucene src lucene.offsets src java org apache lucene analysis standard StandardTokenizer.java 164 cannot find symbol javac symbol method setStartOffset int javac location class org.apache.lucene.analysis.tokenattributes.OffsetAttribute javac offsetAtt.setStartOffset start javac javac lucene src lucene.offsets src java org apache lucene analysis standard StandardTokenizer.java 165 cannot find symbol javac symbol method setEndOffset int javac location class org.apache.lucene.analysis.tokenattributes.OffsetAttribute javac offsetAtt.setEndOffset start termAtt.termLength javac javac lucene src lucene.offsets src java org apache lucene index DocInverterPerThread.java 56 cannot find symbol javac symbol method setStartOffset int javac location class org.apache.lucene.analysis.tokenattributes.OffsetAttribute javac offsetAttribute.setStartOffset startOffset javac javac lucene src lucene.offsets src java org apache lucene index DocInverterPerThread.java 57 cannot find symbol javac symbol method setEndOffset int javac location class org.apache.lucene.analysis.tokenattributes.OffsetAttribute javac offsetAttribute.setEndOffset endOffset javac javac Note Some input files use or override a deprecated API. javac Note Recompile with -Xlint deprecation for details. javac 6 errors me too sorry Eclipse left me blind for some funny reason waiting for test to complete before I commit again ... Span Query api is a perfect example. Can you describe the limitations here in more detail Take a look at SpanNearQuery and SpanOrQuery. 1. They don t provide incremental construction i.e. add method like in BooleanQuery and they can be built only from an array of subqueries. So if you don t know exact amount of subqueries upfront you re busted. You have to use ArrayList which you convert to array to feed into SpanQuery which is converted back to ArrayList inside 2. They can t be edited. If you have a need to iterate over your query tree and modify it in one way or another you need to create brand new instances of Span Query. And here you hit 1 again hard. 3. They can t be even inspected without creating a new array from the backing list see getClauses . I use patched versions of SpanNear OrQueries which still use backing ArrayList but accept it in constructor have utility add method and getClauses returns this very list which allows for zero-cost inspection and easy modification if the need arises. ok maybe this time it will work I hope I managed to clean it up core build and test pass . The only thing that fails is contrib but I guess this has nothing to do with it javac D Repository SerachAndMatch Lucene lucene java trunk contrib highlighter src java org apache lucene search highlight WeightedSpanTermExtractor.java 306 cannot find symbol javac MemoryIndex indexer new MemoryIndex javac javac symbol class MemoryIndex javac location class org.apache.lucene.search.highlight.WeightedSpanTermExtractor javac D Repository SerachAndMatch Lucene lucene java trunk contrib highlighter src java org apache lucene search highlight WeightedSpanTermExtractor.java 306 cannot find symbol javac MemoryIndex indexer new MemoryIndex javac javac symbol class MemoryIndex javac location class org.apache.lucene.search.highlight.WeightedSpanTermExtractor javac Note Some input files use unchecked or unsafe operations. javac Note Recompile with -Xlint unchecked for details. javac 3 errors The only thing that fails is contrib but I guess this has nothing to do with it looks like an issue with highlighters dependency on memory index. what target produces the problem We have seen something like it in the past. ant build-contrib I use patched versions of SpanNear OrQueries which still use backing ArrayList but accept it in constructor have utility add method and getClauses returns this very list which allows for zero-cost inspection and easy modification if the need arises. That sounds useful Ð is it something you can share OK all tests pass. I had to fix a few back-compat tests that were using the new TokenStream API I think because we created the back-compat branch from trunk after the new TokenStream API landed . I ll commit in a day or two. Thanks Eks Thanks Eks 
