Possible slowdown of indexing merging on 3.x vs trunk
Opening an issue to pursue the possible slowdown Marc Sturlese uncovered. Patches from Marc showing the issue thanks Marc . we should run the comparisons as a public static void main i think instead of extending LuceneTestCase. 4.0 could be getting SimpleText etc Same tests just cut over to static void main instead. 3.x junit Time taken indexing 114 junit Time taken closing 20 junit Time taken whole process 134 4.x junit Time taken indexing 133 junit Time taken closing 1 junit Time taken whole process 134 So for me its the same speed. I didnt use the public static void main i used the test as-is except i disabled assertions and forced StandardCodec. I actually see 3.x running slower Trunk Time taken indexing 184 Time taken closing 2 Time taken whole process 187 3.x Time taken indexing 205 Time taken closing 40 Time taken whole process 245 But this is because 3.x does a level 2 merge just before close and the close must wait for that merge to complete. Whereas trunk never gets to the level 2 merge only level 0 1 likely because trunk s RAM efficiency is a better and so it packs more docs into each level 0 segment. If we pushed the number of doc up to maybe 1.1M then we should similarly see trunk trigger a level 2 merge. Really when benchmarking indexing it s best to just close the IW without waiting for merges otherwise you re comparing apples oranges. Seeing others results the problem seems to be merges are very slow on trunk on Snow Leopard where I ve been running the tests . I ve attached the whole stdout for both executions. Soon will run the tests on a debian box and post the results too. Here are the results trunk Time taken indexing 927 Time taken closing 283 Time taken whole process 1211 3.4 Time taken indexing 303 Time taken closing 37 Time taken whole process 340 something is severely wrong on mac os X with NIOFSDirectory when merging on the read side . with mmapdirectory its fast. simplefs is slow too. I ve tried snow leopard and MMap. Trunk went much better than before trunk Time taken indexing 416 Time taken closing 1 Time taken whole process 417 3.4 Time taken indexing 321 Time taken closing 46 Time taken whole process 368 it looks like the bug will especially affect any directory that uses bufferedindexinput NIOFS SimpleFS . The problem is multitermsenum doesnt reuse the sub-docs positionsenums so for each term segment we clone the input and bufferedindexinput.clone sets the clone s buffer to null. so across lots of low freq-terms we re-read 4096 bytes MERGE BUFFER SIZE to refill the buffer on each one... mmapdirectory is less affected because it has no buffer to re-read but seems like fixing the reusing would even help it... Patch fixing the lack of reuse in MultiTermsEnum used during merging . The lack of re-use meant we were cloning 2 IndexInputs frq prx per term being merged For NIOFSDir this then meant we would read the same 4K region of the file over and over and over again. I added a test to catch over-cloning but the test sometimes fails when it gets pulsing codec because that codec does not properly reuse a known issue that we should now fix . This seed will fail due to pulsing codec otv TestForTooMuchCloning.test -seed 6407e19e4835e90d -d7cdae0d4a378eb -ee1a92b677887 i opened LUCENE-3517 for the pulsing bug. But i think we should fix this general bug which affects all codecs first I think LUCENE-3517 might involve attributes-policeman we could do this String bodyCodec CodecProvider.getDefault .getFieldCodec body assumeFalse PulsingCodec fails this test because of over-cloning bodyCodec.equals Pulsing bodyCodec.equals MockRandom New patch. I changed the test to just use random terms and also added the workaround for Pulsing s non-reuse. I discovered SimpleText also fails to properly reuse it failed the new test and fixed that. I think it s ready Thank you Marc and Erick This was a devious issue and severely impacted merge performance for non-MMapDir impls. This bug was only present in 4.0.
