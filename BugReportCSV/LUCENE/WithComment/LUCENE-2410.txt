Optimize PhraseQuery
Looking the scorers for PhraseQuery I think there are some speedups we could do The AND part of the scorer which advances to the next doc that has all the terms in PhraseScorer.doNext should do the same optimizing as BooleanQuery s ConjunctionScorer ie sort terms from rarest to most frequent. I don t think it should use a linked list firstToLast that it does today. We do way too much work now when .score is not called because we go and find all occurrences of the phrase in the doc whereas we should stop only after finding the first and then go and count the rest if .score is called. For the exact case I think we can use two int arrays to find the matches. The first array holds the count of how many times a term in the phrase matched a phrase starting at that position. When that count the number of terms in the phrase it s a match. The 2nd is a gen array holds docID when that count was last touched to avoid clearing. Ie when incrementing the count if the docID gen we reset count to 0. I think this d be faster than the PQ we now use. Downside of this is if you have immense docs position gets very large we d need 2 immense arrays. It d be great to do LUCENE-1252 along with this ie factor PhraseScorer into two AND d sub-scorers LUCENE-1252 is open for this . The first one should be ConjunctionScorer and the 2nd one checks the positions ie either the exact or sloppy scorers . This would mean if the PhraseQuery is AND d w other clauses or a filter is applied we would save CPU by not checking the positions for a doc unless all other AND d clauses accepted the doc. Another thing we should fix Ð PhraseQuery of a single term should rewrite to TermQuery. just doing the easy part here here s the rewrite patch. I checked MultiPhraseQuery already has it. Looks great Robert Ð I think you should go ahead commit that and we ll work on the rest of these optos later. Committed revisions 943493 trunk 943499 3x Attached initial rough patch doing the 1st and 3rd bullets above. Still many nocommits but all tests pass. I only did this for the exact case I don t understand the sloppy case so I modified ExactPhraseScorer to no longer subclass PhraseScorer and instead do everything on its own. I tested on a 20M doc Wikipedia index best of 10 runs Query No. hits Trunk QPS Patch QPS Speedup United States 314K 4.29 11.04 2.6X faster United Kingdom Parliament 7K 20.33 58.57 2.9X faster The speedup is great However there s one problem w the patch that I must fix and will bring these gains down which is it requires 2 int arrays sized to the max position encountered during the search which for a large doc could be very large . I think to make this committable I d have to switch to processing the positions in chunks like BooleanScorer . New patch attached switches over to chunking processing the positions 4096 at once . United States is now 10.66 QPS 2.5X speedup and United Kingdom Parliament is now 54.93 QPS 2.7X speedup . I think it s ready to commit... I ll wait a few days. Attached patch w another optimization if the freq of the 2 rarest terms in the phrase are closish then just use .nextDoc instead of .advance when ANDing. This buys another 15 speedup 12.30 QPS net 2.9X faster than trunk on United States phrase query. Also I fixed MultiPhraseQuery to sort its clauses by approx docFreq the optimization is approx in this case because we can t efficiently compute the docFreq of a position that s unioning 1 term. New patch Ð makes the useAdvance per-Term and adds a safety fallback to .advance if too many .nextDocs are used. Fantastic Phrase queries have often been a bottleneck. Very nice Mike Another improvement we could make for positional queries phrases span queries would be skip lists on the positions maybe in a different codec This would probably be a nice speedup for large docs. Alas.... I think I somehow screwed up my performance tests above. I m testing search perf working on LUCENE-2504 and in comparing search perf from 2.9.x - 3.x I only saw a 20 speedup on the phrase query united states for a 5M doc Wikipedia index. And re-running the test on trunk pre and post this commit I still see only 20 gain.... still not sure what I did wrong. I ll update CHANGES. Two steps forward one step back... sigh. Bulk close for 3.1
