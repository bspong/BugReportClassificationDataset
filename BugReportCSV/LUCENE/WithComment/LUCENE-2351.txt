optimize automatonquery
Mike found a few cases in flex where we have some bad behavior with automatonquery. The problem is similar to a database query planner where sometimes simply doing a full table scan is faster than using an index. We can optimize automatonquery a little bit and get better performance for fuzzy wildcard regex queries. Here is a list of ideas create commonSuffixRef for infinite automata not just really-bad linear scan cases do a null check rather than populating an empty commonSuffixRef localize the linear case to not seek but instead scan when ping-ponging against loops in the state machine add a mechanism to enable disable the terms dict cache e.g. we can disable it for infinite cases and maybe fuzzy N 1 also. change the use of BitSet to OpenBitSet or long gen for path-tracking optimize the backtracking code where it says String is good to go as-is this need not be a full run I think... Patch to adress some of the optos Adds optional boolean useCache to TermsEnum.seek Adds protected FilteredTermsEnum.set getUseTermsCache so each query type can choose defaults to false Switches to long gen in AutomatonTermsEnum.nextString Use commonSuffixRef null to mean there is no suffix The attached patch improves sneaky wildcard query un t on a 5M doc wikipedia index matching 1058 terms 124623 docs from 39.69 QPS - 44.85 QPS best of 5 on flex. But trunk is at 63.19 QPS so we still have more to do... This one should be final public SeekStatus seek BytesRef text throws IOException return seek text true As codecs and other implementations should only override the other one. And it will get inlined. attached is a patch to optimize infinite transitions its extremely ugly likely incorrect in some ways too updated version of the previous patch. should be more correct and faster in my benchmarks. attached is a new approach rids of linearmode adds real support for infinite automata to prevent useless seeking. when any loop is encountered in the state machine a portion of the term dictionary is calculated based on its transition ranges and for this portion the enumeration is instead driven from the terms dictionary rather than the state machine. not really 100 on how i feel about this yet but it seems right. OOOH I like this approach It makes the linear decision local and bounds by linearUpperBound the region so that we don t have to revisit the decision on every term. And it enables efficiently using the suffix And.... it s FAST With this fix the hard query un t on flex is 105 QPS best of 5 on 5 M doc wikipedia index vs 62 QPS on trunk. Yay attached is the same patch as before except it includes a random test for Automaton. I stole the code from TestStressIndexing and create random unicode terms and random regular expressions and verify them against a brain-dead query that just brute forces every term. This found two unrelated bugs automaton didnt handle the empty term correctly. there was a logic bug in UnicodeUtil.nextValidUTF16String these are both also fixed in the patch... will commit soon. Committed revision 929065.
