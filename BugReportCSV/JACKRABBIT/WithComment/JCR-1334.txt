Deadlock due different Thread access while prepare and commit in same Transaction
Since we have configured a j2c resource adapter any modification to the repository ends with a deadlock.lock output.txt- I have added in the Methods of the DefaultISMLocking Class the calling Threadname javacorexxx - the javacore file of the deadlock situationPrepare and commit is indeed called using different threads.do you have any ideas to solve that problem I m not an expert on JTS and JTA but can you somehow change the transaction service to not use CORBA calls to coordinate the transaction hi marcel i have a sessionbean with a containermanaged transaction so i think i can t configure anything. BR clauswhat do you think about to bind the locks not to a thread but rather to the transactioncontext if we run in a xa environment because you can not guarantee that the prepare and commit comes from the same thread in a container managed transaction BR claushi guys what do others think i think we can not per default bind a lock to a thread. this will work good for a standalone server but not in a xa environment with container transactions on a j2ee server we run now for about a half year with jackrabbit in production but without xa transactions and this is no good solution hope somebody will help to solve that problem br claus Patched version of DefaultISMLockingClaus can you please compile and deploy the attached PatchedDefaultISMLocking class into your jackrabbit-jca and add the following element to your workspace.xml as the following sibling of SearchIndex ISMLocking class org.apache.jackrabbit.core.state.PatchedDefaultISMLocking If that works for you I will commit the changes. Thank you.hi marcel thanks for your patch. it works fine on the downgrade method but i have found that i get the same deadlock on acquireReadLock. i have attached the stacktrace and a new PatchedDefaultISMLocking.java with this Locking Class it works fine what do you think about BR clausThe stacktrace you attached still shows the DefaultISMLocking class in use. Can you please run your test with the PatchedDefaultISMLocking hi marcel it has only that name but this is the patched one ... Hmm that looks strange because the thread in the stacktrace should have set the thread that owns the write lock to the current thread thus allowing subsequent read locks by the same thread. btw. your version of PatchedDefaultISMLocking won t work because it allows any thread to read even if another thread is currently writing.ok i see. that was not clear to me but now i understand. hmm so websphere ignores the threads from which they get the readlock. i think they work in xa environemt not with threads when will the downgrade be called because the deadlock comes from the searchmanager. he calls DefaultISMLocking.acquireReadLock and this call comes from a different thread than from downgrade. when will the downgrade be called downgrade is called in SharedItemStateManager.Update.end because the deadlock comes from the searchmanager. he calls DefaultISMLocking.acquireReadLock and this call comes from a different thread than from downgrade. no I don t think so. the stacktrace includes the Update.end call and at the time EventStateCollection.dispatch is called the lock is downgraded and associated with the current thread that later calls acquireReadLock .first thanks for your help ok i will add more log output to get more informations .. hi marcel is it possible that the are running two instances of the DefaultISMLocking Object As i can see there will be generated a instance in the WorkspaceInfo.doInitalize Method and one in the RepositoryImpl.createVersionManager Method. I think on the downgrade Method the active Thread will be set coming from the SharedItemStateManager hold by the Workspace and the deadlock call comes from the VersionItemStateProvider.hasItemState and this call goes to the DefaultISMLocking Object that holds the wrong active Thread. i hope i have explained it that you can follow me - as i mentioned before with the two instances i think this is the problem this is the call stack DefaultISMLocking.acquireWriteLock CurrentThreadName ORB.thread.pool 2 DefaultISMLocking.acquireWriteLock .downgrade CurrentThreadName ORB.thread.pool 0 DefaultISMLocking.RWLock .setActiveWriter ORB.thread.pool 0 Instance org.apache.jackrabbit.core.state.DefaultISMLocking RWLock 765fa6a6 CurrentThreadName ORB.thread.pool 0 Different Thread access Hold Thread in RWLock ORB.thread.pool 2 givenThread ORB.thread.pool 0 ... DefaultISMLocking.RWLock .allowReader activeWriter ORB.thread.pool 2 Instance org.apache.jackrabbit.core.state.DefaultISMLocking RWLock 10b626ba CurrentThreadName ORB.thread.pool 0 the downgrade is called on a other instance of the RWLock then the allowReader comes from a other thread an the hold activeWriter Thread in the RWLock is a different one. i don t know why there are two instances of the DefaultISMLocking Object.can anyone tell me why there are two instances of the DefaultISMLocking Objects This is the reason for my problem. BR clausThe second RWLock probably belongs to the DefaultISMLocking of the version storage. Do you perform versioning operations at the same time hi marcel no i don t perform versioning operations. i only try to add a node. and this is the only operation where the deadlock occurs. please read my comment from 13 Feb 08 09 48 AM there i have hopefully eyplained the deadlock hope you can follow me - The active writer will be changed in the DefaultISMLocking-Instance of the SharedItemStateManager. The allowReader comes from the DefaultISMLocking-Instance hold by the VersionItemStateProvider and in this Instance is the wrong Thread set. thanks lot claus is there anybody out who can tell me why there are 2 instances of DefaultISMLocking Objects we need xa in our environment and so this is a realy important issue for us. thanks clausThe second locking object belongs to the version storage. If that lock has a active writer then your code uses versioning. Is one of your nodes mix versionable Did you also configure the ISMLocking in the repository.xml for version storage as well hi marcel i do not have configured a extra ISMLocking in my repository.xml file because i have applied the patched code directly on the DefaultISMLocking Class My node have only the mix referenceable and mix lockable types. If i look into the last stacktrace i have added i see that the deadlock comes from the SharedItemStateManager.getItemState This calls VersionItemStateProvider.hasItemState. I do not know why this will be called on my node that has no mix versionable as mixintype Anyway .. it tries to acquire a ReadLock with the DefaultISMLocking hold by versionstorage. On the rwLock Object of that instance there was never called downgrade so the rwLock has a other Thread than the rwLock Object of the DefaultISMLocking hold by the SharedItemStateManager. If we call rwLock.setActiveWriter Thread.currentThread in the DefaultISMLocking we must call it on every instance of the DefaultISMLocking with maybe a observation pattern What do you think BR claushi marcel yesterday i have tested again to find a solution for that issue and i have taken the newest packages core common and rmi and i don t know why but your pactch works now fine. the VersionItemStateprovider.hasItemState will not be called anymore so no deadlock occurs. from my side it would be great if you can apply your patch to the trunk. it will work till we dont use versionable nodes but this is no requirement for us .. thanks claus Hi Claus that s good news. however be warned that there still might be an issue. I ran our daily integration test with the patched DefaultISMLocking class and some tests failed with an exception. I ll have to further investigate what causes those errors but once those are resolved I ll commit the changes. Confirmed on jboss 4.2.2.GA using jdk6 and solaris10. The deadlock occurs directly during deploy but was not present using jdk5.hi marcel do you have any news about the errors thanks clausNot yet. I was on vacation and did not have time to work on this issue.hi marcel as written before your patch works fine but i found that in some really rarely situations that i can not reproduce a deadlock has still occured. I have modified your patch as follows to find out when a readlock will be aquired with a wrong active writer public Sync readLock   if activeWriter null activeWriter Thread.currentThread     System.out.println Jackrabbit - PatchedDefaultISMLocking ActiveWriter Thread not the same for readLock ActiveWriter is activeWriter .getName              CurrentThread is Thread.currentThread .getName     Throwable throwable new Throwable     throwable.fillInStackTrace     throwable.printStackTrace     activeWriter Thread.currentThread      return super.readLock i know to set the activewriter to the current Thread is no good idea but i want to know where the issues comes from and my application should not run into a deadlock. And here is the stacktrace 25.06.08 18 13 36 019 CEST 1ea9f0d9 SystemErr R java.lang.Throwable 25.06.08 18 13 36 019 CEST 1ea9f0d9 SystemErr R at org.apache.jackrabbit.core.state.PatchedDefaultISMLocking RWLock.readLock PatchedDefaultISMLocking.java Compiled Code 25.06.08 18 13 36 019 CEST 1ea9f0d9 SystemErr R at org.apache.jackrabbit.core.state.PatchedDefaultISMLocking.acquireReadLock PatchedDefaultISMLocking.java Inlined Compiled Code 25.06.08 18 13 36 019 CEST 1ea9f0d9 SystemErr R at org.apache.jackrabbit.core.state.SharedItemStateManager.acquireReadLock SharedItemStateManager.java Inlined Compiled Code 25.06.08 18 13 36 019 CEST 1ea9f0d9 SystemErr R at org.apache.jackrabbit.core.state.SharedItemStateManager.getItemState SharedItemStateManager.java Compiled Code 25.06.08 18 13 36 019 CEST 1ea9f0d9 SystemErr R at org.apache.jackrabbit.core.state.ChangeLog.undo ChangeLog.java 330 25.06.08 18 13 36 019 CEST 1ea9f0d9 SystemErr R at org.apache.jackrabbit.core.state.XAItemStateManager.rollback XAItemStateManager.java 182 25.06.08 18 13 36 019 CEST 1ea9f0d9 SystemErr R at org.apache.jackrabbit.core.TransactionContext.rollback TransactionContext.java 224 25.06.08 18 13 36 019 CEST 1ea9f0d9 SystemErr R at org.apache.jackrabbit.core.XASessionImpl.rollback XASessionImpl.java 352 25.06.08 18 13 36 019 CEST 1ea9f0d9 SystemErr R at org.apache.jackrabbit.jca.TransactionBoundXAResource.rollback TransactionBoundXAResource.java 76 25.06.08 18 13 36 019 CEST 1ea9f0d9 SystemErr R at com.ibm.ejs.j2c.XATransactionWrapper.rollback XATransactionWrapper.java 1148 25.06.08 18 13 36 019 CEST 1ea9f0d9 SystemErr R at com.ibm.ws.Transaction.JTA.JTAXAResourceImpl.rollback JTAXAResourceImpl.java 347 25.06.08 18 13 36 019 CEST 1ea9f0d9 SystemErr R at com.ibm.ws.Transaction.JTA.RegisteredResources.distributeOutcome RegisteredResources.java 1085 25.06.08 18 13 36 019 CEST 1ea9f0d9 SystemErr R at com.ibm.ws.Transaction.JTA.RegisteredResources.distributeRollback RegisteredResources.java 1908 25.06.08 18 13 36 019 CEST 1ea9f0d9 SystemErr R at com.ibm.ws.Transaction.JTA.TransactionImpl.internalRollback TransactionImpl.java 1511 25.06.08 18 13 36 019 CEST 1ea9f0d9 SystemErr R at com.ibm.ws.Transaction.JTS.TransactionWrapper.rollback TransactionWrapper.java 591 25.06.08 18 13 36 019 CEST 1ea9f0d9 SystemErr R at com.ibm.ws.Transaction.JTS.WSCoordinatorImpl.rollback WSCoordinatorImpl.java 163 25.06.08 18 13 36 019 CEST 1ea9f0d9 SystemErr R at com.ibm.ws.Transaction.JTS. WSCoordinatorImplBase. invoke Unknown Source 25.06.08 18 13 36 019 CEST 1ea9f0d9 SystemErr R at com.ibm.CORBA.iiop.ServerDelegate.dispatchInvokeHandler ServerDelegate.java Compiled Code 25.06.08 18 13 36 019 CEST 1ea9f0d9 SystemErr R at com.ibm.CORBA.iiop.ServerDelegate.dispatch ServerDelegate.java Compiled Code 25.06.08 18 13 36 019 CEST 1ea9f0d9 SystemErr R at com.ibm.rmi.iiop.ORB.process ORB.java Compiled Code 25.06.08 18 13 36 019 CEST 1ea9f0d9 SystemErr R at com.ibm.CORBA.iiop.ORB.process ORB.java Compiled Code 25.06.08 18 13 36 019 CEST 1ea9f0d9 SystemErr R at com.ibm.rmi.iiop.Connection.doWork Connection.java Compiled Code 25.06.08 18 13 36 019 CEST 1ea9f0d9 SystemErr R at com.ibm.rmi.iiop.WorkUnitImpl.doWork WorkUnitImpl.java Compiled Code 25.06.08 18 13 36 019 CEST 1ea9f0d9 SystemErr R at com.ibm.ejs.oa.pool.PooledThread.run ThreadPool.java Compiled Code 25.06.08 18 13 36 019 CEST 1ea9f0d9 SystemErr R at com.ibm.ws.util.ThreadPool Worker.run ThreadPool.java 937 As i can see Websphere has a ThreadPool for the XA Communication and so it is not guaranteed that the same thread that holds the writelock comes to get a redlock. I don t know how to solve that problem BR clausHi I got the same issue on Tomcat with Jencks when using ...rmi.ClientXASession. The prepare and commit requests seem to arrive sometimes at different threads causing a deadlock. I understand that RMI gives no guarantee on which thread requests are served so I fully agree that locking should not be thread based. Btw. I tested with the attached PatchedDefaultISMLocking class and I still get errors although no deadlocks. I get a stale item state exception on commit after deleting a node having versions. So this does not seem to be a solution currently. Roberthi robert i think the patched ISM Locking Class has nothing to do with the stale item exception you get now. I have also found a bug in the LockManager with Transactions see JCR-1702 Maybe you test the attached patch and share your experiences ... thanks clausI have implemented a extension to the current Locking Strategy that binds the lock to thread. Now the lock will not only be bound to the thread but also bound with the Xid to a ThreadLocal Object for the current transaction This patch implements also the code for JCR-1702 patch aahh sorry with this patch the timeout of a xasession will be set to 1200 instead of 5. i had problems that my container websphere does nt set the configured timeout through the jca interfacesValid patch against current trunkCommitted in revision 734400 trunk .Good stuff Merged to the 1.5 branch in revision 743318.
