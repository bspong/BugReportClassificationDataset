OutOfMemoryError when re-indexing the repository
 ERROR 20060825 17 06 40 org.apache.jackrabbit.core.observation.ObservationManagerFactory - Synchronous EventConsumer threw exception. java.lang.OutOfMemoryError when we try to re-index a repository the repository is quite big more then 4 Gb of disk usage and sometimes it stores 40Mb size documents. As attach I put all the last logs we registered with the full stack traces. Related to this whe have also errors with Lucene DEBUG 20060803 08 24 01 org.apache.jackrabbit.core.query.LazyReader - Dump java.io.IOException Invalid header signature read 8656037701166316554 expected -2226271756974174256         at org.apache.jackrabbit.core.query.MsWordTextFilter and then this ones DEBUG 20060803 08 37 17 org.apache.jackrabbit.core.ItemManager - removing item 8637bf5f-4689-4e75-888f-b7b89bef40c8 from cache WARN 20060803 08 40 13 org.apache.jackrabbit.core.RepositoryImpl - Existing lock file at C Wave Repository .lock deteteced. Repository was not shut down properly. ERROR 20060803 09 33 14 org.apache.jackrabbit.core.observation.ObservationManagerFactory - Synchronous EventConsumer threw exception. java.lang.NullPointerException null values not allowed this is our repository.xml configuration for indexing SearchIndex class org.apache.jackrabbit.core.query.lucene.SearchIndex          param name path value wsp.home index          param name textFilterClasses value org.apache.jackrabbit.core.query.lucene.TextPlainTextFilter org.apache.jackrabbit.core.query.MsExcelTextFilter org.apache.jackrabbit.core.query.MsPowerPointTextFilter org.apache.jackrabbit.core.query.MsWordTextFilter org.apache.jackrabbit.core.query.PdfTextFilter org.apache.jackrabbit.core.query.HTMLTextFilter org.apache.jackrabbit.core.query.XMLTextFilter org.apache.jackrabbit.core.query.RTFTextFilter                         org.apache.jackrabbit.core.query.OpenOfficeTextFilter          param name useCompoundFile value true          param name minMergeDocs value 100          param name volatileIdleTime value 3          param name maxMergeDocs value 100000          param name mergeFactor value 10          param name bufferSize value 10          param name cacheSize value 1000          param name forceConsistencyCheck value false          param name autoRepair value true                  param name respectDocumentOrder value false          param name analyzer value org.apache.lucene.analysis.standard.StandardAnalyzer SearchIndex Your log files seem to indicate that some of your content is corrupt Caused by java.lang.IllegalArgumentException invalid QName literal at org.apache.jackrabbit.name.QName.valueOf QName.java 618 at org.apache.jackrabbit.core.state.util.Serializer.deserialize Serializer.java 124 at org.apache.jackrabbit.core.state.obj.ObjectPersistenceManager.load ObjectPersistenceManager.java 206 ... 61 more Please note that using the ObjectPersistenceManager on a production system is not recommended because it is not transactional. You should consider using DerbyPersistenceManager as your version storage.To reproduce this issue I tried to re-index a repository with 100 000 nodes. I was able to re-index the repository with as little as 32 mb heap size. My profiler did not show any exceptional memory usage in the search index. The memory usage was actually quite low. Can you please try to re-index your repository without the text filters Maybe there is a memory leak in one of the filters when an exception is thrown on an invalid or corrupt document. Having a heap dump for analysis would also be helpful. Can you please run the re-indexing process with the following JVM option -Xrunhprof heap sites doe n This will allow you to create a heap dump on a Ctrl-Break on Windows or kill -QUIT on Unix on the JVM process. Thanks a lot.Hi Marcel we think the problem is in PdfTextFilter or in PdfBox libraries. We are not sure about that and we still investigate in that direction. It seems that after an exception something don t free the resources.2 things with pdfbox If you dont religiously close the streams it causes oom problems and a GC wont get to the finalize fast enough to avoid OOM 2. PDFBox has to build the entire document including all the graphics images before it can render the text. If you have a refactored PDF you can get 1000 s of graphics line segments this causes PDBBox to use lots of CPU and Heap converting to a text stream. I am using PDFBox in a different search engine in the same way and it randomly causes lots of problems with refactored PDF files. HTHI tried to re-index my repsoitory without the text filters and it works fine. So the bug is in one of the text filters ... These text filters i used before org.apache.jackrabbit.core.query.lucene.TextPlainTextFilter org.apache.jackrabbit.core.query.MsExcelTextFilter org.apache.jackrabbit.core.query.MsPowerPointTextFilter org.apache.jackrabbit.core.query.MsWordTextFilter org.apache.jackrabbit.core.query.PdfTextFilter org.apache.jackrabbit.core.query.HTMLTextFilter org.apache.jackrabbit.core.query.XMLTextFilter org.apache.jackrabbit.core.query.RTFTextFilter org.apache.jackrabbit.core.query.OpenOfficeTextFilter So i can test to re-index the repository without some filters ... Please give me a hint wich one i should use hi marcel the vm argument -Xrunhprof heap sites doe n does not work in my case. the re-index process stops after about 1-2 minutes with a outofmemory-error is there another way to get a dump file claushi all in my case the most file types in my repository are word documents. if i remove the org.apache.jackrabbit.core.query.MsWordTextFilter class the re-index process works fine. but if i enable the filter the process ends with a outofmemory error. i think we must look for a memory leak ... clausDoes this issue still occur now that RuntimeExceptions are being catched per the JCR-574 fix Claus wrote is there another way to get a dump file Acutally there is. jdk 1.4.2-12 supports the option -XX HeapDumpOnOutOfMemoryError With this option the JVM will create a dump it goes out of memory.Hi Jukka .. The issue JCR-574 is very different to this issue. The Problem was that the LazyReader has only catched Exceptions not Runtime Exceptions. The Problem here is that i get a OutOfMemoryException while re-indexing a huge Repository. This is for me a very big problem because i can not work in a Production environment with Jackrabbit because we have about 4-5 million documents doc xls pdf . If i have to re-index the repsoitory i can not to this. I will try the vm-argument what marcel wrote. clausI would assume that the OutOfMemoryException is triggered by the parsing of some large Word document especially since you reported that the problem does not occur if you disable the Word document filter. Thus if we catch the OutOfMemoryException caused by a single document it will should not interrupt the whole indexing process. Any memory garbage should then get collected automatically unless the document parser stores information statically.ok i didn t think about this .. i will try the reindexing process with the fixed LazyReader and we will see if it works... but at the moment i start the repository with this arguments -Xms1550m -Xmx3000m i do not think that only one document causes the outofmemoryexception. i think there are objects they will not be garbaged clausI must give Jukka right. I have testetd to reindex the repsoitory and it works fine now. For me this issue can be closed .. Thanks lot clausThanks for testing this out Resolved as Fixed.
