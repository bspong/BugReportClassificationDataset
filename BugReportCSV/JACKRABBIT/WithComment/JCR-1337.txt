Optimize first execution queries for DescendantSelfAxisWeight ChildAxisQuery
The first execution of a query involving DescendantSelfAxisWeight ChildAxisQuery is slow. Consecutive queries are faster because the hierarchy is cachedWhen per-document payloads and unique doc ids are implemented in lucene we might be able to optimize the hierarchical cache because we might base it on uid instead of the current doc id based on uid we might keep the hierarchical cache intact for ever even after segment merges and pre-warming caching will be trivial See dev list lucene mail about the subject http mail-archives.apache.org mod mbox lucene-java-dev 200801.mbox 3c4795CE64.6090206 gmail.com 3eIn the meantime we could also initialize the cache when the index reader is created.Proposed patch.That is a quick you patch have there - Looks like a useful optimization i only have concerns in its current form IIUC merging of 2 indexes also introduces a new CachingIndexReader if the new formed index is large initializeParents delegatee might consume a lot of time and cpu. Therefor a background thread with low priority might be better or would this again impose synchronization problems perhaps Also since this patch is only pre-warming newly created CachingIndexReaders and initializes its parents it does still imply slow uncached queries when 2 large indexes merge and create a new CachingIndexReaders. These two indexes might contains many parents of lucene docs residing in other segments which obviously are not re-warmed strange non-existing word but hopefully clear - so still having not the hierarchical cache up2date. Think this patch is an improvement but does not cover the entire pre-warming issue and might lead to hickups for large index merges. Perhaps a nonstop background thread with low priority from time to time updating the hierarchical cache would be more efficient. OTOH with low priority threads we are pretty much jvm dependant on how they are handled isn t WDYT I already had those changes in my checkout Populating the cache is quite fast. On my machine I m able to initialize the cache at a rate of 180k nodes a second. But you are right the tough work still is to resolve DocIds that point to other segments. I d argue that those DocIds are only a fraction unless you have an application that heavily modifies existing content.I ve added logging information to the CachingIndexReader which now write the percentage of UUIDDocIds.180k nodes a sec is really fast but might also be in your test setup where the parent ids are not scattered around over many different indexes. It might get quite a bit slower then isn t Even if 2 large indexes merge the amount of time needed for pre-warming is small compared to the time taken for merging I suppose I ve added logging information to the CachingIndexReader which now write the percentage of UUIDDocIds This is nice info to monitor Reading the parent information is independent of how the ids are distributed. The read is fast because there is no random access involved. The lucene documents can be read sequentially from disk. ... pre-warming is small compared to the time taken for merging I suppose yes I think that s correct. On my machine indexes are merged at about 10k nodes per second. Though merging is completely done in the background while initializing the cache using the current patch is done by the first thread that accesses the new index segment after the merge. Reading the parent information is independent of how the ids are distributed. The read is fast because there is no random access involved. The lucene documents can be read sequentially from disk. But if the number of foreignParents becomes larger through many node updates for example then looking up parent information and thus pre-warming becomes slower or am i missing something If  else if info.parent null                foreignParents                parents info.docId DocId.create info.parent             needs to be executed more often I suppose the initializeParents becomes slower. Either I am missing something or my previous comment was unclear. On my machine indexes are merged at about 10k nodes per second. Though merging is completely done in the background while initializing the cache using the current patch is done by the first thread that accesses the new index segment after the merge. Might background pre-warming be an option But if the number of foreignParents becomes larger through many node updates for example then looking up parent information and thus pre-warming becomes slower or am i missing something the only difference between DocId.create String and DocId.create int is parsing the uuid string. I m not sure how significant that is. It is certainly slower than the int variant but it s much less called. Might background pre-warming be an option Done. The index merger thread forces initialization of the CachingIndexReader before the index segments are replaced. the only difference between DocId.create String and DocId.create int is parsing the uuid string. I m not sure how significant that is. It is certainly slower than the int variant but it s much less called. Ooow that is certainly not what i meant i am certainly not pushing on the difference between parsing a uuid string and just an int I thought that for building the hierarchy cache in your pre-warming the getDocumentNumber MultiIndexReader reader in DocId.UUIDDocId was also executed. I scanned your patch and must have interpreted it incorrectly also because I assumed it would do some parts regarding looking up parents . I assumed it would call the expensive getDocumentNumber MultiIndexReader reader .... I am a little puzzled ATM how it works and what is pre-warmed. I just need to test your patch because I am clearly off track. Disregard my remarks I ll try to find some time to take a better look. Sry for any confusion Done. The index merger thread forces initialization of the CachingIndexReader before the index segments are replaced. having a hard time keep up with you - I ll try your patch tomorrow evening. Curious about the statistics I ll keep yo posted I m sorry that s probably my fault. I didn t describe the patch well enough. The UUIDDocIds are not resolved when the cache is filled. The pre-warming is limited to creating the DocId instances. Because most DocIds are PlainDocIds which do not need to be resolved the pre-warming is quite effective IMO. Because most DocIds are PlainDocIds which do not need to be resolved the pre-warming is quite effective IMO Yes I already figured this out. It is indeed effective as long as the indexes have not very many deleted updated nodes when the number of parent nodes in foreign segment grows . I agree this is an easy first iteration optimization. Also when the number of foreign parent might get very high users good choose to recreate the index by deleting it if it is an option ofcourse - I think that when Lucene would have support for uid in the future we might also be able to pre-warm the parents when they are in different indexes. 1 for your patch though i have not yet had time to test performance gain. Still try to find some time for it soon Applied most recent patch in revision 615223. Ard are you OK with setting this issue to fixed I think once there are more features available in lucene we can create new jira issue.I am ok with it. Not sure either when and how exactly Lucene will add the uid feature. If it makes it to a release we can indeed create an issue to make advantage of it Do you close the issue Forgot to commit new class FieldSelectors. Done in revision 615227.Merged into 1.4 branch together with changes from JCR-1884 in revision 748135
