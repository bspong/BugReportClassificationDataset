<!-- 
RSS generated by JIRA (5.2.8#851-sha1:3262fdc28b4bc8b23784e13eadc26a22399f5d88) at Tue Jul 16 13:16:26 UTC 2013

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/LUCENE-2324/LUCENE-2324.xml?field=key&field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>5.2.8</version>
        <build-number>851</build-number>
        <build-date>26-02-2013</build-date>
    </build-info>

<item>
            <title>[LUCENE-2324] Per thread DocumentsWriters that write their own private segments</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-2324</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;See &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2293&quot; title=&quot;IndexWriter has hard limit on max concurrency&quot;&gt;&lt;del&gt;LUCENE-2293&lt;/del&gt;&lt;/a&gt; for motivation and more details.&lt;/p&gt;

&lt;p&gt;I&apos;m copying here Mike&apos;s summary he posted on 2293:&lt;/p&gt;

&lt;p&gt;Change the approach for how we buffer in RAM to a more isolated&lt;br/&gt;
approach, whereby IW has N fully independent RAM segments&lt;br/&gt;
in-process and when a doc needs to be indexed it&apos;s added to one of&lt;br/&gt;
them. Each segment would also write its own doc stores and&lt;br/&gt;
&quot;normal&quot; segment merging (not the inefficient merge we now do on&lt;br/&gt;
flush) would merge them. This should be a good simplification in&lt;br/&gt;
the chain (eg maybe we can remove the *PerThread classes). The&lt;br/&gt;
segments can flush independently, letting us make much better&lt;br/&gt;
concurrent use of IO &amp;amp; CPU.&lt;/p&gt;</description>
                <environment></environment>
            <key id="12459100">LUCENE-2324</key>
            <summary>Per thread DocumentsWriters that write their own private segments</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png">Resolved</status>
                    <resolution id="1">Fixed</resolution>
                                <assignee username="michaelbusch">Michael Busch</assignee>
                                <reporter username="michaelbusch">Michael Busch</reporter>
                        <labels>
                    </labels>
                <created>Mon, 15 Mar 2010 05:40:56 +0000</created>
                <updated>Thu, 28 Apr 2011 16:19:55 +0100</updated>
                    <resolved>Thu, 28 Apr 2011 16:19:55 +0100</resolved>
                                            <fixVersion>Realtime Branch</fixVersion>
                                <component>core/index</component>
                        <due></due>
                    <votes>1</votes>
                        <watches>8</watches>
                                                    <comments>
                    <comment id="12845199" author="michaelbusch" created="Mon, 15 Mar 2010 06:33:31 +0000"  >&lt;p&gt;Here is an interesting article about allocation/deallocation on modern JVMs:&lt;br/&gt;
&lt;a href=&quot;http://www.ibm.com/developerworks/java/library/j-jtp09275.html&quot; class=&quot;external-link&quot;&gt;http://www.ibm.com/developerworks/java/library/j-jtp09275.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;And here is a snippet that mentions how pooling is generally not faster anymore:&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;Allocation in JVMs was not always so fast &amp;#8211; early JVMs indeed had poor allocation and garbage collection performance, which is almost certainly where this myth got started. In the very early days, we saw a lot of &quot;allocation is slow&quot; advice &amp;#8211; because it was, along with everything else in early JVMs &amp;#8211; and performance gurus advocated various tricks to avoid allocation, such as object pooling. (Public service announcement: Object pooling is now a serious performance loss for all but the most heavyweight of objects, and even then it is tricky to get right without introducing concurrency bottlenecks.) However, a lot has happened since the JDK 1.0 days; the introduction of generational collectors in JDK 1.2 has enabled a much simpler approach to allocation, greatly improving performance. &lt;/p&gt;
&lt;hr /&gt;
</comment>
                    <comment id="12845261" author="mikemccand" created="Mon, 15 Mar 2010 09:47:41 +0000"  >&lt;p&gt;Sounds great &amp;#8211; let&apos;s test it in practice.&lt;/p&gt;</comment>
                    <comment id="12845398" author="michaelbusch" created="Mon, 15 Mar 2010 16:33:50 +0000"  >&lt;p&gt;Reply to Mike&apos;s comment on &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2293&quot; title=&quot;IndexWriter has hard limit on max concurrency&quot;&gt;&lt;del&gt;LUCENE-2293&lt;/del&gt;&lt;/a&gt;: &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2293?focusedCommentId=12845263&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#action_12845263&quot; class=&quot;external-link&quot;&gt;https://issues.apache.org/jira/browse/LUCENE-2293?focusedCommentId=12845263&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#action_12845263&lt;/a&gt;&lt;/p&gt;


&lt;blockquote&gt;
&lt;p&gt;I think we can do even better, ie, that class wastes RAM for the single posting case (intStart, byteStart, lastDocID, docFreq, lastDocCode, lastDocPosition are not needed).&lt;/p&gt;

&lt;p&gt;EG we could have a separate class dedicated to the singleton case. When term is first encountered it&apos;s enrolled there. We&apos;d probably need a separate hash to store these (though not necessarily?). If it&apos;s seen again it&apos;s switched to the full posting.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Hmm I think we&apos;d need a separate hash.  Otherwise you have to subclass PostingList for the different cases (freq. vs. non-frequent terms) and do instanceof checks? Or with the parallel arrays idea maybe we could encode more information in the dense ID? E.g. use one bit to indicate if that term occurred more than once. &lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;I mean instead of allocating an instance per unique term, we assign an integer ID (dense, ie, 0, 1, 2...).&lt;/p&gt;

&lt;p&gt;And then we have an array for each member now in FreqProxTermsWriter.PostingList, ie int[] docFreqs, int [] lastDocIDs, etc. Then to look up say the lastDocID for a given postingID you just get lastDocIDs&lt;span class=&quot;error&quot;&gt;&amp;#91;postingID&amp;#93;&lt;/span&gt;. If we&apos;re worried about oversize allocation overhead, we can make these arrays paged... but that&apos;d slow down each access.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yeah I like that idea. I&apos;ve done something similar for representing trees - I had a very compact Node class with no data but such a dense ID, and arrays that stored the associated data.  Very easy to add another data type with no RAM overhead (you only use the amount of RAM the data needs).&lt;/p&gt;

&lt;p&gt;Though, the price you pay is for dereferencing multiple times for each array?  &lt;br/&gt;
And how much RAM would we safe? The pointer for the PostingList object (4-8 bytes), plus the size of the object header - how much is that in Java? &lt;/p&gt;

&lt;p&gt;Seems ilke it&apos;s 8 bytes: &lt;a href=&quot;http://www.codeinstructions.com/2008/12/java-objects-memory-structure.html&quot; class=&quot;external-link&quot;&gt;http://www.codeinstructions.com/2008/12/java-objects-memory-structure.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;So in a 32Bit JVM we would safe 4 bytes (pointer) + 8 bytes (header) - 4 bytes (ID) = 8 bytes.  For fields with tons of unique terms that might be worth it?  &lt;/p&gt;</comment>
                    <comment id="12845400" author="michaelbusch" created="Mon, 15 Mar 2010 16:43:31 +0000"  >&lt;blockquote&gt;
&lt;p&gt;Sounds great - let&apos;s test it in practice.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I have to admit that I need to catch up a bit on the flex branch.  I was wondering if it makes sense to make these kinds of experiments (pooling vs. non-pooling) with the flex code? Is it as fast as trunk already, or are there related nocommits left that affect indexing performance?  I would think not much of the flex changes should affect the in-memory indexing performance (in TermsHash*).&lt;/p&gt;</comment>
                    <comment id="12845408" author="earwin" created="Mon, 15 Mar 2010 17:05:05 +0000"  >&lt;p&gt;&amp;gt; Seems ilke it&apos;s 8 bytes&lt;br/&gt;
Object header is two words, so that&apos;s 16bytes for 64bit arch. (probably 12 for 64bit+CompressedOops?)&lt;/p&gt;

&lt;p&gt;Also, GC time is (roughly) linear in number of objects on heap, so replacing single huge array of objects with few huge primitive arrays for their fields does miracles to your GC delays.&lt;/p&gt;</comment>
                    <comment id="12845426" author="mikemccand" created="Mon, 15 Mar 2010 17:52:25 +0000"  >&lt;blockquote&gt;&lt;p&gt;Hmm I think we&apos;d need a separate hash. Otherwise you have to subclass PostingList for the different cases (freq. vs. non-frequent terms) and do instanceof checks? Or with the parallel arrays idea maybe we could encode more information in the dense ID? E.g. use one bit to indicate if that term occurred more than once.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Or 2 sets of parallel arrays (one for the singletons).... or, something.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;So in a 32Bit JVM we would safe 4 bytes (pointer) + 8 bytes (header) - 4 bytes (ID) = 8 bytes. For fields with tons of unique terms that might be worth it?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;And also the GC cost.&lt;/p&gt;

&lt;p&gt;But it seems like specializing singleton fields will be the bigger win.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I was wondering if it makes sense to make these kinds of experiments (pooling vs. non-pooling) with the flex code?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Last I tested (a while back now) indexing perf was the same &amp;#8211; need to&lt;br/&gt;
test again w/ recent changes (eg terms index is switching to packed&lt;br/&gt;
ints).  For pooling vs not I&apos;d just do the experiment on trunk?&lt;/p&gt;

&lt;p&gt;And most of this change (changing how postings data is buffered in&lt;br/&gt;
RAM) is &quot;above&quot; flex I expect.&lt;/p&gt;

&lt;p&gt;But if for some reason you need to start changing index postings&lt;br/&gt;
format then you should probably do that on flex.&lt;/p&gt;</comment>
                    <comment id="12845428" author="mikemccand" created="Mon, 15 Mar 2010 17:57:50 +0000"  >&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;Seems ilke it&apos;s 8 bytes&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Object header is two words, so that&apos;s 16bytes for 64bit arch. (probably 12 for 64bit+CompressedOops?)&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Right, and the pointer&apos;d also be 8 bytes (but compact int stays at 4&lt;br/&gt;
bytes) so net/net on 64bit JRE savings would be 16-20 bytes per term.&lt;/p&gt;

&lt;p&gt;Another thing we could do if we cutover to parallel arrays is to&lt;br/&gt;
switch to packed ints.  Many of these fields are horribly wasteful as&lt;br/&gt;
ints, eg docFreq or lastPosition.&lt;/p&gt;</comment>
                    <comment id="12846028" author="jasonrutherglen" created="Tue, 16 Mar 2010 17:50:16 +0000"  >&lt;p&gt;Carrying over from &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2312&quot; title=&quot;Search on IndexWriter&amp;#39;s RAM Buffer&quot;&gt;LUCENE-2312&lt;/a&gt;.  I&apos;m proposing we for starters have a byte slice writer, lock, move or copy&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/help_16.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; the bytes from the writable byte pool/writer to a read only byte block pool, unlock.  This sounds like a fairly self-contained thing that can be unit tested at a low level.&lt;/p&gt;

&lt;p&gt;Mike, can you add a bit as to how this could work?  Also, what is the IntBlockPool used for?  &lt;/p&gt;</comment>
                    <comment id="12846037" author="jasonrutherglen" created="Tue, 16 Mar 2010 17:59:51 +0000"  >&lt;p&gt;Are there going to be issues with the char array buffers as well (ie, will we need to also flush them for concurrency?)&lt;/p&gt;</comment>
                    <comment id="12846084" author="michaelbusch" created="Tue, 16 Mar 2010 19:29:17 +0000"  >&lt;p&gt;Shall we not first try to remove the downstream *PerThread classes and make the DocumentsWriter single-threaded without locking.  Then we add a PerThreadDocumentsWriter and DocumentsWriterThreadBinder, which talks to the PerThreadDWs and IW talks to DWTB.  We can pick other names &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;When that&apos;s done we can think about what kind of locking/synchronization/volatile stuff we need for &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2312&quot; title=&quot;Search on IndexWriter&amp;#39;s RAM Buffer&quot;&gt;LUCENE-2312&lt;/a&gt;.&lt;/p&gt;</comment>
                    <comment id="12846102" author="jasonrutherglen" created="Tue, 16 Mar 2010 20:18:22 +0000"  >&lt;p&gt;Michael,&lt;/p&gt;

&lt;p&gt;For &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2312&quot; title=&quot;Search on IndexWriter&amp;#39;s RAM Buffer&quot;&gt;LUCENE-2312&lt;/a&gt;, I think the searching isn&apos;t going to be an&lt;br/&gt;
issue, I&apos;ve got basic per thread doc writers working (though not&lt;br/&gt;
thoroughly tested). I didn&apos;t see a great need to rework all the&lt;br/&gt;
classes, which even if we did, I&apos;m not sure helps with the byte&lt;br/&gt;
array read write issues? I&apos;d prefer to get a proof of concept&lt;br/&gt;
more or less working, then refine it from there. I think there&apos;s&lt;br/&gt;
two main design/implementation issues before we can roll&lt;br/&gt;
something out:&lt;/p&gt;

&lt;p&gt;1) A new skip list implementation that at specific intervals&lt;br/&gt;
writes a new skip (ie, single level). Right now in trunk we have&lt;br/&gt;
a multilevel skiplist that requires ahead of time the number of&lt;br/&gt;
docs.&lt;/p&gt;

&lt;p&gt;2) Figure out the low -&amp;gt; high levels of byte/char/int array&lt;br/&gt;
visibility to reader threads. The main challenge here is the&lt;br/&gt;
fact that the DW related code that utilizes this is really hard&lt;br/&gt;
for me to understand enough to know what can be changed, without&lt;br/&gt;
the side effect being bunches of other broken stuff. If there&lt;br/&gt;
was a Directory like class abstraction we could simply override&lt;br/&gt;
and reimplement, we could do that, and maybe there is one, I&apos;m&lt;br/&gt;
not sure yet. &lt;/p&gt;

&lt;p&gt;However if reworking the PerThread classes somehow makes the tie&lt;br/&gt;
into the IO (eg, the byte array pooling) system abstracted and&lt;br/&gt;
easier, then I&apos;m all for it.&lt;/p&gt;</comment>
                    <comment id="12846110" author="jasonrutherglen" created="Tue, 16 Mar 2010 20:45:41 +0000"  >&lt;p&gt;NormsWriterPerField has a growing norm byte array, we&apos;d need a way to read/write lock it... &lt;/p&gt;

&lt;p&gt;I think we have concurrency issues in the TermsHash table?  Maybe it&apos;d need to be rewritten to use ConcurrentHashMap?&lt;/p&gt;</comment>
                    <comment id="12846112" author="jasonrutherglen" created="Tue, 16 Mar 2010 20:50:09 +0000"  >&lt;p&gt;Actually TermsHashField doesn&apos;t need to be concurrent, it&apos;s only being written to and the terms concurrent skiplist (was a btree) holds the reference to the posting list.  So I think we&apos;re good there because terms enum never accesses the terms hash.  Nice!&lt;/p&gt;
</comment>
                    <comment id="12846128" author="michaelbusch" created="Tue, 16 Mar 2010 21:13:17 +0000"  >&lt;p&gt;I think we all agree that we want to have a single writer thread, multi reader thread model.  Only then the thread-safety problems in &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2312&quot; title=&quot;Search on IndexWriter&amp;#39;s RAM Buffer&quot;&gt;LUCENE-2312&lt;/a&gt; can be reduced to visibility (no write-locking).  So I think making this change first makes most sense.  It involves a bit boring refactoring work unfortunately. &lt;/p&gt;</comment>
                    <comment id="12846220" author="jasonrutherglen" created="Wed, 17 Mar 2010 00:04:24 +0000"  >&lt;p&gt;Michael, Agreed, can you outline how you think we should proceed then?&lt;/p&gt;</comment>
                    <comment id="12846586" author="michaelbusch" created="Wed, 17 Mar 2010 21:06:57 +0000"  >&lt;blockquote&gt;&lt;p&gt;Michael, Agreed, can you outline how you think we should proceed then?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Sorry for not responding earlier...&lt;/p&gt;

&lt;p&gt;I&apos;m currently working on removing the PostingList object pooling, because it makes TermsHash and TermsHashPerThread much easier.  Have written the patch and all tests pass, though I haven&apos;t done performance testing yet.  Making TermsHash and TermsHashPerThread smaller will also make the patch here easier which will remove them. I&apos;ll post the patch soon. &lt;/p&gt;

&lt;p&gt;Next steps I think here are to make everything downstream of DocumentsWriter single-threaded (removal of *PerThread) classes.  Then we need to write the DocumentsWriterThreadBinder and have to think about how to apply deletes, commits and rollbacks to all DocumentsWriter instances.  &lt;/p&gt;</comment>
                    <comment id="12846591" author="michaelbusch" created="Wed, 17 Mar 2010 21:12:28 +0000"  >&lt;p&gt;All tests pass but I have to review if with the changes the memory consumption calculation still works correctly. Not sure if the junits test that?&lt;/p&gt;

&lt;p&gt;Also haven&apos;t done any performance testing yet.  &lt;/p&gt;</comment>
                    <comment id="12849806" author="jasonrutherglen" created="Thu, 25 Mar 2010 17:56:22 +0000"  >&lt;p&gt;Michael, I&apos;m guessing this patch needs to be updated as per &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2329&quot; title=&quot;Use parallel arrays instead of PostingList objects&quot;&gt;&lt;del&gt;LUCENE-2329&lt;/del&gt;&lt;/a&gt;?  &lt;/p&gt;</comment>
                    <comment id="12849808" author="jasonrutherglen" created="Thu, 25 Mar 2010 18:03:30 +0000"  >&lt;p&gt;Actually, I just browsed the patch again, I don&apos;t think it implements private doc writers as of yet?  &lt;/p&gt;

&lt;p&gt;I think you&apos;re right, we can get this issue completed.  &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2312&quot; title=&quot;Search on IndexWriter&amp;#39;s RAM Buffer&quot;&gt;LUCENE-2312&lt;/a&gt;&apos;s path looks clear at this point.  Shall I take a whack at it?&lt;/p&gt;</comment>
                    <comment id="12849819" author="michaelbusch" created="Thu, 25 Mar 2010 18:27:36 +0000"  >&lt;p&gt;Hey Jason,&lt;/p&gt;

&lt;p&gt;Disregard my patch here.  I just experimented with removal of pooling, but then did &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2329&quot; title=&quot;Use parallel arrays instead of PostingList objects&quot;&gt;&lt;del&gt;LUCENE-2329&lt;/del&gt;&lt;/a&gt; instead.  TermsHash and TermsHashPerThread are now much simpler, because all the pooling code is gone after 2329 was committed.  Should make it a little easier to get this patch done.&lt;/p&gt;

&lt;p&gt;Sure it&apos;d be awesome if you could provide a patch here.  I can help you, we should just frequently post patches here so that we don&apos;t both work on the same areas.&lt;/p&gt;
</comment>
                    <comment id="12849844" author="jasonrutherglen" created="Thu, 25 Mar 2010 19:01:15 +0000"  >&lt;p&gt;Michael, I&apos;m working on a patch and will post one (hopefully) shortly.&lt;/p&gt;</comment>
                    <comment id="12849899" author="michaelbusch" created="Thu, 25 Mar 2010 21:00:30 +0000"  >&lt;p&gt;Awesome!&lt;/p&gt;</comment>
                    <comment id="12849965" author="jasonrutherglen" created="Fri, 26 Mar 2010 00:47:08 +0000"  >&lt;p&gt;I&apos;m a little confused in the flushedDocCount, remap deletes conversion portions of DocWriter.  flushedDocCount is used as a global counter, however when we move to per thread doc writers, it won&apos;t be global anymore.  Is there a different (easier) way to perform remap deletes?  &lt;/p&gt;</comment>
                    <comment id="12850056" author="mikemccand" created="Fri, 26 Mar 2010 09:47:46 +0000"  >&lt;p&gt;Good question...&lt;/p&gt;

&lt;p&gt;When we buffer delete Term/Query we record the current docID as of when that delete had arrived (so that interleaved delete/adds are resolved properly).  The docID we record is &quot;absolute&quot; (ie, adds in the base flushedDocCount), so that we can decouple when deletes are materialized (moved into the deletedDocs BitVectors) from when new segments are flushed.&lt;/p&gt;

&lt;p&gt;I think we have a couple options.&lt;/p&gt;

&lt;p&gt;Option 1 is to use a relative (within the current segment) docID when the deleted Term/Query/docID is first buffered, but then make it absolute only when the segment is finally flushed.&lt;/p&gt;

&lt;p&gt;Option 2 is to use a relative docID, but do away with the decoupling, ie force deletions to always flush at the same time the segment is flushed.&lt;/p&gt;

&lt;p&gt;I think I like option 1 the best &amp;#8211; I suspect the decoupling gains us performance as it allows us to batch up more deletions (doing deletions in batch gets better locality, and also means opening/closing readers left often, in the non-pooling case).&lt;/p&gt;</comment>
                    <comment id="12850211" author="jasonrutherglen" created="Fri, 26 Mar 2010 16:54:32 +0000"  >&lt;p&gt;Mike, lets do option 1.  I think the process of making the doc id absolute is simply adding up the previous segments num docs to be the base?  &lt;/p&gt;

&lt;p&gt;Option 2 would use reader cloning?&lt;/p&gt;
</comment>
                    <comment id="12850214" author="mikemccand" created="Fri, 26 Mar 2010 16:59:58 +0000"  >&lt;blockquote&gt;&lt;p&gt;I think the process of making the doc id absolute is simply adding up the previous segments num docs to be the base?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Right.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Option 2 would use reader cloning?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I don&apos;t think so &amp;#8211; I think it&apos;d have to pull a SegmentReader for every segment every time we flush a new segment, to resolve the deletions.  In the non-pooled case that&apos;d be a newly opened SegmentReader for every segment in the index every time a new segment is flushed.&lt;/p&gt;</comment>
                    <comment id="12850223" author="jasonrutherglen" created="Fri, 26 Mar 2010 17:15:24 +0000"  >&lt;p&gt;Currently the doc writer manages the ram buffer size, however&lt;br/&gt;
this needs to be implemented across doc writers for this issue&lt;br/&gt;
to be complete. IW addDoc returns doFlush from DW. I don&apos;t think&lt;br/&gt;
doFlush will be useful anymore?&lt;/p&gt;

&lt;p&gt;A slightly different memory management needs to be designed.&lt;br/&gt;
Right now we allow the user to set the max ram buffer size and&lt;br/&gt;
when the doc writer&apos;s buffers exceed the ram limit, the buffer&lt;br/&gt;
is flushed and the process is complete.&lt;/p&gt;

&lt;p&gt;With this issue, the flush logic probably needs to be bumped up&lt;br/&gt;
into IW, and flushing becomes a multi-docwriter ram usage&lt;br/&gt;
examination. For starters, if the aggregate ram usage of all doc&lt;br/&gt;
writers exceeds the IWC defined ram buffer size, we need to&lt;br/&gt;
schedule flushing the doc writer with the greatest ram usage? I&lt;br/&gt;
wonder if there&apos;s something I&apos;m missing here in regards to&lt;br/&gt;
synchronization issues with DW?&lt;/p&gt;</comment>
                    <comment id="12850227" author="jasonrutherglen" created="Fri, 26 Mar 2010 17:26:04 +0000"  >&lt;p&gt;Following up on the previous comment, if the current thread (the one calling add doc) is also the one that needs to do the flushing, then only the thread attached to the doc writer with the greatest ram usage can/should do the flushing?&lt;/p&gt;</comment>
                    <comment id="12850235" author="michaelbusch" created="Fri, 26 Mar 2010 17:34:16 +0000"  >&lt;p&gt;The easiest would be if each DocumentsWriterPerThread had a fixed buffer size, then they can flush fully independently and you don&apos;t need to manage RAM globally across threads.&lt;/p&gt;

&lt;p&gt;Of course then you&apos;d need two config parameters: number of concurrent threads and buffer size per thread.&lt;/p&gt;</comment>
                    <comment id="12850244" author="mikemccand" created="Fri, 26 Mar 2010 17:41:41 +0000"  >&lt;p&gt;But if 1 thread tends to index lots of biggish docs... don&apos;t we want to allow it to use up more than 1/nth?&lt;/p&gt;

&lt;p&gt;Ie we don&apos;t want to flush unless total RAM usage has hit the limit?&lt;/p&gt;</comment>
                    <comment id="12850253" author="jasonrutherglen" created="Fri, 26 Mar 2010 17:52:22 +0000"  >&lt;blockquote&gt;&lt;p&gt;Of course then you&apos;d need two config parameters: number of&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;concurrent threads and buffer size per thread.&lt;/p&gt;

&lt;p&gt;I&apos;m not sure how we&apos;d enforce the number of threads? Or we&apos;d&lt;br/&gt;
have to re-implement the wait system implemented in DW? In&lt;br/&gt;
practice, each thread&apos;s DW will probably have roughly the same&lt;br/&gt;
ram buffer size so if they each have the same max size, that&apos;d&lt;br/&gt;
be ok. I don&apos;t think we can limit the number of threads though,&lt;br/&gt;
because we&apos;d need to then implement interleaving doc ids? &lt;/p&gt;</comment>
                    <comment id="12850260" author="jasonrutherglen" created="Fri, 26 Mar 2010 18:01:19 +0000"  >&lt;p&gt;Also, I think we can remove the sync on doFlushInternal?&lt;/p&gt;</comment>
                    <comment id="12850262" author="michaelbusch" created="Fri, 26 Mar 2010 18:02:50 +0000"  >&lt;blockquote&gt;
&lt;p&gt;But if 1 thread tends to index lots of biggish docs... don&apos;t we want to allow it to use up more than 1/nth?&lt;br/&gt;
Ie we don&apos;t want to flush unless total RAM usage has hit the limit?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Sure that&apos;d be the disadvantage.  But is that a realistic scenario?  That the &quot;avg. document size per thread&quot; differ significantly in an application?  &lt;/p&gt;</comment>
                    <comment id="12850265" author="michaelbusch" created="Fri, 26 Mar 2010 18:06:33 +0000"  >&lt;blockquote&gt;
&lt;p&gt;I&apos;m not sure how we&apos;d enforce the number of threads? Or we&apos;d&lt;br/&gt;
have to re-implement the wait system implemented in DW? &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I was thinking we were going to do that... having a fixed number of DocumentsWriterPerThread instances, and a ThreadBinder that let&apos;s a thread wait if the perthread is not available.  You don&apos;t need to interleave docIds then?  &lt;/p&gt;</comment>
                    <comment id="12850290" author="mikemccand" created="Fri, 26 Mar 2010 18:35:43 +0000"  >&lt;blockquote&gt;&lt;p&gt;But is that a realistic scenario? That the &quot;avg. document size per thread&quot; differ significantly in an application?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think this could happen, eg if an app uses different threads for indexing different sources of docs.  Not all apps index only 140 character docs from all threads &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/wink.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;I think for this same reason the ThreadBinder should have affinity, ie, try to schedule the same thread to the same DW, assuming it&apos;s free.  If it&apos;s not free and another DW is free you should use the other one.&lt;/p&gt;</comment>
                    <comment id="12850292" author="mikemccand" created="Fri, 26 Mar 2010 18:38:11 +0000"  >&lt;blockquote&gt;&lt;p&gt;I was thinking we were going to do that... having a fixed number of DocumentsWriterPerThread instances, and a ThreadBinder that let&apos;s a thread wait if the perthread is not available. You don&apos;t need to interleave docIds then?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think we&apos;d have a fixed max?  And then we&apos;d create a new DW when all existing ones are in-use and we&apos;re not yet at the max?&lt;/p&gt;

&lt;p&gt;Eg if it&apos;s a thread pool of size 50 that&apos;s indexing, but, the rate of docs is very slow such that in practice only one of these 50 threads is indexing at once, we&apos;d only use one DW.  And we&apos;d flush when that DW hits the RAM limit.&lt;/p&gt;</comment>
                    <comment id="12850295" author="mikemccand" created="Fri, 26 Mar 2010 18:42:11 +0000"  >&lt;p&gt;Yes, doFlushInternal should no longer be sync&apos;d.  It should have a small sync&apos;d at the end where it 1) inserts the new SegmentInfo into the in-memory segments, and 2) computes the new base for remapping the deletes.  I think it can then release the sync while it does the remapping of buffered deletes?&lt;/p&gt;</comment>
                    <comment id="12850312" author="michaelbusch" created="Fri, 26 Mar 2010 19:20:54 +0000"  >&lt;blockquote&gt;&lt;p&gt;Not all apps index only 140 character docs from all threads &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;What a luxury! &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;I think for this same reason the ThreadBinder should have affinity, ie, try to schedule the same thread to the same DW, assuming it&apos;s free. If it&apos;s not free and another DW is free you should use the other one.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;If you didn&apos;t have such an affinity but use a random assignment of DWs to threads, would that balance the RAM usage across DWs without a global RAM management?&lt;/p&gt;</comment>
                    <comment id="12850362" author="jasonrutherglen" created="Fri, 26 Mar 2010 21:32:41 +0000"  >&lt;p&gt;The patch is not committable.  &lt;/p&gt;

&lt;p&gt;The basics are here.  We can worry about max threads later.  For now there&apos;s a doc writer per thread.  I still don&apos;t fully understand remap deletes, so it&apos;s commented out.  There&apos;s a flushedDocCount per DW, and a global one in IW.  &lt;/p&gt;

&lt;p&gt;IWC is accessed directly where possible.&lt;/p&gt;

&lt;p&gt;When running TestIndexWriter, the DW resumeAllThreads assertion fails.&lt;/p&gt;

&lt;p&gt;If the total ram buffer size is too large, and the current thread&apos;s DW is the largest, it&apos;s flushed.  The next test case can be in regards to the memory management.  &lt;/p&gt;</comment>
                    <comment id="12850440" author="jasonrutherglen" created="Sat, 27 Mar 2010 01:09:53 +0000"  >&lt;p&gt;In regards to remap deletes. I looked at buffered deletes in&lt;br/&gt;
depth when implementing &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2047&quot; title=&quot;IndexWriter should immediately resolve deleted docs to docID in near-real-time mode&quot;&gt;LUCENE-2047&lt;/a&gt;. I think deletes are fairly&lt;br/&gt;
simple, though there&apos;s all this logic to manage them because&lt;br/&gt;
they&apos;re buffered. I&apos;m liking deleting in the foreground and&lt;br/&gt;
queuing the deleted doc ids per segment as a way to avoid&lt;br/&gt;
remapping absolute doc ids in commit merge. Hopefully this&lt;br/&gt;
simplifies things?&lt;/p&gt;</comment>
                    <comment id="12850449" author="jasonrutherglen" created="Sat, 27 Mar 2010 01:41:33 +0000"  >&lt;p&gt;Mike, can you describe the difference between DW&apos;s deletesInRAM and deletesFlushed?  I have some idea, however maybe some clarification will help.  &lt;/p&gt;</comment>
                    <comment id="12850500" author="mikemccand" created="Sat, 27 Mar 2010 09:44:35 +0000"  >&lt;p&gt;I don&apos;t think we should delete in FG &amp;#8211; I suspect this&apos;ll give net/net worse performance, due to loss of locality.  It also means you must always keep readers available, which is an unnecessary cost for non-NRT apps.&lt;/p&gt;

&lt;p&gt;deletesInRAM are those deletes done during the current segment.  deletesFlushed absorbs deletesInRAM on successful segment flush.  We have to segregate the two for proper recovery if we fail to flush the RAM buffer, eg say you hit a disk full while flushing a new segment, and then you close your IW successfully.  We have to make sure in that case that deletesInRAM are discarded.&lt;/p&gt;</comment>
                    <comment id="12850754" author="jasonrutherglen" created="Mon, 29 Mar 2010 00:31:53 +0100"  >&lt;blockquote&gt;&lt;p&gt;I don&apos;t think we should delete in FG - I suspect this&apos;ll&lt;br/&gt;
give net/net worse performance, due to loss of locality. It also&lt;br/&gt;
means you must always keep readers available, which is an&lt;br/&gt;
unnecessary cost for non-NRT apps. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Right, I agree it&apos;s best to buffer the deleted terms/queries...&lt;br/&gt;
The comments below touch on why this came up as a thought.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;deletesInRAM are those deletes done during the current&lt;br/&gt;
segment. deletesFlushed absorbs deletesInRAM on successful&lt;br/&gt;
segment flush. We have to segregate the two for proper recovery&lt;br/&gt;
if we fail to flush the RAM buffer, eg say you hit a disk full&lt;br/&gt;
while flushing a new segment, and then you close your IW&lt;br/&gt;
successfully. We have to make sure in that case that&lt;br/&gt;
deletesInRAM are discarded. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Ok, this helps... &lt;/p&gt;

&lt;p&gt;The design issue I&apos;m running into is the buffered deletes &quot;num&lt;br/&gt;
limit&quot; which seems to be the highest doc id of DW at the time&lt;br/&gt;
the delete docs method is called? Then when apply deletes is&lt;br/&gt;
called, deletes are made only up to &quot;num limit&quot;? This is to&lt;br/&gt;
insure that documents (with for example a del term) that are&lt;br/&gt;
added after the delete docs call, are not deleted when apply&lt;br/&gt;
deletes is called, unless another call to del docs is made again&lt;br/&gt;
(at which point the &quot;num limit&quot; is set to the current DW max doc&lt;br/&gt;
id).&lt;/p&gt;

&lt;p&gt;If the above is true, in order to not remap deletes on each&lt;br/&gt;
merge, would we need to maintain this &quot;num limit&quot; variable per&lt;br/&gt;
DW? I don&apos;t think the global remap is useful with per thread DWs&lt;br/&gt;
because a DW could fail, and we wouldn&apos;t know the order in&lt;br/&gt;
segment infos it could/would&apos;ve been placed? Whereas with a&lt;br/&gt;
single DW, we know the new segment will be placed last in&lt;br/&gt;
segment infos.&lt;/p&gt;

&lt;p&gt;We could maintain a global deleted terms/queries queue, but then&lt;br/&gt;
we&apos;d also need to maintain a term -&amp;gt; &quot;num limit&quot; map per DW? It&lt;br/&gt;
seems a bit redundant, but maybe it&apos;s ok?&lt;/p&gt;
</comment>
                    <comment id="12850760" author="michaelbusch" created="Mon, 29 Mar 2010 01:34:37 +0100"  >&lt;p&gt;Yes, we would need to buffer terms/queries per DW and also per DW the BufferedDeletes.Num.  The docID spaces in two DWs will be completely independent of each other after this change.&lt;/p&gt;


&lt;p&gt;One potential problem that we (I think) even today have is the following: If you index with multiple threads, and then call e.g. deleteDocuments(Term) with one of the indexer threads while you keep adding documents with the other threads, it&apos;s not clear to the caller when exactly the deleteDocuments(Term) will happen.  It depends on the thread scheduling. &lt;/p&gt;

&lt;p&gt;Going back to the idea I mentioned here:&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2293?focusedCommentId=12841407&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#action_12841407&quot; class=&quot;external-link&quot;&gt;https://issues.apache.org/jira/browse/LUCENE-2293?focusedCommentId=12841407&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#action_12841407&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I mentioned the idea of having a sequence ID, that gets incremented on add, delete, update.  What if we had even with separate DWs a global sequence ID?  The sequence ID would tell you unambiguously which action happened when.  The add/update/delete methods could return the sequenceID that was assigned to that particular action.  &lt;/p&gt;

&lt;p&gt;Then we could e.g. track the delete terms globally together with the sequenceID of the corresponding delete call, while we still apply deletes during flush.  Since sequenceIDs enforce a strict ordering we can figure out to how many docs per DW we need to apply the delete terms.&lt;/p&gt;

&lt;p&gt;Later when we switch to real-time deletes (when the RAM is searchable) we will simply store the sequenceIDs in the deletes int[] array which I mentioned in my comment on &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2293&quot; title=&quot;IndexWriter has hard limit on max concurrency&quot;&gt;&lt;del&gt;LUCENE-2293&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Does this make sense?&lt;/p&gt;</comment>
                    <comment id="12850766" author="michaelbusch" created="Mon, 29 Mar 2010 01:58:40 +0100"  >&lt;blockquote&gt;&lt;p&gt;I think for this same reason the ThreadBinder should have affinity&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Mike, can you explain what the advantages of this kind of thread affinity are?  I was always wondering why the DocumentsWriter code currently makes efforts to assign a ThreadState always to the same Thread?  Is that being done for performance reasons?  &lt;/p&gt;</comment>
                    <comment id="12850780" author="jasonrutherglen" created="Mon, 29 Mar 2010 03:35:20 +0100"  >&lt;blockquote&gt;&lt;p&gt;I mentioned the idea of having a sequence ID, that gets&lt;br/&gt;
incremented on add, delete, update. What if we had even with&lt;br/&gt;
separate DWs a global sequence ID? The sequence ID would tell&lt;br/&gt;
you unambiguously which action happened when. The&lt;br/&gt;
add/update/delete methods could return the sequenceID that was&lt;br/&gt;
assigned to that particular action.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think adding the global sequence id makes sense and would be&lt;br/&gt;
simple to add (eg, AtomicLong). However, in the apply deletes&lt;br/&gt;
method how would we know which doc to stop deleting at? How&lt;br/&gt;
would the seq id map to a DW&apos;s doc id?&lt;/p&gt;

&lt;p&gt;If we have independent buffered deletes per DW-thread, then how&lt;br/&gt;
will we keep track of the memory usage? eg, we&apos;ll be flushing&lt;br/&gt;
DWs independently, but will not want to double count the same&lt;br/&gt;
term/query&apos;s size for memory tracking? I&apos;m not sure how we&apos;d do&lt;br/&gt;
that without having a global deletes manager that individual DWs&lt;br/&gt;
interact with (maybe have a ref count per term/query?). The deletes&lt;br/&gt;
manager would have the size/ram usage method, not individual&lt;br/&gt;
DWs. The DWs would need to keep a hash map of term -&amp;gt;&lt;br/&gt;
BufferedDeletes.Num.&lt;/p&gt;</comment>
                    <comment id="12850792" author="michaelbusch" created="Mon, 29 Mar 2010 05:48:48 +0100"  >&lt;blockquote&gt;
&lt;p&gt;However, in the apply deletes&lt;br/&gt;
method how would we know which doc to stop deleting at? How&lt;br/&gt;
would the seq id map to a DW&apos;s doc id?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;We could have a global deletes-map that stores seqID -&amp;gt; DeleteAction.  DeleteAction either contains a Term or a Query, and in addition an int &quot;flushCount&quot; (I&apos;ll explain in a bit what flushCount is used for.)&lt;/p&gt;

&lt;p&gt;Each DocumentsWriterPerThread would have a growing array that contains each seqID that &quot;affected&quot; that DWPT, i.e. the seqIDs of &lt;b&gt;all&lt;/b&gt; deletes, plus the seqIDs of the adds/updates performed by that particular DWPT.  One bit of a seqID in that array can indicate if it&apos;s a delete or add/update.&lt;/p&gt;

&lt;p&gt;When it&apos;s time to flush we sort the array by increasing seqID and then loop a single time through it to find the seqIDs of all DeleteActions.  During the loop we count the number of adds/updates to determine the number of docs the DeleteActions affect.  After applying the deletes the DWPT makes a synchronized call to the global deletes-map and increments the flushCount int for each applied DeleteAction.  If flushCount==numThreadStates (== number of DWPT instances) the corresponding DeleteAction entry can be removed, because it was applied to all DWPT.&lt;/p&gt;

&lt;p&gt;I think this should work?  Or is there a simpler solution?&lt;/p&gt;</comment>
                    <comment id="12850857" author="mikemccand" created="Mon, 29 Mar 2010 10:40:23 +0100"  >&lt;p&gt;Yeah I think we&apos;re gonna need the global sequenceID in some form &amp;#8211; my Options 1 or 2 can&apos;t work because the interleaving issue (as seen/required by the app) is a global thing.&lt;/p&gt;</comment>
                    <comment id="12850864" author="mikemccand" created="Mon, 29 Mar 2010 10:53:47 +0100"  >
&lt;blockquote&gt;&lt;p&gt;Mike, can you explain what the advantages of this kind of thread affinity are? I was always wondering why the DocumentsWriter code currently makes efforts to assign a ThreadState always to the same Thread? Is that being done for performance reasons?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;It&apos;s for performance. I expect there are apps where a given&lt;br/&gt;
thread/pool indexes certain kind of docs, ie, the app threads&lt;br/&gt;
themselves have &quot;affinity&quot; for docs with similar term distributions.&lt;br/&gt;
In which case, it&apos;s best (most RAM efficient) if those docs w/&lt;br/&gt;
presumably similar term stats are sent back to the same DW.  If you&lt;br/&gt;
mix in different term stats into one buffer you get worse RAM&lt;br/&gt;
efficiency.&lt;/p&gt;

&lt;p&gt;Also, for better RAM efficiency you want &lt;b&gt;fewer&lt;/b&gt; DWs... because we get&lt;br/&gt;
more RAM efficiency the higher the freq of the terms... but of course&lt;br/&gt;
you want more DWs for better CPU efficiency whenever that many threads&lt;br/&gt;
are running at once.&lt;/p&gt;

&lt;p&gt;Net/net CPU efficiency should trump RAM efficiency, I think, so if&lt;br/&gt;
there is a conflict we should favor CPU efficiency.&lt;/p&gt;

&lt;p&gt;Though, thread affinity doesn&apos;t seem that CPU costly to implement?&lt;br/&gt;
Lookup the DW your thread first used... if it&apos;s free, seize it.  If&lt;br/&gt;
it&apos;s not, fallback to any DW that&apos;s free.&lt;/p&gt;</comment>
                    <comment id="12851017" author="jasonrutherglen" created="Mon, 29 Mar 2010 18:41:11 +0100"  >&lt;p&gt;Michael B.: What you&apos;re talking about here:&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2324?focusedCommentI&quot; class=&quot;external-link&quot;&gt;https://issues.apache.org/jira/browse/LUCENE-2324?focusedCommentI&lt;/a&gt;&lt;br/&gt;
d=12850792page=com.atlassian.jira.plugin.system.issuetabpanels%3A&lt;br/&gt;
comment-tabpanel#action_12850792 is a transaction log?&lt;/p&gt;

&lt;p&gt;I&apos;m not sure we need that level of complexity just yet? How&lt;br/&gt;
would we make the transaction log memory efficient? Are there&lt;br/&gt;
other uses you foresee? Maybe there&apos;s a simpler solution for the&lt;br/&gt;
BufferedDeletes.Num per DW problem that could make use of global&lt;br/&gt;
sequence ids? I&apos;d prefer to continue to use the per term/query&lt;br/&gt;
max doc id. There aren&apos;t performance issues with concurrently&lt;br/&gt;
accessing and updating maps, so a global sync lock as the DW map&lt;br/&gt;
values are updated should be OK?&lt;/p&gt;</comment>
                    <comment id="12851078" author="michaelbusch" created="Mon, 29 Mar 2010 21:35:26 +0100"  >&lt;blockquote&gt;
&lt;p&gt;I&apos;m not sure we need that level of complexity just yet? How&lt;br/&gt;
would we make the transaction log memory efficient?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Is that really so complex?  You only need one additional int per doc in the DWPTs, and the global map for the delete terms.  You don&apos;t need to buffer the actual terms per DWPT.  I thought that&apos;s quite efficient?  But I&apos;m totally open to other ideas.&lt;/p&gt;

&lt;p&gt;I can try tonight to code a prototype of this - I don&apos;t think it would be very complex actually.  But of course there might be complications I haven&apos;t thought of.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Are there other uses you foresee?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Not really for the &quot;transaction log&quot; as you called it.  I&apos;d remove that log once we switch to deletes in the FG (when the RAM buffer is searchable).  But a nice thing would be for add/update/delete to return the seqID, and also the if RAMReader in the future had an API to check up to which seqID it&apos;s able to &quot;see&quot;.  Then it&apos;s very clear to user of the API where a given reader is at.  &lt;br/&gt;
For this to work we have to assign the seqID at the &lt;b&gt;end&lt;/b&gt; of a call.  E.g. when adding a large document, which takes a long time to process, it should get the seqID assigned after the &quot;work&quot; is done and right before the addDocument() call returns.  &lt;/p&gt;
</comment>
                    <comment id="12851099" author="jasonrutherglen" created="Mon, 29 Mar 2010 22:12:17 +0100"  >&lt;blockquote&gt;&lt;p&gt;You only need one additional int per doc in the DWPTs, and the global map for the delete terms.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Ok, lets give it a try, it&apos;ll be more clear with the prototype.  &lt;/p&gt;

&lt;p&gt;The clarify, the apply deletes doc id up to will be the flushed doc count saved per term/query per DW, though it won&apos;t be saved, it&apos;ll be derived from the sequence id int array where the action has been encoded into the seq id int?&lt;/p&gt;
</comment>
                    <comment id="12851142" author="michaelbusch" created="Mon, 29 Mar 2010 23:32:19 +0100"  >&lt;blockquote&gt;
&lt;p&gt;The clarify, the apply deletes doc id up to will be the flushed doc count saved per term/query per DW, though it won&apos;t be saved, it&apos;ll be derived from the sequence id int array where the action has been encoded into the seq id int?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yeah, that&apos;s the idea.  Let&apos;s see if it works &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                    <comment id="12853594" author="jasonrutherglen" created="Tue, 6 Apr 2010 00:23:48 +0100"  >&lt;p&gt;Michael B, I  was going to wait for your prototype before continuing... &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                    <comment id="12853751" author="michaelbusch" created="Tue, 6 Apr 2010 06:14:52 +0100"  >&lt;p&gt;Sorry, Jason, I got sidetracked with &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2329&quot; title=&quot;Use parallel arrays instead of PostingList objects&quot;&gt;&lt;del&gt;LUCENE-2329&lt;/del&gt;&lt;/a&gt; and other things at work.  I&apos;ll try to write the sequence ID stuff asap.  However, there&apos;s more we need to do here that is sort of independent of the deleted docs problem.  E.g. removing all the downstream perThread classes.   &lt;/p&gt;

&lt;p&gt;We should work with the flex code from now on, as the flex branch will be merged into trunk soon.&lt;/p&gt;</comment>
                    <comment id="12857097" author="michaelbusch" created="Wed, 14 Apr 2010 22:14:14 +0100"  >&lt;p&gt;The patch removes all *PerThread classes downstream of DocumentsWriter.&lt;/p&gt;

&lt;p&gt;This simplifies a lot of the flushing logic in the different consumers.  The patch also removes FreqProxMergeState, because we don&apos;t have to interleave posting lists from different threads anymore of course.  I really like these simplifications!&lt;/p&gt;

&lt;p&gt;There is still a lot to do:  The changes in DocumentsWriter and IndexWriter are currently just experimental to make everything compile.  Next I will introduce DocumentsWriterPerThread and implement the sequenceID logic (which was discussed here in earlier comments) and the new RAM management.  I also want to go through the indexing chain once again - there are probably a few more things to clean up or simplify.&lt;/p&gt;

&lt;p&gt;The patch compiles and actually a surprising amount of tests pass.  Only multi-threaded tests seem to fail,&lt;br/&gt;
which is not very surprising, considering I removed all thread-handling logic from DocumentsWriter. &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; &lt;/p&gt;

&lt;p&gt;So this patch isn&apos;t working yet - just wanted to post my current progress.  &lt;/p&gt;</comment>
                    <comment id="12857112" author="jasonrutherglen" created="Wed, 14 Apr 2010 22:46:57 +0100"  >&lt;p&gt;Michael, nice!  I guess I should&apos;ve spent more time removing the PerThread classes, but now we&apos;re pretty much there.  Indeed the simplification should make things a lot better.  I guess I&apos;ll wait for the next patch to work on something.&lt;/p&gt;</comment>
                    <comment id="12857124" author="mikemccand" created="Wed, 14 Apr 2010 23:25:58 +0100"  >&lt;p&gt;This is awesome Michael!  Much simpler... nor more FreqProxMergeState, nor the logic to interleave/synchronize writing to the doc stores.  I like it!&lt;/p&gt;</comment>
                    <comment id="12857164" author="michaelbusch" created="Thu, 15 Apr 2010 01:52:09 +0100"  >&lt;blockquote&gt;
&lt;p&gt;It&apos;s for performance. I expect there are apps where a given&lt;br/&gt;
thread/pool indexes certain kind of docs, ie, the app threads&lt;br/&gt;
themselves have &quot;affinity&quot; for docs with similar term distributions.&lt;br/&gt;
In which case, it&apos;s best (most RAM efficient) if those docs w/&lt;br/&gt;
presumably similar term stats are sent back to the same DW. If you&lt;br/&gt;
mix in different term stats into one buffer you get worse RAM&lt;br/&gt;
efficiency.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I do see your point, but I feel like we shouldn&apos;t optimize/make compromises for this use case.  Mainly, because I think apps with such an affinity that you describe are very rare?  The usual design is a queued ingestion pipeline, where a pool of indexer threads take docs out of a queue and feed them to an IndexWriter, I think?  In such a world the threads wouldn&apos;t have an affinity for similar docs.&lt;/p&gt;

&lt;p&gt;And if a user really has so different docs, maybe the right answer would be to have more than one single index?  Even if today an app utilizes the thread affinity, this only results in maybe somewhat faster indexing performance, but the benefits would be lost after flusing/merging.  &lt;/p&gt;

&lt;p&gt;If we assign docs randomly to available DocumentsWriterPerThreads, then we should on average make good use of the overall memory?  Alternatively we could also select the DWPT from the pool of available DWPTs that has the highest amount of free memory?  &lt;/p&gt;

&lt;p&gt;Having a fully decoupled memory management is compelling I think, mainly because it makes everything so much simpler.  A DWPT could decide itself when it&apos;s time to flush, and the other ones can keep going independently.  &lt;/p&gt;

&lt;p&gt;If you do have a global RAM management, how would the flushing work?  E.g. when a global flush is triggered because all RAM is consumed, and we pick the DWPT with the highest amount of allocated memory for flushing, what will the other DWPTs do during that flush?  Wouldn&apos;t we have to pause the other DWPTs to make sure we don&apos;t exceed the maxRAMBufferSize?&lt;br/&gt;
Of course we could say &quot;always flush when 90% of the overall memory is consumed&quot;, but how would we know that the remaining 10% won&apos;t fill up during the time the flush takes?  &lt;/p&gt;</comment>
                    <comment id="12857373" author="mikemccand" created="Thu, 15 Apr 2010 17:08:31 +0100"  >&lt;blockquote&gt;&lt;p&gt;The usual design is a queued ingestion pipeline, where a pool of indexer threads take docs out of a queue and feed them to an IndexWriter, I think?&lt;/p&gt;&lt;/blockquote&gt;

&lt;blockquote&gt;&lt;p&gt;Mainly, because I think apps with such an affinity that you describe are very rare?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Hmm I suspect it&apos;s not that rare....  yes one design is a single&lt;br/&gt;
indexing queue w/ dedicated thread pool only for indexing, but a push&lt;br/&gt;
model is equal valid, where your app already has separate threads (or&lt;br/&gt;
thread pools) servicing different content sources, so when a doc&lt;br/&gt;
arrives to one of those source-specific threads, it&apos;s that thread that&lt;br/&gt;
indexes it, rather than handing off to a separately pool.&lt;/p&gt;

&lt;p&gt;Lucene is used in a very wide variety of apps &amp;#8211; we shouldn&apos;t optimize&lt;br/&gt;
the indexer on such hard app specific assumptions.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;And if a user really has so different docs, maybe the right answer would be to have more than one single index?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Hmm but the app shouldn&apos;t have to resort to this... (it doesn&apos;t have&lt;br/&gt;
to today).&lt;/p&gt;

&lt;p&gt;But... could we allow an add/updateDocument call to express this&lt;br/&gt;
affinity, explicitly?  If you index homogenous docs you wouldn&apos;t use&lt;br/&gt;
it, but, if you index drastically different docs that fall into clear&lt;br/&gt;
&quot;categories&quot;, expressing the affinity can get you a good gain in&lt;br/&gt;
indexing throughput.&lt;/p&gt;

&lt;p&gt;This may be the best solution, since then one could pass the affinity&lt;br/&gt;
even through a thread pool, and then we would fallback to thread&lt;br/&gt;
binding if the document class wasn&apos;t declared?&lt;/p&gt;

&lt;p&gt;I mean this is virtually identical to &quot;having more than one index&quot;,&lt;br/&gt;
since the DW is like its own index.  It just saves some of the&lt;br/&gt;
copy-back/merge cost of addIndexes...&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Even if today an app utilizes the thread affinity, this only results in maybe somewhat faster indexing performance, but the benefits would be lost after flusing/merging.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes this optimization is only about the initial flush, but, it&apos;s&lt;br/&gt;
potentially sizable.  Merging matters less since typically it&apos;s not&lt;br/&gt;
the bottleneck (happens in the BG, quickly enough).&lt;/p&gt;

&lt;p&gt;On the right apps, thread affinity can make a huge difference.  EG if&lt;br/&gt;
you allow up to 8 thread states, and the threads are indexing content&lt;br/&gt;
w/ highly divergent terms (eg, one language per thread, or, docs w/&lt;br/&gt;
very different field names), in the worst case you&apos;ll be up to 1/8 as&lt;br/&gt;
efficient since each term must now be copied in up to 8 places&lt;br/&gt;
instead of one.  We have a high per-term RAM cost (reduced thanks to&lt;br/&gt;
the parallel arrays, but, still high).&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;If we assign docs randomly to available DocumentsWriterPerThreads, then we should on average make good use of the overall memory?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;It really depends on the app &amp;#8211; if the term space is highly thread&lt;br/&gt;
dependent (above examples) you an end up flush much more frequently for&lt;br/&gt;
a given RAM buffer.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Alternatively we could also select the DWPT from the pool of available DWPTs that has the highest amount of free memory?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Hmm... this would be kinda costly binder?  You&apos;d need a pqueue?&lt;br/&gt;
Thread affinity (or the explicit affinity) is a single&lt;br/&gt;
map/array/member lookup.  But it&apos;s an interesting idea...&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;If you do have a global RAM management, how would the flushing work? E.g. when a global flush is triggered because all RAM is consumed, and we pick the DWPT with the highest amount of allocated memory for flushing, what will the other DWPTs do during that flush? Wouldn&apos;t we have to pause the other DWPTs to make sure we don&apos;t exceed the maxRAMBufferSize?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The other DWs would keep indexing &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;  That&apos;s the beauty of this&lt;br/&gt;
approach... a flush of one DW doesn&apos;t stop all other DWs from&lt;br/&gt;
indexing, unliked today.&lt;/p&gt;

&lt;p&gt;And you want to serialize the flushing right?  Ie, only one DW flushes&lt;br/&gt;
at a time (the others keep indexing).&lt;/p&gt;

&lt;p&gt;Hmm I suppose flushing more than one should be allowed (OS/IO have&lt;br/&gt;
alot of concurrency, esp since IO goes into write cache)... perhaps&lt;br/&gt;
that&apos;s the best way to balance index vs flush time?  EG we pick one to&lt;br/&gt;
flush @ 90%, if we cross 95% we pick another to flush, another at&lt;br/&gt;
100%, etc.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Of course we could say &quot;always flush when 90% of the overall memory is consumed&quot;, but how would we know that the remaining 10% won&apos;t fill up during the time the flush takes?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Regardless of the approach for document -&amp;gt; DW binding, this is an&lt;br/&gt;
issue (ie it&apos;s non-differentiating here)?  Ie the other DWs continue&lt;br/&gt;
to consume RAM while one DW is flushing.  I think the low/high water&lt;br/&gt;
mark is an OK solution here?  Or the tiered flushing (I think I like&lt;br/&gt;
that better &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; ).&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Having a fully decoupled memory management is compelling I think, mainly because it makes everything so much simpler. A DWPT could decide itself when it&apos;s time to flush, and the other ones can keep going independently.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I&apos;m all for simplifying things, which you&apos;ve already nicely done here,&lt;br/&gt;
but not of it&apos;s at the cost of a non-trivial potential indexing perf&lt;br/&gt;
loss.  We&apos;re already taking a perf hit here, since the doc stores&lt;br/&gt;
can&apos;t be shared... I think that case is justifiable (good&lt;br/&gt;
simplification).&lt;/p&gt;</comment>
                    <comment id="12857375" author="tsmith" created="Thu, 15 Apr 2010 17:13:03 +0100"  >&lt;blockquote&gt;&lt;p&gt;But... could we allow an add/updateDocument call to express this affinity, explicitly?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;i would love to be able to explicitly define a &quot;segment&quot; affinity for documents i&apos;m feeding&lt;/p&gt;

&lt;p&gt;this would then allow me to say: &lt;br/&gt;
all docs from table a has affinity 1&lt;br/&gt;
all docs from table b has affinity 2&lt;/p&gt;

&lt;p&gt;this would ideally result in indexing documents from each table into a different segment (obviously, i would then need to be able to have segment merging be affinity aware so optimize/merging would only merge segments that share an affinity)&lt;/p&gt;</comment>
                    <comment id="12857380" author="jasonrutherglen" created="Thu, 15 Apr 2010 17:17:31 +0100"  >&lt;blockquote&gt;&lt;p&gt;only one DW flushes at a time (the others keep indexing).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think it&apos;s best to simply flush at 90% for now. We already&lt;br/&gt;
exceed the ram buffer size because of over allocation? Perhaps&lt;br/&gt;
we can view the ram buffer size as a rough guideline not a hard&lt;br/&gt;
and fast limit because, lets face it, we&apos;re using Java which is&lt;br/&gt;
about as inexact when it comes to RAM consumption as it gets?&lt;br/&gt;
Also, hopefully it would move the patch along faster and more&lt;br/&gt;
complex algorithms could easily be added later. &lt;/p&gt;</comment>
                    <comment id="12857381" author="mikemccand" created="Thu, 15 Apr 2010 17:20:33 +0100"  >&lt;blockquote&gt;
&lt;p&gt;i would love to be able to explicitly define a &quot;segment&quot; affinity for documents i&apos;m feeding&lt;/p&gt;

&lt;p&gt;this would then allow me to say: &lt;br/&gt;
all docs from table a has affinity 1&lt;br/&gt;
all docs from table b has affinity 2&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Right, this is exactly what affinity would be good for &amp;#8211; so IW would&lt;br/&gt;
try to send &quot;table a&quot; docs their own DW(s) and &quot;table b&quot; docs to their&lt;br/&gt;
own DW(s), which should give faster indexing than randomly binding to&lt;br/&gt;
DWs.&lt;/p&gt;

&lt;p&gt;But:&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;this would ideally result in indexing documents from each table into a different segment (obviously, i would then need to be able to have segment merging be affinity aware so optimize/merging would only merge segments that share an affinity)&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This part I was not proposing &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;The affinity would just be an optimization hint in creating the&lt;br/&gt;
initial flushed segments, so IW can speed up indexing.&lt;/p&gt;

&lt;p&gt;Probably if you really want to keep the segments segregated like that,&lt;br/&gt;
you should in fact index to separate indices?&lt;/p&gt;</comment>
                    <comment id="12857385" author="tsmith" created="Thu, 15 Apr 2010 17:27:02 +0100"  >&lt;blockquote&gt;&lt;p&gt;Probably if you really want to keep the segments segregated like that, you should in fact index to separate indices?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Thats what i&apos;m currently thinking i&apos;ll have to do&lt;/p&gt;

&lt;p&gt;however it would be ideal if i could either subclass IndexWriter or use IndexWriter directly with this affinity concept (potentially writing my own segment merger that is affinity aware)&lt;br/&gt;
that makes it so i can easily use near real time indexing, as only one IndexWriter will be in the mix, as well as make managing deletes and a whole other host of issues with multiple indexes disappear&lt;br/&gt;
Also makes it so i can configure memory settings across all &quot;affinity groups&quot; instead of having to dynamically create them, each with their own memory bounds&lt;/p&gt;</comment>
                    <comment id="12858591" author="michaelbusch" created="Mon, 19 Apr 2010 17:48:51 +0100"  >&lt;blockquote&gt;
&lt;p&gt;But... could we allow an add/updateDocument call to express this&lt;br/&gt;
affinity, explicitly? If you index homogenous docs you wouldn&apos;t use&lt;br/&gt;
it, but, if you index drastically different docs that fall into clear&lt;br/&gt;
&quot;categories&quot;, expressing the affinity can get you a good gain in&lt;br/&gt;
indexing throughput.&lt;/p&gt;

&lt;p&gt;This may be the best solution, since then one could pass the affinity&lt;br/&gt;
even through a thread pool, and then we would fallback to thread&lt;br/&gt;
binding if the document class wasn&apos;t declared?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I would like this if we then also added an API that can be used to specify the&lt;br/&gt;
per-DWPT RAM size.  E.g. if someone has such an app where different threads&lt;br/&gt;
index docs of different sizes, then the DW that indexes big docs can be given&lt;br/&gt;
more memory?&lt;/p&gt;

&lt;p&gt;What I&apos;m mainly trying to avoid is synchronization points between the&lt;br/&gt;
different DWPTs.  For example, currently the same ByteBlockAllocator is shared&lt;br/&gt;
between the different threads, so all its methods need to be synchronized.&lt;/p&gt;


&lt;blockquote&gt;
&lt;p&gt;The other DWs would keep indexing  That&apos;s the beauty of this&lt;br/&gt;
approach... a flush of one DW doesn&apos;t stop all other DWs from&lt;br/&gt;
indexing, unliked today.&lt;/p&gt;

&lt;p&gt;And you want to serialize the flushing right? Ie, only one DW flushes&lt;br/&gt;
at a time (the others keep indexing).&lt;/p&gt;

&lt;p&gt;Hmm I suppose flushing more than one should be allowed (OS/IO have&lt;br/&gt;
alot of concurrency, esp since IO goes into write cache)... perhaps&lt;br/&gt;
that&apos;s the best way to balance index vs flush time? EG we pick one to&lt;br/&gt;
flush @ 90%, if we cross 95% we pick another to flush, another at&lt;br/&gt;
100%, etc.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Oh I don&apos;t want to disallow flushing in parallel!  I think it makes perfect&lt;br/&gt;
sense to allow more than one DW to flush at the same time.  If each DWPT has a&lt;br/&gt;
private max buffer size, then it can decide on its own when it&apos;s time to&lt;br/&gt;
flush.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Hmm I suppose flushing more than one should be allowed (OS/IO have&lt;br/&gt;
alot of concurrency, esp since IO goes into write cache)... perhaps&lt;br/&gt;
that&apos;s the best way to balance index vs flush time? EG we pick one to&lt;br/&gt;
flush @ 90%, if we cross 95% we pick another to flush, another at&lt;br/&gt;
100%, etc.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;If we allow flushing in parallel and also allow specifying the max RAM per&lt;br/&gt;
DWPT, then there doesn&apos;t even have to be any cross-thread RAM tracking?  Each&lt;br/&gt;
DWPT could just flush when its own buffer is full?&lt;/p&gt;

&lt;p&gt;So let&apos;s summarize: &lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;Expose a ThreadBinder API for controlling number of DWPT instances and&lt;br/&gt;
thread affinity of DWPTs explicitly. (We can later decide if we want to also&lt;br/&gt;
support such an affinity after a segment was flushed, as Tim is asking for.&lt;br/&gt;
But that should IMO not be part of this patch.)&lt;/li&gt;
	&lt;li&gt;Also expose an API for specifying the RAM buffer size per DWPT.&lt;/li&gt;
	&lt;li&gt;Allow flushing in parallel (multiple DWPTs can flush at the same time). A&lt;br/&gt;
DWPT flushes when its buffer is full, independent of what the other DWPTs are&lt;br/&gt;
doing.&lt;/li&gt;
	&lt;li&gt;The default implementation of the ThreadBinder API assigns threads to DWPT&lt;br/&gt;
randomly and gives each DWPT 1/n-th of the overall memory.&lt;/li&gt;
	&lt;li&gt;The DWPT RAM value must be updateable.  E.g. when you first start indexing&lt;br/&gt;
only one DWPT should be created with the max RAM.  Then when multiple threads&lt;br/&gt;
are used for adding documents another DWPT should be added and the RAM&lt;br/&gt;
value of the already existing one should be reduced, and possibly a flush of that&lt;br/&gt;
DWPT needs to be triggered.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;How does this sound?&lt;/p&gt;</comment>
                    <comment id="12858623" author="jasonrutherglen" created="Mon, 19 Apr 2010 19:28:31 +0100"  >&lt;p&gt;Michael B, sounds good.  The approach outlined is straightforward and covers the edge cases.  &lt;/p&gt;</comment>
                    <comment id="12858819" author="mikemccand" created="Tue, 20 Apr 2010 12:14:24 +0100"  >&lt;p&gt;I still think this &quot;zero sync&apos;d code&quot; at the cost of perf loss /&lt;br/&gt;
exposing per-DWPT details is taking things too far.  You&apos;re cutting&lt;br/&gt;
into the bone...&lt;/p&gt;

&lt;p&gt;I don&apos;t think we should apps to be setting per-DWPT RAM limits, or,&lt;br/&gt;
even expose to apps how IW manages threads (this is an impl. detail).&lt;/p&gt;

&lt;p&gt;I think we should keep the approach we have today &amp;#8211; you set the&lt;br/&gt;
overall RAM limit and IW internally manages flushing when that&lt;br/&gt;
allotted RAM is full.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;E.g. if someone has such an app where different threads&lt;br/&gt;
index docs of different sizes, then the DW that indexes big docs can be given&lt;br/&gt;
more memory?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Hmm this isn&apos;t really fair &amp;#8211; the app in general can&apos;t predict how&lt;br/&gt;
many docs of each type will come in, how IW allocates RAM for&lt;br/&gt;
different kinds of docs (this is an impl detail), etc.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;What I&apos;m mainly trying to avoid is synchronization points between the&lt;br/&gt;
different DWPTs. For example, currently the same ByteBlockAllocator is shared&lt;br/&gt;
between the different threads, so all its methods need to be synchronized.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I understand the motivation, but...&lt;/p&gt;

&lt;p&gt;Is this sync really so bad?  First, we should move all&lt;br/&gt;
allocators/pools to per-DWPT, so they don&apos;t need to be sync&apos;d.&lt;/p&gt;

&lt;p&gt;Then, all that needs to be sync&apos;d is the tracking of net RAM used (a&lt;br/&gt;
single long), and then the logic to pick the DWPT(s) to flush?  So&lt;br/&gt;
then each DWPT would allocate its own RAM (unsync&apos;d), track its own&lt;br/&gt;
RAM used (unsync&apos;d), and update the total (in tiny sync block) after&lt;br/&gt;
the update (add/del) is serviced?&lt;/p&gt;

&lt;p&gt;We&apos;re still gonna need sync&apos;d code, anyway (global sequence ID,&lt;br/&gt;
grabbing a DWPT), right?  We can put this &quot;go flush a DWPT&quot; logic in&lt;br/&gt;
the same block if we really have to?  It feels like we&apos;re going to&lt;br/&gt;
great lengths (cutting into the bone) to avoid a trivial cost (the&lt;br/&gt;
minor complexity of managing flushing based on aggregate RAM used).&lt;/p&gt;

&lt;blockquote&gt;
&lt;ol&gt;
	&lt;li&gt;Expose a ThreadBinder API for controlling number of DWPT instances and&lt;br/&gt;
thread affinity of DWPTs explicitly. (We can later decide if we want to also&lt;br/&gt;
support such an affinity after a segment was flushed, as Tim is asking for.&lt;br/&gt;
But that should IMO not be part of this patch.)&lt;/li&gt;
	&lt;li&gt;Also expose an API for specifying the RAM buffer size per DWPT.&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;I don&apos;t think we should expose so much.&lt;/p&gt;

&lt;p&gt;I think, instead, we should add an optional method to Document (eg&lt;br/&gt;
set/getSourceID or something), that&apos;d reference which &quot;source&quot; this&lt;br/&gt;
doc comes from.  The app would set it, optionally, as a &quot;hint&quot; to IW.&lt;/p&gt;

&lt;p&gt;The source ID should not be publically tied to DWPT &amp;#8211; how IW&lt;br/&gt;
optimizes based on this &quot;hint&quot; from the app is really an impl detail.&lt;br/&gt;
Yes, today we&apos;ll use it for DWPT affinity; tomorrow, who knows.  EG,&lt;br/&gt;
that source ID need not be less than the max DWPTs.&lt;/p&gt;

&lt;p&gt;When source ID isn&apos;t provided we&apos;d fallback to the same &quot;best guess&quot;&lt;br/&gt;
we have today (same thread = same source ID).&lt;/p&gt;

&lt;p&gt;The javadoc would be something like &quot;as a hint to IW, to possibly&lt;br/&gt;
improve its indexing performance, if you have docs from difference&lt;br/&gt;
sources you should set the source ID on your Document&quot;.  And&lt;br/&gt;
how/whether IW makes use of this information is &quot;under the hood&quot;...&lt;/p&gt;

&lt;p&gt;We can do this as a separate issue... it&apos;s fairly orthogonal.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Allow flushing in parallel (multiple DWPTs can flush at the same time). &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;+1&lt;/p&gt;

&lt;p&gt;This would be a natural way to protect against too much RAM usage&lt;br/&gt;
while flush(es) are happening.  Start one flush going, but keep&lt;br/&gt;
indexing docs into the other DWTPs... if RAM usage grows too much&lt;br/&gt;
beyond your first trigger and before that first flush has finished,&lt;br/&gt;
start a 2nd DWPT flushing, etc.  This is naturally self-regulating,&lt;br/&gt;
since the &quot;mutator&quot; threads are tied up doing the flushing...&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;The DWPT RAM value must be updateable. E.g. when you first start indexing&lt;br/&gt;
only one DWPT should be created with the max RAM. Then when multiple threads&lt;br/&gt;
are used for adding documents another DWPT should be added and the RAM&lt;br/&gt;
value of the already existing one should be reduced, and possibly a flush of that&lt;br/&gt;
DWPT needs to be triggered.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This isn&apos;t great... I mean it&apos;s weird that on adding say a 3rd&lt;br/&gt;
indexing thread I suddenly see a flush triggered even though I&apos;m&lt;br/&gt;
nowehere near the RAM limit.  Then, later, if I cut back to using only&lt;br/&gt;
2 threads, I still only ever use up to 2/3rd of my RAM buffer.  IW&apos;s&lt;br/&gt;
API really shouldn&apos;t have such &quot;surprising&quot; behavior where how many /&lt;br/&gt;
which threads come through it so drastically affect it&apos;s flushing&lt;br/&gt;
behavior.&lt;/p&gt;</comment>
                    <comment id="12858948" author="jasonrutherglen" created="Tue, 20 Apr 2010 16:50:55 +0100"  >&lt;p&gt;I get Michael B&apos;s motivation however Mike M&apos;s global ram limit&lt;br/&gt;
is probably better from a user perspective and it would be a&lt;br/&gt;
sync only on a long RAM used variable, so we should be good? &lt;/p&gt;

&lt;p&gt;Sounds like we use the current thread affinity system (ie, a&lt;br/&gt;
hash map), that when the max threads is reached, new threads get&lt;br/&gt;
kind of round robined onto existing DWPTs? &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;if RAM usage grows too much beyond your first trigger and&lt;br/&gt;
before that first flush has finished, start a 2nd DWPT flushing,&lt;br/&gt;
etc. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;What&apos;s the definition of the &quot;too much&quot; portion of the above&lt;br/&gt;
statement?&lt;/p&gt;</comment>
                    <comment id="12858949" author="michaelbusch" created="Tue, 20 Apr 2010 16:51:22 +0100"  >&lt;blockquote&gt;
&lt;p&gt;I still think this &quot;zero sync&apos;d code&quot; at the cost of perf loss /&lt;br/&gt;
exposing per-DWPT details is taking things too far. You&apos;re cutting&lt;br/&gt;
into the bone... &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;No worries - I haven&apos;t started implementing the RAM management part yet. &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;


&lt;blockquote&gt;
&lt;p&gt;I don&apos;t think we should apps to be setting per-DWPT RAM limits, or,&lt;br/&gt;
even expose to apps how IW manages threads (this is an impl. detail).&lt;/p&gt;

&lt;p&gt;I think we should keep the approach we have today - you set the&lt;br/&gt;
overall RAM limit and IW internally manages flushing when that&lt;br/&gt;
allotted RAM is full.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think the reason why we have two different APIs in mind (you: sourceID, I:&lt;br/&gt;
expert thread binder API) is that we&apos;re having different goals with them? You&lt;br/&gt;
want to make the out-of-the-box indexing performance as good as possible, and&lt;br/&gt;
users should have to set a minimum amount of easy-to-understand parameters&lt;br/&gt;
(such as buffer size in MB). I think that&apos;s the right thing to do of course.&lt;br/&gt;
(though that doesn&apos;t prevent us from adding an expert API in addition, as we&lt;br/&gt;
always have)&lt;/p&gt;

&lt;p&gt;I&apos;m thinking a lot about real-time indexing and the searchable RAM buffer&lt;br/&gt;
these days, so the thread-binder API could help you to have more control over&lt;br/&gt;
where your docs will actually end up and which reader will see them. But I&lt;br/&gt;
think too that this API would be very &quot;expert&quot; and not many people would use&lt;br/&gt;
it.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;We can do this as a separate issue... it&apos;s fairly orthogonal.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yeah I was just thinking the same - I agree.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Is this sync really so bad? First, we should move all&lt;br/&gt;
allocators/pools to per-DWPT, so they don&apos;t need to be sync&apos;d.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;OK cool that we agree on that. I was worried you wanted to have global pools&lt;br/&gt;
too, if it&apos;s only the single long it&apos;s not very complicated, I agree.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;We&apos;re still gonna need sync&apos;d code, anyway (global sequence ID,&lt;br/&gt;
grabbing a DWPT), right? We can put this &quot;go flush a DWPT&quot; logic in&lt;br/&gt;
the same block if we really have to? It feels like we&apos;re going to&lt;br/&gt;
great lengths (cutting into the bone) to avoid a trivial cost (the&lt;br/&gt;
minor complexity of managing flushing based on aggregate RAM used).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Sorry if I&apos;m being annoying &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; Yeah sure, there will be several sync&apos;d spots.&lt;br/&gt;
If we don&apos;t share any data structures between threads that hold indexed (and&lt;br/&gt;
in the future searchable) data I&apos;m happy.&lt;/p&gt;

&lt;p&gt;I haven&apos;t spent as much time as you thinking about the current RAM management&lt;br/&gt;
yet and the current code that ensures thread safety - still learning some&lt;br/&gt;
parts of the code. I do appreciate all your patient feedback!&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;This isn&apos;t great... I mean it&apos;s weird that on adding say a 3rd&lt;br/&gt;
indexing thread I suddenly see a flush triggered even though I&apos;m&lt;br/&gt;
nowehere near the RAM limit. Then, later, if I cut back to using only&lt;br/&gt;
2 threads, I still only ever use up to 2/3rd of my RAM buffer. IW&apos;s&lt;br/&gt;
API really shouldn&apos;t have such &quot;surprising&quot; behavior where how many /&lt;br/&gt;
which threads come through it so drastically affect it&apos;s flushing&lt;br/&gt;
behavior.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yeah I don&apos;t really like that either. Let&apos;s not do that. I had first not&lt;br/&gt;
thought about that disadvantage, added this point later to the list, and never&lt;br/&gt;
really liked it. (and knew you would complain about it &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; )&lt;/p&gt;

&lt;p&gt;My goal is to have a default indexing chain that isn&apos;t slower than the one we&lt;br/&gt;
have today, but searchable and that very fast. That&apos;s not trivial, but I think&lt;br/&gt;
we can do it!&lt;/p&gt;

&lt;p&gt;I&apos;ll implement the global flush trigger and make all pools DWPT-local. The&lt;br/&gt;
explicit thread-binder or sourceID APIs we can worry about later, as we agreed&lt;br/&gt;
above.&lt;/p&gt;
</comment>
                    <comment id="12859375" author="mikemccand" created="Wed, 21 Apr 2010 15:42:12 +0100"  >&lt;blockquote&gt;
&lt;p&gt;I think the reason why we have two different APIs in mind (you: sourceID, I:&lt;br/&gt;
expert thread binder API) is that we&apos;re having different goals with&lt;br/&gt;
them?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes, I think you&apos;re right!&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;You want to make the out-of-the-box indexing performance as good as&lt;br/&gt;
possible, and users should have to set a minimum amount of&lt;br/&gt;
easy-to-understand parameters (such as buffer size in MB). I think&lt;br/&gt;
that&apos;s the right thing to do of course.  (though that doesn&apos;t prevent&lt;br/&gt;
us from adding an expert API in addition, as we always have)&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Right &amp;#8211; simple things should be simple and complex things should be&lt;br/&gt;
possible.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;I&apos;m thinking a lot about real-time indexing and the searchable RAM buffer&lt;br/&gt;
these days, so the thread-binder API could help you to have more control over&lt;br/&gt;
where your docs will actually end up and which reader will see them. But I&lt;br/&gt;
think too that this API would be very &quot;expert&quot; and not many people would use&lt;br/&gt;
it.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;In fact now I want both &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;Ie, make it possible (optional) to declare the sourceID, and IW&lt;br/&gt;
optimizes based on this hint.&lt;/p&gt;

&lt;p&gt;But, also, letting advanced apps directly control individual DWPTs.&lt;br/&gt;
(I think a new experimental Indexer interface can work well here...).&lt;/p&gt;

&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;We can do this as a separate issue... it&apos;s fairly orthogonal.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yeah I was just thinking the same - I agree.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;OK I&apos;ll open this...&lt;/p&gt;


&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;Is this sync really so bad? First, we should move all allocators/pools to per-DWPT, so they don&apos;t need to be sync&apos;d.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;OK cool that we agree on that. I was worried you wanted to have global pools&lt;br/&gt;
too, if it&apos;s only the single long it&apos;s not very complicated, I agree.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yeah let&apos;s not do global pools (anymore!)...&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Sorry if I&apos;m being annoying &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;No, you&apos;re not!  You&apos;re asking good questions (as usual)!&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;My goal is to have a default indexing chain that isn&apos;t slower than the one we&lt;br/&gt;
have today, but searchable and that very fast. That&apos;s not trivial, but I think&lt;br/&gt;
we can do it!&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This is an awesome goal, and I agree very reachable.  Though the&lt;br/&gt;
deletes/sequence ID/merging/NRT interaction is going to be fun...&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;I&apos;ll implement the global flush trigger and make all pools DWPT-local. The&lt;br/&gt;
explicit thread-binder or sourceID APIs we can worry about later, as we agreed&lt;br/&gt;
above.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;OK thanks.&lt;/p&gt;</comment>
                    <comment id="12859387" author="mikemccand" created="Wed, 21 Apr 2010 16:11:55 +0100"  >&lt;blockquote&gt;
&lt;p&gt;Sounds like we use the current thread affinity system (ie, a&lt;br/&gt;
hash map), that when the max threads is reached, new threads get&lt;br/&gt;
kind of round robined onto existing DWPTs?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yeah something along those lines... and clearing out all mappings for&lt;br/&gt;
a given DWPT when it flushes.&lt;/p&gt;

&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;if RAM usage grows too much beyond your first trigger and before that first flush has finished, start a 2nd DWPT flushing, etc.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;What&apos;s the definition of the &quot;too much&quot; portion of the above&lt;br/&gt;
statement?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;We could do something simple, eg, at 90% RAM used, you flush your&lt;br/&gt;
first DWPT.  At 110% RAM used, you flush all DWPTs.  And take linear&lt;br/&gt;
steps in between?&lt;/p&gt;

&lt;p&gt;EG if I have 5 DWPTs, I&apos;d flush first one at 90%, 2nd at 95%, 3rd at&lt;br/&gt;
100%, 4th at 105% and 5th at 110%.&lt;/p&gt;

&lt;p&gt;Of course, if flushing is fast, then RAM is quickly freed up, then we&lt;br/&gt;
only flush 1 DWPT at a time... we only need these tiers to&lt;br/&gt;
self-regulate RAM consumed from ongoing indexing vs time it takes to&lt;br/&gt;
do the flush.&lt;/p&gt;</comment>
                    <comment id="12859489" author="michaelbusch" created="Wed, 21 Apr 2010 20:11:44 +0100"  >&lt;p&gt;How shall we actually handle flushing by maxBufferedDocs?  It&apos;d be the easiest to just make this a per-DWPT flush trigger.  Of course that&apos;d be a change in runtime behavior, but I think that&apos;s acceptable?&lt;/p&gt;

&lt;p&gt;And flushing by maxBufferedDeleteTerms is also tricky, because that needs to be a flush across all DWPTs?  With searchable RAM buffers and deletes in the foreground this trigger should actually not be necessary anymore?  We would probably apply deletes when the realtime IndexReader is (re)opened?&lt;/p&gt;</comment>
                    <comment id="12859511" author="jasonrutherglen" created="Wed, 21 Apr 2010 21:16:06 +0100"  >&lt;blockquote&gt;&lt;p&gt;flushing by maxBufferedDeleteTerms is also tricky,&lt;br/&gt;
because that needs to be a flush across all DWPTs&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think we simply keep track of maxBufferedDocs and&lt;br/&gt;
maxBufferedDeleteTerms globally. We can&apos;t deprecate because&lt;br/&gt;
that&apos;d be a pain for users. &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;With searchable RAM buffers and deletes in the foreground&lt;br/&gt;
this trigger should actually not be necessary anymore?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The deletes aren&apos;t entirely in the foreground, only the RAM&lt;br/&gt;
buffer deletes. Deletes to existing segments would use the&lt;br/&gt;
existing clone and delete mechanism. I asked about foreground&lt;br/&gt;
deletes to existing segments before, and we agreed that it&apos;s not&lt;br/&gt;
a good idea due to locality of terms/postings.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;We would probably apply deletes when the realtime&lt;br/&gt;
IndexReader is (re)opened?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Right, that&apos;s how the existing apply deletes basically works.&lt;br/&gt;
That&apos;d be the same. Oh ok, I see, perhaps the global deletes&lt;br/&gt;
manager could cache the deletes for the existing segments, the&lt;br/&gt;
DWPTs can keep track of their deletes separately. We probably&lt;br/&gt;
won&apos;t apply segment or DWPT deletes in the foreground? We&apos;d&lt;br/&gt;
cache them separately, and update ram consumption and unique&lt;br/&gt;
term/query counts globally. &lt;/p&gt;</comment>
                    <comment id="12859590" author="michaelbusch" created="Thu, 22 Apr 2010 00:59:51 +0100"  >&lt;blockquote&gt;
&lt;p&gt;The deletes aren&apos;t entirely in the foreground, only the RAM&lt;br/&gt;
buffer deletes. Deletes to existing segments would use the&lt;br/&gt;
existing clone and delete mechanism.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think deletes should all be based on the new sequenceIDs. Today the buffered&lt;br/&gt;
deletes depend on a value that can change (numDocs changes when segments are&lt;br/&gt;
merged). But with sequenceIDs they&apos;re absolute: each delete operation will&lt;br/&gt;
have a sequenceID assigned, and the ordering of all write operations (add,&lt;br/&gt;
update, delete) is unambiguously defined by the sequenceIDs. Remember that&lt;br/&gt;
addDocument/updateDocument/deleteDocuments will all return the sequenceID.&lt;/p&gt;

&lt;p&gt;This means that we don&apos;t have to remap deletes anymore. Also the pooled&lt;br/&gt;
SegmentReaders that read the flushed segments can use arrays of sequenceIDs&lt;br/&gt;
instead of BitSets? Of course that needs more memory, but even if you add 1M&lt;br/&gt;
docs without ever calling IW.commit/close you only need 8MB - I think that&apos;s&lt;br/&gt;
acceptable. And this size is independent on how many times you call&lt;br/&gt;
reopen/clone on the realtime readers, because they can all share the same&lt;br/&gt;
deletes array.&lt;/p&gt;

&lt;p&gt;We can also modify IW.commit/close to return the latest sequenceID. This would&lt;br/&gt;
be very nice for document tracking. E.g. when you hit an aborting exception&lt;br/&gt;
after you flushed already some segments, then even though you must discard&lt;br/&gt;
everything in the DW&apos;s buffer you can still call commit/close to commit&lt;br/&gt;
everything that doesn&apos;t have to be discarded. IW.commit/close would in that&lt;br/&gt;
case return the sequenceID of the latest write operation that was successfully&lt;br/&gt;
committed, i.e. that would be visible to an IndexReader. Though we have to be&lt;br/&gt;
careful here: multiple segments can have interleaving sequenceIDs, so we must&lt;br/&gt;
discard every segment that has one or more sequenceIDs greater than the lowest&lt;br/&gt;
one in the DW. So we still need the push-deletes logic, that keeps RAM deletes&lt;br/&gt;
separate from the flushed ones until flushing was successful.&lt;/p&gt;

&lt;p&gt;DW/IW need to keep track of the largest sequenceID that is &lt;b&gt;safe&lt;/b&gt;, i.e. that&lt;br/&gt;
could be committed even if DW hits an aborting exception. Some invariants: &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;All deletes with sequenceID smaller or equal to safeSeqID have already been&lt;br/&gt;
applied to the deletes arrays of the flushed segments. &lt;/li&gt;
	&lt;li&gt;All deletes with sequenceID greater than safeSeqID are in a deletesInRAM&lt;br/&gt;
buffer. &lt;/li&gt;
	&lt;li&gt;safeSeqID is always smaller than any buffered doc or buffered delete in DW&lt;br/&gt;
or DWPT.&lt;/li&gt;
	&lt;li&gt;safeSeqID is always equal to the maximum sequenceID of one, and only one,&lt;br/&gt;
flushed segment.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;When IW.close/commit is called then safeSeqID is returned. If no aborting&lt;br/&gt;
exception occurred it equals the highest sequenceID ever assigned (during that&lt;br/&gt;
IW &quot;session&quot;). In any case it&apos;s always the sequenceID of the latest write&lt;br/&gt;
operation an IndexReader will &quot;see&quot; that you open after IW.close/commit&lt;br/&gt;
finished.&lt;/p&gt;

&lt;p&gt;But this would be nice for apps to track which docs made it successfully into&lt;br/&gt;
the index. Apps can then externally keep a log to figure out what they have to&lt;br/&gt;
reply in case of exceptions.&lt;/p&gt;

&lt;p&gt;Does all this make sense? &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; This is very complex stuff, I wouldn&apos;t be&lt;br/&gt;
surprised if there&apos;s something I didn&apos;t think about.&lt;/p&gt;</comment>
                    <comment id="12859737" author="michaelbusch" created="Thu, 22 Apr 2010 09:17:06 +0100"  >&lt;blockquote&gt;
&lt;p&gt;I think we simply keep track of maxBufferedDocs and&lt;br/&gt;
maxBufferedDeleteTerms globally. We can&apos;t deprecate because&lt;br/&gt;
that&apos;d be a pain for users. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The problem with maxBufferedDocs is that you can only enforce it by&lt;br/&gt;
&quot;stopping the world&quot;, i.e. preventing other DWPTs from adding docs while&lt;br/&gt;
one DWPT is flushing.  Unless we also introduce the 90%-95% etc flush&lt;br/&gt;
triggers for maxBufferedDocs.&lt;/p&gt;

&lt;p&gt;The other option would be to have maxBufferedDocs per DWPT.  Then&lt;br/&gt;
the theoretical total limit of docs in RAM would be &lt;br/&gt;
maxNumberThreadStates * maxBufferedDocs.&lt;/p&gt;</comment>
                    <comment id="12859850" author="mikemccand" created="Thu, 22 Apr 2010 16:46:43 +0100"  >&lt;blockquote&gt;&lt;p&gt;The problem with maxBufferedDocs is that you can only enforce it by &quot;stopping the world&quot;&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Why not just stop the world for maxBufferedDocs/maxBufferedDelDocs?&lt;br/&gt;
It should be pretty simple to implement?&lt;/p&gt;

&lt;p&gt;Obviously you&apos;ll take a perf hit if you flush by these counts, so, you&lt;br/&gt;
really should flush by RAM instead... But some apps need this (eg&lt;br/&gt;
setting up indices for ParallelReader, until we get SliceWriter&lt;br/&gt;
working...).&lt;/p&gt;</comment>
                    <comment id="12859900" author="michaelbusch" created="Thu, 22 Apr 2010 18:01:00 +0100"  >&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;The problem with maxBufferedDocs is that you can only enforce it by &quot;stopping the world&quot;&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Why not just stop the world for maxBufferedDocs/maxBufferedDelDocs?&lt;br/&gt;
It should be pretty simple to implement?&lt;/p&gt;

&lt;p&gt;Obviously you&apos;ll take a perf hit if you flush by these counts, so, you&lt;br/&gt;
really should flush by RAM instead... But some apps need this (eg&lt;br/&gt;
setting up indices for ParallelReader, until we get SliceWriter&lt;br/&gt;
working...).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I&apos;m not sure I understand how this would help for ParallelReader?&lt;br/&gt;
I think you can&apos;t use multi-threaded indexing even today, because you&lt;br/&gt;
have no control over the order in which the docs will make it into the&lt;br/&gt;
index.  &lt;/p&gt;

&lt;p&gt;So having maxBufferedDocs per DWPT seems tempting to me.  Then you know&lt;br/&gt;
that each written segment will have exactly a size of maxBufferedDocs,&lt;br/&gt;
so this is much more predictable.  And if you index with a single &lt;br/&gt;
thread only the behavior is identical to a &quot;global&quot; maxBufferedDocs&lt;br/&gt;
flush trigger.&lt;/p&gt;</comment>
                    <comment id="12859906" author="mikemccand" created="Thu, 22 Apr 2010 18:08:41 +0100"  >&lt;blockquote&gt;
&lt;p&gt;I&apos;m not sure I understand how this would help for ParallelReader?&lt;br/&gt;
I think you can&apos;t use multi-threaded indexing even today, because you&lt;br/&gt;
have no control over the order in which the docs will make it into the&lt;br/&gt;
index.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Well, to set up indexes for PR today, you have to run IndexWriter in a very degraded state &amp;#8211; flush by doc count, use a single thread, turn off concurrent merging (use SMS), use LogDocMergePolicy.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;So having maxBufferedDocs per DWPT seems tempting to me. Then you know&lt;br/&gt;
that each written segment will have exactly a size of maxBufferedDocs,&lt;br/&gt;
so this is much more predictable. And if you index with a single &lt;br/&gt;
thread only the behavior is identical to a &quot;global&quot; maxBufferedDocs&lt;br/&gt;
flush trigger.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Yeah, maybe that&apos;d be sufficient...?  It&apos;d sort of &quot;match&quot; the current behaviour, in that you get segments flushed to the index with that many docs.&lt;/p&gt;</comment>
                    <comment id="12859910" author="jasonrutherglen" created="Thu, 22 Apr 2010 18:18:09 +0100"  >&lt;blockquote&gt;&lt;p&gt;It&apos;d sort of &quot;match&quot; the current behaviour, in that you get segments flushed to the index with that many docs.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;It seems like the consensus is, most users don&apos;t use the maxBufferedDocs feature, however for the ones that do, we can stop the world and flush all segments?  That way we can focus on the RAM consumed use case?  &lt;/p&gt;</comment>
                    <comment id="12859940" author="michaelbusch" created="Thu, 22 Apr 2010 19:24:27 +0100"  >&lt;p&gt;No, I think the consensus is to have a DWPT maxBufferedDocs flush trigger, not a global one.&lt;/p&gt;</comment>
                    <comment id="12859942" author="jasonrutherglen" created="Thu, 22 Apr 2010 19:29:14 +0100"  >&lt;blockquote&gt;&lt;p&gt;I think the consensus is to have a DWPT maxBufferedDocs flush trigger, not a global one. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Ok, so we&apos;ll need to implement dynamic resizing of the maxBufferedDocs per DWPT in the case where the maxNumberThreadStates changes?&lt;/p&gt;</comment>
                    <comment id="12859953" author="michaelbusch" created="Thu, 22 Apr 2010 19:53:18 +0100"  >&lt;blockquote&gt;&lt;p&gt;No, I think the consensus is to have a DWPT maxBufferedDocs flush trigger, not a global one. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think we should just keep it simple.  If you set maxBufferedDocs to 1000, then every thread will flush at 1000 docs.&lt;br/&gt;
If you set maxThreadStates to 5, then you could at the same time in theory have 5000 docs in memory.&lt;/p&gt;

&lt;p&gt;We just have to explain this in the javadocs.  It&apos;s a change that we should also mention in the backwards-compatibility section.&lt;/p&gt;</comment>
                    <comment id="12859957" author="jasonrutherglen" created="Thu, 22 Apr 2010 20:03:56 +0100"  >&lt;blockquote&gt;&lt;p&gt;This means that we don&apos;t have to remap deletes&lt;br/&gt;
anymore.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Good.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;the pooled SegmentReaders that read the flushed segments&lt;br/&gt;
can use arrays of sequenceIDs instead of BitSets?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Ok, that&apos;d be an improvement. In order to understand all the&lt;br/&gt;
logic, I&apos;d need to see a prototype, I can&apos;t really regurgitate&lt;br/&gt;
what I&apos;ve read thus far. Are there any concurrency issues with&lt;br/&gt;
the seq arrays? Like say if a reader is iterating a posting&lt;br/&gt;
list? I guess not because we&apos;re always incrementing? &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;safeSeqID is always smaller than any buffered doc or&lt;br/&gt;
buffered delete in DW or DWPT.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Can you elaborate on this? Wouldn&apos;t safeSeqID be greater than&lt;br/&gt;
buffered doc or deleted doc due to add&lt;br/&gt;
docs and interleaving?&lt;/p&gt;

&lt;p&gt;Also can we modify the terminology, perhaps committed seq id is&lt;br/&gt;
better? To me that&apos;s a little more clear. Maybe it&apos;d be good to&lt;br/&gt;
start a wiki on this so we can have a master doc we&apos;re all&lt;br/&gt;
referring to (kind of a Solr thing that because the projects are&lt;br/&gt;
merged we can try for?) &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/wink.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;
</comment>
                    <comment id="12859966" author="jasonrutherglen" created="Thu, 22 Apr 2010 20:16:36 +0100"  >&lt;blockquote&gt;&lt;p&gt;We just have to explain this in the javadocs. It&apos;s a change that we should also mention in the backwards-compatibility section.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;We&apos;re punting eh?&lt;/p&gt;</comment>
                    <comment id="12876714" author="jasonrutherglen" created="Tue, 8 Jun 2010 17:25:00 +0100"  >&lt;p&gt;Michael B, what&apos;s the status?  I&apos;m thinking of working on it.&lt;/p&gt;</comment>
                    <comment id="12877597" author="michaelbusch" created="Thu, 10 Jun 2010 23:15:22 +0100"  >&lt;p&gt;I just created a new branch for this (and related RT features) here:&lt;br/&gt;
&lt;a href=&quot;https://svn.apache.org/repos/asf/lucene/dev/branches/realtime_search&quot; class=&quot;external-link&quot;&gt;https://svn.apache.org/repos/asf/lucene/dev/branches/realtime_search&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I&apos;ll commit the latest version of my patch soon (have to update to trunk and make it compile)...&lt;/p&gt;</comment>
                    <comment id="12890641" author="michaelbusch" created="Wed, 21 Jul 2010 11:22:14 +0100"  >&lt;p&gt;Finally a new version of the patch! (Sorry for keeping you guys waiting...)&lt;/p&gt;

&lt;p&gt;It&apos;s not done yet, but it compiles (against realtime branch!) and &amp;gt;95% of the core test cases pass.&lt;/p&gt;

&lt;p&gt;Work done in addition to last patch:&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Added DocumentsWriterPerThread&lt;/li&gt;
	&lt;li&gt;Reimplemented big parts of DocumentsWriter&lt;/li&gt;
	&lt;li&gt;Added DocumentsWriterThreadPool which is an extension point for different pool implementation.  The default impl is&lt;br/&gt;
  the ThreadAffinityDocumentsWriterThreadPool, which does what the old code did (try to assign a DWPT always to &lt;br/&gt;
  the same thread).  It should be easy now to add Document#getSourceID() and another pool that can assign threads&lt;br/&gt;
  based on the sourceID.&lt;/li&gt;
	&lt;li&gt;Initial implementation of sequenceIDs.  Currently they&apos;re only used to keep track of deletes and not for&lt;br/&gt;
  e.g. NRT readers yet.&lt;/li&gt;
	&lt;li&gt;Lots of other changes here and there.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;TODOs:&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Implement flush-by-ram logic&lt;/li&gt;
	&lt;li&gt;Implement logic to discard deletes from the deletes buffer&lt;/li&gt;
	&lt;li&gt;Finish sequenceID handling: IW#commit() and IW#close() should return ID of last flushed sequenceID&lt;/li&gt;
	&lt;li&gt;Maybe change delete logic:  currently deletes are applied when a segment is flushed.  Maybe we can keep it this way&lt;br/&gt;
  in the realtime-branch though, because that&apos;s most likely what we want to do once the RAM buffer is searchable and&lt;br/&gt;
  deletes are cheaper as they can then be done in-memory before flush&lt;/li&gt;
	&lt;li&gt;Fix unit tests (mostly exception handling and thread safety)&lt;/li&gt;
	&lt;li&gt;New test cases, e.g. for sequenceID testing&lt;/li&gt;
	&lt;li&gt;Simplify code:  In some places I copied code around, which can probably be further simplified&lt;/li&gt;
	&lt;li&gt;I started removing some of the old setters/getters in IW which are not in IndexWriterConfig - need to finish that,&lt;br/&gt;
  or revert those changes and use a different patch&lt;/li&gt;
	&lt;li&gt;Fix nocommits&lt;/li&gt;
	&lt;li&gt;Performance testing&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I&apos;m planning to commit this soon to the realtime branch, even though it&apos;s obviously not done yet.  But it&apos;s a big &lt;br/&gt;
patch and changes will be easier to track with an svn history.&lt;/p&gt;</comment>
                    <comment id="12890643" author="michaelbusch" created="Wed, 21 Jul 2010 11:32:46 +0100"  >&lt;p&gt;OK, I committed to the branch.  I&apos;ll try tomorrow to merge trunk into the branch.  I was already warned that there will most likely be lots of conflicts - so help is welcome! &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;  &lt;/p&gt;</comment>
                    <comment id="12890789" author="jasonrutherglen" created="Wed, 21 Jul 2010 18:34:30 +0100"  >&lt;p&gt;Michael, thanks for posting and committing the patch.  I&apos;ll be taking a look.&lt;/p&gt;

&lt;p&gt;Before I/we forget, maybe we can describe what we discussed about RT features such as the terms dictionary (the AtomicIntArray linked list, possible usage of a btree), the multi-level skip list (static levels), and other such features at: &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2312&quot; class=&quot;external-link&quot;&gt;https://issues.apache.org/jira/browse/LUCENE-2312&lt;/a&gt;&lt;/p&gt;</comment>
                    <comment id="12890790" author="jasonrutherglen" created="Wed, 21 Jul 2010 18:37:28 +0100"  >&lt;p&gt;We need to update the indexing chain comment in DocumentsWriterPerThread&lt;/p&gt;</comment>
                    <comment id="12890812" author="michaelbusch" created="Wed, 21 Jul 2010 19:28:51 +0100"  >&lt;blockquote&gt;
&lt;p&gt;We need to update the indexing chain comment in DocumentsWriterPerThread &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;There&apos;s a lot of code cleanup to do.  I just wanted to checkpoint what I have so far.&lt;/p&gt;


&lt;blockquote&gt;
&lt;p&gt;Before I/we forget, maybe we can describe what we discussed about RT features such as the terms dictionary (the AtomicIntArray linked list, possible usage of a btree), the multi-level skip list (static levels), and other such features at: &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2312&quot; class=&quot;external-link&quot;&gt;https://issues.apache.org/jira/browse/LUCENE-2312&lt;/a&gt;&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yeah, I will.  But first I need to catch up on sleep &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                    <comment id="12890893" author="jasonrutherglen" created="Wed, 21 Jul 2010 22:02:26 +0100"  >&lt;p&gt;Looks like we&apos;re not using MergeDocIDRemapper anymore?&lt;/p&gt;</comment>
                    <comment id="12890905" author="jasonrutherglen" created="Wed, 21 Jul 2010 22:22:26 +0100"  >&lt;blockquote&gt;&lt;p&gt;Implement logic to discard deletes from the deletes&lt;br/&gt;
buffer&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Michael, where in the code is this supposed to occur?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Implement flush-by-ram logic&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I&apos;ll make a go of this.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Maybe change delete logic: currently deletes are applied&lt;br/&gt;
when a segment is flushed. Maybe we can keep it this way in the&lt;br/&gt;
realtime-branch though, because that&apos;s most likely what we want&lt;br/&gt;
to do once the RAM buffer is searchable and deletes are cheaper&lt;br/&gt;
as they can then be done in-memory before flush&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think we&apos;ll keep things this way for this issue (ie, per&lt;br/&gt;
thread document writers), however for &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2312&quot; title=&quot;Search on IndexWriter&amp;#39;s RAM Buffer&quot;&gt;LUCENE-2312&lt;/a&gt; I think we&apos;ll&lt;br/&gt;
want to implement foreground deletes (eg, updating the deleted&lt;br/&gt;
docs sequences int[]).&lt;/p&gt;</comment>
                    <comment id="12891085" author="mikemccand" created="Thu, 22 Jul 2010 11:27:35 +0100"  >&lt;p&gt;This is looking awesome Michael!  I love the removal of *PerThread &amp;#8211;&lt;br/&gt;
they are all logically absorbed into DWPT, so everything is now per&lt;br/&gt;
thread.&lt;/p&gt;

&lt;p&gt;I still see usage of docStoreOffset, but aren&apos;t we doing away with&lt;br/&gt;
shared doc stores with the cutover to DWPT?&lt;/p&gt;

&lt;p&gt;I think you can further simplify DocumentsWriterPerThread.DocWriter;&lt;br/&gt;
in fact I think you can remove it &amp;amp; all subclasses in consumers!  The&lt;br/&gt;
consumers can simply directly write their files.  The only reason this&lt;br/&gt;
class was created was because we have to interleave docs when writing&lt;br/&gt;
the doc stores; this is no longer needed since doc stores are again&lt;br/&gt;
private to the segment.  I think we don&apos;t need PerDocBuffer, either.&lt;br/&gt;
And this also simplifies RAM usage tracking!&lt;/p&gt;

&lt;p&gt;Also, we don&apos;t need separate closeDocStore; it should just be closed&lt;br/&gt;
during flush.&lt;/p&gt;

&lt;p&gt;I like the ThreadAffinityDocumentsWriterThreadPool; it&apos;s the default&lt;br/&gt;
right (I see some tests explicitly setting in on IWC; not sure why)?&lt;/p&gt;

&lt;p&gt;We should make the in-RAM deletes impl somehow pluggable?&lt;/p&gt;</comment>
                    <comment id="12891228" author="michaelbusch" created="Thu, 22 Jul 2010 17:19:20 +0100"  >&lt;p&gt;Thanks, Mike - great feedback! (as always)&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;I still see usage of docStoreOffset, but aren&apos;t we doing away with&lt;br/&gt;
shared doc stores with the cutover to DWPT?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Do we want all segments that one DWPT writes to share the same&lt;br/&gt;
doc store, i.e. one doc store per DWPT, or remove doc stores &lt;br/&gt;
entirely?&lt;/p&gt;


&lt;blockquote&gt;
&lt;p&gt;I think you can further simplify DocumentsWriterPerThread.DocWriter;&lt;br/&gt;
in fact I think you can remove it &amp;amp; all subclasses in consumers!&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I agree!  Now that a high number of testcases pass it&apos;s less scary&lt;br/&gt;
to modify even more code &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;  - will do this next.&lt;/p&gt;


&lt;blockquote&gt;
&lt;p&gt;Also, we don&apos;t need separate closeDocStore; it should just be closed&lt;br/&gt;
during flush.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;OK sounds good.&lt;/p&gt;


&lt;blockquote&gt;
&lt;p&gt;I like the ThreadAffinityDocumentsWriterThreadPool; it&apos;s the default&lt;br/&gt;
right (I see some tests explicitly setting in on IWC; not sure why)?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;It&apos;s actually only TestStressIndexing2 and it sets it to use a different &lt;br/&gt;
number of max thread states than the default.&lt;/p&gt;


&lt;blockquote&gt;
&lt;p&gt;We should make the in-RAM deletes impl somehow pluggable?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Do you mean so that it&apos;s customizable how deletes are handled? &lt;br/&gt;
E.g. doing live deletes vs. lazy deletes on flush?&lt;br/&gt;
I think that&apos;s a good idea.  E.g. at Twitter we&apos;ll do live deletes always&lt;br/&gt;
to get the lowest latency (and we don&apos;t have too many deletes),&lt;br/&gt;
but that&apos;s probably not the best default for everyone.&lt;br/&gt;
So I agree that making this customizable is a good idea.&lt;/p&gt;

&lt;p&gt;It&apos;d also be nice to have a more efficient data structure to buffer the&lt;br/&gt;
deletes.  With many buffered deletes the java hashmap approach&lt;br/&gt;
will not be very efficient.  Terms could be written into a byte pool,&lt;br/&gt;
but what should we do with queries?&lt;/p&gt;</comment>
                    <comment id="12891241" author="yseeley@gmail.com" created="Thu, 22 Jul 2010 17:56:05 +0100"  >&lt;blockquote&gt;&lt;p&gt;It&apos;d also be nice to have a more efficient data structure to buffer the deletes. With many buffered deletes the java hashmap approach will not be very efficient. Terms could be written into a byte pool, but what should we do with queries?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;IMO, terms are an order of magnitude more important than queries.  Most deletes will be by some sort of unique id, and will be in the same field.&lt;/p&gt;

&lt;p&gt;Perhaps a single byte[] with length prefixes (like the field cache has).  A single int could then represent a term (it would just be an offset into the byte[], which is field-specific, so no need to store the field each time).&lt;/p&gt;

&lt;p&gt;We could then build a treemap or hashmap that natively used an int[]... but that may not be necessary (depending on how deletes are applied).  Perhaps a sort could be done right before applying, and duplicate terms could be handled at that time.&lt;/p&gt;

&lt;p&gt;Anyway, I&apos;m only casually following this issue, but I&apos;ts looking like really cool stuff!&lt;/p&gt;</comment>
                    <comment id="12891256" author="mikemccand" created="Thu, 22 Jul 2010 18:30:21 +0100"  >&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;I still see usage of docStoreOffset, but aren&apos;t we doing away with shared doc stores with the cutover to DWPT?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Do we want all segments that one DWPT writes to share the same&lt;br/&gt;
doc store, i.e. one doc store per DWPT, or remove doc stores &lt;br/&gt;
entirely?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Oh good question... a single DWPT can in fact continue to share doc&lt;br/&gt;
store across the segments it flushes.&lt;/p&gt;

&lt;p&gt;Hmm, but... this opto only helps in that we don&apos;t have to merge the&lt;br/&gt;
doc stores if we merge segments that already share their doc stores.&lt;br/&gt;
But if (say) I have 2 threads indexing, and I&apos;m indexing lots of docs&lt;br/&gt;
and each DWPT has written 5 segments, we will then merge these 10&lt;br/&gt;
segments, and must merge the doc stores at that point.  So the sharing&lt;br/&gt;
isn&apos;t really buying us much (just not closing old files &amp;amp; opening new&lt;br/&gt;
ones, which is presumably negligible)?&lt;/p&gt;

&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;I think you can further simplify DocumentsWriterPerThread.DocWriter; in fact I think you can remove it &amp;amp; all subclasses in consumers!&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I agree! Now that a high number of testcases pass it&apos;s less scary&lt;br/&gt;
to modify even more code  - will do this next.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Also, we don&apos;t need separate closeDocStore; it should just be closed during flush.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;OK sounds good.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Super &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;I like the ThreadAffinityDocumentsWriterThreadPool; it&apos;s the default right (I see some tests explicitly setting in on IWC; not sure why)?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;It&apos;s actually only TestStressIndexing2 and it sets it to use a different &lt;br/&gt;
number of max thread states than the default.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Ahh OK great.&lt;/p&gt;

&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;We should make the in-RAM deletes impl somehow pluggable?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Do you mean so that it&apos;s customizable how deletes are handled? &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Actually I was worried about the long[] sequenceIDs (adding 8 bytes&lt;br/&gt;
RAM per buffered doc) &amp;#8211; this could be a biggish hit to RAM efficiency&lt;br/&gt;
for small docs.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt; E.g. doing live deletes vs. lazy deletes on flush?&lt;br/&gt;
I think that&apos;s a good idea. E.g. at Twitter we&apos;ll do live deletes always&lt;br/&gt;
to get the lowest latency (and we don&apos;t have too many deletes),&lt;br/&gt;
but that&apos;s probably not the best default for everyone.&lt;br/&gt;
So I agree that making this customizable is a good idea.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yeah, this too &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;Actually deletions today are not applied on flush &amp;#8211; they continue to&lt;br/&gt;
be buffered beyond flush, and then get applied just before a merge&lt;br/&gt;
kicks off.  I think we should keep this (as an option and probably as&lt;br/&gt;
the default) &amp;#8211; it&apos;s important for apps w/ large indices that don&apos;t use&lt;br/&gt;
NRT (and don&apos;t pool readers) because it&apos;s costly to open readers.&lt;/p&gt;

&lt;p&gt;So it sounds like we should support &quot;lazy&quot; (apply-before-merge like&lt;br/&gt;
today) and &quot;live&quot; (live means resolve deleted Term/Query -&amp;gt; docID(s)&lt;br/&gt;
synchronously inside deleteDocuments, right?).&lt;/p&gt;

&lt;p&gt;Live should also be less performant because of less temporal locality&lt;br/&gt;
(vs lazy).&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;It&apos;d also be nice to have a more efficient data structure to buffer the&lt;br/&gt;
deletes. With many buffered deletes the java hashmap approach&lt;br/&gt;
will not be very efficient. Terms could be written into a byte pool,&lt;br/&gt;
but what should we do with queries?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I agree w/ Yonik: let&apos;s worry only about delete by Term (not Query)&lt;br/&gt;
for now.&lt;/p&gt;

&lt;p&gt;Maybe we could reuse (factor out) TermsHashPerField&apos;s custom hash&lt;br/&gt;
here, for the buffered Terms?  It efficiently maps a BytesRef --&amp;gt; int.&lt;/p&gt;

&lt;p&gt;Another thing: it looks like finishFlushedSegment is sync&apos;d on the IW&lt;br/&gt;
instance, but, it need not be sync&apos;d for all of that?  EG&lt;br/&gt;
readerPool.get(), applyDeletes, building the CFS, may not need to be&lt;br/&gt;
inside the sync block?&lt;/p&gt;</comment>
                    <comment id="12891262" author="michaelbusch" created="Thu, 22 Jul 2010 18:51:15 +0100"  >&lt;blockquote&gt;
&lt;p&gt;Perhaps a single byte[] with length prefixes (like the field cache has).  A single int could then represent a term (it would just be an offset into the byte[], which is field-specific, so no need to store the field each time).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yeah that&apos;s pretty much how TermsHashPerField works.  I agree with Mike, &lt;br/&gt;
let&apos;s reuse that code.&lt;/p&gt;


&lt;blockquote&gt;
&lt;p&gt;Hmm, but... this opto only helps in that we don&apos;t have to merge the&lt;br/&gt;
doc stores if we merge segments that already share their doc stores.&lt;br/&gt;
But if (say) I have 2 threads indexing, and I&apos;m indexing lots of docs&lt;br/&gt;
and each DWPT has written 5 segments, we will then merge these 10&lt;br/&gt;
segments, and must merge the doc stores at that point. So the sharing&lt;br/&gt;
isn&apos;t really buying us much (just not closing old files &amp;amp; opening new&lt;br/&gt;
ones, which is presumably negligible)?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yeah that&apos;s true.  I agree it won&apos;t help much. I think we should just &lt;br/&gt;
remove the doc stores, great simplification (which should also make &lt;br/&gt;
parallel indexing a bit easier &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; ).  &lt;/p&gt;


&lt;blockquote&gt;
&lt;p&gt;Another thing: it looks like finishFlushedSegment is sync&apos;d on the IW&lt;br/&gt;
instance, but, it need not be sync&apos;d for all of that? EG&lt;br/&gt;
readerPool.get(), applyDeletes, building the CFS, may not need to be&lt;br/&gt;
inside the sync block?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Thanks for the hint.  I need to carefully go over all the synchronization, &lt;br/&gt;
there are likely more problems.  &lt;/p&gt;</comment>
                    <comment id="12891264" author="yseeley@gmail.com" created="Thu, 22 Jul 2010 19:00:44 +0100"  >&lt;blockquote&gt;&lt;p&gt;Yeah that&apos;s pretty much how TermsHashPerField works. I agree with Mike, let&apos;s reuse that code.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Do we even need to maintain a hash over it though, or can we simply keep a list (and allow dup terms until it&apos;s time to apply them)?&lt;/p&gt;</comment>
                    <comment id="12891334" author="jasonrutherglen" created="Thu, 22 Jul 2010 22:06:14 +0100"  >&lt;blockquote&gt;&lt;p&gt;I think we should just remove the doc stores&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Right, I think we should remove sharing doc stores between&lt;br/&gt;
segments. And in general, RT apps will likely not want to use&lt;br/&gt;
doc stores if they are performing numerous updates and/or&lt;br/&gt;
deletes. We can explicitly state this in the javadocs.&lt;/p&gt;

&lt;p&gt;I&apos;m thinking we could explore efficient deleted docs as sequence&lt;br/&gt;
ids in a different issue, specifically storing them in a short[]&lt;br/&gt;
and wrapping around.  &lt;/p&gt;</comment>
                    <comment id="12891657" author="michaelbusch" created="Fri, 23 Jul 2010 17:20:17 +0100"  >&lt;blockquote&gt;
&lt;p&gt;I think we don&apos;t need PerDocBuffer, either.&lt;br/&gt;
And this also simplifies RAM usage tracking!&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The nice thing about that buffer is that on&lt;br/&gt;
non-aborting exceptions we can simply skip&lt;br/&gt;
the whole document, because nothing gets&lt;br/&gt;
written to the stores until finishDocument &lt;br/&gt;
is called.&lt;/p&gt;</comment>
                    <comment id="12891663" author="mikemccand" created="Fri, 23 Jul 2010 17:28:59 +0100"  >&lt;blockquote&gt;

&lt;blockquote&gt;&lt;p&gt;I think we don&apos;t need PerDocBuffer, either.  And this also simplifies RAM usage tracking!&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The nice thing about that buffer is that on&lt;br/&gt;
non-aborting exceptions we can simply skip&lt;br/&gt;
the whole document, because nothing gets&lt;br/&gt;
written to the stores until finishDocument &lt;br/&gt;
is called.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Well, though, if we did write it &quot;live&quot;, since the doc is marked deleted, it would be &quot;harmless&quot; right?&lt;/p&gt;

&lt;p&gt;Only difference is it took up some disk space (which should be minor given that it&apos;s not &quot;normal&quot; to have lots of docs hitting non-aborting exceptions).&lt;/p&gt;

&lt;p&gt;Also... when the doc is large, the fact that we double buffer (write first to RAMFile then to the store) can be costly.  Ie we consume 1X the stored-field size in transient RAM, vs writing directly to disk, I think?&lt;/p&gt;</comment>
                    <comment id="12891664" author="michaelbusch" created="Fri, 23 Jul 2010 17:35:26 +0100"  >&lt;blockquote&gt;&lt;/blockquote&gt;
&lt;p&gt;Well, though, if we did write it &quot;live&quot;, since the doc is marked deleted, it would be &quot;harmless&quot; right?&lt;/p&gt;

&lt;p&gt;Only difference is it took up some disk space (which should be minor given that it&apos;s not &quot;normal&quot; to have lots of docs hitting non-aborting exceptions).&lt;/p&gt;

&lt;p&gt;Also... when the doc is large, the fact that we double buffer (write first to RAMFile then to the store) can be costly.  Ie we consume 1X the stored-field size in transient RAM, vs writing directly to disk, I think?&lt;br/&gt;
{quote]&lt;/p&gt;

&lt;p&gt;Yeah I totally agree that this would be a nice performance win - I got first excited too about removing that extra buffering layer.  But we probably have to handle exceptions a bit differently then?  Because now all exceptions thrown in processDocument() are considered non-aborting, because they nothing is actually written to disk in processDocument().&lt;/p&gt;

&lt;p&gt;We only abort when we encounter an exception in finishDocument(), because then the files might be corrupted.&lt;/p&gt;

&lt;p&gt;So we probably just have to catch the IOExceptions somewhere else to figure out if it was e.g. a (non-aborting) exception from the TokenStream vs. e.g. a disk full (aborting) exception from StoredFieldsWriter.&lt;/p&gt;</comment>
                    <comment id="12891986" author="mikemccand" created="Sat, 24 Jul 2010 15:49:34 +0100"  >&lt;p&gt;Yes, you&apos;re right!  So, after this change, an exception while processing stored fields / term vectors must handled as an aborting exception.&lt;/p&gt;</comment>
                    <comment id="12892174" author="shaie" created="Mon, 26 Jul 2010 06:13:43 +0100"  >&lt;p&gt;Is it possible that as part of this issue (or this effort), you&apos;ll think of opening PTDW for easier extensions (such as Parallel Indexing)? If it can easily be done by marking some methods as protected instead of (package-)private, then I think it&apos;s worth it. We can mark them as lucene.internal or something ... And perhaps even allow setting the DW on IW, for really expert use?&lt;/p&gt;

&lt;p&gt;If you think that should be part of the overhaul refactoring you&apos;ve once suggested (Michael B.), then I&apos;m ok with that as well. Just thinking that it&apos;s better to hit the iron while it&apos;s still hot &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;.&lt;/p&gt;</comment>
                    <comment id="12892924" author="michaelbusch" created="Tue, 27 Jul 2010 21:55:53 +0100"  >&lt;blockquote&gt;
&lt;p&gt;Is it possible that as part of this issue (or this effort), you&apos;ll think of opening PTDW for easier extensions (such as Parallel Indexing)?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yeah I&apos;d like to make some progress on parallel indexing too.  I think now that DWPT is roughly working I can start thinking about what further changes are necessary in the indexer.&lt;/p&gt;</comment>
                    <comment id="12896519" author="mikemccand" created="Mon, 9 Aug 2010 13:23:54 +0100"  >&lt;p&gt;BTW, randomly, &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2186&quot; title=&quot;First cut at column-stride fields (index values storage)&quot;&gt;&lt;del&gt;LUCENE-2186&lt;/del&gt;&lt;/a&gt; has already roughly factored out the BytesRef hash from TermsHashPerField... so if we want also to reuse that here, we can share.&lt;/p&gt;</comment>
                    <comment id="12910262" author="mikemccand" created="Thu, 16 Sep 2010 19:49:40 +0100"  >&lt;p&gt;Is this near-comittable?  Ie &quot;just&quot; the DWPT cutover?  This part seems separable from making each DWPT&apos;s buffer searchable?&lt;/p&gt;

&lt;p&gt;I&apos;m running some tests w/ 20 indexing threads and I think the sync&apos;d flush is a big bottleneck...&lt;/p&gt;</comment>
                    <comment id="12910268" author="jasonrutherglen" created="Thu, 16 Sep 2010 20:01:45 +0100"  >&lt;blockquote&gt;&lt;p&gt;I think the sync&apos;d flush is a big bottleneck&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Is this because indexing stops while the DWPT segment is being flushed to disk or are you referring to a different sync?&lt;/p&gt;</comment>
                    <comment id="12910276" author="mikemccand" created="Thu, 16 Sep 2010 20:24:28 +0100"  >&lt;blockquote&gt;&lt;p&gt;Is this because indexing stops while the DWPT segment is being flushed to disk or are you referring to a different sync?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I&apos;m talking about Lucene trunk today (ie before this patch).&lt;/p&gt;

&lt;p&gt;Yes, because indexing of all 20 threads is blocked while a single thread moves the RAM buffer to disk.  But, with this patch, each thread will privately move its own RAM buffer to disk, not blocking the rest.&lt;/p&gt;

&lt;p&gt;With 20 threads I&apos;m seeing ~4 seconds of concurrent indexing and then 6-8 seconds to flush (w/ 256 MB RAM buffer).&lt;/p&gt;</comment>
                    <comment id="12910483" author="michaelbusch" created="Fri, 17 Sep 2010 07:57:50 +0100"  >&lt;blockquote&gt;&lt;p&gt;Is this near-comittable?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think we need to:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;merge trunk and make tests pass&lt;/li&gt;
	&lt;li&gt;finish flushing by RAM&lt;/li&gt;
	&lt;li&gt;make deletes work again&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Then it should be ready to commit.  Sorry, was so busy the last weeks that I couldn&apos;t make much progress.&lt;/p&gt;</comment>
                    <comment id="12910527" author="mikemccand" created="Fri, 17 Sep 2010 11:28:37 +0100"  >&lt;p&gt;I just posted some details about the concurrency bottleneck of flush at &lt;a href=&quot;http://chbits.blogspot.com/2010/09/lucenes-indexing-is-fast.html&quot; class=&quot;external-link&quot;&gt;http://chbits.blogspot.com/2010/09/lucenes-indexing-is-fast.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;It&apos;s pretty bad &amp;#8211; w/ only 6 threads, I see flush taking 54% of the time, ie for 54% of the time all threads are blocked on indexing while the flush runs.&lt;/p&gt;</comment>
                    <comment id="12910535" author="simonw" created="Fri, 17 Sep 2010 12:02:04 +0100"  >&lt;blockquote&gt;&lt;p&gt;BTW, randomly, &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2186&quot; title=&quot;First cut at column-stride fields (index values storage)&quot;&gt;&lt;del&gt;LUCENE-2186&lt;/del&gt;&lt;/a&gt; has already roughly factored out the BytesRef hash from TermsHashPerField... so if we want also to reuse that here, we can share.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;do you guys still need the BytesHash being factored out since I currently work on splitting stand alone parts out of &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2186&quot; title=&quot;First cut at column-stride fields (index values storage)&quot;&gt;&lt;del&gt;LUCENE-2186&lt;/del&gt;&lt;/a&gt;. If so that would be the next on the list... let me know&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I just posted some details about the concurrency bottleneck of flush at &lt;a href=&quot;http://chbits.blogspot.com/2010/09/lucenes-indexing-is-fast.html&quot; class=&quot;external-link&quot;&gt;http://chbits.blogspot.com/2010/09/lucenes-indexing-is-fast.html&lt;/a&gt;&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;nice post mike &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;
</comment>
                    <comment id="12911144" author="jasonrutherglen" created="Sun, 19 Sep 2010 02:07:07 +0100"  >&lt;p&gt;Simon, I think the BytesHash being factored is useful, though not a must have for committing the flush by DWPT code.&lt;/p&gt;

&lt;p&gt;Mike, I need to finish the unit tests for &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2573&quot; title=&quot;Tiered flushing of DWPTs by RAM with low/high water marks&quot;&gt;&lt;del&gt;LUCENE-2573&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Michael, what is the issue with deletes?  We don&apos;t need deletes to use sequence ids yet?  Maybe we should open a separate issue to make deletes work for the realtime/DWPT branch?&lt;/p&gt;</comment>
                    <comment id="12912312" author="jasonrutherglen" created="Mon, 20 Sep 2010 03:46:46 +0100"  >&lt;p&gt;Simon, on second thought, lets go ahead and factor out BytesHash, do you want to submit a patch for the realtime branch and post it here or should I?&lt;/p&gt;</comment>
                    <comment id="12912313" author="jasonrutherglen" created="Mon, 20 Sep 2010 03:50:19 +0100"  >&lt;p&gt;I opened an issue for the deletes &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2655&quot; title=&quot;Get deletes working in the realtime branch&quot;&gt;&lt;del&gt;LUCENE-2655&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;</comment>
                    <comment id="12969836" author="jasonrutherglen" created="Thu, 9 Dec 2010 18:12:11 +0000"  >&lt;p&gt;Now that &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2680&quot; title=&quot;Improve how IndexWriter flushes deletes against existing segments&quot;&gt;&lt;del&gt;LUCENE-2680&lt;/del&gt;&lt;/a&gt; is implemented we can get deletes working properly in the&lt;br/&gt;
realtime, which is really the DWPT branch at this point. Given the significant&lt;br/&gt;
changes made since it&apos;s creation, rather than continue with a realtime branch,&lt;br/&gt;
I propose we patch into trunk the DWPT changes. &lt;/p&gt;</comment>
                    <comment id="12969850" author="michaelbusch" created="Thu, 9 Dec 2010 18:30:58 +0000"  >&lt;p&gt;Ideally we should merge trunk into realtime after &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2680&quot; title=&quot;Improve how IndexWriter flushes deletes against existing segments&quot;&gt;&lt;del&gt;LUCENE-2680&lt;/del&gt;&lt;/a&gt; is committed, get everything working there, and then merge realtime back into trunk?&lt;/p&gt;

&lt;p&gt;I agree that it totally makes sense to get DWPT into trunk as soon as possible (ie. not wait until all realtime stuff is done).&lt;/p&gt;</comment>
                    <comment id="12969856" author="jasonrutherglen" created="Thu, 9 Dec 2010 18:37:39 +0000"  >&lt;p&gt;&amp;gt; and then merge realtime back into trunk&lt;/p&gt;

&lt;p&gt;I&apos;d prefer to simply create a patch of the DWPT changes, which granted&apos;ll be large, however we&apos;re through most of the hurdles and at this point, the patch&apos;ll be straightforward to implement and test using the existing unit tests?&lt;/p&gt;</comment>
                    <comment id="12969865" author="michaelbusch" created="Thu, 9 Dec 2010 18:58:42 +0000"  >&lt;p&gt;Not sure if that&apos;s much easier though, because what you said is true:  the realtime branch currently is basically the DWPT branch.&lt;/p&gt;</comment>
                    <comment id="12970262" author="jasonrutherglen" created="Fri, 10 Dec 2010 18:46:54 +0000"  >&lt;p&gt;Michael,&lt;/p&gt;

&lt;p&gt;I don&apos;t have commit access to the realtime branch. A patch is probably easier&lt;br/&gt;
for other people to work on and look at than the branch? Also, I have some time&lt;br/&gt;
to work on integrating DWPT and the changes since the branch creation are&lt;br/&gt;
fairly significant and probably require mostly manual merging. &lt;/p&gt;</comment>
                    <comment id="12970287" author="michaelbusch" created="Fri, 10 Dec 2010 19:25:24 +0000"  >&lt;p&gt;I started merging yesterday the latest trunk into realtime.  The merge is rather hard, as you might imagine &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;  &lt;br/&gt;
But I&apos;m down from 600 compile errors to ~100.  I can try to finish it this weekend.&lt;/p&gt;

&lt;p&gt;But I don&apos;t want to block you, if you want to go the patch route and have time now don&apos;t wait for me.&lt;/p&gt;</comment>
                    <comment id="12970289" author="michaelbusch" created="Fri, 10 Dec 2010 19:31:14 +0000"  >&lt;blockquote&gt;&lt;p&gt;I started merging yesterday the latest trunk into realtime.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;As part of this I want to clean up the branch a bit and remove unnecessary changes (like refactorings) to make the merge back into trunk less difficult.  When I&apos;m done with the merge we should patch &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2680&quot; title=&quot;Improve how IndexWriter flushes deletes against existing segments&quot;&gt;&lt;del&gt;LUCENE-2680&lt;/del&gt;&lt;/a&gt; into realtime.  (or commit to trunk and merge trunk into realtime again)&lt;/p&gt;</comment>
                    <comment id="12970341" author="jasonrutherglen" created="Fri, 10 Dec 2010 23:49:52 +0000"  >&lt;blockquote&gt;&lt;p&gt;The merge is rather hard, as you might imagine&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I&apos;d hold off until &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2680&quot; title=&quot;Improve how IndexWriter flushes deletes against existing segments&quot;&gt;&lt;del&gt;LUCENE-2680&lt;/del&gt;&lt;/a&gt; is committed to trunk, otherwise the merge work may be redundant.  &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;commit to trunk and merge trunk into realtime again&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think it&apos;s pretty much ready for trunk as the tests pass.  &lt;/p&gt;

&lt;p&gt;Merging realtime/DWPT branch back to trunk should be easier if we try to wrap up DWPT in a short period of time, eg, plow ahead until it&apos;s merge-able.&lt;/p&gt;
</comment>
                    <comment id="12973734" author="jasonrutherglen" created="Tue, 21 Dec 2010 16:20:16 +0000"  >&lt;p&gt;As per Michael B&apos;s email, I&apos;ll start on integrating deletes and flush-by-RAM aka &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2573&quot; title=&quot;Tiered flushing of DWPTs by RAM with low/high water marks&quot;&gt;&lt;del&gt;LUCENE-2573&lt;/del&gt;&lt;/a&gt;, along the way the deadlock issue could be uncovered, which test is it occurring on?&lt;/p&gt;</comment>
                    <comment id="12974484" author="jasonrutherglen" created="Thu, 23 Dec 2010 04:29:58 +0000"  >&lt;p&gt;Also, it&apos;d be great if we could summarize the changes trunk -&amp;gt; DWPT branch.  &lt;/p&gt;</comment>
                    <comment id="12974526" author="jasonrutherglen" created="Thu, 23 Dec 2010 04:54:41 +0000"  >&lt;p&gt;Here&apos;s ant test-core output.  Looks like it&apos;s deadlocking in TestIndexWriter?  There are some IR.reopen failures, a null pointer, and a delete count I&apos;ll look at.&lt;/p&gt;</comment>
                    <comment id="12974529" author="jasonrutherglen" created="Thu, 23 Dec 2010 05:33:29 +0000"  >&lt;p&gt;Small patch fixing the num deletes test null pointer. &lt;/p&gt;

&lt;p&gt;The TestIndexReaderReopen failure seems to have something to do with flushing deletes.&lt;/p&gt;</comment>
                    <comment id="12977443" author="jasonrutherglen" created="Tue, 4 Jan 2011 20:15:53 +0000"  >&lt;p&gt;Looks like the problem with TestIndexReaderReopen is the test opens an IW, tried to queue deletes, however because there isn&apos;t an existing DWPT (at least when flush is called), the deletes haven&apos;t been recorded and so they&apos;re not applied to the index.  Perhaps we need to init DW with at least 1 DWPT?&lt;/p&gt;</comment>
                    <comment id="12977465" author="jasonrutherglen" created="Tue, 4 Jan 2011 21:20:38 +0000"  >&lt;p&gt;Also I think the deadlock is happening in DW update doc where we&apos;re calling delete term across all DWPTs. How will we guarantee point-in-timeness when RT is turned on? I guess with the sequence ids?&lt;/p&gt;</comment>
                    <comment id="12977481" author="jasonrutherglen" created="Tue, 4 Jan 2011 21:53:43 +0000"  >&lt;p&gt;I added an assertion showing the lack of DWPTs when delete is called.&lt;/p&gt;

&lt;p&gt;deleteTermNoWait (which skips flush control) is called in update doc and now deadlock doesn&apos;t occur when executing TestIndexWriter.&lt;/p&gt;</comment>
                    <comment id="12977560" author="jasonrutherglen" created="Wed, 5 Jan 2011 00:50:09 +0000"  >&lt;p&gt;Taking a step back, I&apos;m not sure flush control should be global, as flushing is&lt;br/&gt;
entirely per thread now? If we&apos;re adding a delete term for every DWPT, if one&lt;br/&gt;
is flushing do we wait or do we simply queue it up? I don&apos;t think we can wait&lt;br/&gt;
in the delete call for a DWPT to completely flush?&lt;/p&gt;

&lt;p&gt;So we&apos;ll likely need to delete in a PerThreadTask that&apos;s executed on each&lt;br/&gt;
existing DWPT. How we guarantee concurrency seems a little odd here as what if&lt;br/&gt;
a new DWPT is spun up while we&apos;re deleting in another thread? Perhaps we should&lt;br/&gt;
simply spin up on DW init, the max thread state number of DWPTs? This way we&lt;br/&gt;
always have &amp;gt; 0 available, and we can perhaps lock on seq id when adding&lt;br/&gt;
deletes to all DWPTs (it&apos;s a fast call). We may want to simply skip any&lt;br/&gt;
flushing DWPTs when adding deletes?&lt;/p&gt;

&lt;p&gt;In browsing the code, FlushControl isn&apos;t used in very many places. This&apos;ll get&lt;br/&gt;
a little bit more fleshed out when we integrate &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2573&quot; title=&quot;Tiered flushing of DWPTs by RAM with low/high water marks&quot;&gt;&lt;del&gt;LUCENE-2573&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;</comment>
                    <comment id="12977585" author="jasonrutherglen" created="Wed, 5 Jan 2011 01:52:56 +0000"  >&lt;p&gt;I think we can get DWPTs working sans &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2573&quot; title=&quot;Tiered flushing of DWPTs by RAM with low/high water marks&quot;&gt;&lt;del&gt;LUCENE-2573&lt;/del&gt;&lt;/a&gt; for now.  We can do this by setting a hard max buffer size, deletes, etc per DWPT.  The values will be from IWC divided by the max thread states.  &lt;/p&gt;</comment>
                    <comment id="12977833" author="jasonrutherglen" created="Wed, 5 Jan 2011 16:49:17 +0000"  >&lt;p&gt;Perhaps it&apos;s best to place the RAM tracking into FlushControl where the RAM&lt;br/&gt;
consumed by deleted query, terms, and added documents can be recorded, so that&lt;br/&gt;
the proper flush decision may be made in it, a central global object. To get this idea&lt;br/&gt;
working we&apos;d need to implement &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2573&quot; title=&quot;Tiered flushing of DWPTs by RAM with low/high water marks&quot;&gt;&lt;del&gt;LUCENE-2573&lt;/del&gt;&lt;/a&gt; in FlushControl. I&apos;ll likely get&lt;br/&gt;
started on this.&lt;/p&gt;</comment>
                    <comment id="12977892" author="jasonrutherglen" created="Wed, 5 Jan 2011 18:32:07 +0000"  >&lt;p&gt;Another model we could implement is a straight queuing. This&apos;d give us total&lt;br/&gt;
ordering on all IW calls. Documents, deletes, and flushes would be queued up&lt;br/&gt;
and executed asynchronously. For example in today&apos;s DWPT code we will still&lt;br/&gt;
block document additions while flushing because we&apos;re tying a thread to a given&lt;br/&gt;
DWPT. If a thread&apos;s DWPT is flushing, wouldn&apos;t we want to simply assign the doc&lt;br/&gt;
add to a different non-flushing DWPT to gain full efficiency? This seems more&lt;br/&gt;
easily doable with a queuing model. If we want synchronous flushing then we&apos;d&lt;br/&gt;
place a flush event in the queue and wait for it to complete executing. How&lt;br/&gt;
does this sound? &lt;/p&gt;
</comment>
                    <comment id="12978059" author="mikemccand" created="Thu, 6 Jan 2011 00:31:15 +0000"  >&lt;blockquote&gt;
&lt;p&gt;Taking a step back, I&apos;m not sure flush control should be global, as flushing is&lt;br/&gt;
entirely per thread now?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think flush control must be global?  Ie when we&apos;ve used too much RAM we start flushing?&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;If we&apos;re adding a delete term for every DWPT, if one&lt;br/&gt;
is flushing do we wait or do we simply queue it up? I don&apos;t think we can wait&lt;br/&gt;
in the delete call for a DWPT to completely flush?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I believe we can drop the delete in that case.  We only need to buffer into DWPTs that have at least 1 doc.&lt;/p&gt;</comment>
                    <comment id="12978060" author="mikemccand" created="Thu, 6 Jan 2011 00:32:37 +0000"  >{queue}&lt;br/&gt;
Another model we could implement is a straight queuing. This&apos;d give us total&lt;br/&gt;
ordering on all IW calls. Documents, deletes, and flushes would be queued up&lt;br/&gt;
and executed asynchronously. For example in today&apos;s DWPT code we will still&lt;br/&gt;
block document additions while flushing because we&apos;re tying a thread to a given&lt;br/&gt;
DWPT. If a thread&apos;s DWPT is flushing, wouldn&apos;t we want to simply assign the doc&lt;br/&gt;
add to a different non-flushing DWPT to gain full efficiency? This seems more&lt;br/&gt;
easily doable with a queuing model. If we want synchronous flushing then we&apos;d&lt;br/&gt;
place a flush event in the queue and wait for it to complete executing. How&lt;br/&gt;
does this sound?{queue}
&lt;p&gt;I think we should have to add queueing to all incoming ops...&lt;/p&gt;

&lt;p&gt;If a given DWPT is flushing then we pick another?  Ie the binding logic would naturally avoid DWPTs that are not available &amp;#8211; either because another thread has it, or it&apos;s flushing.  But it would prefer to use the same DWPT it used last time, if possible (affinity).&lt;/p&gt;</comment>
                    <comment id="12978065" author="jasonrutherglen" created="Thu, 6 Jan 2011 01:00:33 +0000"  >&lt;p&gt;We&apos;re going to great lengths it seems to emulate a producer consumer queue (eg,&lt;br/&gt;
ordering of calls with sequence ids, thread pooling) without actually&lt;br/&gt;
implementing one. A fixed size blocking queue would simply block threads as&lt;br/&gt;
needed and would probably look cleaner in code. We could still implement thread&lt;br/&gt;
affinities though I simply can&apos;t see most applications requiring affinity, so&lt;br/&gt;
perhaps we can avoid it for now and put it back in later? &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I think flush control must be global? Ie when we&apos;ve used too much RAM we&lt;br/&gt;
start flushing?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Right, it should. I&apos;m just not sure we still need FC&apos;s global waiting during&lt;br/&gt;
flush, that&apos;d seem to go away because the RAM usage tracking is in DW. If we&lt;br/&gt;
record the new incremental RAM used (which I think we do) per add/update/delete&lt;br/&gt;
then we can enable a pluggable user defined flush policy. &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt; If a given DWPT is flushing then we pick another? Ie the binding logic&lt;br/&gt;
would naturally avoid DWPTs that are not available - either because another&lt;br/&gt;
thread has it, or it&apos;s flushing. But it would prefer to use the same DWPT it&lt;br/&gt;
used last time, if possible (affinity). &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;However once the affinity DWPT flush completed, we&apos;d need logic to revert back&lt;br/&gt;
to the original?&lt;/p&gt;

&lt;p&gt;I think the 5% model of &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2573&quot; title=&quot;Tiered flushing of DWPTs by RAM with low/high water marks&quot;&gt;&lt;del&gt;LUCENE-2573&lt;/del&gt;&lt;/a&gt; may typically yield flushing that occurs in&lt;br/&gt;
near intervals of each other, ie, it&apos;s going to slow down the aggregate&lt;br/&gt;
indexing if they&apos;re flushing on top of each other. Maybe we should start at 60%&lt;br/&gt;
then the multiple of 40% divided by maxthreadstate - 1? Ideally we&apos;d&lt;br/&gt;
statistically optimize the flush interval per machine, eg, SSDs and RAM disks&lt;br/&gt;
will likely require only a small flush percentage interval.&lt;/p&gt;


</comment>
                    <comment id="12978075" author="jasonrutherglen" created="Thu, 6 Jan 2011 01:49:39 +0000"  >&lt;blockquote&gt;&lt;p&gt;I believe we can drop the delete in that case. We only need to buffer&lt;br/&gt;
into DWPTs that have at least 1 doc.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Right if the DWPT&apos;s flushing we can skip it. In the queue model we&apos;d consume and locate&lt;br/&gt;
the existing DWPTs, adding the delete to each DWPT not flushing. However in the&lt;br/&gt;
zero DWPT case we still need to record a delete somewhere, most likely we&apos;d&lt;br/&gt;
need to create a zero doc DWPT? Oh wait, we need to add the delete to the last&lt;br/&gt;
segment? Ah, I can fix that in the existing code (eg, fix the reopen test case&lt;br/&gt;
failures).&lt;/p&gt;</comment>
                    <comment id="12978129" author="jasonrutherglen" created="Thu, 6 Jan 2011 05:04:12 +0000"  >&lt;p&gt;Same as the last patch, however default deletes is added to DW to which deletes are added to when there are no available DWPTs.  On flush all threads, default deletes is applied to the last segment with no doc limit.  &lt;/p&gt;

&lt;p&gt;TestIndexReaderReopen now passes.&lt;/p&gt;</comment>
                    <comment id="12978140" author="jasonrutherglen" created="Thu, 6 Jan 2011 05:20:10 +0000"  >&lt;p&gt;Here&apos;s a new test.out, I&apos;ll look at TestCheckIndex which should probably work.  &lt;/p&gt;

&lt;p&gt;&quot;IndexFileDeleter doesn&apos;t know about file&quot; seems odd.  We&apos;re OOMing in TestIndexWriter because we&apos;re not flushing by RAM (eg, it currently defaults to return false).&lt;/p&gt;</comment>
                    <comment id="12978401" author="mikemccand" created="Thu, 6 Jan 2011 16:35:38 +0000"  >&lt;blockquote&gt;
&lt;p&gt;We&apos;re going to great lengths it seems to emulate a producer consumer queue (eg,&lt;br/&gt;
ordering of calls with sequence ids, thread pooling) without actually&lt;br/&gt;
implementing one. A fixed size blocking queue would simply block threads as&lt;br/&gt;
needed and would probably look cleaner in code. We could still implement thread&lt;br/&gt;
affinities though I simply can&apos;t see most applications requiring affinity, so&lt;br/&gt;
perhaps we can avoid it for now and put it back in later?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I&apos;m not sure we should queue.  I wonder how much this&apos;d slow down the single threaded case?&lt;/p&gt;

&lt;p&gt;Also: I thought we don&apos;t have sequence IDs anymore?  (At least, for landing DWPT; after that (for &quot;true RT&quot;) we need something like sequence IDs?).&lt;/p&gt;

&lt;p&gt;I think thread/doc-class affinity is fairly important.  Docs compress better if they are indexed together with similar docs.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I&apos;m just not sure we still need FC&apos;s global waiting during flush, that&apos;d seem to go away because the RAM usage tracking is in DW.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;We shouldn&apos;t do global waiting anymore &amp;#8211; this is what&apos;s great about DWPT.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;However once the affinity DWPT flush completed, we&apos;d need logic to revert back to the original?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I don&apos;t think so?  I mean a DWPT post-flush is a clean slate.  Some other thread/doc-class can stick to it.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;I think the 5% model of &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2573&quot; title=&quot;Tiered flushing of DWPTs by RAM with low/high water marks&quot;&gt;&lt;del&gt;LUCENE-2573&lt;/del&gt;&lt;/a&gt; may typically yield flushing that occurs in&lt;br/&gt;
near intervals of each other, ie, it&apos;s going to slow down the aggregate&lt;br/&gt;
indexing if they&apos;re flushing on top of each other. Maybe we should start at 60%&lt;br/&gt;
then the multiple of 40% divided by maxthreadstate - 1? Ideally we&apos;d&lt;br/&gt;
statistically optimize the flush interval per machine, eg, SSDs and RAM disks&lt;br/&gt;
will likely require only a small flush percentage interval.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yeah we&apos;ll have to run tests to try to gauge the best &quot;default&quot; policy.  And you&apos;re right that it&apos;ll depend on the relative strength of IO vs CPU on the machine.  Fast IO system means we can flush &quot;later&quot;.&lt;/p&gt;</comment>
                    <comment id="12978403" author="mikemccand" created="Thu, 6 Jan 2011 16:37:26 +0000"  >&lt;blockquote&gt;
&lt;p&gt;Right if the DWPT&apos;s flushing we can skip it. In the queue model we&apos;d consume and locate&lt;br/&gt;
the existing DWPTs, adding the delete to each DWPT not flushing.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;We have an issue open for the app to state that the delete will not apply to any docs indexed in the current session... once we do that, then, the deletes don&apos;t need to be buffered on any DWPTs; just on the &quot;latest&quot; already flushed segment in the index.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;However in the&lt;br/&gt;
zero DWPT case we still need to record a delete somewhere, most likely we&apos;d&lt;br/&gt;
need to create a zero doc DWPT? Oh wait, we need to add the delete to the last&lt;br/&gt;
segment? Ah, I can fix that in the existing code (eg, fix the reopen test case&lt;br/&gt;
failures).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes, to the last flushed segment.&lt;/p&gt;</comment>
                    <comment id="12978409" author="jasonrutherglen" created="Thu, 6 Jan 2011 16:56:35 +0000"  >&lt;p&gt;I added a FlushPolicy class, deletes record the ram used to DWPT and DW.  Recording to DW is for global ram used.  The TIW OOM is still occurring however.  The delete calls to FlushControl are gone, I&apos;m not sure what&apos;s going to be left of it.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I&apos;m not sure we should queue. I wonder how much this&apos;d slow down the single threaded case?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes, that&apos;s a good point.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;We shouldn&apos;t do global waiting anymore - this is what&apos;s great about DWPT.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;However we&apos;ll have global waiting for the flush all threads case.  I think that can move down to DW though.  Or should it simply be a sync in/on IW?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I thought we don&apos;t have sequence IDs anymore?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The seqid lock was there, however it was removed in the last update.  I think we need to clearly document the various locks and sync points as right now it&apos;s not clear enough to prevent deadlock situations.&lt;/p&gt;</comment>
                    <comment id="12978466" author="michaelbusch" created="Thu, 6 Jan 2011 19:16:28 +0000"  >&lt;blockquote&gt;
&lt;p&gt;I believe we can drop the delete in that case. We only need to buffer into DWPTs that have at least 1 doc.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yeah sounds right.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;If a given DWPT is flushing then we pick another? Ie the binding logic would naturally avoid DWPTs that are not available - either because another thread has it, or it&apos;s flushing. But it would prefer to use the same DWPT it used last time, if possible (affinity).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This is actually what should be happening currently if the (default) ThreadAffinityThreadPool is used.  I&apos;ve to check the code again and maybe write a test specifically for that.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Also: I thought we don&apos;t have sequence IDs anymore? (At least, for landing DWPT; after that (for &quot;true RT&quot;) we need something like sequence IDs?).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;True, sequenceIDs are gone since the last merge.  And yes, I still think we&apos;ll need them for RT.  Even for the non-RT case sequenceIDs would have nice benefits.  If methods like addDocument(), deleteDocuments(), etc. return the sequenceID they&apos;d define a strict ordering on those operations and make it transparent for the application, which would be beneficial for document tracking and log replay.&lt;/p&gt;

&lt;p&gt;But anyway, let&apos;s add seqIDs back after the DWPT changes are done and in trunk.&lt;/p&gt;


&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;We shouldn&apos;t do global waiting anymore - this is what&apos;s great about DWPT.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;However we&apos;ll have global waiting for the flush all threads case. I think that can move down to DW though. Or should it simply be a sync in/on IW?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;True, the only global lock that locks all thread states happens when flushAllThreads is called.  This is called when IW explicitly triggers a flush, e.g. on close/commit.  &lt;br/&gt;
However, maybe this is not the right approach?  I guess we don&apos;t really need the global lock.  A thread performing the &quot;global flush&quot; could still acquire each thread state before it starts flushing, but return a threadState to the pool once that particular threadState is done flushing?&lt;/p&gt;

&lt;p&gt;A related question is: Do we want to piggyback on multiple threads when a global flush happens?  Eg. Thread 1 called commit, Thread 2 shortly afterwards addDocument().  When should addDocument() happen? &lt;br/&gt;
a) After all DWPTs finished flushing? &lt;br/&gt;
b) After at least one DWPT finished flushing and is available again?&lt;br/&gt;
c) Or should Thread 2 be used to help flushing DWPTs in parallel with Thread 1?  &lt;/p&gt;

&lt;p&gt;a) is currently implemented, but I think not really what we want.&lt;br/&gt;
b) is probably best for RT, because it means the lowest indexing latency for the new document to be added.&lt;br/&gt;
c) probably means the best overall throughput (depending even on hardware like disk speed, etc)&lt;/p&gt;

&lt;p&gt;For whatever option we pick, we&apos;ll have to carefully think about error handling.  It&apos;s quite straightforward for a) (just commit all flushed segments to SegmentInfos when the global flush completed succesfully).  But for b) and c) it&apos;s unclear what should happen if a DWPT flush fails after some completed already successfully before.&lt;/p&gt;</comment>
                    <comment id="12978504" author="jasonrutherglen" created="Thu, 6 Jan 2011 21:00:34 +0000"  >&lt;blockquote&gt;&lt;p&gt;actually what should be happening currently if the (default)&lt;br/&gt;
ThreadAffinityThreadPool is used. I&apos;ve to check the code again and maybe write&lt;br/&gt;
a test specifically for that.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Lets try to test it, though I&apos;m not immediately sure how the test case&apos;d look. &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;let&apos;s add seqIDs back after the DWPT changes are done and in trunk.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Right.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;True, the only global lock that locks all thread states happens when&lt;br/&gt;
flushAllThreads is called. This is called when IW explicitly triggers a flush,&lt;br/&gt;
e.g. on close/commit. However, maybe this is not the right approach?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think this is fine for the DWPT branch as flush, commit, and close are&lt;br/&gt;
explicitly blocked commands issued by the user. If we implemented something&lt;br/&gt;
more complex now, it wouldn&apos;t carry over to RT because the DWPTs don&apos;t require&lt;br/&gt;
flushing to search on them. Which leads to the main drawback probably being for&lt;br/&gt;
NRT, eg, get reader. Hmm... In that case a stop the world flush does affect&lt;br/&gt;
overall indexing performance. Perhaps we can add flush and not block all DWPTs&lt;br/&gt;
in a separate issue after the DWPT branch is merged to trunk, if there&apos;s user&lt;br/&gt;
need?  Or perhaps it&apos;s easy to implement, I&apos;m still trying to get a feel for the&lt;br/&gt;
lock progression in the branch. &lt;/p&gt;

&lt;p&gt;In the indexing many documents case, the DWPTs&apos;ll be flushed by the tiered RAM&lt;br/&gt;
system. It&apos;s the bulk add case where we don&apos;t want to block all threads/DWPTs&lt;br/&gt;
at once, eg, I think our main goal is to fix Mike&apos;s performance test, with NRT&lt;br/&gt;
being secondary or even a distraction.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;But for b) and c) it&apos;s unclear what should happen if a DWPT flush fails&lt;br/&gt;
after some completed already successfully before.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Right, all that&apos;d be solved if we bulk moved IW to a Scala-like asynchronous&lt;br/&gt;
queuing model. However it&apos;s probably a bit too much to do right now. Perhaps in&lt;br/&gt;
the bulk add-many-docs case we&apos;ll need a callback for errors? No because the&lt;br/&gt;
add doc method call that triggers the flush will report any exception(s).&lt;/p&gt;</comment>
                    <comment id="12978516" author="jasonrutherglen" created="Thu, 6 Jan 2011 21:25:05 +0000"  >&lt;p&gt;When we&apos;re OOMing the ram used reported by DW is 0.  We&apos;re probably not adding to the RAM used value somewhere.&lt;/p&gt;</comment>
                    <comment id="12978563" author="jasonrutherglen" created="Thu, 6 Jan 2011 23:11:18 +0000"  >&lt;p&gt;RAM accounting is slightly improved, we had two variables in DW keeping track of it.  The extraneous is removed, however we&apos;re still OOMing in: ant test-core -Dtestcase=TestIndexWriter -Dtestmethod=testIndexingThenDeleting&lt;/p&gt;</comment>
                    <comment id="12979129" author="mikemccand" created="Sat, 8 Jan 2011 14:00:14 +0000"  >&lt;blockquote&gt;&lt;p&gt;I guess we don&apos;t really need the global lock. A thread performing the &quot;global flush&quot; could still acquire each thread state before it starts flushing, but return a threadState to the pool once that particular threadState is done flushing?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Good question... we could (in theory) also flush them concurrently?  But, since we don&apos;t &quot;own&quot; the threads in IW, we can&apos;t easily do that, so I think no global lock, go through all DWPTs w/ current thread and flush, sequentially?  So all that&apos;s guaranteed after the global flush() returns is that all state present prior to when flush() is invoked, is moved to disk.  Ie if addDocs are still happening concurrently then the DWPTs will start filling up again even while the &quot;global flush&quot; runs.  That&apos;s fine.&lt;/p&gt;

&lt;blockquote&gt;

&lt;p&gt;A related question is: Do we want to piggyback on multiple threads when a global flush happens? Eg. Thread 1 called commit, Thread 2 shortly afterwards addDocument(). When should addDocument() happen? &lt;br/&gt;
a) After all DWPTs finished flushing? &lt;br/&gt;
b) After at least one DWPT finished flushing and is available again?&lt;br/&gt;
c) Or should Thread 2 be used to help flushing DWPTs in parallel with Thread 1?&lt;/p&gt;

&lt;p&gt;a) is currently implemented, but I think not really what we want.&lt;br/&gt;
b) is probably best for RT, because it means the lowest indexing latency for the new document to be added.&lt;br/&gt;
c) probably means the best overall throughput (depending even on hardware like disk speed, etc)&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think start simple &amp;#8211; the addDocument always happens?  Ie it&apos;s never coordinated w/ the ongoing flush.  It picks a free DWPT like normal, and since flush is single threaded, there should always be a free DWPT?&lt;/p&gt;

&lt;p&gt;Longer term c) would be great, or, if IW has an ES then it&apos;d send multiple flush jobs to the ES.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;For whatever option we pick, we&apos;ll have to carefully think about error handling. It&apos;s quite straightforward for a) (just commit all flushed segments to SegmentInfos when the global flush completed succesfully). But for b) and c) it&apos;s unclear what should happen if a DWPT flush fails after some completed already successfully before.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think we should continue what we do today?  Ie, if it&apos;s an &apos;aborting&apos; exception, then the entire segment held by that DWPT is discarded?  And we then throw this exc back to caller (and don&apos;t try to flush any other segments)?&lt;/p&gt;</comment>
                    <comment id="12979138" author="jasonrutherglen" created="Sat, 8 Jan 2011 15:32:17 +0000"  >&lt;blockquote&gt;&lt;p&gt;So all that&apos;s guaranteed after the global flush() returns is that all&lt;br/&gt;
state present prior to when flush() is invoked, is moved to disk. Ie if addDocs&lt;br/&gt;
are still happening concurrently then the DWPTs will start filling up again&lt;br/&gt;
even while the &quot;global flush&quot; runs. That&apos;s fine.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;What if the user wants a guaranteed hard flush of all state up to the point of&lt;br/&gt;
the flush call (won&apos;t they want this sometimes with getReader)? If we&apos;re&lt;br/&gt;
flushing sequentially (without pausing all threads) we&apos;re removing that? Maybe&lt;br/&gt;
we&apos;ll need to give the option of global lock/stop or sequential flush?&lt;/p&gt;

&lt;p&gt;Also I think we need to clear the thread bindings of a DWPT just prior to the&lt;br/&gt;
flush of the DWPT? Otherwise (when multiple threads are mapped to a single&lt;br/&gt;
DWPT) the other threads will wait on the &lt;span class=&quot;error&quot;&gt;&amp;#91;main&amp;#93;&lt;/span&gt; DWPT flush when they should be&lt;br/&gt;
spinning up a new DWPT? &lt;/p&gt;

&lt;p&gt;Then, what happens to reusing the DWPT if we&apos;re flushing it, and we spin a new&lt;br/&gt;
DWPT (effectively replacing the old DWPT), eg, we&apos;re going to lose the byte[]&lt;br/&gt;
recycling? Maybe we need to and share and sync the byte[] pooling between DWPTs&lt;br/&gt;
or will that noticeably affect indexing performance? &lt;/p&gt;</comment>
                    <comment id="12979139" author="jasonrutherglen" created="Sat, 8 Jan 2011 15:35:14 +0000"  >&lt;p&gt;Also, don&apos;t we need the global lock for commit/close?&lt;/p&gt;</comment>
                    <comment id="12979146" author="mikemccand" created="Sat, 8 Jan 2011 16:17:38 +0000"  >&lt;blockquote&gt;
&lt;p&gt;What if the user wants a guaranteed hard flush of all state up to the point of&lt;br/&gt;
the flush call (won&apos;t they want this sometimes with getReader)? If we&apos;re&lt;br/&gt;
flushing sequentially (without pausing all threads) we&apos;re removing that? Maybe&lt;br/&gt;
we&apos;ll need to give the option of global lock/stop or sequential flush?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;What&apos;s a &quot;hard flush&quot;?&lt;/p&gt;

&lt;p&gt;With the proposed approach, all docs added (or in the process of being added) will make it into the flushed segments once the flush returns; newly added docs after the flush call started may or not make it.  But this is fine?  I mean, if the app has stronger requirements then it should externally sync?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Also I think we need to clear the thread bindings of a DWPT just prior to the flush of the DWPT? &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Right.&lt;/p&gt;

&lt;p&gt;As soon as a DWPT is pulled from production for flushing, it loses all thread affinity and becomes unavailable until its flush finishes.  When a thread needs a DWPT, it tries to pick the one it last had (affinity) but if that one&apos;s busy, it picks a new one.  If none are available but we are below our max DWPT count, it spins up a new one?&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Then, what happens to reusing the DWPT if we&apos;re flushing it, and we spin a new&lt;br/&gt;
DWPT (effectively replacing the old DWPT), eg, we&apos;re going to lose the byte[]&lt;br/&gt;
recycling?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Why would we lose them?  Wouldn&apos;t that DWPT just go back into rotation once the flush is done?&lt;/p&gt;</comment>
                    <comment id="12979149" author="jasonrutherglen" created="Sat, 8 Jan 2011 16:36:46 +0000"  >&lt;blockquote&gt;&lt;p&gt;As soon as a DWPT is pulled from production for flushing, it loses all thread affinity and becomes unavailable until its flush finishes. When a thread needs a DWPT, it tries to pick the one it last had (affinity) but if that one&apos;s busy, it picks a new one. If none are available but we are below our max DWPT count, it spins up a new one?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Right.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;With the proposed approach, all docs added (or in the process of being added) will make it into the flushed segments once the flush returns; newly added docs after the flush call started may or not make it. But this is fine? I mean, if the app has stronger requirements then it should externally sync?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Ok.  The proposed change is simply the thread calling add doc will flush it&apos;s DWPT if needed, take it offline while doing so, and return it when completed.  I think the risk is a new DWPT likely will have been created during flush, which&apos;d make the returning DWPT inutile?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Why would we lose them? Wouldn&apos;t that DWPT just go back into rotation once the flush is done?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes, we just need to change the existing code a bit then.&lt;/p&gt;

&lt;p&gt;However I think we may still need the global lock for close, eg, today we&apos;re preventing the user from adding docs during close, after this issue is merged that behavior would change?  &lt;/p&gt;</comment>
                    <comment id="12979162" author="jasonrutherglen" created="Sat, 8 Jan 2011 17:35:18 +0000"  >&lt;p&gt;And there&apos;s the case of the thread calling flush doesn&apos;t yet have a DWPT, it&apos;s going to need to get one assigned to it, however the one assigned may not be the max ram consumer.  What&apos;ll we do then?  If the user explicitly called flush we can a) do nothing b) flush (the max ram consumer) thread&apos;s DWPT, however that gets hairy with wait notifies (almost like the global lock?).&lt;/p&gt;</comment>
                    <comment id="12979189" author="mikemccand" created="Sat, 8 Jan 2011 19:45:02 +0000"  >&lt;blockquote&gt;&lt;p&gt;The proposed change is simply the thread calling add doc will flush it&apos;s DWPT if needed, take it offline while doing so, and return it when completed.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Wait &amp;#8211; this is the &quot;addDocument&quot; case right?  (I thought we were still talking about the &quot;flush the world&quot; case...).&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I think the risk is a new DWPT likely will have been created during flush, which&apos;d make the returning DWPT inutile?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;A new DWPT will have been created only if more than one thread is indexing docs right?  In which case this is fine?  Ie the old DWPT (just flushed) will just go back into rotation, and when another thread comes in it can take it?&lt;/p&gt;

&lt;p&gt;But, you&apos;re right: maybe we should sometimes &quot;prune&quot; DWPTs.  Or simply stop recycling any RAM, so that a just-flushed DWPT is an empty shell.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;However I think we may still need the global lock for close, eg, today we&apos;re preventing the user from adding docs during close, after this issue is merged that behavior would change?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Well, the threads still adding docs will hit AlreadyClosedException?  (But, that&apos;s just &quot;best effort&quot;).  The behavior of calling IW.close while other threads are still adding docs has never been defined (and, shouldn&apos;t be) except that we won&apos;t corrupt your index, and we&apos;ll get all docs indexed before .close was called, committed.  So I think even for this case we don&apos;t need a global lock.&lt;/p&gt;</comment>
                    <comment id="12979190" author="mikemccand" created="Sat, 8 Jan 2011 19:46:47 +0000"  >&lt;blockquote&gt;
&lt;p&gt;And there&apos;s the case of the thread calling flush doesn&apos;t yet have a DWPT, it&apos;s going to need to get one assigned to it, however the one assigned may not be the max ram consumer. What&apos;ll we do then? If the user explicitly called flush we can a) do nothing b) flush (the max ram consumer) thread&apos;s DWPT, however that gets hairy with wait notifies (almost like the global lock?).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Wait &amp;#8211; why would the thread calling flush need to have a DWPT assigned to it?  You&apos;re talking about the &quot;flush the world&quot; case?  (Ie the app calls IW.commit or IW.getReader).  In this case the thread just one by one pulls all DWPTs that have any indexed docs out of production, flushes them, clears them, and returns them to production?&lt;/p&gt;</comment>
                    <comment id="12979229" author="jasonrutherglen" created="Sat, 8 Jan 2011 23:00:39 +0000"  >&lt;blockquote&gt;&lt;p&gt;the &quot;flush the world&quot; case? (Ie the app calls IW.commit or&lt;br/&gt;
IW.getReader). In this case the thread just one by one pulls all DWPTs that&lt;br/&gt;
have any indexed docs out of production, flushes them, clears them, and returns&lt;br/&gt;
them to production?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The 2 cases are: A) Flush every DWPT sequentually (aka flush the world) and &lt;br/&gt;
B) flush by RAM usage when adding docs or deleting. A is clear! I think with B&lt;br/&gt;
we&apos;re saying even if the calling thread is bound to DWPT #1, if DWPT #2 is&lt;br/&gt;
greater in size and the aggregate RAM usage exceeds the max, using the calling&lt;br/&gt;
thread, we take DWPT #2 out of production, flush, and return it?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;The behavior of calling IW.close while other threads are still adding&lt;br/&gt;
docs has never been defined (and, shouldn&apos;t be) except that we won&apos;t corrupt&lt;br/&gt;
your index, and we&apos;ll get all docs indexed before .close was called, committed.&lt;br/&gt;
So I think even for this case we don&apos;t need a global lock.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Great, that simplifies and clarifies that we do not require a global lock.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;But, you&apos;re right: maybe we should sometimes &quot;prune&quot; DWPTs. Or simply&lt;br/&gt;
stop recycling any RAM, so that a just-flushed DWPT is an empty shell.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I&apos;m not sure how we&apos;d prune, typically object pools have a separate eviction&lt;br/&gt;
thread, I think that&apos;s going overboard? Maybe we can simply throw out the DWPT&lt;br/&gt;
and put recycling byte[]s and/or pooling DWPTs back in later if it&apos;s necessary?&lt;/p&gt;
</comment>
                    <comment id="12979243" author="jasonrutherglen" created="Sun, 9 Jan 2011 00:53:45 +0000"  >&lt;p&gt;To further clarify, we also no longer have global aborts?  Each abort only applies to an individual DWPT?  &lt;/p&gt;</comment>
                    <comment id="12979247" author="michaelbusch" created="Sun, 9 Jan 2011 01:22:58 +0000"  >&lt;blockquote&gt;&lt;p&gt;I think the risk is a new DWPT likely will have been created during flush, which&apos;d make the returning DWPT inutile.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The DWPT will not be removed from the pool, just marked as busy during flush, like as its state is busy (or currently called &quot;non-idle&quot; in the code) during addDocumentI().  So no new DWPT would be created during flush if the maxThreadState limit was already reached.&lt;/p&gt;

</comment>
                    <comment id="12979248" author="michaelbusch" created="Sun, 9 Jan 2011 01:31:25 +0000"  >&lt;blockquote&gt;
&lt;p&gt;I think start simple - the addDocument always happens? Ie it&apos;s never coordinated w/ the ongoing flush. It picks a free DWPT like normal, and since flush is single threaded, there should always be a free DWPT?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yeah I agree.  The change I&apos;ll make then is to not have the global lock and return a DWPT immediately to the pool and set it to &apos;idle&apos; after its flush completed.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;I think we should continue what we do today? Ie, if it&apos;s an &apos;aborting&apos; exception, then the entire segment held by that DWPT is discarded? And we then throw this exc back to caller (and don&apos;t try to flush any other segments)?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;What I meant was the following situation: Suppose we have two DWPTs and IW.commit() is called.  The first DWPT finishes flushing successfully, is returned to the pool and idle again.  The second DWPT flush fails with an aborting exception.  Should the segment of the first DWPT make it into the index or not?  I think segment 1 shouldn&apos;t be committed, ie. a global flush should be all or nothing.  This means we would have to delay the commit of the segments until all DWPTs flushed successfully.&lt;/p&gt;</comment>
                    <comment id="12979252" author="jasonrutherglen" created="Sun, 9 Jan 2011 02:09:21 +0000"  >&lt;blockquote&gt;&lt;p&gt;I think segment 1 shouldn&apos;t be committed, ie. a global flush should be all or nothing. This means we would have to delay the commit of the segments until all DWPTs flushed successfully.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;If a DWPT aborts during flush, we simply throw an exception, however we still keep the successfully flushed segment(s).  If there&apos;s an abort on any DWPT during commit then we throw away any successfully flushed segments as well.  I think that makes sense, eg, all or nothing.&lt;/p&gt;</comment>
                    <comment id="12979308" author="mikemccand" created="Sun, 9 Jan 2011 12:00:57 +0000"  >&lt;blockquote&gt;
&lt;p&gt;I think with B&lt;br/&gt;
we&apos;re saying even if the calling thread is bound to DWPT #1, if DWPT #2 is&lt;br/&gt;
greater in size and the aggregate RAM usage exceeds the max, using the calling&lt;br/&gt;
thread, we take DWPT #2 out of production, flush, and return it?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Right &amp;#8211; the thread affinity has nothing to do with which thread gets to flush which DWPT.  Once flush is triggered, the thread doing the flushing is free to flush any DWPT.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Maybe we can simply throw out the DWPT&lt;br/&gt;
and put recycling byte[]s and/or pooling DWPTs back in later if it&apos;s necessary?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;OK let&apos;s start there and put back re-use only if we see a real perf issue?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;What I meant was the following situation: Suppose we have two DWPTs and IW.commit() is called. The first DWPT finishes flushing successfully, is returned to the pool and idle again. The second DWPT flush fails with an aborting exception. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Hmm, tricky.  I think I&apos;d lean towards keeping segment 1.  Discarding it would be inconsistent w/ aborts hit during the &quot;flushed by RAM&quot; case?  EG if seg 1 was flushed due to RAM usage, succeeds, and then later seg 2 is flushed due to RAM usage, but aborts.  In this case we would still keep seg 1?&lt;/p&gt;

&lt;p&gt;I think aborting a flush should only lose the docs in that one DWPT (as it is today).&lt;/p&gt;

&lt;p&gt;Remember, a call to commit may succeed in flushing seg 1 to disk, and updating the in-memory segment infos, but on hitting the aborting exc to seg 2, will throw that to the caller, not having committed &lt;b&gt;any&lt;/b&gt; change to the index.  Exceptions thrown during the prepareCommit (phase 1) part of commit mean nothing is changed in the index.&lt;/p&gt;

&lt;p&gt;Alternatively... we could abort the entire IW session (as eg we handle OOME today) if ever an aborting exception was hit?  This might be cleaner?  But it&apos;s really a &quot;nuke the world&quot; option which scares me.  EG it could be a looong indexing session (app doesn&apos;t call commit() until the end) and we could be throwing away &lt;b&gt;alot&lt;/b&gt; of progress.&lt;/p&gt;</comment>
                    <comment id="12979382" author="jasonrutherglen" created="Sun, 9 Jan 2011 18:01:07 +0000"  >&lt;blockquote&gt;&lt;p&gt;Once flush is triggered, the thread doing the flushing is free to flush any DWPT.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;OK.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;OK let&apos;s start there and put back re-use only if we see a real perf issue?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think that&apos;s best.  Balancing RAM isn&apos;t implemented in the branch, we can&apos;t predict the future usage of DWPT(s) (which could languish consuming RAM with byte[]s well after they&apos;re flushed due to a sudden drop in the number of calling threads external to IW).&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;But it&apos;s really a &quot;nuke the world&quot; option which scares me. EG it could be a looong indexing session (app doesn&apos;t call commit() until the end) and we could be throwing away alot of progress.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Right.  Another option is to on commit try to flush all segments, meaning even if one DWPT/segment aborts, continue on with the other DWPTs (ie, a best effort).  Then perhaps throw an exception with a report of which segment flushes succeeded, or simply return a report object detailing what happened during commit (somewhat expert usage though).  Either way I think we need to give a few options to the user, then choose a default and see if it sticks.  The default should probably be &quot;best effort&quot;.&lt;/p&gt;
</comment>
                    <comment id="12979558" author="mikemccand" created="Mon, 10 Jan 2011 12:06:07 +0000"  >&lt;p&gt;I think on commit if we hit an aborting exception flushing a given DWPT, we throw it then &amp;amp; there.&lt;/p&gt;

&lt;p&gt;Any segs already flushed remain flushed (but not committed).  Any segs not yet flushed remain not yet flushed...&lt;/p&gt;</comment>
                    <comment id="12979625" author="jasonrutherglen" created="Mon, 10 Jan 2011 16:26:12 +0000"  >&lt;blockquote&gt;&lt;p&gt;Any segs already flushed remain flushed (but not committed). Any segs not yet flushed remain not yet flushed...&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;If the segment are flushed, then they will be deleted?  Or they will be made available in a subsequent and completely successful commit?&lt;/p&gt;</comment>
                    <comment id="12979649" author="michaelbusch" created="Mon, 10 Jan 2011 17:10:15 +0000"  >&lt;blockquote&gt;&lt;p&gt;Longer term c) would be great, or, if IW has an ES then it&apos;d send multiple flush jobs to the ES.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Lost in abbreviations &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; - Can you remind me what &apos;ES&apos; is?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;But, you&apos;re right: maybe we should sometimes &quot;prune&quot; DWPTs. Or simply stop recycling any RAM, so that a just-flushed DWPT is an empty shell.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I&apos;m not sure I understand what the problem here with recycling RAM is.  Could someone elaborate?&lt;/p&gt;</comment>
                    <comment id="12979654" author="michaelbusch" created="Mon, 10 Jan 2011 17:16:16 +0000"  >&lt;blockquote&gt;&lt;p&gt;I think aborting a flush should only lose the docs in that one DWPT (as it is today).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yeah I&apos;m convinced now I don&apos;t want the &quot;nuke the world&quot; approach.  Btw, Mike, you&apos;re very good with giving things intuitive names &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;


&lt;blockquote&gt;&lt;p&gt;I think on commit if we hit an aborting exception flushing a given DWPT, we throw it then &amp;amp; there.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes sounds good.&lt;/p&gt;


&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;Any segs already flushed remain flushed (but not committed). Any segs not yet flushed remain not yet flushed...&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;If the segment are flushed, then they will be deleted? Or they will be made available in a subsequent and completely successful commit?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The aborting exception might be thrown due to a disk-full situation.  This can be fixed and commit() called again, which then would flush the remaining DWPTs and commit all flushed segments.&lt;br/&gt;
Otherwise, those flushed segments will be orphaned and deleted sometime later by a different IW because they don&apos;t belong to any SegmentInfos.&lt;/p&gt;</comment>
                    <comment id="12979655" author="jasonrutherglen" created="Mon, 10 Jan 2011 17:17:21 +0000"  >&lt;blockquote&gt;&lt;p&gt;Lost in abbreviations  - Can you remind me what &apos;ES&apos; is?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I read it as ExecutorService, ie, a thread pool.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I&apos;m not sure I understand what the problem here with recycling RAM is. Could someone elaborate?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Mainly that we could have DWPT(s) lying around unused, consuming &lt;span class=&quot;error&quot;&gt;&amp;#91;recycled&amp;#93;&lt;/span&gt; RAM, eg, from a sudden drop in the number of incoming threads after a flush.  This is a drop the code, and put it back in if that was a bad idea solution.&lt;/p&gt;
</comment>
                    <comment id="12979671" author="michaelbusch" created="Mon, 10 Jan 2011 18:16:29 +0000"  >&lt;blockquote&gt;
&lt;p&gt;Mainly that we could have DWPT(s) lying around unused, consuming &lt;span class=&quot;error&quot;&gt;&amp;#91;recycled&amp;#93;&lt;/span&gt; RAM, eg, from a sudden drop in the number of incoming threads after a flush. This is a drop the code, and put it back in if that was a bad idea solution.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Ah thanks, got it.  &lt;/p&gt;


&lt;blockquote&gt;&lt;p&gt;Or simply stop recycling any RAM, so that a just-flushed DWPT is an empty shell.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;+1&lt;/p&gt;</comment>
                    <comment id="12979713" author="mikemccand" created="Mon, 10 Jan 2011 19:50:11 +0000"  >&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;Lost in abbreviations - Can you remind me what &apos;ES&apos; is?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I read it as ExecutorService, ie, a thread pool.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes, sorry that&apos;s what I meant.&lt;/p&gt;

&lt;p&gt;Ie someday IW can take an ES too and farm things out to it when it could make use of concurrency (like flush the world).  But that&apos;s for later &amp;lt;/dream&amp;gt;.&lt;/p&gt;</comment>
                    <comment id="12981192" author="michaelbusch" created="Thu, 13 Jan 2011 09:33:36 +0000"  >&lt;p&gt;I made some progress with the concurrency model, especially removing the need for various locks to make everything easier.&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;DocumentsWriterPerThreadPool.ThreadState now extends ReentrantLock, which means that standard methods like lock() and unlock() can be used to reserve a DWPT for a task.&lt;/li&gt;
	&lt;li&gt;The max. number of DWPTs allowed (config.maxThreadStates) is instantiated up-front.  Creating a DWPT is cheap, so this is not a performance concern; this makes it easier to push config changes to the DWPTs without synchronizing on the pool and without having to worry about newly created DWPTs getting the same config settings.&lt;/li&gt;
	&lt;li&gt;DocumentsWriterPerThreadPool.getActivePerThreadsIterator() gives the caller a static snapshot of the active DWPTs at the time the iterator was acquired, e.g. for flushAllThreads() or DW.abort().  Here synchronizing on the pool isn&apos;t necessary either.&lt;/li&gt;
	&lt;li&gt;deletes are now pushed to DW.pendingDeletes() if no active DWPTs are present.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;TODOs:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;fix remaining testcases that still fail&lt;/li&gt;
	&lt;li&gt;fix RAM tracking and flush-by-RAM&lt;/li&gt;
	&lt;li&gt;write new testcases to test thread pool, thread assignment, etc&lt;/li&gt;
	&lt;li&gt;review if all cases that were discussed in the recent comments here work as expected (likely not &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; )&lt;/li&gt;
	&lt;li&gt;performance testing and code cleanup&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="12981305" author="jasonrutherglen" created="Thu, 13 Jan 2011 16:06:23 +0000"  >&lt;blockquote&gt;&lt;p&gt;DocumentsWriterPerThreadPool.ThreadState now extends ReentrantLock, which means that standard methods like lock() and unlock() can be used to reserve a DWPT for a task.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Really?  That makes synchronized seem simpler?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;the max. number of DWPTs allowed (config.maxThreadStates) is instantiated up-front. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;What about the memory used, eg, the non-use of byte[] recycling?  I guess it&apos;ll be cleared on flush.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;fix RAM tracking and flush-by-RAM&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I created a BytesUsed object that cascades the changes to parent BytesUsed objects, this allows each individual SD, DWPT, DW, etc to keep track of their bytes used, while also propagating the changes to the higher level objects, eg, SD -&amp;gt; DWPT, DWPT -&amp;gt; DW.&lt;/p&gt;</comment>
                    <comment id="12981380" author="michaelbusch" created="Thu, 13 Jan 2011 17:21:22 +0000"  >&lt;blockquote&gt;&lt;p&gt;Really?  That makes synchronized seem simpler?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Well look at ThreadAffinityDocumentsWriterThreadPool.  There I&apos;m able to use things like tryLock() and getQueueLength().&lt;br/&gt;
Also DocumentsWriterPerThreadPool has a getAndLock() method, that can be used by DW for addDocument(), whereas DW.flush(), which needs to iterate the DWPTs, can lock the individual DWPTs directly.  I think it&apos;s simpler, but I&apos;m open to other suggestions of course &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;


&lt;blockquote&gt;&lt;p&gt;What about the memory used, eg, the non-use of byte[] recycling? I guess it&apos;ll be cleared on flush.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yeah, sure.  That is independent on whether they&apos;re all created upfront or not.  But yeah, after flush or abort we need to clear the DWPT&apos;s state to make sure they&apos;re not consuming unused RAM (as you described in your earlier comment).&lt;/p&gt;</comment>
                    <comment id="12981388" author="earwin" created="Thu, 13 Jan 2011 17:36:46 +0000"  >&lt;p&gt;Maan, this comment list is infinite.&lt;br/&gt;
How do I currently get the ..er.. current version? Latest branch + latest Jason&apos;s patch?&lt;/p&gt;

&lt;p&gt;Regardless of everything else, I&apos;d ask you not to extend random things &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; at least if you can&apos;t say is-a about them.&lt;br/&gt;
DocumentsWriterPerThreadPool.ThreadState IS A ReentrantLock? No. So you&apos;re better off encapsulating it rather than extending.&lt;br/&gt;
Same can be applied to SegmentInfos that extends Vector :/&lt;/p&gt;</comment>
                    <comment id="12981390" author="michaelbusch" created="Thu, 13 Jan 2011 17:55:53 +0000"  >&lt;blockquote&gt;&lt;p&gt;How do I currently get the ..er.. current version?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Just do &apos;svn up&apos; on the RT branch.&lt;/p&gt;


&lt;blockquote&gt;&lt;p&gt;Regardless of everything else, I&apos;d ask you not to extend random things&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This was a conscious decision, not random.  Extending ReentrantLock is not an uncommon pattern, e.g. ConcurrentHashMap.Segment does exactly that.  ThreadState basically is nothing but a lock that has a reference to the corresponding DWPT it protects.&lt;/p&gt;

&lt;p&gt;I encourage you to look at the code.&lt;/p&gt;</comment>
                    <comment id="12981519" author="jasonrutherglen" created="Thu, 13 Jan 2011 22:33:30 +0000"  >&lt;blockquote&gt;&lt;p&gt;look at ThreadAffinityDocumentsWriterThreadPool. There I&apos;m able to use things like tryLock() and getQueueLength().&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Makes sense, I had only read the DocumentsWriterPerThreadPool part.&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;DWPT.perDocAllocator and freeLevel can be removed?&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;DWPT&apos;s RecyclingByteBlockAllocator -&amp;gt; DirectAllocator?&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Looks like the deletes handling is updated to the patch&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;I don&apos;t think we need FlushControl anymore as the RAM tracking should occur in DW and there&apos;s no need for IW to &lt;span class=&quot;error&quot;&gt;&amp;#91;globally&amp;#93;&lt;/span&gt; wait for flushes.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;The locking is more clear now, I can see DW.updateDocument locks the threadstate as does flushAllThreads.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I&apos;ll reincorporate the RAM tracking, and then will try the unit tests again.  I&apos;m curious if the file not found errors are gone.&lt;/p&gt;</comment>
                    <comment id="12981548" author="michaelbusch" created="Thu, 13 Jan 2011 23:14:54 +0000"  >&lt;blockquote&gt;&lt;p&gt;DWPT.perDocAllocator and freeLevel can be removed?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;done.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;DWPT&apos;s RecyclingByteBlockAllocator -&amp;gt; DirectAllocator?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;done. Also removed more recycling code.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I don&apos;t think we need FlushControl anymore as the RAM tracking should occur in DW and there&apos;s no need for IW to &lt;span class=&quot;error&quot;&gt;&amp;#91;globally&amp;#93;&lt;/span&gt; wait for flushes.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I removed flushControl from DW.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I&apos;m curious if the file not found errors are gone.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think there&apos;s something wrong with TermVectors - several related test cases fail. We need to investigate more.&lt;/p&gt;</comment>
                    <comment id="12981563" author="jasonrutherglen" created="Thu, 13 Jan 2011 23:54:40 +0000"  >&lt;blockquote&gt;&lt;p&gt;I think there&apos;s something wrong with TermVectors - several related test cases fail. We need to investigate more.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes, there are far more than the last revision!  I don&apos;t think it&apos;s just TV however.&lt;/p&gt;</comment>
                    <comment id="12981581" author="jasonrutherglen" created="Fri, 14 Jan 2011 00:40:51 +0000"  >&lt;p&gt;Here&apos;s the latest test.out.  There&apos;s a lot of these:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
[junit] junit.framework.AssertionFailedError: IndexFileDeleter doesn&apos;t know about file _5.fdt
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1156)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1088)
    [junit] 	at org.apache.lucene.index.IndexWriter.filesExist(IndexWriter.java:3273)
    [junit] 	at org.apache.lucene.index.IndexWriter.startCommit(IndexWriter.java:3321)
    [junit] 	at org.apache.lucene.index.IndexWriter.prepareCommit(IndexWriter.java:2339)
    [junit] 	at org.apache.lucene.index.IndexWriter.commitInternal(IndexWriter.java:2410)
    [junit] 	at org.apache.lucene.index.IndexWriter.closeInternal(IndexWriter.java:1083)
    [junit] 	at org.apache.lucene.index.IndexWriter.close(IndexWriter.java:1027)
    [junit] 	at org.apache.lucene.index.IndexWriter.close(IndexWriter.java:991)
    [junit] 	at org.apache.lucene.index.TestAddIndexes.testMergeAfterCopy(TestAddIndexes.java:432)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                    <comment id="12981827" author="jasonrutherglen" created="Fri, 14 Jan 2011 17:06:23 +0000"  >&lt;p&gt;I&apos;m taking a guess here, however the ThreadAffinityDocumentsWriterThreadPool.getAndLock method looks a little suspicious as we&apos;re iterating on ThreadStates and on a non-concurrent hashmap calling put while not in a lock?  &lt;/p&gt;</comment>
                    <comment id="12981830" author="jasonrutherglen" created="Fri, 14 Jan 2011 17:12:29 +0000"  >&lt;p&gt;Also multiple threads can call DocumentsWriterPerThread.addDocument and that&apos;s resulting in this:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;[junit] java.lang.AssertionError: omitTermFreqAndPositions:&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt; postings.docFreqs[termID]:0
    [junit]     at org.apache.lucene.index.FreqProxTermsWriterPerField.addTerm(FreqProxTermsWriterPerField.java:143)
    [junit]     at org.apache.lucene.index.TermsHashPerField.add(TermsHashPerField.java:234)
    [junit]     at org.apache.lucene.index.DocInverterPerField.processFields(DocInverterPerField.java:91)
    [junit]     at org.apache.lucene.index.DocFieldProcessor.processDocument(DocFieldProcessor.java:274)
    [junit]     at org.apache.lucene.index.DocumentsWriterPerThread.addDocument(DocumentsWriterPerThread.java:184)
    [junit]     at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:374)
    [junit]     at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1403)
    [junit]     at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1375)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                    <comment id="12981832" author="michaelbusch" created="Fri, 14 Jan 2011 17:20:38 +0000"  >&lt;blockquote&gt;&lt;p&gt;as we&apos;re iterating on ThreadStates and on a non-concurrent hashmap calling put while not in a lock? &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The threadBindings hashmap is a ConcurrentHashMap and the getActivePerThreadsIterator() is threadsafe I believe.&lt;/p&gt;</comment>
                    <comment id="12981839" author="jasonrutherglen" created="Fri, 14 Jan 2011 17:34:58 +0000"  >&lt;blockquote&gt;&lt;p&gt;The threadBindings hashmap is a ConcurrentHashMap and the getActivePerThreadsIterator() is threadsafe I believe.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Sorry yes CHM is used, it all looks thread safe, but there must be multiple threads accessing a single DWPT at the same time for some of these errors to be occurring.  &lt;/p&gt;</comment>
                    <comment id="12981895" author="jasonrutherglen" created="Fri, 14 Jan 2011 19:39:32 +0000"  >&lt;p&gt;Also, why are we always (well, likely) assigning the DWPT to a different thread state if tryLock returns false?  If there&apos;s a lot of contention (eg, far more incoming threads than DWPTs), then won&apos;t the thread assignation code become a hotspot?&lt;/p&gt;

&lt;p&gt;In ThreadAffinityDocumentsWriterThreadPool.clearThreadBindings(ThreadState perThread) we&apos;re actually clearing the entire map.  When this&apos;s called in IW.flush (which is unsynced on IW), if there are multiple concurrent flushes, then perhaps a single DWPT is in use by multiple threads.  To safeguard against this and perhaps more easily add an assertion, maybe we should lock on the DWPT rather than ThreadState?&lt;/p&gt;</comment>
                    <comment id="12982200" author="michaelbusch" created="Sun, 16 Jan 2011 02:30:22 +0000"  >&lt;p&gt;I just committed fixes for some failing tests.  &lt;br/&gt;
Eg. the addIndexes() problem is now fixed.  The problem was that I had accidentally removed the following line in DW.addIndexes():&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
 &lt;span class=&quot;code-comment&quot;&gt;// Update SI appropriately
&lt;/span&gt; info.setDocStore(info.getDocStoreOffset(), newDsName, info.getDocStoreIsCompoundFile());
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;info.setDocStore() calls clearFiles(), which empties a SegmentInfo-local cache of all filenames that belong to the corresponding segment.  Since addIndexes() changes the segment name, it is important to refill that cache with the new file names.&lt;/p&gt;

&lt;p&gt;This was a sneaky bug.  We should probably call clearFiles() explicitly there in addIndexes().  For now I added a comment.&lt;/p&gt;</comment>
                    <comment id="12982204" author="jasonrutherglen" created="Sun, 16 Jan 2011 02:55:07 +0000"  >&lt;p&gt;Are you also merging trunk in as svn up yields a lot of updates.&lt;/p&gt;

&lt;p&gt;There are new test failures in: TestSnapshotDeletionPolicy&lt;/p&gt;</comment>
                    <comment id="12982205" author="jasonrutherglen" created="Sun, 16 Jan 2011 03:01:03 +0000"  >&lt;p&gt;The TestStressIndexing2 errors remind me of what I saw when working on &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2680&quot; title=&quot;Improve how IndexWriter flushes deletes against existing segments&quot;&gt;&lt;del&gt;LUCENE-2680&lt;/del&gt;&lt;/a&gt;.  I&apos;ll take a look.  They weren&apos;t there in the previous revisions of this branch.&lt;/p&gt;</comment>
                    <comment id="12982213" author="jasonrutherglen" created="Sun, 16 Jan 2011 03:41:02 +0000"  >&lt;p&gt;In DW.flushAllThreads we&apos;re accessing indexWriter.segmentInfos while we&apos;re not synced on IW, so the segment infos vector may be changing as we&apos;re accessing it.  I&apos;m not sure how we can reasonably solve this, I don&apos;t think cloning segment infos will work.  In trunk, doFlush is sync&apos;ed on IW and so doesn&apos;t run into these problems.  Perhaps for the flush all threads case we should simply sync on IW?&lt;/p&gt;</comment>
                    <comment id="12982220" author="jasonrutherglen" created="Sun, 16 Jan 2011 04:39:15 +0000"  >&lt;p&gt;DW.deleteTerms iterates on DWPTs without acquiring the ThreadState.lock, instead DWPT.deleteTerms is synced (on DWPT).  I think if a flush is occurring then deletes can get in at the same time?  I don&apos;t think BufferedDeletes supports that?&lt;/p&gt;</comment>
                    <comment id="12982360" author="jasonrutherglen" created="Sun, 16 Jan 2011 18:53:17 +0000"  >&lt;p&gt;I removed ThreadState and DWPT now extends ReentrantLock.  &lt;/p&gt;

&lt;p&gt;I added assertions such as delete terms assert numDocsInRAM &amp;gt; 0; which fails.  I think we should focus on cleaning up the locking, ie, do we need to use synchronized in DWPT?  Perhaps everything should use the RL?&lt;/p&gt;</comment>
                    <comment id="12982364" author="jasonrutherglen" created="Sun, 16 Jan 2011 19:09:19 +0000"  >&lt;p&gt;I just noticed this.  I think this line outside of any locking is probably not good for the concurrency of updateDoc.  Meaning all kinds of things can sneak in like flushes, before we get to adding the delete to all DWPTs?  This&apos;s part of what was tricky with &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2680&quot; title=&quot;Improve how IndexWriter flushes deletes against existing segments&quot;&gt;&lt;del&gt;LUCENE-2680&lt;/del&gt;&lt;/a&gt;, we had to keep the locking on DW for updateDoc.  Maybe to test this&apos;s an issue we can assert the count of the deletes, ala, FlushControl (which was added I think to ensure concurrency correctness)?&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
  &lt;span class=&quot;code-comment&quot;&gt;// delete term from other DWPTs later, so that &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; thread
&lt;/span&gt;  &lt;span class=&quot;code-comment&quot;&gt;// doesn&apos;t have to lock multiple DWPTs at the same time
&lt;/span&gt;  &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (delTerm != &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;) {
    deleteTerm(delTerm, perThread);
  }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                    <comment id="12982367" author="jasonrutherglen" created="Sun, 16 Jan 2011 19:33:13 +0000"  >&lt;p&gt;Maybe getAndLock should accept a delTerm, and lock on every non-flushing, non-aborting, numDocs &amp;gt; 0, DWPT, and add the delTerm to them, then unlock each locked DWPT?  This is analogous to how trunk adds the delTerm in the synced DW.getThreadState method?&lt;/p&gt;</comment>
                    <comment id="12982374" author="jasonrutherglen" created="Sun, 16 Jan 2011 20:14:57 +0000"  >&lt;p&gt;Here&apos;s a potential plan for the locking.  I think we still need a global lock for flush status, abort, and deletes.  DocumentsWriterPerThreadPool has the getAndLock method so maybe we should add a global lock on it.&lt;/p&gt;

&lt;p&gt;1) Add DWPTP.setFlushStatus(DWPT)&lt;br/&gt;
2) Add DWPTP.setAbortStatus(DWPT)&lt;br/&gt;
3) Add DWPTP.getFlushStatus(DWPT)&lt;br/&gt;
4) Add DWPTP.getAbortStatus(DWPT)&lt;br/&gt;
5) Add DWPTP.getAndLock(Thread requestingThread, DocumentsWriter documentsWriter, Document doc, Term delTerm)&lt;br/&gt;
6) Add DWPTP.&lt;span class=&quot;error&quot;&gt;&amp;#91;deleteTerm,deleteQuery&amp;#93;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Where each of these methods acquires a lock on DWPTP.  &lt;/p&gt;

&lt;p&gt;I think it&apos;ll look somewhat like FlushControl, however the common parameter will be an idx to the appropriate DWPT, eg, getFlushPending(int idx).  Alternatively instead of placing this in DWPTP, DW is another possible candidate.&lt;/p&gt;</comment>
                    <comment id="12982900" author="michaelbusch" created="Mon, 17 Jan 2011 22:23:33 +0000"  >&lt;p&gt;My last commit yesterday made almost all test cases pass.&lt;/p&gt;

&lt;p&gt;The ones that test flush-by-ram are still failing.  Also TestStressIndexing2 still fails.  The reason has to do with how deletes are pushed into bufferedDeletes.  E.g. if I call addDocument() instead of updateDocument() in TestStressIndexing.IndexerThread then the test passes. &lt;/p&gt;

&lt;p&gt;I need to look more into that problem, but otherwise it&apos;s looking good and we&apos;re pretty close!&lt;/p&gt;</comment>
                    <comment id="12982916" author="jasonrutherglen" created="Mon, 17 Jan 2011 23:33:44 +0000"  >&lt;p&gt;Very nice!  Looks like we needed all kinds of IW syncs?  I noticed that in addition to TestStressIndexing2, TestNRTThreads was also failing.  The attached patch fixes both by adding a sync on DW for deletes (and the update doc delete term).  Time to add the RAM usage?&lt;/p&gt;</comment>
                    <comment id="12982926" author="jasonrutherglen" created="Mon, 17 Jan 2011 23:57:27 +0000"  >&lt;p&gt;Looks like TestNRTThreads is still sometimes failing, if I moved the sync around then it passes and TestStressIndexing2 fails.  &lt;/p&gt;</comment>
                    <comment id="12983048" author="jasonrutherglen" created="Tue, 18 Jan 2011 07:05:01 +0000"  >&lt;p&gt;Ok, TestNRTThreads works after 10+ iterations.  TestStressIndexing2 works most of the time however with enough iterations, eg, &quot;ant test-core -Dtestcase=TestStressIndexing2 -Dtests.iter=30&quot; it fails.  I think that deletes are sneaking in because we&apos;re not sync&apos;ed on DW as we&apos;re flushing the DWPT.  Ideally some assertions would pick this up.&lt;/p&gt;</comment>
                    <comment id="12983154" author="mikemccand" created="Tue, 18 Jan 2011 11:30:59 +0000"  >&lt;p&gt;I ran a quick perf test here: I built the 10M Wikipedia index,&lt;br/&gt;
Standard codec, using 6 threads.  Trunk took 541.6 sec; RT took 518.2&lt;br/&gt;
sec (only a bit faster), but the test wasn&apos;t really fair because it&lt;br/&gt;
flushed @ docCount=12870.&lt;/p&gt;

&lt;p&gt;But I can&apos;t test flush by RAM &amp;#8211; that&apos;s not working yet on RT right?&lt;/p&gt;

&lt;p&gt;(The search results matched, which is nice!)&lt;/p&gt;

&lt;p&gt;Then I ran a single-threaded test.  Trunk took 1097.1 sec and RT took&lt;br/&gt;
1040.5 sec &amp;#8211; a bit faster!  Presumably in the noise (we don&apos;t expect&lt;br/&gt;
a speedup?), but excellent that it&apos;s not slower...&lt;/p&gt;

&lt;p&gt;I think we lost infoStream output on the details of flushing?  I can&apos;t&lt;br/&gt;
see when which DWPTs are flushing...&lt;/p&gt;</comment>
                    <comment id="12983209" author="jasonrutherglen" created="Tue, 18 Jan 2011 14:45:41 +0000"  >&lt;blockquote&gt;&lt;p&gt;I can&apos;t test flush by RAM - that&apos;s not working yet on RT right?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Right, we&apos;re only flushing by doc count, so we could be flushing segments that are too small?  However we can see some of the concurrency gains by not sync&apos;ing on IW and allowing documents updates to continue while flushing.&lt;/p&gt;</comment>
                    <comment id="12983246" author="michaelbusch" created="Tue, 18 Jan 2011 16:19:53 +0000"  >&lt;blockquote&gt;
&lt;p&gt;I ran a quick perf test here: I built the 10M Wikipedia index,&lt;br/&gt;
Standard codec, using 6 threads. Trunk took 541.6 sec; RT took 518.2&lt;br/&gt;
sec (only a bit faster), but the test wasn&apos;t really fair because it&lt;br/&gt;
flushed @ docCount=12870.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Thanks for running the tests!&lt;br/&gt;
Hmm that&apos;s a bit disappointing - we were hoping for more speedup.  &lt;br/&gt;
Flushing by docCount is currently per DWPT, so every initial segment&lt;br/&gt;
in your test had 12870 docs. I guess there&apos;s a lot of merging happening.&lt;/p&gt;

&lt;p&gt;Maybe you could rerun with higher docCount?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;But I can&apos;t test flush by RAM - that&apos;s not working yet on RT right?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;True.  I&apos;m going to add that soonish.  There&apos;s one thread-safety bug &lt;br/&gt;
related to deletes that needs to be fixed too.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Then I ran a single-threaded test. Trunk took 1097.1 sec and RT took&lt;br/&gt;
1040.5 sec - a bit faster! Presumably in the noise (we don&apos;t expect&lt;br/&gt;
a speedup?), but excellent that it&apos;s not slower...&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yeah I didn&apos;t expect much speedup - cool! &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;  Maybe because some &lt;br/&gt;
code is gone, like the WaitQueue, not sure how much overhead that &lt;br/&gt;
added in the single-threaded case.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;I think we lost infoStream output on the details of flushing? I can&apos;t&lt;br/&gt;
see when which DWPTs are flushing...&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Oh yeah, good point, I&apos;ll add some infoStream messages to DWPT!&lt;/p&gt;</comment>
                    <comment id="12983316" author="mikemccand" created="Tue, 18 Jan 2011 18:35:40 +0000"  >
&lt;p&gt;The branch is looking very nice!!  Very clean &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;Random comments:&lt;/p&gt;

&lt;p&gt;Why does DW.anyDeletions need to be sync&apos;d?&lt;/p&gt;

&lt;p&gt;Missing headers on at least DocumentsWriterPerThreadPool,&lt;br/&gt;
ThreadAffinityDWTP.&lt;/p&gt;

&lt;p&gt;IWC.setIndexerThreadPool&apos;s javadoc is stale.&lt;/p&gt;

&lt;p&gt;On ThreadAffinityDWTP... it may be better if we had a single queue,&lt;br/&gt;
where threads wait in line, if no DWPT is available?  And when a DWPT&lt;br/&gt;
finishes it then notifies any waiting threads?  (Ie, instead of queue-per-DWPT).&lt;/p&gt;

&lt;p&gt;I see the fieldInfos.update(dwpt.getFieldInfos()) (in&lt;br/&gt;
DW.updateDocument) &amp;#8211; is there a risk that two threads bring a new&lt;br/&gt;
field into existence at the same time, but w/ different config?  Eg&lt;br/&gt;
one doc omitsTFAP and the other doesn&apos;t?  Or, on flush, does each DWPT&lt;br/&gt;
use its private FieldInfos to correctly flush the segment?  (Hmm: do&lt;br/&gt;
we seed each DWPT w/ the original FieldInfos created by IW on init?).&lt;/p&gt;

&lt;p&gt;How are we handling the case of open IW, do delete-by-term but no&lt;br/&gt;
added docs?&lt;/p&gt;

&lt;p&gt;Does DW.pushDeletes really need to sync on IW?  BufferedDeletes is&lt;br/&gt;
sync&apos;d already.&lt;/p&gt;

&lt;p&gt;DW.substractFlushedDocs is mis-spelled (not sure it&apos;s used though).&lt;/p&gt;

&lt;p&gt;In DW.deleteTerms... shouldn&apos;t we skip a DWPT if it has no buffered&lt;br/&gt;
docs?&lt;/p&gt;</comment>
                    <comment id="12983346" author="michaelbusch" created="Tue, 18 Jan 2011 19:31:20 +0000"  >&lt;blockquote&gt;&lt;p&gt;Why does DW.anyDeletions need to be sync&apos;d?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Hmm good point.  Actually only the call to DW.pendingDeletes.any() needs to be synced, but not the loop that calls the DWPTs.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;In ThreadAffinityDWTP... it may be better if we had a single queue,&lt;br/&gt;
where threads wait in line, if no DWPT is available? And when a DWPT&lt;br/&gt;
finishes it then notifies any waiting threads? (Ie, instead of queue-per-DWPT).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Whole foods instead of safeway? &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;br/&gt;
Yeah that would be fairer.  A large doc (= a full cart) wouldn&apos;t block unlucky other docs.  I&apos;ll make that change, good idea!&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;I see the fieldInfos.update(dwpt.getFieldInfos()) (in&lt;br/&gt;
DW.updateDocument) - is there a risk that two threads bring a new&lt;br/&gt;
field into existence at the same time, but w/ different config? Eg&lt;br/&gt;
one doc omitsTFAP and the other doesn&apos;t? Or, on flush, does each DWPT&lt;br/&gt;
use its private FieldInfos to correctly flush the segment? (Hmm: do&lt;br/&gt;
we seed each DWPT w/ the original FieldInfos created by IW on init?).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Every DWPT has its own private FieldInfos.  When a segment is flushed the DWPT uses its private FI and then it updates the original DW.fieldInfos (from IW), which is a synchronized call.  &lt;/p&gt;

&lt;p&gt;The only consumer of DW.getFieldInfos() is SegmentMerger in IW.  Hmm, given that IW.flush() isn&apos;t synchronized anymore I assume this can lead into a problem?  E.g. the SegmentMerger gets a FieldInfos that&apos;s &quot;newer&quot; than the list of segments it&apos;s trying to flush?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;How are we handling the case of open IW, do delete-by-term but no added docs?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;DW has a SegmentDeletes (pendingDeletes) which gets pushed to the last segment.  We only add delTerms to DW.pendingDeletes if we couldn&apos;t push it to any DWPT.  Btw. I think the whole pushDeletes business isn&apos;t working correctly yet, I&apos;m looking into it.  I need to understand the code that coalesces the deletes better. &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;In DW.deleteTerms... shouldn&apos;t we skip a DWPT if it has no buffered docs?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yeah, I did that already, but not committed yet.&lt;/p&gt;</comment>
                    <comment id="12983613" author="michaelbusch" created="Wed, 19 Jan 2011 08:38:33 +0000"  >&lt;p&gt;So I&apos;m wondering about the following problem with deletes:&lt;/p&gt;

&lt;p&gt;Suppose we open a new IW on an existing index with 2 segments _1 and _2. IW is set to maxBufferedDocs=1000.  The app starts indexing with two threads, so two DWPTs are created.  DWPT1 starts working on _3, DWPT2 on _4.  Both &quot;remember&quot; that they must apply their deletes only to segments _1 and _2.  After adding 500 docs thread 2 stops indexing for an hour, but thread 1 keeps working.  While thread 2 is sleeping several segment flushes (_3, _5, _6, etc) happen.&lt;/p&gt;

&lt;p&gt;Now thread 2 wakes up again and adds another 500 docs, and also some deletes, so DWPT2 has to flush finally.  How can it figure out to which docs the deletes to apply to?  _1 and _2 are probably gone a long time ago.  If we apply the deletes to all of _3 this would be a mistake too.&lt;/p&gt;

&lt;p&gt;I&apos;m starting to think there&apos;s no way around sequenceIds?  Even without RT.&lt;/p&gt;</comment>
                    <comment id="12983720" author="mikemccand" created="Wed, 19 Jan 2011 14:21:26 +0000"  >&lt;p&gt;I like the grocery store analogy!  Yes, just like that &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Every DWPT has its own private FieldInfos. When a segment is flushed the DWPT uses its private FI and then it updates the original DW.fieldInfos (from IW), which is a synchronized call.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;OK that sounds good.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Hmm, given that IW.flush() isn&apos;t synchronized anymore I assume this can lead into a problem? E.g. the SegmentMerger gets a FieldInfos that&apos;s &quot;newer&quot; than the list of segments it&apos;s trying to flush?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes... or, the FieldInfos changes (due to flush) while SegmentMerger is still merging.  Probably SR should make a deep copy of the FieldInfos when it starts?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;DW has a SegmentDeletes (pendingDeletes) which gets pushed to the last segment. We only add delTerms to DW.pendingDeletes if we couldn&apos;t push it to any DWPT. Btw. I think the whole pushDeletes business isn&apos;t working correctly yet, I&apos;m looking into it. I need to understand the code that coalesces the deletes better.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;OK &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;How can it figure out to which docs the deletes to apply to? _1 and _2 are probably gone a long time ago. If we apply the deletes to all of _3 this would be a mistake too.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Hmmm... I think you&apos;re right (this is a problem).&lt;/p&gt;

&lt;p&gt;I think we also have problems w/ updateDocument?  That call is&lt;br/&gt;
supposed be atomic (ie the del &amp;amp; addDoc can never be separately&lt;br/&gt;
committed), but, I think if one DWPT (holding the del term) gets&lt;br/&gt;
flushed but another (holding the del term and the added doc) aborts&lt;br/&gt;
and then you commit, you could see the del &quot;make it&quot; but not the&lt;br/&gt;
addDocument?&lt;/p&gt;

&lt;p&gt;Finally, we are wanting to allow out-of-order-merges soon... so&lt;br/&gt;
that eg BSMP becomes much simpler to implement &amp;amp; bring to core,&lt;br/&gt;
but, these buffered deletes also make that more complicated.&lt;/p&gt;

&lt;p&gt;Gonna have to mull on this...&lt;/p&gt;</comment>
                    <comment id="12983756" author="jasonrutherglen" created="Wed, 19 Jan 2011 15:25:27 +0000"  >&lt;blockquote&gt;&lt;p&gt;we are wanting to allow out-of-order-merges soon&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Then the coalescing of deletes won&apos;t work.  &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I&apos;m starting to think there&apos;s no way around sequenceIds? Even without RT.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This sounds about right, however would the sequence-ids be for all segments (instead of only for DW)?&lt;/p&gt;</comment>
                    <comment id="12984285" author="mikemccand" created="Thu, 20 Jan 2011 17:10:20 +0000"  >&lt;p&gt;OK I think Michael&apos;s example can be solved, with a small change to the delete buffering.&lt;/p&gt;

&lt;p&gt;When a delete arrives, we should buffer in each DWPT, but also buffer into the &quot;global&quot; deletes pool (held in DocumentsWriter).&lt;/p&gt;

&lt;p&gt;Whenever any DWPT is flushed, that global pool is pushed.&lt;/p&gt;

&lt;p&gt;Then, the buffered deletes against each DWPT are carried (as usual) along w/ the segment that&apos;s flushed from that DWPT, but those buffered deletes &lt;b&gt;only&lt;/b&gt; apply to the docs in that one segment.&lt;/p&gt;

&lt;p&gt;The pushed deletes from the global pool apply to all prior segments (ie, they &quot;coalesce&quot;).&lt;/p&gt;

&lt;p&gt;This way, the deletes that will be applied to the already flushed segments are aggressively pushed.&lt;/p&gt;

&lt;p&gt;Separately, I think we should relax the error semantics for updateDocument: if an aborting exception occurs (eg disk full while flushing a segment), then it&apos;s possible that the &quot;delete&quot; from an updateDocument will have applied but the &quot;add&quot; did not.  Outside of error cases, of course, updateDocument will continue to be atomic (ie a commit() can never split the delete &amp;amp; add).  Then the updateDocument case is handled as just an &lt;span class=&quot;error&quot;&gt;&amp;#91;atomic wrt flush&amp;#93;&lt;/span&gt; add &amp;amp; delete.&lt;/p&gt;</comment>
                    <comment id="12984291" author="jasonrutherglen" created="Thu, 20 Jan 2011 17:26:56 +0000"  >&lt;blockquote&gt;&lt;p&gt;When a delete arrives, we should buffer in each DWPT, but also buffer into the &quot;global&quot; deletes pool (held in DocumentsWriter).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This&apos;ll work, however it seems like it&apos;s going to be a temporary solution if we implement sequence-ids properly and/or implement non-sequential merges.  In fact, with shared doc-store gone, what&apos;s holding up non-sequential merging?&lt;/p&gt;</comment>
                    <comment id="12984343" author="mikemccand" created="Thu, 20 Jan 2011 19:21:25 +0000"  >&lt;blockquote&gt;&lt;p&gt;In fact, with shared doc-store gone, what&apos;s holding up non-sequential merging?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Nothing really!  We could/should go do it right now... I think it should be trivial.  Then, we should fixup our default MP to behave more like BSMP!!  Immense segments are merged only pair wise, and no inadvertent optimizing...&lt;/p&gt;

&lt;p&gt;I think the buffered deletes will work fine for non-sequential merging &amp;#8211; we&apos;d do the same coalescing we do now, only applying deletes on-demand to the to-be-merged segs, etc.&lt;/p&gt;

&lt;p&gt;We just have to make sure the merged segment is appended to the end of the index (well, what was the end as of when the merge kicked off); this way I think we can continue w/ the invariant that buffered deletes apply to all segments to their &quot;left&quot;?&lt;/p&gt;</comment>
                    <comment id="12984379" author="jasonrutherglen" created="Thu, 20 Jan 2011 20:25:36 +0000"  >&lt;blockquote&gt;&lt;p&gt;We could/should go do it right now&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Nice!&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I think the buffered deletes will work fine for non-sequential merging - we&apos;d do the same coalescing we do now, only applying deletes on-demand to the to-be-merged segs, etc.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think this is going to make IW deletes even more hairy and hard to understand!  Though if we keep the option of using a BV for deletes then there&apos;s probably no choice.  If we implemented sequence-id deletes using a short[], then we&apos;re only increasing the RAM usage by 16 times, though we then do not need to clone which can generate excessive garbage (in a high flush &lt;span class=&quot;error&quot;&gt;&amp;#91;N&amp;#93;&lt;/span&gt;RT enviro).  &lt;/p&gt;</comment>
                    <comment id="12984738" author="mikemccand" created="Fri, 21 Jan 2011 13:54:14 +0000"  >&lt;blockquote&gt;&lt;p&gt;If we implemented sequence-id deletes using a short[], then we&apos;re only increasing the RAM usage by 16 times, though we then do not need to clone which can generate excessive garbage (in a high flush &lt;span class=&quot;error&quot;&gt;&amp;#91;N&amp;#93;&lt;/span&gt;RT enviro).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;How would we handle wraparound (in a concurrent way)?  Also, 16 fold increase in RAM usage is not cheap!&lt;/p&gt;

&lt;p&gt;I think, instead, we should recycle the bit vectors?  See, what we do now is really quite silly:  we drop the BV, let GC recycle it, allocate a new BV (same size), copy in nearly the same bits that we just discarded, set a few more bits.&lt;/p&gt;

&lt;p&gt;If instead we had a pool that&apos;d hold recently freed BVs (for a given segment), but, also tracked their &quot;state&quot; ie what &quot;delete gen&quot; they were at, and then when we need a new BV for that same segment, we pull a free one, catch it up (replay the deletes that had arrived since it was created), and use it, that&apos;s very fast?  Ie the cost is about as good as it can be &amp;#8211; incremental to the number of deletes actually recorded.  And the added RAM is &quot;a few bits&quot; per doc, where exactly how many &quot;a few&quot; is is decided by your app, ie, how many in-flight readers it &quot;tends&quot; to keep open.&lt;/p&gt;</comment>
                    <comment id="12984780" author="jasonrutherglen" created="Fri, 21 Jan 2011 15:55:20 +0000"  >&lt;blockquote&gt;&lt;p&gt;How would we handle wraparound (in a concurrent way)? Also, 16 fold increase in RAM usage is not cheap!&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Instantiate a new array, and the next reader&apos;s seqid is set to 0, while a second value is incremented to guarantee uniqueness of the reader.&lt;/p&gt;

&lt;p&gt;A short&apos;s 8 bytes * 500,000 docs (in the RAM buffer, is that a lot?) = ~4 MB?  Right, it&apos;ll eat into the RAM buffer but it&apos;s not extreme (or is it?!).&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;we drop the BV, let GC recycle it, allocate a new BV (same size), copy in nearly the same bits that we just discarded, set a few more bits.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Right, that&apos;s probably our best option for DF, BV, norms, and any other similar array.  I did propose that a while back, and I&apos;m not sure why, but I don&apos;t think you were a big fan:  &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-1574&quot; title=&quot;PooledSegmentReader, pools SegmentReader underlying byte arrays&quot;&gt;LUCENE-1574&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Would this also be used for DW&apos;s deletes?  At 16 times the minimum size of the sequence-ids then the pooled approach would allow the equivalent of 16 BVs!  The paged approach I think&apos;ll have issues in a low reader latency enviro, ie, create overhead from all the changes.  Whereas an array is fast to change, and fast to copy.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;also tracked their &quot;state&quot; ie what &quot;delete gen&quot; they were at, and then when we need a new BV for that same segment, we pull a free one, catch it up&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Couldn&apos;t we simply use System.arraycopy and be done?&lt;/p&gt;




</comment>
                    <comment id="12984784" author="jasonrutherglen" created="Fri, 21 Jan 2011 15:58:30 +0000"  >&lt;p&gt;Sorry, pre-coffee math, a short is 2 bytes! * 500,000 = ~1 MB.  That&apos;s a little less painful.&lt;/p&gt;</comment>
                    <comment id="12985089" author="mikemccand" created="Sat, 22 Jan 2011 11:11:40 +0000"  >&lt;blockquote&gt;&lt;p&gt;Right, it&apos;ll eat into the RAM buffer but it&apos;s not extreme (or is it?!).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think this could be acceptable (2 bytes per doc) as long as it&apos;s only for the docIDs in iW&apos;s RAM buffer, and not for docs in flushed segments?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I did propose that a while back, and I&apos;m not sure why, but I don&apos;t think you were a big fan: &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-1574&quot; title=&quot;PooledSegmentReader, pools SegmentReader underlying byte arrays&quot;&gt;LUCENE-1574&lt;/a&gt;&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Ugh, you&apos;re right!  Back then I wasn&apos;t a fan.... but, back then I didn&apos;t realize we could also reuse the contents of the bit vector (not just the allocated RAM), using a replay log.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Would this also be used for DW&apos;s deletes?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;It&apos;s tempting &amp;#8211; but let&apos;s first see how it works out for the flushed segments.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;The paged approach I think&apos;ll have issues in a low reader latency enviro, ie, create overhead from all the changes. Whereas an array is fast to change, and fast to copy.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;You mean paged BV right?  I think that, and more generally any transactional data structure (eg like Zoie&apos;s wrapped bloom filter / HashSet approach) is too much added cost for searching.  Using RT/NRT shouldn&apos;t slow down searching, ie I prefer the cost be front loaded into the reopen than backloaded into all searches.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Couldn&apos;t we simply use System.arraycopy and be done?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Well... System.arraycopy, while fast, is still O(N).  Yes, it has a small constant in front, but for a large index that cost will start to dominate.  Vs the cost of replaying the log, assuming the log is &quot;smallish&quot;, is linear in the number of deletes since this BV&apos;s last reader.  Still I expect we&apos;ll need a hybrid approach &amp;#8211; if the number of deletes in the log is too many then we fallback to System.arraycopy.&lt;/p&gt;</comment>
                    <comment id="12985148" author="jasonrutherglen" created="Sat, 22 Jan 2011 16:11:45 +0000"  >&lt;blockquote&gt;&lt;p&gt;the cost of replaying the log, assuming the log is &quot;smallish&quot;&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This is recording and replaying the doc-ids?  How/when does a previous BV become &apos;free&apos; to be used by the next reader?  What if they&apos;re open at the same time?  And if it&apos;s a previous previous reader that&apos;s been closed, won&apos;t that be quite a few docids to save?  Eg, a delete-by-query has removed thousands of docs, I guess we&apos;d use System.arraycopy then.  The most usual case is updateDocument with &lt;span class=&quot;error&quot;&gt;&amp;#91;N&amp;#93;&lt;/span&gt;RT, which&apos;d generate few doc-ids.  &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;System.arraycopy, while fast, is still O(N)&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Right, the larger segments will really adversely affect performance, as they do today, however the indexing is so much slower with NRT + clone that it&apos;s not noticeable.  &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Using RT/NRT shouldn&apos;t slow down searching&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Right!  The cost needs to be in the indexing and/or reopen threads.&lt;/p&gt;</comment>
                    <comment id="12985395" author="mikemccand" created="Sun, 23 Jan 2011 18:53:14 +0000"  >&lt;p&gt;One problem I realized in discussing stuff w/ Simon....&lt;/p&gt;

&lt;p&gt;The DWPT-private FieldInfos we now make are dangerous since they break bulk merging of stored fields and term vectors, I think?&lt;/p&gt;

&lt;p&gt;Somehow, we have to let each DWPT have some privacy, but, the field name -&amp;gt; number binding should be &quot;global&quot;.  I think Simon is going to open a separate issue to make something possible along these lines...&lt;/p&gt;

&lt;p&gt;I&apos;ll make a test case that asserts bulk merging is &quot;working&quot; w/ threaded indexing... would be good to know it&apos;s working properly today, too &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                    <comment id="12985468" author="mikemccand" created="Mon, 24 Jan 2011 00:04:51 +0000"  >&lt;p&gt;I&apos;m getting compilation errors after update the RT branch &amp;#8211; eg, BlockTermState is missing?&lt;/p&gt;</comment>
                    <comment id="12985547" author="simonw" created="Mon, 24 Jan 2011 07:33:25 +0000"  >&lt;blockquote&gt;
&lt;p&gt;Somehow, we have to let each DWPT have some privacy, but, the field name -&amp;gt; number binding should be &quot;global&quot;. I think Simon is going to open a separate issue to make something possible along these lines...&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Yep, I opened &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2881&quot; title=&quot;Track FieldInfo per segment instead of per-IW-session&quot;&gt;&lt;del&gt;LUCENE-2881&lt;/del&gt;&lt;/a&gt; for this&lt;/p&gt;</comment>
                    <comment id="12986556" author="jasonrutherglen" created="Tue, 25 Jan 2011 18:01:03 +0000"  >&lt;p&gt;The compilation errors are gone, TestNRTThreads and TestStressIndexing2 are still failing.  I think we need to implement Mike&apos;s idea: &lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2324?focusedCommentId=12984285&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#action_12984285&quot; class=&quot;external-link&quot;&gt;https://issues.apache.org/jira/browse/LUCENE-2324?focusedCommentId=12984285&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#action_12984285&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;then retest.  &lt;/p&gt;

&lt;p&gt;Is a test deadlocking somewhere, ant hasn&apos;t returned.&lt;/p&gt;</comment>
                    <comment id="12987827" author="jasonrutherglen" created="Thu, 27 Jan 2011 23:29:56 +0000"  >&lt;p&gt;I think we have a logical issue with updateDoc in that we&apos;re adding the delTerm&lt;br/&gt;
to &lt;b&gt;all&lt;/b&gt; DWPTs, however we&apos;re flushing DWPTs un-synced. This means that a&lt;br/&gt;
delTerm would be applied to a segment prior to the DWPT with the new doc being&lt;br/&gt;
flushed. This can easily happen because even though getReader syncs on flush,&lt;br/&gt;
doFlush is not synced, meaning another concurrent flush may begin, merges may&lt;br/&gt;
be performed, all without the new doc being made available. The solution is&lt;br/&gt;
probably as simple as treating updateDoc deletes as &lt;b&gt;special&lt;/b&gt; and storing them&lt;br/&gt;
in a Map&amp;lt;SegmentInfo/DWPT,&amp;lt;Term,Integer&amp;gt;&amp;gt; that belongs to the DWPT that&lt;br/&gt;
received the new doc (instead of adding the delTerm to each DWPT).&lt;/p&gt;

&lt;p&gt;This updateDocDeletesMap would be flushed only when the DWPT [with the updated&lt;br/&gt;
doc] is flushed, and if a DWPT no longer exists, we can add the&lt;br/&gt;
delTerm to the last segment. &lt;/p&gt;</comment>
                    <comment id="12988043" author="mikemccand" created="Fri, 28 Jan 2011 11:41:43 +0000"  >&lt;p&gt;Ahh, I see the problem now &amp;#8211; because we don&apos;t do a full sync on flushing all DWPTs for getReader (or, commit), we can see the delete part of an updateDocument sneak into a the &quot;commit&quot; without the corresponding add.&lt;/p&gt;

&lt;p&gt;I think this means we have to do a full stop on getReader or commit?&lt;/p&gt;

&lt;p&gt;Jason I don&apos;t think special tracking of updateDocument deletes will work.  EG, the document I&apos;m replacing could be in another DWPT?&lt;/p&gt;</comment>
                    <comment id="12988079" author="jasonrutherglen" created="Fri, 28 Jan 2011 13:30:18 +0000"  >&lt;blockquote&gt;&lt;p&gt;Jason I don&apos;t think special tracking of updateDocument deletes will work. EG, the document I&apos;m replacing could be in another DWPT?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Right.  I think that&apos;s why we&apos;d need to keep track of &lt;span class=&quot;error&quot;&gt;&amp;#91;unfortunately&amp;#93;&lt;/span&gt; the delTerms of all DWPTs per DWPT.  Then when a DWPT flushes it&apos;s deletes and documents, it&apos;ll flush delTerms to the other DWPTs.  Whereas today, in the updateDoc call we&apos;re flushing the delTerm to all DWPTs (this could be too early), which would logically seem to break atomicity.  &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;we have to do a full stop on getReader or commit?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Update doc isn&apos;t sync&apos;d so we&apos;d need to stop it as well?  Where are we gaining flush concurrency then?  &lt;/p&gt;</comment>
                    <comment id="12988082" author="mikemccand" created="Fri, 28 Jan 2011 13:42:21 +0000"  >&lt;blockquote&gt;&lt;p&gt;Where are we gaining flush concurrency then?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Even if we do full stop for commit and getReader, we&apos;ve still gained concurrency on the batch indexing case.&lt;/p&gt;

&lt;p&gt;So, I think we can actually avoid full stop for commit/getReader.&lt;/p&gt;

&lt;p&gt;The current plan w/ deletes is that a delete gets buffered 1) into the global pool (stored in DW and pushed whenever any DWPT flushes), as well as 2) per DWPT.  The per-DWPT pools apply &lt;b&gt;only&lt;/b&gt; to the segment flushed from that DWPT, while the global pool applies during coalescing (ie to all &quot;prior&quot; segments).&lt;/p&gt;

&lt;p&gt;To avoid the full-stop, I think during the flush we can have two global delete pools.  We carefully sweep all DWPTs and flush each, in succession.  Any DWPT not yet flushed is free to continue indexing as normal, putting deletes into the first global pool, flushing as normal.  But, a DWPT that has been flushed by the &quot;sweeper&quot; must instead put deletes for an updateDocument carefully into the 2nd pool, and not buffer the delete into DWPTs not yet flushed.&lt;/p&gt;

&lt;p&gt;Basically, as the sweeper visits each DWPT, it&apos;s segregating them into &quot;pre commit point&quot; and &quot;post commit point&quot;.&lt;/p&gt;

&lt;p&gt;We also must then ensure that no &quot;post commit point&quot; DWPT is allowed to flush until all &quot;pre commit point&quot; DWPTs have been visited by the sweeper.&lt;/p&gt;</comment>
                    <comment id="12988090" author="jasonrutherglen" created="Fri, 28 Jan 2011 14:36:18 +0000"  >&lt;blockquote&gt;&lt;p&gt;Even if we do full stop for commit and getReader, we&apos;ve still gained concurrency on the batch indexing case.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Right, I&apos;m still worried this case has update doc concurrency issues.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;The current plan w/ deletes is that a delete gets buffered 1) into the global pool (stored in DW and pushed whenever any DWPT flushes), as well as 2) per DWPT. The per-DWPT pools apply only to the segment flushed from that DWPT, while the global pool applies during coalescing (ie to all &quot;prior&quot; segments).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Ok, this is very clear now.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;We also must then ensure that no &quot;post commit point&quot; DWPT is allowed to flush until all &quot;pre commit point&quot; DWPTs have been visited by the sweeper.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Ah, this&apos;s the key that should solve the edge cases.&lt;/p&gt;</comment>
                    <comment id="12988457" author="mikemccand" created="Sat, 29 Jan 2011 13:27:32 +0000"  >&lt;p&gt;Actually, instead of buffering the delete term against each DWPT, and pushing the buffered delete packets when we flush the DWPT...&lt;/p&gt;

&lt;p&gt;Why don&apos;t we simply resolve the delete &quot;live&quot;?&lt;/p&gt;

&lt;p&gt;See, TermsHashPerField holds all indexed terms in a hash table, so, lookup is very fast (much faster than what we do today, where segment first gets flushed, then we load an SR and use terms dict to seek to the right term).&lt;/p&gt;

&lt;p&gt;We could eg add a parallel int[] array (ie a new int field, per unique term) that&apos;d hold the docIDUpto.  On flush, we&apos;d then skip any docIDs less than the docIDUpto for that term.&lt;/p&gt;

&lt;p&gt;Alternatively, we could keep a separate hash (as we have right now), but then, on flushing the segment, we apply the deletes as we flush.&lt;/p&gt;</comment>
                    <comment id="12988460" author="mikemccand" created="Sat, 29 Jan 2011 14:12:47 +0000"  >&lt;blockquote&gt;&lt;p&gt;Alternatively, we could keep a separate hash (as we have right now), but then, on flushing the segment, we apply the deletes as we flush.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I&apos;ll open a separate issue for this &amp;#8211; I think it&apos;s a low hanging fruit.&lt;/p&gt;</comment>
                    <comment id="12988462" author="mikemccand" created="Sat, 29 Jan 2011 14:40:22 +0000"  >&lt;p&gt;OK, I opened &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2897&quot; title=&quot;apply delete-by-Term and docID immediately to newly flushed segments&quot;&gt;&lt;del&gt;LUCENE-2897&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;</comment>
                    <comment id="12988463" author="jasonrutherglen" created="Sat, 29 Jan 2011 14:41:07 +0000"  >&lt;p&gt;I had to read this a few times, yes it&apos;s very elegant as we&apos;re skipping the postings that otherwise would be deleted immediately after flush, and we&apos;re reusing the terms map already in DWPT.&lt;/p&gt;</comment>
                    <comment id="12998873" author="simonw" created="Thu, 24 Feb 2011 14:48:58 +0000"  >&lt;p&gt;Can anyone gimme a quick statement about what is left here or what the status of this issue is? I am at the point where I need to do some rather big changes to DocValues which I would not need if we have DWPT so I might rather help here before wasting time.&lt;/p&gt;</comment>
                    <comment id="12999244" author="michaelbusch" created="Fri, 25 Feb 2011 07:27:17 +0000"  >&lt;blockquote&gt;
&lt;p&gt;Somehow, we have to let each DWPT have some privacy, but, the field name -&amp;gt; number binding should be &quot;global&quot;. I think Simon is going to open a separate issue to make something possible along these lines...&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This is done now (&lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2881&quot; title=&quot;Track FieldInfo per segment instead of per-IW-session&quot;&gt;&lt;del&gt;LUCENE-2881&lt;/del&gt;&lt;/a&gt;) and merged into the RT branch.&lt;/p&gt;


&lt;blockquote&gt;
&lt;p&gt;The current plan w/ deletes is that a delete gets buffered 1) into the global pool (stored in DW and pushed whenever any DWPT flushes), as well as 2) per DWPT. The per-DWPT pools apply only to the segment flushed from that DWPT, while the global pool applies during coalescing (ie to all &quot;prior&quot; segments).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I implemented and committed this approach.  It&apos;s looking pretty good - almost all tests pass.  Only TestStressIndexing2 is sometimes failing - but only when updateDocument() is called, not when I modify the test to only use add, delete-by-term and delete-by-query. &lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;To avoid the full-stop, I think during the flush we can have two global delete pools. We carefully sweep all DWPTs and flush each, in succession. Any DWPT not yet flushed is free to continue indexing as normal, putting deletes into the first global pool, flushing as normal. But, a DWPT that has been flushed by the &quot;sweeper&quot; must instead put deletes for an updateDocument carefully into the 2nd pool, and not buffer the delete into DWPTs not yet flushed.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I haven&apos;t done this yet - it might fix the failing test I described.&lt;/p&gt;</comment>
                    <comment id="12999247" author="michaelbusch" created="Fri, 25 Feb 2011 07:31:50 +0000"  >&lt;blockquote&gt;
&lt;p&gt;Can anyone gimme a quick statement about what is left here or what the status of this issue is? I am at the point where I need to do some rather big changes to DocValues which I would not need if we have DWPT so I might rather help here before wasting time.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think it&apos;s very close!  The new deletes approach is implemented, and various bugs are fixed.  Also the latest trunk is merged in (including &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2881&quot; title=&quot;Track FieldInfo per segment instead of per-IW-session&quot;&gt;&lt;del&gt;LUCENE-2881&lt;/del&gt;&lt;/a&gt;).  &lt;br/&gt;
Outstanding issues are to fix the updateDocument() problems, and finish flush-by-RAM (&lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2573&quot; title=&quot;Tiered flushing of DWPTs by RAM with low/high water marks&quot;&gt;&lt;del&gt;LUCENE-2573&lt;/del&gt;&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Other than TestStressIndexing2 and TestNRTThreads (updateDocument problem) and a few tests that rely on flush-by-RAM, all core and contrib tests are passing now.&lt;/p&gt;</comment>
                    <comment id="12999518" author="simonw" created="Fri, 25 Feb 2011 19:50:32 +0000"  >&lt;blockquote&gt;&lt;p&gt;Outstanding issues are to fix the updateDocument() problems, and finish flush-by-RAM (&lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2573&quot; title=&quot;Tiered flushing of DWPTs by RAM with low/high water marks&quot;&gt;&lt;del&gt;LUCENE-2573&lt;/del&gt;&lt;/a&gt;).&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Seems like &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2573&quot; title=&quot;Tiered flushing of DWPTs by RAM with low/high water marks&quot;&gt;&lt;del&gt;LUCENE-2573&lt;/del&gt;&lt;/a&gt; is more isolated than the updateDocument() issue so I think I can spend time on that one without interfering with what you are working on. I might need some time to get into what has been done so far, might come back here or on the list if I have questions. &lt;/p&gt;</comment>
                    <comment id="13005155" author="mikemccand" created="Thu, 10 Mar 2011 16:53:42 +0000"  >&lt;p&gt;One nice side effect of DWPT is it should allow us to get over the 2GB RAM buffer limit, assuming you use multiple threads.&lt;/p&gt;

&lt;p&gt;Ie I can set my RAM buffer to 10 GB, and if I&apos;m using 5 threads, it should work.&lt;/p&gt;

&lt;p&gt;Not sure it&apos;s really meaningful in practice, since in past tests I haven&apos;t seen any gains over ~128 or 256 MB buffer... but maybe that&apos;s changed now.&lt;/p&gt;</comment>
                    <comment id="13005159" author="jasonrutherglen" created="Thu, 10 Mar 2011 17:00:43 +0000"  >&lt;p&gt;Is the max optimal DWPT size related to the size of the terms hash, or is it likely something else?&lt;/p&gt;</comment>
                    <comment id="13005172" author="mikemccand" created="Thu, 10 Mar 2011 17:16:34 +0000"  >&lt;blockquote&gt;&lt;p&gt;Is the max optimal DWPT size related to the size of the terms hash, or is it likely something else?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Bigger really should be better I think.&lt;/p&gt;

&lt;p&gt;Because 1) the RAM efficiency ought to scale up very well, as you see a given term in more and more docs (hmm, though, maybe not, because from Zipf&apos;s law, half your terms will be singletons no matter how many docs you index), and 2) less merging is required.&lt;/p&gt;

&lt;p&gt;I&apos;m not sure why in the past perf seemd to taper off and maybe get worst after RAM buffer was over 256 MB... we should definitely re-test.&lt;/p&gt;</comment>
                    <comment id="13005177" author="jasonrutherglen" created="Thu, 10 Mar 2011 17:25:16 +0000"  >&lt;blockquote&gt;&lt;p&gt;Because 1) the RAM efficiency ought to scale up very well, as you see a given term in more and more docs (hmm, though, maybe not, because from Zipf&apos;s law, half your terms will be singletons no matter how many docs you index), and 2) less merging is required.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I&apos;m not sure how we handled concurrency on the terms hash before, however with DWPTs there won&apos;t be contention regardless.  It&apos;d be nice if we could build 1-2 GB segment&apos;s in RAM, I think that would greatly reduce the number merges that are required downstream.  Eg, then there&apos;s less need for merging by size, and most merges would be caused by the number/percentage of deletes.  If it turns out the low DF terms are causing the slowdown, maybe there is a different hashing system that could be used.&lt;/p&gt;</comment>
                    <comment id="13005188" author="mikemccand" created="Thu, 10 Mar 2011 17:29:36 +0000"  >&lt;p&gt;Concurrency today is similar to DWPT in that we simply write into multiple segments in RAM.&lt;/p&gt;

&lt;p&gt;But the, on flush, we do a merge (in RAM) of these segments and write a single on-disk segment.&lt;/p&gt;

&lt;p&gt;Vs this change which instead writes N on-disk segments and lets &quot;normal&quot; merging merge them.&lt;/p&gt;

&lt;p&gt;I think making a different data structure to hold low-DF terms would actually be a big boost in RAM efficiency.  The RAM-per-unique-term is fairly high...&lt;/p&gt;</comment>
                    <comment id="13005631" author="jasonrutherglen" created="Fri, 11 Mar 2011 13:33:49 +0000"  >&lt;blockquote&gt;&lt;p&gt;I think making a different data structure to hold low-DF terms would actually be a big boost in RAM efficiency. The RAM-per-unique-term is fairly high...&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;However we&apos;re not sure why a largish 1+ GB RAM buffer seems to slow down?  If we&apos;re round robin indexing against the DWPTs I think they&apos;ll have a similar number of unique terms as today, even though each DWPT will be smaller in size total size from each containing 1/Nth docs.  &lt;/p&gt;</comment>
                    <comment id="13005642" author="mikemccand" created="Fri, 11 Mar 2011 14:11:22 +0000"  >&lt;p&gt;The slowdown could have been due to the merge sort by docID that we do today on flush.&lt;/p&gt;

&lt;p&gt;Ie, if a given term X occurrs in 6 DWPTs (today) then we merge-sort the docIDs from the postings of that term, which is costly.  (The &quot;normal&quot; merge that will merge these DWPTs after this issue lands just append by docIDs).&lt;/p&gt;

&lt;p&gt;So maybe after this lands we&apos;ll see only faster performance the larger the RAM buffer &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;  That would be nice!&lt;/p&gt;</comment>
                    <comment id="13005651" author="jasonrutherglen" created="Fri, 11 Mar 2011 14:23:24 +0000"  >&lt;blockquote&gt;&lt;p&gt;Ie, if a given term X occurrs in 6 DWPTs (today) then we merge-sort the docIDs from the postings of that term, which is costly. (The &quot;normal&quot; merge that will merge these DWPTs after this issue lands just append by docIDs).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Right, this is the same principal motivation behind implementing DWPTs for use with realtime search, eg, the doc-id interleaving is too expensive to be performed at query time.&lt;/p&gt;</comment>
                    <comment id="13019803" author="simonw" created="Thu, 14 Apr 2011 13:21:22 +0100"  >&lt;p&gt;guys I opened &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-3023&quot; title=&quot;Land DWPT on trunk&quot;&gt;&lt;del&gt;LUCENE-3023&lt;/del&gt;&lt;/a&gt; to land on trunk! can I close this and we iterate on &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-3023&quot; title=&quot;Land DWPT on trunk&quot;&gt;&lt;del&gt;LUCENE-3023&lt;/del&gt;&lt;/a&gt; from now on?&lt;/p&gt;

&lt;p&gt;simon&lt;/p&gt;</comment>
                    <comment id="13026342" author="simonw" created="Thu, 28 Apr 2011 16:19:55 +0100"  >&lt;p&gt;we land this on trunk via &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-3023&quot; title=&quot;Land DWPT on trunk&quot;&gt;&lt;del&gt;LUCENE-3023&lt;/del&gt;&lt;/a&gt; &lt;/p&gt;</comment>
                </comments>
                <issuelinks>
                        <issuelinktype id="10032">
                <name>Blocker</name>
                                <outwardlinks description="blocks">
                            <issuelink>
            <issuekey id="12458987">LUCENE-2312</issuekey>
        </issuelink>
                    </outwardlinks>
                                                <inwardlinks description="is blocked by">
                            <issuelink>
            <issuekey id="12504248">LUCENE-3028</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12500948">LUCENE-2956</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12496483">LUCENE-2881</issuekey>
        </issuelink>
                    </inwardlinks>
                            </issuelinktype>
                        <issuelinktype id="10030">
                <name>Reference</name>
                                <outwardlinks description="relates to">
                            <issuelink>
            <issuekey id="12470358">LUCENE-2573</issuekey>
        </issuelink>
                    </outwardlinks>
                                                <inwardlinks description="is related to">
                            <issuelink>
            <issuekey id="12458038">LUCENE-2293</issuekey>
        </issuelink>
                    </inwardlinks>
                            </issuelinktype>
                    </issuelinks>
                <attachments>
                    <attachment id="12441770" name="ASF.LICENSE.NOT.GRANTED--lucene-2324.patch" size="172178" author="michaelbusch" created="Wed, 14 Apr 2010 22:14:14 +0100" />
                    <attachment id="12450048" name="lucene-2324.patch" size="317160" author="michaelbusch" created="Wed, 21 Jul 2010 11:22:14 +0100" />
                    <attachment id="12468626" name="LUCENE-2324.patch" size="6371" author="jasonrutherglen" created="Tue, 18 Jan 2011 07:05:01 +0000" />
                    <attachment id="12468605" name="LUCENE-2324.patch" size="2095" author="jasonrutherglen" created="Mon, 17 Jan 2011 23:33:44 +0000" />
                    <attachment id="12468506" name="LUCENE-2324.patch" size="16370" author="jasonrutherglen" created="Sun, 16 Jan 2011 18:53:17 +0000" />
                    <attachment id="12439916" name="LUCENE-2324.patch" size="62279" author="jasonrutherglen" created="Fri, 26 Mar 2010 21:32:40 +0000" />
                    <attachment id="12467686" name="LUCENE-2324-SMALL.patch" size="18402" author="jasonrutherglen" created="Thu, 6 Jan 2011 23:11:18 +0000" />
                    <attachment id="12467645" name="LUCENE-2324-SMALL.patch" size="12215" author="jasonrutherglen" created="Thu, 6 Jan 2011 16:56:35 +0000" />
                    <attachment id="12467614" name="LUCENE-2324-SMALL.patch" size="7522" author="jasonrutherglen" created="Thu, 6 Jan 2011 05:04:12 +0000" />
                    <attachment id="12467477" name="LUCENE-2324-SMALL.patch" size="5679" author="jasonrutherglen" created="Tue, 4 Jan 2011 21:53:42 +0000" />
                    <attachment id="12466859" name="LUCENE-2324-SMALL.patch" size="3053" author="jasonrutherglen" created="Thu, 23 Dec 2010 05:33:29 +0000" />
                    <attachment id="12468471" name="test.out" size="133941" author="jasonrutherglen" created="Sun, 16 Jan 2011 02:55:07 +0000" />
                    <attachment id="12468325" name="test.out" size="57019" author="jasonrutherglen" created="Fri, 14 Jan 2011 00:40:51 +0000" />
                    <attachment id="12467618" name="test.out" size="33428" author="jasonrutherglen" created="Thu, 6 Jan 2011 05:20:10 +0000" />
                    <attachment id="12466857" name="test.out" size="26087" author="jasonrutherglen" created="Thu, 23 Dec 2010 04:54:41 +0000" />
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>15.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Mon, 15 Mar 2010 09:47:41 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>11470</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>25401</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                    <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                <customfieldname>Time in Status</customfieldname>
                <customfieldvalues>
                    
                </customfieldvalues>
            </customfield>
                            </customfields>
    </item>
</channel>
</rss>