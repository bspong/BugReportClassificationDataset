<!-- 
RSS generated by JIRA (5.2.8#851-sha1:3262fdc28b4bc8b23784e13eadc26a22399f5d88) at Tue Jul 16 12:58:40 UTC 2013

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/LUCENE-2293/LUCENE-2293.xml?field=key&field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>5.2.8</version>
        <build-number>851</build-number>
        <build-date>26-02-2013</build-date>
    </build-info>

<item>
            <title>[LUCENE-2293] IndexWriter has hard limit on max concurrency</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-2293</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;DocumentsWriter has this nasty hardwired constant:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; MAX_THREAD_STATE = 5;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;which probably I should have attached a //nocommit to the moment I&lt;br/&gt;
wrote it &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/wink.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;That constant sets the max number of thread states to 5.  This means,&lt;br/&gt;
if more than 5 threads enter IndexWriter at once, they will &quot;share&quot;&lt;br/&gt;
only 5 thread states, meaning we gate CPU concurrency to 5 running&lt;br/&gt;
threads inside IW (each thread must first wait for the last thread to&lt;br/&gt;
finish using the thread state before grabbing it).&lt;/p&gt;

&lt;p&gt;This is bad because modern hardware can make use of more than 5&lt;br/&gt;
threads.  So I think an immediate fix is to make this settable&lt;br/&gt;
(expert), and increase the default (8?).&lt;/p&gt;

&lt;p&gt;It&apos;s tricky, though, because the more thread states, the less RAM&lt;br/&gt;
efficiency you have, meaning the worse indexing throughput.  So you&lt;br/&gt;
shouldn&apos;t up and set this to 50: you&apos;ll be flushing too often.&lt;/p&gt;

&lt;p&gt;But... I think a better fix is to re-think how threads write state&lt;br/&gt;
into DocumentsWriter.  Today, a single docID stream is assigned across&lt;br/&gt;
threads (eg one thread gets docID=0, next one docID=1, etc.), and each&lt;br/&gt;
thread writes to a private RAM buffer (living in the thread state),&lt;br/&gt;
and then on flush we do a merge sort.  The merge sort is inefficient&lt;br/&gt;
(does not currently use a PQ)... and, wasteful because we must&lt;br/&gt;
re-decode every posting byte.&lt;/p&gt;

&lt;p&gt;I think we could change this, so that threads write to private RAM&lt;br/&gt;
buffers, with a private docID stream, but then instead of merging on&lt;br/&gt;
flush, we directly flush each thread as its own segment (and, allocate&lt;br/&gt;
private docIDs to each thread).  We can then leave merging to CMS&lt;br/&gt;
which can already run merges in the BG without blocking ongoing&lt;br/&gt;
indexing (unlike the merge we do in flush, today).&lt;/p&gt;

&lt;p&gt;This would also allow us to separately flush thread states.  Ie, we&lt;br/&gt;
need not flush all thread states at once &amp;#8211; we can flush one when it&lt;br/&gt;
gets too big, and then let the others keep running.  This should be a&lt;br/&gt;
good concurrency gain since is uses IO &amp;amp; CPU resources &quot;throughout&quot;&lt;br/&gt;
indexing instead of &quot;big burst of CPU only&quot; then &quot;big burst of IO&lt;br/&gt;
only&quot; that we have today (flush today &quot;stops the world&quot;).&lt;/p&gt;

&lt;p&gt;One downside I can think of is... docIDs would now be &quot;less&lt;br/&gt;
monotonic&quot;, meaning if N threads are indexing, you&apos;ll roughly get&lt;br/&gt;
in-time-order assignment of docIDs.  But with this change, all of one&lt;br/&gt;
thread state would get 0..N docIDs, the next thread state&apos;d get&lt;br/&gt;
N+1...M docIDs, etc.  However, a single thread would still get&lt;br/&gt;
monotonic assignment of docIDs.&lt;/p&gt;</description>
                <environment></environment>
            <key id="12458038">LUCENE-2293</key>
            <summary>IndexWriter has hard limit on max concurrency</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png">Closed</status>
                    <resolution id="1">Fixed</resolution>
                                <assignee username="mikemccand">Michael McCandless</assignee>
                                <reporter username="mikemccand">Michael McCandless</reporter>
                        <labels>
                    </labels>
                <created>Wed, 3 Mar 2010 20:58:13 +0000</created>
                <updated>Fri, 10 May 2013 11:44:51 +0100</updated>
                    <resolved>Mon, 15 Mar 2010 14:13:07 +0000</resolved>
                                            <fixVersion>4.0-ALPHA</fixVersion>
                                <component>core/index</component>
                        <due></due>
                    <votes>3</votes>
                        <watches>3</watches>
                                                    <comments>
                    <comment id="12840911" author="michaelbusch" created="Wed, 3 Mar 2010 22:02:12 +0000"  >&lt;p&gt;Good timing - a couple days ago I was thinking about how threading could be changed in the indexer.&lt;/p&gt;

&lt;p&gt;The other downside is that you would have to buffer deleted docs and queries separately for each thread state, because you have to keep the private docID? So that would nee a bit more memory.&lt;/p&gt;

&lt;p&gt;Couldn&apos;t we make the DocumentsWriter and all related down-stream classes single-threaded then? The IndexWriter (or a new class) would have the doc queue, basically a load balancer, that multiple DocumentsWriter instances would pull from as soon as they are done inverting the previous document?&lt;/p&gt;

&lt;p&gt;This would allow us to simplify the indexer chain a lot - we could get rid of all the *PerThread classes. We&apos;d also have to separate then the docstores from the DocumentsWriter, so that multiple DocumentsWriter instances could share it. (what I&apos;d like to do anyway for &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2026&quot; title=&quot;Refactoring of IndexWriter&quot;&gt;LUCENE-2026&lt;/a&gt; anyway).&lt;/p&gt;</comment>
                    <comment id="12840918" author="jasonrutherglen" created="Wed, 3 Mar 2010 22:21:19 +0000"  >&lt;p&gt;Mike, good one!  Would having a doc id stream per thread make implementing a searchable RAM buffer easier?&lt;/p&gt;</comment>
                    <comment id="12840949" author="earwin" created="Wed, 3 Mar 2010 23:35:51 +0000"  >&lt;blockquote&gt;&lt;p&gt;The IndexWriter (or a new class) would have the doc queue, basically a load balancer, that multiple DocumentsWriter instances would pull from as soon as they are done inverting the previous document?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I hope we won&apos;t lose monotonic docIDs for a singlethreaded indexation somewhere along that path.&lt;/p&gt;</comment>
                    <comment id="12840952" author="michaelbusch" created="Wed, 3 Mar 2010 23:48:29 +0000"  >&lt;blockquote&gt;&lt;p&gt;I hope we won&apos;t lose monotonic docIDs for a singlethreaded indexation somewhere along that path.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;No. The order in the single threaded case won&apos;t be different from today with the changes Mike is proposing.&lt;/p&gt;</comment>
                    <comment id="12841106" author="shaie" created="Thu, 4 Mar 2010 07:25:21 +0000"  >&lt;blockquote&gt;&lt;p&gt;The IndexWriter (or a new class) would have the doc queue, basically a load balancer, that multiple DocumentsWriter instances would pull from as soon as they are done inverting the previous document?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Today, DW enforces thread binding - the same thread will always receive the same ThreadState. This allows applications who distribute the documents between threads based on some criteria, to get a locality of Documents indexed by each thread. I can&apos;t think of why an application would rely on that, but still that&apos;s something that happens.&lt;/p&gt;

&lt;p&gt;Also, in the pull approach, Lucene would introduce another place where it allocates threads. Not only would we need to allow setting that concurrency level, we&apos;d also need to allow overriding how a thread is instantiated. That will change the way applications are written today - I assume lots of applications that are multi-threaded rely on the multiple threads to index the documents. But now those threads won&apos;t do anything besides register a document in a queue. Therefore such applications will need to move to single-threaded indexing (because multi-threaded gives them nothing), and control the threads IW allocates.&lt;/p&gt;

&lt;p&gt;I personally prefer to leave multi-threaded indexing to the application. If it anyway contains a queue of incoming documents (from the outside) and allocates threads to process them in parallel (for example to parse rich text documents, fetch content from remote machines etc.), we wouldn&apos;t want them to do all this just to waste those threads at the end and let IW control another level of concurrency.&lt;/p&gt;

&lt;p&gt;Another downside of such approach is that it breaks backward compatibility in a new way we&apos;ve never considered. If the application allocates threads from a pool, and we introduce a new IW/DW w/ concurrency level=3 (for example), then the application will suddenly spawn more threads that it intended to. Perhaps it chose to use SMS, or overrode CMS to handle the threads allocation, but it&apos;s definitely not ready to handle another thread allocator.&lt;/p&gt;

&lt;p&gt;Another thing is that the queue cannot be of just Document objects, but a DocAndOp objects to account for add/delete/updates ... another complication.&lt;/p&gt;

&lt;p&gt;My preference is to keep the queue to the application.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;The other downside is that you would have to buffer deleted docs and queries separately for each thread state&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Just for clarity - you&apos;ll need to do it with the queue approach as well, right? I mean, a DW which pulled an operation from the queue, which is a DELETE op, will need to cache that DELETE so that it will be executed on all documents that were indexed up until flush. So that does not save anything vs. if we change DW to flush by ThreadState.&lt;/p&gt;

&lt;p&gt;Instead, I prefer to take advantage of the application&apos;s concurrency level in the following way:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Each thread will continue to write documents to a ThreadState. We&apos;ll allow changing the MAX_LEVEL, so if an app wants to get more concurrency, it can.
	&lt;ul&gt;
		&lt;li&gt;MAX_LEVEL will set the number of ThreadState objects available.&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;All threads will obtain memory buffers from a pull which will be limited by IW&apos;s RAM limit.&lt;/li&gt;
	&lt;li&gt;When a thread finishes indexing a document and realizes the pool has been exhausted, it flushes its ThreadState.
	&lt;ul&gt;
		&lt;li&gt;At that moment, that ThreadState is pulled out of the &apos;active&apos; list and is flushed. When it&apos;s done, it reclaims its used buffers and being put again in the active list.&lt;/li&gt;
		&lt;li&gt;New threads that come in will simply pick a ThreadState from the pool (but we&apos;ll bind them to that instance until it&apos;s flushed) and add documents to them.&lt;/li&gt;
		&lt;li&gt;That way, we hijack an application thread to do the flushing, which is anyway what happens today.&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;That way we are less likely to reach a state like Mike described - &quot;big burst of CPU only&quot; then &quot;big burst of IO only&quot; - and more likely to balance the two.&lt;/p&gt;

&lt;p&gt;If the application wants to be single threaded, we allow it to be like that all the way through, not introducing more thread allocations. Otherwise, we let it control its concurrency level and use it to our needs.&lt;/p&gt;</comment>
                    <comment id="12841120" author="michaelbusch" created="Thu, 4 Mar 2010 08:18:43 +0000"  >&lt;blockquote&gt;
&lt;p&gt;Also, in the pull approach, Lucene would introduce another place where it allocates threads.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;What I described is not much different from what&apos;s happening today. DocumentsWriter has already a WaitQueue, that ensures that the docs are written in the right order.&lt;/p&gt;

&lt;p&gt;I simply tried to suggest a way to refactor our classes... functionally the same as what Mike suggested. I shouldn&apos;t have said &quot;pulled from&quot; (the queue).&lt;/p&gt;</comment>
                    <comment id="12841127" author="shaie" created="Thu, 4 Mar 2010 08:48:12 +0000"  >&lt;blockquote&gt;&lt;p&gt;What I described is not much different from what&apos;s happening today.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Maybe I didn&apos;t understand then:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;basically a load balancer, that multiple DocumentsWriter instances would pull from as soon as they are done inverting the previous document?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Who adds documents to that queue and what are the DW instances? The way I read it, I understood those are different threads than the application threads. If I misunderstood that, could you please clarify?&lt;/p&gt;

&lt;p&gt;Also, I thought that each thread writes to different ThreadState does not ensure documents are written in order, but that finally when DW flushes, the different ThreadStates are merged together and one segment is written, somehow restores the orderness ...&lt;/p&gt;

&lt;p&gt;If only WaitQueue was documented &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;.&lt;/p&gt;

&lt;p&gt;I obviously don&apos;t know that part of the code as well as you. So if I misunderstood your meaning, I&apos;d appreciate if you clarify it for me. What I would like to avoid is having Lucene allocate indexing threads on its own.&lt;/p&gt;

&lt;p&gt;Also, is my proposal above different than what you suggest?&lt;/p&gt;</comment>
                    <comment id="12841135" author="michaelbusch" created="Thu, 4 Mar 2010 09:11:47 +0000"  >&lt;p&gt;Sorry - after reading my comment again I can see why it was confusing. Loadbalancer wasn&apos;t a very good analogy.&lt;/p&gt;

&lt;p&gt;I totally agree that Lucene should still piggyback on the application&apos;s threads and not start its own thread for document inversion.&lt;/p&gt;

&lt;p&gt;Today, as you said, does the DocumentsWriter manage a certain number of thread states, has the WaitQueue, and its own memory management.&lt;/p&gt;

&lt;p&gt;What I was thinking was that it would be simpler if the DocumentsWriter was only used by a single thread. The IndexWriter would have multiple DocumentsWriters and do the thread binding (+waitqueue). This would make the code in DocumentsWriter and the downstream classes simpler. The side-effect is that each DocumentsWriter would manage its own memory. &lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Also, I thought that each thread writes to different ThreadState does not ensure documents are written in order, but that finally when DW flushes, the different ThreadStates are merged together and one segment is written, somehow restores the orderness ...&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Stored fields are written to an on-disk stream (docstore) in order. The WaitQueue takes care of finishing the docs in the right order. &lt;br/&gt;
The postings are written into TermHashes per threadstate in parallel. The doc ids are in increasing order, but can have gaps. E.g. Threadstate 1 inverts doc 1 and 3, Threadstate 2 inverts doc 2. When it&apos;s time to flush the whole buffer these different TermHash postingslists get interleaved.&lt;/p&gt;</comment>
                    <comment id="12841140" author="shaie" created="Thu, 4 Mar 2010 09:22:48 +0000"  >&lt;p&gt;Ok so I think I understand now. You propose to change IW to bind a Thread to a DW, instead of that being done inside DW. And therefore it will simplify DW&apos;s code ... I wonder if that won&apos;t complicate IW code in return? Perhaps we&apos;ll gain a lot of simplification on DW, so a bit of complexity on IW will be ok.&lt;/p&gt;

&lt;p&gt;If we do that .. why not renaming DW to SegmentWriter? If each DW will eventually flush its own Segment, the name would make more sense?&lt;/p&gt;

&lt;p&gt;BTW, I was thinking that an application can emulate this sort of thing even today (well ... to some extent - w/o deletes). It can create an IW for each indexing thread and at the end call addIndexes. What we&apos;d need to introduce on IW to make it efficient though is something like addRawIndexes, which will just update the segments file about the new segments, but won&apos;t attempt to merge them and clean deletes out of them.&lt;br/&gt;
I think I want this API anyway for being able to add segments faster to an index, if e.g. you don&apos;t care about the merges at the moment ... but that is separate issue.&lt;/p&gt;

&lt;p&gt;Then I think what I proposed is more or less the same as you propose, therefore I&apos;m fine with that approach. When a DW/SW realizes it exhausted its memory pool, it just flushes and new threads will bind to other DW/SW.&lt;/p&gt;

&lt;p&gt;Thanks for the explanation on WaitQueue.&lt;/p&gt;</comment>
                    <comment id="12841193" author="earwin" created="Thu, 4 Mar 2010 11:24:51 +0000"  >&lt;blockquote&gt;&lt;p&gt;I wonder if that won&apos;t complicate IW code in return? Perhaps we&apos;ll gain a lot of simplification on DW, so a bit of complexity on IW will be ok.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;That will get rid of all that *PerThread insanity for each DW component, if I&apos;m getting it right. That&apos;s -13 classes. Yay for the issue!&lt;/p&gt;

&lt;p&gt;On a random sidenote, can we group things like these into subpackages? Having 132 files in oal.index is somewhat intimidating when trying to read/understand things.&lt;/p&gt;</comment>
                    <comment id="12841225" author="mikemccand" created="Thu, 4 Mar 2010 12:58:07 +0000"  >&lt;p&gt;I agree IW should not spawn its own threads.  It should piggy back on&lt;br/&gt;
incoming threads.&lt;/p&gt;

&lt;p&gt;On whether we can remove the &quot;perThread&quot; layer throughout the chain &amp;#8211;&lt;br/&gt;
that would be compelling.  But, we should scrutinize what that layer&lt;br/&gt;
does throughout the current chain to assess what we might lose.&lt;/p&gt;

&lt;p&gt;But, I was proposing a bigger change (call it &quot;private RAM segments&quot;):&lt;br/&gt;
there would be multiple DWs, each one writing to its own private RAM&lt;br/&gt;
segment (each one getting private docID assignment) &lt;b&gt;and&lt;/b&gt; its own doc&lt;br/&gt;
stores.&lt;/p&gt;

&lt;p&gt;There would be no more WaitQueue in IW.&lt;/p&gt;

&lt;p&gt;Each DW would flush its own segment privately.  They would not all&lt;br/&gt;
flush at once (merging their postings) like we must do today because&lt;br/&gt;
they &quot;share&quot; a single docID space.&lt;/p&gt;

&lt;p&gt;As I understand it, this would be step towards how Lucy handles&lt;br/&gt;
concurrency during indexing.  Ie, it&apos;d make the DWs nearly fully&lt;br/&gt;
independent from one another, and then IW is just there to dispatch/do&lt;br/&gt;
merging/etc.  (In Lucy each writer is a separate process, I think &amp;#8211;&lt;br/&gt;
VERY independent).&lt;/p&gt;

&lt;p&gt;We could do both changes, too (remove the &quot;perThread&quot; layer of&lt;br/&gt;
indexing chaing and switch to private RAM segments) &amp;#8211; I think they&lt;br/&gt;
are actually orthogonal.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;The other downside is that you would have to buffer deleted docs and queries separately for each thread state, because you have to keep the private docID? So that would nee a bit more memory.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Right.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Mike, good one! Would having a doc id stream per thread make implementing a searchable RAM buffer easier?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes &amp;#8211; they would just appear like sub segments.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I hope we won&apos;t lose monotonic docIDs for a singlethreaded indexation somewhere along that path.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;We won&apos;t.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Instead, I prefer to take advantage of the application&apos;s concurrency level in the following way:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;Each thread will continue to write documents to a ThreadState. We&apos;ll allow changing the MAX_LEVEL, so if an app wants to get more concurrency, it can.&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;MAX_LEVEL will set the number of ThreadState objects available.&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
	&lt;li&gt;All threads will obtain memory buffers from a pull which will be limited by IW&apos;s RAM limit.&lt;/li&gt;
	&lt;li&gt;When a thread finishes indexing a document and realizes the pool has been exhausted, it flushes its ThreadState.&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;At that moment, that ThreadState is pulled out of the &apos;active&apos; list and is flushed. When it&apos;s done, it reclaims its used buffers and being put again in the active list.&lt;/li&gt;
	&lt;li&gt;New threads that come in will simply pick a ThreadState from the pool (but we&apos;ll bind them to that instance until it&apos;s flushed) and add documents to them.&lt;/li&gt;
	&lt;li&gt;That way, we hijack an application thread to do the flushing, which is anyway what happens today.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;+1 &amp;#8211; this I think matches what I was thinking.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;If only WaitQueue was documented&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Sorry &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/sad.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;But WaitQueue would go away with this change.  We would no longer have&lt;br/&gt;
shared doc stores!&lt;/p&gt;</comment>
                    <comment id="12841235" author="shaie" created="Thu, 4 Mar 2010 13:21:37 +0000"  >&lt;p&gt;Perhaps instead of buffering the delete Terms/Queries somewhere central, when a delete by term is performed by a certain DW, it can register it immediately on all existing DWs. Each DW will record the doc ID up until which this term delete should be executed, and when it&apos;s its time to flush, will apply all the deletes that were accumulated on itself. It&apos;ll be like doing a Parallel segment deletes (but maybe I&apos;m too into Parallel Indexing &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;).&lt;/p&gt;

&lt;p&gt;This should not affect any documents that were added to any DW after the delete happened, and if we simply do it (sycned) across all active DWs, I think we should be fine?&lt;/p&gt;</comment>
                    <comment id="12841342" author="jasonrutherglen" created="Thu, 4 Mar 2010 15:44:02 +0000"  >&lt;blockquote&gt;&lt;p&gt;But WaitQueue would go away with this change.  We would no longer have shared doc stores!&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Cool, most of the DW code is intuitive except the shared doc stores because it&apos;s hard to when see when a doc store ends.  Also the interleaving is a bit difficult to visualize.  I look forward to checking out DW after this change.  &lt;/p&gt;</comment>
                    <comment id="12841347" author="mikemccand" created="Thu, 4 Mar 2010 15:50:54 +0000"  >&lt;p&gt;Yes, I think each DW will have to record its own buffered delete Term/Query, mapping to its docID at the time the delete arrived.&lt;/p&gt;

&lt;p&gt;Syncing across all of them would work but may be overkill.  I think we could instead have a lock free collection (need not even be FIFO &amp;#8211; the order doesn&apos;t matter) into which we add all Term/Query that are deleted.  Then, any time a thread hits that DW to add a document, it must first service that queue, by popping out all Term/Query stored in it and enrolling them the un-synchronized map of Term/Query -&amp;gt; docID).&lt;/p&gt;</comment>
                    <comment id="12841388" author="michaelbusch" created="Thu, 4 Mar 2010 17:06:58 +0000"  >&lt;blockquote&gt;
&lt;p&gt;But, I was proposing a bigger change (call it &quot;private RAM segments&quot;):&lt;br/&gt;
there would be multiple DWs, each one writing to its own private RAM&lt;br/&gt;
segment (each one getting private docID assignment) and its own doc&lt;br/&gt;
stores.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Cool! I wasn&apos;t sure if you wanted to give them private doc stores too. +1, I like it.&lt;/p&gt;
</comment>
                    <comment id="12841395" author="mikemccand" created="Thu, 4 Mar 2010 17:31:13 +0000"  >&lt;blockquote&gt;&lt;p&gt;Cool! I wasn&apos;t sure if you wanted to give them private doc stores too. +1, I like it.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I wasn&apos;t sure either &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/wink.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;  Ie, I forgot about that aspect of my proposal until it was raised in the discussion... but I think that&apos;d be necessary.&lt;/p&gt;

&lt;p&gt;This will be a perf hit, when building up a big new index.  But since doc stores now merge by bulk copy (when there are no deletions) hopefully the impact isn&apos;t too much.  And, hopefully it&apos;s more than made up for by the improvement in IO/CPU interleaved concurrency.&lt;/p&gt;

&lt;p&gt;I&apos;ll work out a patch to at least make the hardwired 5 configurable... but does anyone out there wanna work out the &quot;private RAM segments&quot;?&lt;/p&gt;</comment>
                    <comment id="12841407" author="michaelbusch" created="Thu, 4 Mar 2010 17:47:35 +0000"  >&lt;blockquote&gt;&lt;p&gt;Yes, I think each DW will have to record its own buffered delete Term/Query, mapping to its docID at the time the delete arrived. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think in the future deletes in DW could work like this:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;DW keeps of course track of a private sequence id, which gets incremented in the add, delete, update calls&lt;/li&gt;
	&lt;li&gt;a DW has a getReader() call, the reader can search the ram buffer&lt;/li&gt;
	&lt;li&gt;when DW.gerReader() gets called, then the new reader remembers the current seqID at the time it was opened - let&apos;s call it RAMReader.seqID; if such a reader gets reopened, simply its seqID gets updated.&lt;/li&gt;
	&lt;li&gt;we keep an growing int array with the size of DW&apos;s maxDoc, which replaces the usual deletes bitset&lt;/li&gt;
	&lt;li&gt;when DW.updateDocument() or .deleteDocument() needs to delete a doc we do that right away, before inverting the new doc. We can do that by running a query using a RAMReader to find all docs that must be deleted. Instead of flipping a bit in a bitset, for each hit we now keep track of when it was deleted:&lt;/li&gt;
&lt;/ul&gt;


&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-comment&quot;&gt;// init each slot in deletes array with -1
&lt;/span&gt;&lt;span class=&quot;code-keyword&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; NOT_DELETED = &lt;span class=&quot;code-object&quot;&gt;Integer&lt;/span&gt;.MAX_INT;
...
Arrays.fill(deletes, NOT_DELETED);

...

&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; void deleteDocument(Query q) {
  reopen RAMReader
  run query q using RAMReader
  &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; each hit {
    &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; hitDocId = ...
    &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (deletes[hitDocId] == NOT_DELETED) {
      deletes[hitDocId] = DW.seqID;
    }
  }
...
  DW.seqID++;
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now no matter of how often you (re)open RAMReaders, they can share the deletes array. No cloning like with the BitSet approach would be necessary:&lt;/p&gt;

&lt;p&gt;When the RAMReader iterates posting lists it&apos;s as simple as this to treat deletes docs correctly. Instead of doing this in RAMTermDocs.next():&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
  &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (deletedDocsBitSet.get(doc)) {
    skip &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; doc
 }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;we can now do:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
  &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (deletes[doc] &amp;lt; ramReader.seqID) {
    skip &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; doc
  }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Here is an example:&lt;br/&gt;
1. Add 3 docs with DW.addDocument() &lt;br/&gt;
2. User opens ramReader_a&lt;br/&gt;
3. Delete doc 1&lt;br/&gt;
4. User opens ramReader_b&lt;/p&gt;


&lt;p&gt;After 1: DW.seqID = 2; deletes[]=&lt;/p&gt;
{MAX_INT, MAX_INT, MAX_INT}
&lt;p&gt;After 2: ramReader_a.seqID = 2&lt;br/&gt;
After 3: DW.seqID = 3; deletes[]=&lt;/p&gt;
{MAX_INT, 2, MAX_INT}
&lt;p&gt;After 3: ramReader_b.seqID = 3&lt;/p&gt;

&lt;p&gt;Note that both ramReader_a and ramReader_b share the same deletes[] array. Now when ramReader_a is used to read posting lists, it will not treat doc 1 as deleted, because (deletes&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt; &amp;lt; ramReader_a.seqID) = (2 &amp;lt; 2) = false; But ramReader_b will see it as deleted, because (deletes&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt; &amp;lt; ramReader_b.seqID) = (2 &amp;lt; 3) = true.&lt;/p&gt;

&lt;p&gt;What do you think about this approach for the future when we have a searchable DW buffer?&lt;/p&gt;</comment>
                    <comment id="12841463" author="shaie" created="Thu, 4 Mar 2010 19:24:18 +0000"  >&lt;p&gt;What about the following scenario:&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;A document is added w/ term A to DW1&lt;/li&gt;
	&lt;li&gt;A document is added w/ term A to DW2 (by another thread)&lt;/li&gt;
	&lt;li&gt;A deleteDocuments(Term-A) is issued against DW1 (could be even 3, where A does not exist)&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;I thought that when (3) happens, the delete-by-term needs to be issued against all DWs, so that later when they apply their deletes they&apos;ll &lt;b&gt;remember&lt;/b&gt; to do so. Issuing that against all DWs will record the docID of each DW up until which the delete should apply.&lt;/p&gt;

&lt;p&gt;We could move to doing the delete right-away, by reopening a DW reader, and we could move to storing deletes in int[] rather than bit set. But I&apos;m not sure I understand how your proposal will handle the scenario I&apos;ve described.&lt;/p&gt;

&lt;p&gt;Also, I don&apos;t see the advantage of moving to store the deletes in int[] rather than bitset ... is it just to avoid calling the get(doc)?&lt;/p&gt;</comment>
                    <comment id="12841545" author="michaelbusch" created="Thu, 4 Mar 2010 21:43:50 +0000"  >&lt;blockquote&gt;
&lt;p&gt;I thought that when (3) happens, the delete-by-term needs to be issued against all DWs, so that later when they apply their deletes they&apos;ll remember to do so. Issuing that against all DWs will record the docID of each DW up until which the delete should apply.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes, you still need to apply deletes on all DWs. My approach is not different in that regard.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Also, I don&apos;t see the advantage of moving to store the deletes in int[] rather than bitset ... is it just to avoid calling the get(doc)?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The big advantage is that all (re)opened readers can share the single int[] array. If you use a bitset you need to clone it for each reader. With the int[] reopening becomes basically free from a deletes perspective.&lt;/p&gt;</comment>
                    <comment id="12841617" author="michaelbusch" created="Thu, 4 Mar 2010 23:49:58 +0000"  >&lt;blockquote&gt;&lt;p&gt;The big advantage is that all (re)opened readers can share the single int[] array.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Dirty reads will be a problem with sharing the array. An AtomicIntegerArray could be used. We need to experiment how expensive that would be. &lt;/p&gt;</comment>
                    <comment id="12841697" author="shaie" created="Fri, 5 Mar 2010 04:13:07 +0000"  >&lt;p&gt;But if each DW maintains its own doc IDs, separately from the others, what will be stored in the int[]? DW1 deleted docID 0 (its 0) and DW4 deleted the same. The two documents are not the same one ... no?&lt;/p&gt;

&lt;p&gt;Won&apos;t this complicate the entire solution? What I liked about keeping each DW separate (and call it SegmentWriter) is that it really operates on its own. When a delete happens on IW, it is synced so that it could be registered on all DWs. But besides that, the DWs don&apos;t know about each other nor care. Code should be really simple that way - the only thing that will be shared is the pool of buffers.&lt;/p&gt;

&lt;p&gt;I guess I&apos;m missing something, because you&apos;re far more knowledgeable in this code than I am ... &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                    <comment id="12841744" author="michaelbusch" created="Fri, 5 Mar 2010 08:18:44 +0000"  >&lt;blockquote&gt;
&lt;p&gt;But if each DW maintains its own doc IDs, separately from the others, what will be stored in the int[]? DW1 deleted docID 0 (its 0) and DW4 deleted the same. The two documents are not the same one ... no? &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;In DW you don&apos;t delete by docID. You can only delete by term or query. You have to run the (term)query in all DWs to determine if any of the DWs have one or more matching docs that have to be deleted.&lt;/p&gt;

&lt;p&gt;Today the queries and/or terms are buffered, along with the maxDocID at the time the delete or update was called. They are applied just after the DW buffer was flushed to a segment, be cause that&apos;s the first time the docs are searchable and the delete queries can be executed.&lt;/p&gt;

&lt;p&gt;In the future, when we can search the DW buffer(s), you can apply the deletes right away. Using this int[] approach for deletes will avoid the need of cloning bitsets in each reopen. &lt;/p&gt;</comment>
                    <comment id="12841745" author="michaelbusch" created="Fri, 5 Mar 2010 08:27:21 +0000"  >&lt;blockquote&gt;
&lt;p&gt;Won&apos;t this complicate the entire solution? What I liked about keeping each DW separate (and call it SegmentWriter) is that it really operates on its own. When a delete happens on IW, it is synced so that it could be registered on all DWs. But besides that, the DWs don&apos;t know about each other nor care. Code should be really simple that way - the only thing that will be shared is the pool of buffers.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;What I&apos;m proposing is not different or makes it more complicated. Either way, you have to apply all deletes on all DWs, because you delete by query or term.&lt;/p&gt;

&lt;p&gt;This might not be the right time for this proposal, because it&apos;ll only work with searchable DW buffers. But I wanted to mention this idea already, so that we can keep it in mind. And hopefully we can work on searchable DW buffers soon.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;but does anyone out there wanna work out the &quot;private RAM segments&quot;?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I would like to try to help, but I&apos;m likely not going to have enough time right now to write an entire patch for this big change myself.&lt;/p&gt;</comment>
                    <comment id="12841800" author="mikemccand" created="Fri, 5 Mar 2010 10:23:16 +0000"  >&lt;blockquote&gt;&lt;p&gt;I think in the future deletes in DW could work like this: &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This approach looks great Michael! Allows you to efficiently share a single int[] for deletions across many reopened RAM segment readers. &lt;/p&gt;

&lt;p&gt;How will we handle flushing? Ie, when a RAM segment is flushed to disk, it&apos;d have to remain alive so long as a reader is still using it? Which means we can&apos;t recycle its buffers until all open readers using it are closed? Or... we could forcefully somehow cut it over to the identical now-on-disk segment? &lt;/p&gt;

&lt;p&gt;This is a great approach for speeding up NRT &amp;#8211; NRT readers will no longer have to flush. It&apos;s similar in spirit to &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-1313&quot; title=&quot;Near Realtime Search (using a built in RAMDirectory)&quot;&gt;&lt;del&gt;LUCENE-1313&lt;/del&gt;&lt;/a&gt;, but that issue is still flushing segments (but, into an intermediate RAMDir).&lt;/p&gt;</comment>
                    <comment id="12841821" author="shaie" created="Fri, 5 Mar 2010 11:34:15 +0000"  >&lt;p&gt;Michael - I see that we were on the same page. Probably I misread your description. I know that in IW deletes are applied by Term/Query and I thought that that delete should be registered on all DWs so they can apply it later. I&apos;m glad that you think like that as well.&lt;/p&gt;

&lt;p&gt;So about the int[], would that be of the size of the index (flushed and unflushed) segments? Suppose that:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;I&apos;ve indexed 5 documents, flushed. (IDs 0-4)&lt;/li&gt;
	&lt;li&gt;Indexed 2 on DW1. (IDs 0,1)&lt;/li&gt;
	&lt;li&gt;Indexed 2 on DW2. (IDs 0,1)&lt;/li&gt;
	&lt;li&gt;Delete by term which affects: flushed IDs 1, 4, DW1-0, DW2 - 0, 1&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Would the int[] be of size 9, and the deleted IDs be 1, 4, 5, 7, 8? How would DW1- be mapped to 5, and DW2-0,1 be mapped to 7 and 8? Will the int[] be initially of size 5 and after DW1 flushes expand to 7, and ID=5 will be set (and afterwards expand to 9 with IDs 7,8)? If so then I understand.&lt;/p&gt;

&lt;p&gt;Why would using an int[] be any better than sharing a bitset?&lt;/p&gt;</comment>
                    <comment id="12841915" author="michaelbusch" created="Fri, 5 Mar 2010 16:30:06 +0000"  >&lt;blockquote&gt;
&lt;p&gt;This is a great approach for speeding up NRT - NRT readers will no longer have to flush. It&apos;s similar in spirit to &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-1313&quot; title=&quot;Near Realtime Search (using a built in RAMDirectory)&quot;&gt;&lt;del&gt;LUCENE-1313&lt;/del&gt;&lt;/a&gt;, but that issue is still flushing segments (but, into an intermediate RAMDir).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I agree! Thinking further about this: Each (re)opened RAM segment reader needs to also remember the maxDoc of the corresponding DW at the time it was (re)opened. This way we can prevent a RAM reader to read postinglists beyond that maxDoc, even if the writer thread keeps building the lists in parallel. This allows us to guarantee the point-in-time requirements.&lt;/p&gt;

&lt;p&gt;Also, the PostingList objects we store in the TermHash already contain a lastDocID (if I remember correctly). So when a RAM reader termEnum iterates the dictionary it can skip all terms where term.lastDocID &amp;gt; RAMReader.maxDoc.&lt;/p&gt;

&lt;p&gt;It&apos;s quite neat that all we have to do in reopen then is to update ramReader.maxDoc and ramReader.seqID.&lt;/p&gt;

&lt;p&gt;Of course one big thing is still missing: keeping the term dictionary sorted. In order to implement the full IndexReader interface, specifically TermEnum, it&apos;s necessary to give each RAM reader a point-in-time sorted dictionary. At least in one direction, as a TermEnum only seeks forward.&lt;/p&gt;

&lt;p&gt;I think we have two options here: Either we try to keep the dictionary always sorted, whenever a term is added. I guess then we&apos;d have to implement a b-tree or something similar?&lt;/p&gt;

&lt;p&gt;The second option I can think of is to add a &quot;nextTerm&quot; pointer to TermHash.Postinglist. This allows us to build up a linked list across all terms. When a ramReader is opened we would sort all terms, but not by changing their position in the hash - instead by building the single-linked list in sorted order.&lt;/p&gt;

&lt;p&gt;When a new reader gets (re)opened we need to mergesort the new terms into the linked list. I guess it&apos;s easy to get this implemented lock-free. E.g. if you have the linked list a-&amp;gt;c, and you want to add b in the middle, you set b-&amp;gt;c before changing a-&amp;gt;c. Then it&apos;s undefined if an in-flight older reader would see term b. The old reader must not return b, since b was added after the old reader was (re)opened. So either case is fine: either it doesn&apos;t see b cause the link wasn&apos;t updated yet, or it sees it but doesn&apos;t return it, because b.lastDocID&amp;gt;ramReader.maxDoc.&lt;/p&gt;

&lt;p&gt;The downside is that we will have to pay the price of sorting in reader.reopen, which however should be cheap if readers are reopened frequently. Not sure though if this linkedlist approach is more or less compelling than something like a btree?&lt;/p&gt;

&lt;p&gt;Btw: Shall we open a new &quot;searchable DW buffer&quot; issue or continue using this issue for these discussions?&lt;/p&gt;</comment>
                    <comment id="12841923" author="michaelbusch" created="Fri, 5 Mar 2010 16:40:28 +0000"  >&lt;blockquote&gt;&lt;p&gt;So about the int[], would that be of the size of the index (flushed and unflushed) segments? Suppose that:&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Each DW would have its own int[]. The size would correspond to the number of docs the DW has in its buffer.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;I&apos;ve indexed 5 documents, flushed. (IDs 0-4)&lt;br/&gt;
Indexed 2 on DW1. (IDs 0,1)&lt;br/&gt;
Indexed 2 on DW2. (IDs 0,1)&lt;br/&gt;
Delete by term which affects: flushed IDs 1, 4, DW1-0, DW2 - 0, 1&lt;br/&gt;
Would the int[] be of size 9, and the deleted IDs be 1, 4, 5, 7, 8? How would DW1- be mapped to 5, and DW2-0,1 be mapped to 7 and 8? Will the int[] be initially of size 5 and after DW1 flushes expand to 7, and ID=5 will be set (and afterwards expand to 9 with IDs 7,8)? If so then I understand.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;DW1 will have an int[] of size 2, and DW2 will also have a separate int[] of size 2.&lt;/p&gt;

&lt;p&gt;I think you were thinking of one big int[] across the entire index? I believe you will understand the whole approach now when you think of the int[]s as per ram segment.&lt;/p&gt;</comment>
                    <comment id="12841942" author="shaie" created="Fri, 5 Mar 2010 17:31:26 +0000"  >&lt;p&gt;Thanks Michael. If the int[] are per DW then it&apos;s starting to make sense &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;. I will read this issue again, especially the last several posts.&lt;/p&gt;

&lt;p&gt;I wanted to propose earlier to move the discussion to a seperate issue and close this one by allowing better control over the concurrency level. So a +1 from me on that.&lt;/p&gt;</comment>
                    <comment id="12841960" author="jasonrutherglen" created="Fri, 5 Mar 2010 18:05:02 +0000"  >&lt;blockquote&gt;&lt;p&gt;I think we have two options here: Either we try to keep the dictionary always sorted, whenever a term is added. I guess then we&apos;d have to implement a b-tree or something similar?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The int[] for deletes makes sense, I guess because we&apos;re assuming the number of docs in the RAM buffer won&apos;t be too large.  Can&apos;t we simply instantiate a new terms array (merging in the new terms) for each reopen?  &lt;/p&gt;

&lt;p&gt;Won&apos;t we need to wait for flex and this issue to be completed before tackling this?&lt;/p&gt;</comment>
                    <comment id="12844495" author="mikemccand" created="Fri, 12 Mar 2010 13:36:01 +0000"  >&lt;p&gt;We&apos;ll add this (max internal concurrency, now hardwired to 5) to IWC once it&apos;s in...&lt;/p&gt;</comment>
                    <comment id="12844828" author="jasonrutherglen" created="Sat, 13 Mar 2010 07:45:58 +0000"  >&lt;blockquote&gt;&lt;p&gt;but does anyone out there wanna work out the &quot;private RAM&lt;br/&gt;
segments&quot;?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I didn&apos;t see this before, I figured private RAM segments was on&lt;br/&gt;
the roadmap for this issue, it sounds like it&apos;ll be a different&lt;br/&gt;
one? &lt;/p&gt;

&lt;p&gt;Mike, can you outline what would need to change? It seems like&lt;br/&gt;
large amounts of code could be removed (i.e.&lt;br/&gt;
FreqProxFieldMergeState)? The *PerThread classes? If so, I think&lt;br/&gt;
it would go over my head (because I don&apos;t have a mental mapping&lt;br/&gt;
of how all the classes tie together). &lt;/p&gt;</comment>
                    <comment id="12844848" author="mikemccand" created="Sat, 13 Mar 2010 10:25:15 +0000"  >&lt;p&gt;I think this issue has these steps:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;Allow the 5 to be changed (trivial first step) &amp;#8211; I&apos;ll do this&lt;br/&gt;
    after &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2294&quot; title=&quot;Create IndexWriterConfiguration and store all of IW configuration there&quot;&gt;&lt;del&gt;LUCENE-2294&lt;/del&gt;&lt;/a&gt; is in&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Change the approach for how we buffer in RAM to a more isolated&lt;br/&gt;
    approach, whereby IW has N fully independent RAM segments&lt;br/&gt;
    in-process and when a doc needs to be indexed it&apos;s added to one of&lt;br/&gt;
    them.  Each segment would also write its own doc stores and&lt;br/&gt;
    &quot;normal&quot; segment merging (not the inefficient merge we now do on&lt;br/&gt;
    flush) would merge them.  This should be a good simplification in&lt;br/&gt;
    the chain (eg maybe we can remove the *PerThread classes).  The&lt;br/&gt;
    segments can flush independently, letting us make much better&lt;br/&gt;
    concurrent use of IO &amp;amp; CPU.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Enable NRT readers to directly search these RAM segments.  This&lt;br/&gt;
    entails recording deletes on the RAM segments as an int[].  We&lt;br/&gt;
    need to solve the Term sorting issue... (b-tree, or, simply&lt;br/&gt;
    sort-on-demand the first time a query needs it, though that cost&lt;br/&gt;
    increases the larger your RAM segments get, ie, not incremental to&lt;br/&gt;
    the # docs you just added).  Also, we have to solve what happens&lt;br/&gt;
    to a reader using a RAM segment that&apos;s been flushed.  Perhaps we&lt;br/&gt;
    don&apos;t reuse RAM at that point, ie, rely on GC to reclaim once all&lt;br/&gt;
    readers using that RAM segmeent have closed.  We should do this&lt;br/&gt;
    part under a separate issue (&lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2312&quot; title=&quot;Search on IndexWriter&amp;#39;s RAM Buffer&quot;&gt;LUCENE-2312&lt;/a&gt;).&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="12844893" author="jasonrutherglen" created="Sat, 13 Mar 2010 15:04:45 +0000"  >&lt;blockquote&gt;&lt;p&gt;Change the approach for how we buffer in RAM to a more&lt;br/&gt;
isolated approach&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Would we reuse the DocumentsWriter class, and assign one to each&lt;br/&gt;
thread? Then start to rework DW on down in the code tree,&lt;br/&gt;
removing the per thread logic? Or do we need to do something&lt;br/&gt;
more dramatic?&lt;/p&gt;</comment>
                    <comment id="12844900" author="mikemccand" created="Sat, 13 Mar 2010 15:20:56 +0000"  >&lt;p&gt;Probably one DW instance per thread?  Seems like that&apos;d work?&lt;/p&gt;

&lt;p&gt;And possibly remove *PerThread throughout the default indexing chain?&lt;/p&gt;</comment>
                    <comment id="12844911" author="mikemccand" created="Sat, 13 Mar 2010 16:04:43 +0000"  >&lt;p&gt;Simple patch, just adds maxThreadStates setting to IndexWriterConfig.&lt;/p&gt;</comment>
                    <comment id="12844926" author="jasonrutherglen" created="Sat, 13 Mar 2010 17:35:53 +0000"  >&lt;blockquote&gt;&lt;p&gt;Probably one DW instance per thread? Seems like that&apos;d work? &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Ok&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;And possibly remove *PerThread throughout the default indexing chain?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I like removing this as there&apos;s many loops per thread right now, it&apos;s not easy to glance at and know what&apos;s going on.  &lt;/p&gt;</comment>
                    <comment id="12845047" author="michaelbusch" created="Sun, 14 Mar 2010 08:19:29 +0000"  >&lt;blockquote&gt;
&lt;p&gt;but does anyone out there wanna work out the &quot;private RAM segments&quot;?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Shall we use this issue for the private RAM segments? Or do you want to commit the simple patch, close this one and open a new issue?&lt;/p&gt;</comment>
                    <comment id="12845048" author="michaelbusch" created="Sun, 14 Mar 2010 08:24:58 +0000"  >&lt;p&gt;I&apos;m tempted to get rid of the pooling for PostingLIst objects.  The objects are very small and Java does a good job since 1.5 with object creation and gc.  I even read that the JVM guys think that pooling can be slower than not-pooling.&lt;/p&gt;

&lt;p&gt;Also, I&apos;ve mostly seen gc performance problems so far if there were a big number of long-living objects - it makes the mark time of the garbage collection very long.  Pooling of course exactly gets you in such a situation.&lt;/p&gt;

&lt;p&gt;So what do you think about removing the pooling of the PostingList objects?  &lt;/p&gt;</comment>
                    <comment id="12845056" author="mikemccand" created="Sun, 14 Mar 2010 09:44:28 +0000"  >&lt;blockquote&gt;&lt;p&gt;Or do you want to commit the simple patch, close this one and open a new issue?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;How about a new issue?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Also, I&apos;ve mostly seen gc performance problems so far if there were a big number of long-living objects - So what do you think about removing the pooling of the PostingList objects?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;It&apos;s not only the GC cost, it&apos;s also the cost of init&apos;ing these objects.  EG filling in 0s for all the fields, when we&apos;re gonna overwrite them anyway.&lt;/p&gt;

&lt;p&gt;But, let&apos;s test on modern JREs to confirm this?  I do agree pooling adds code complexity, so, if it&apos;s not buying us anything (or very little) we should remove it.&lt;/p&gt;

&lt;p&gt;The worst case should be docs with many unique terms...  Though... to reduce our per-unique-term RAM cost, we may want to move away from separate postings object per term to parallel arrays.  We also could do something different for singleton terms vs the rest (if Zipf&apos;s law is applying, half the terms should be singletons; if it&apos;s not, you could have many more singleton terms...).  I&apos;d do this as an experimental indexing chain &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                    <comment id="12845159" author="michaelbusch" created="Sun, 14 Mar 2010 23:33:37 +0000"  >&lt;blockquote&gt;&lt;p&gt;How about a new issue?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;OK, will open one.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;(if Zipf&apos;s law is applying, half the terms should be singletons; if it&apos;s not, you could have many more singleton terms...)&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yeah we should utilize our knowledge of term distribution to optimize in-memory postings.  For example, currently a nice optimization would be to store the first posting in the PostingList object and only allocate slices once you see the second occurrence (similar to the pulsing codec)?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Though... to reduce our per-unique-term RAM cost, we may want to move away from separate postings object per term to parallel arrays.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;What exactly do you mean with parallel arrays? Parallel to the termHash array?  Then the termsHash array would not be an array of PostingList objects anymore, but an array of pointers into the char[] array?  And you&apos;d have e.g. a parallel int[] array for df, another int[] for pointers into the postings byte pool, etc? Something like that?&lt;/p&gt;</comment>
                    <comment id="12845190" author="michaelbusch" created="Mon, 15 Mar 2010 05:43:13 +0000"  >&lt;p&gt;OK I opened &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2324&quot; title=&quot;Per thread DocumentsWriters that write their own private segments&quot;&gt;&lt;del&gt;LUCENE-2324&lt;/del&gt;&lt;/a&gt;.  We can close this one after you committed your patch, Mike.&lt;/p&gt;</comment>
                    <comment id="12845263" author="mikemccand" created="Mon, 15 Mar 2010 09:58:07 +0000"  >&lt;blockquote&gt;&lt;p&gt;For example, currently a nice optimization would be to store the first posting in the PostingList object and only allocate slices once you see the second occurrence (similar to the pulsing codec)?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think we can do even better, ie, that class wastes RAM for the single posting case (intStart, byteStart, lastDocID, docFreq, lastDocCode, lastDocPosition are not needed).&lt;/p&gt;

&lt;p&gt;EG we could have a separate class dedicated to the singleton case.  When term is first encountered it&apos;s enrolled there.  We&apos;d probably need a separate hash to store these (though not necessarily?).  If it&apos;s seen again it&apos;s switched to the full posting.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;What exactly do you mean with parallel arrays? Parallel to the termHash array? Then the termsHash array would not be an array of PostingList objects anymore, but an array of pointers into the char[] array? And you&apos;d have e.g. a parallel int[] array for df, another int[] for pointers into the postings byte pool, etc? Something like that?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I mean instead of allocating an instance per unique term, we assign an integer ID (dense, ie, 0, 1, 2...).&lt;/p&gt;

&lt;p&gt;And then we have an array for each member now in FreqProxTermsWriter.PostingList, ie int[] docFreqs, int [] lastDocIDs, etc.  Then to look up say the lastDocID for a given postingID you just get lastDocIDs&lt;span class=&quot;error&quot;&gt;&amp;#91;postingID&amp;#93;&lt;/span&gt;.  If we&apos;re worried about oversize allocation overhead, we can make these arrays paged... but that&apos;d slow down each access.&lt;/p&gt;</comment>
                    <comment id="12845391" author="michaelbusch" created="Mon, 15 Mar 2010 16:14:33 +0000"  >&lt;p&gt;I&apos;ll reply on &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2324&quot; title=&quot;Per thread DocumentsWriters that write their own private segments&quot;&gt;&lt;del&gt;LUCENE-2324&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;</comment>
                </comments>
                <issuelinks>
                        <issuelinktype id="10032">
                <name>Blocker</name>
                                                <inwardlinks description="is blocked by">
                            <issuelink>
            <issuekey id="12458093">LUCENE-2294</issuekey>
        </issuelink>
                    </inwardlinks>
                            </issuelinktype>
                        <issuelinktype id="10030">
                <name>Reference</name>
                                <outwardlinks description="relates to">
                            <issuelink>
            <issuekey id="12459100">LUCENE-2324</issuekey>
        </issuelink>
                    </outwardlinks>
                                            </issuelinktype>
                    </issuelinks>
                <attachments>
                    <attachment id="12438683" name="LUCENE-2293.patch" size="6322" author="mikemccand" created="Sat, 13 Mar 2010 16:04:43 +0000" />
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>1.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Wed, 3 Mar 2010 22:02:12 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>11500</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>25432</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                    <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                <customfieldname>Time in Status</customfieldname>
                <customfieldvalues>
                    
                </customfieldvalues>
            </customfield>
                            </customfields>
    </item>
</channel>
</rss>