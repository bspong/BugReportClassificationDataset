<!-- 
RSS generated by JIRA (5.2.8#851-sha1:3262fdc28b4bc8b23784e13eadc26a22399f5d88) at Tue Jul 16 13:26:47 UTC 2013

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/LUCENE-1821/LUCENE-1821.xml?field=key&field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>5.2.8</version>
        <build-number>851</build-number>
        <build-date>26-02-2013</build-date>
    </build-info>

<item>
            <title>[LUCENE-1821] Weight.scorer() not passed doc offset for &quot;sub reader&quot;</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-1821</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Now that searching is done on a per segment basis, there is no way for a Scorer to know the &quot;actual&quot; doc id for the document&apos;s it matches (only the relative doc offset into the segment)&lt;/p&gt;

&lt;p&gt;If using caches in your scorer that are based on the &quot;entire&quot; index (all segments), there is now no way to index into them properly from inside a Scorer because the scorer is not passed the needed offset to calculate the &quot;real&quot; docid&lt;/p&gt;

&lt;p&gt;suggest having Weight.scorer() method also take a integer for the doc offset&lt;/p&gt;

&lt;p&gt;Abstract Weight class should have a constructor that takes this offset as well as a method to get the offset&lt;br/&gt;
All Weights that have &quot;sub&quot; weights must pass this offset down to created &quot;sub&quot; weights&lt;/p&gt;


&lt;p&gt;Details on workaround:&lt;br/&gt;
In order to work around this, you must do the following:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Subclass IndexSearcher&lt;/li&gt;
	&lt;li&gt;Add &quot;int getIndexReaderBase(IndexReader)&quot; method to your subclass&lt;/li&gt;
	&lt;li&gt;during Weight creation, the Weight must hold onto a reference to the passed in Searcher (casted to your sub class)&lt;/li&gt;
	&lt;li&gt;during Scorer creation, the Scorer must be passed the result of YourSearcher.getIndexReaderBase(reader)&lt;/li&gt;
	&lt;li&gt;Scorer can now rebase any collected docids using this offset&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Example implementation of getIndexReaderBase():&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-comment&quot;&gt;// NOTE: more efficient implementation can be done &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; you cache the result &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; gatherSubReaders in your constructor
&lt;/span&gt;&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; getIndexReaderBase(IndexReader reader) {
  &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (reader == getReader()) {
    &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; 0;
  } &lt;span class=&quot;code-keyword&quot;&gt;else&lt;/span&gt; {
    List readers = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; ArrayList();
    gatherSubReaders(readers);
    Iterator iter = readers.iterator();
    &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; maxDoc = 0;
    &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; (iter.hasNext()) {
      IndexReader r = (IndexReader)iter.next();
      &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (r == reader) {
        &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; maxDoc;
      } 
      maxDoc += r.maxDoc();
    } 
  }
  &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; -1; &lt;span class=&quot;code-comment&quot;&gt;// reader not in searcher
&lt;/span&gt;}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Notes:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;This workaround makes it so you cannot serialize your custom Weight implementation&lt;/li&gt;
&lt;/ul&gt;
</description>
                <environment></environment>
            <key id="12433377">LUCENE-1821</key>
            <summary>Weight.scorer() not passed doc offset for &quot;sub reader&quot;</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png">Closed</status>
                    <resolution id="1">Fixed</resolution>
                                <assignee username="simonw">Simon Willnauer</assignee>
                                <reporter username="tsmith">Tim Smith</reporter>
                        <labels>
                    </labels>
                <created>Tue, 18 Aug 2009 21:59:42 +0100</created>
                <updated>Fri, 10 May 2013 11:43:48 +0100</updated>
                    <resolved>Fri, 14 Jan 2011 18:12:53 +0000</resolved>
                            <version>2.9</version>
                                <fixVersion>4.0-ALPHA</fixVersion>
                                <component>core/search</component>
                        <due></due>
                    <votes>0</votes>
                        <watches>0</watches>
                                                    <comments>
                    <comment id="12744711" author="tsmith" created="Tue, 18 Aug 2009 22:06:54 +0100"  >&lt;p&gt;since Weight.explain() is passed the &quot;sub reader&quot;, it also should be passed the offset in order to calculate the &quot;real&quot; docid&lt;/p&gt;</comment>
                    <comment id="12744715" author="markrmiller@gmail.com" created="Tue, 18 Aug 2009 22:07:57 +0100"  >&lt;p&gt;I think we would prefer that you didn&apos;t cache by multi-reader. We want to encourage cache by segment.&lt;/p&gt;

&lt;p&gt;In my mind, I don&apos;t think we should try and squeeze this into 2.9. We can see if anyone disagrees.&lt;/p&gt;</comment>
                    <comment id="12744716" author="tsmith" created="Tue, 18 Aug 2009 22:09:17 +0100"  >&lt;p&gt;For the explain case, if Searcher had the following method, the base offset would not have to be passed in (could be inferred):&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; getIndexReaderBase(IndexReader reader);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                    <comment id="12744718" author="tsmith" created="Tue, 18 Aug 2009 22:11:14 +0100"  >&lt;p&gt;I have a really crazy cache that for performance and memory reasons has to be based on the Multi-Reader (and this is very important in my application)&lt;/p&gt;

&lt;p&gt;looking closer at the per segment searching, i cannot upgrade to 2.9 (which i really want to do) because of this&lt;/p&gt;</comment>
                    <comment id="12744719" author="markrmiller@gmail.com" created="Tue, 18 Aug 2009 22:11:17 +0100"  >&lt;p&gt;Can you give a use example? What are you caching in the Scorer index wide? It really seems you should be doing it per segment ...&lt;/p&gt;</comment>
                    <comment id="12744721" author="tsmith" created="Tue, 18 Aug 2009 22:16:06 +0100"  >&lt;p&gt;I&apos;m caching an &quot;index&quot; into the terms for the document (Think of the &quot;StringIndex&quot; without the String[])&lt;br/&gt;
all i care about is that integer index value&lt;/p&gt;

&lt;p&gt;breaking this out into a per segment cache would then require me to also store along with the int[] index array a String[] &lt;br/&gt;
this then gets really slow and nasty when trying to run through the required algorithms to use the cache for my application&lt;/p&gt;</comment>
                    <comment id="12744732" author="tsmith" created="Tue, 18 Aug 2009 22:25:07 +0100"  >&lt;p&gt;I think i found a workaround (to allow me to upgrade)&lt;/p&gt;

&lt;p&gt;I already subclass IndexSearcher, so i could just add the following method (and use this instead of regular search())&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; void mySearch(Weight weight, Collector collector) &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; IOException {
    collector.setNextReader(reader, 0);
    Scorer scorer = weight.scorer(reader, !collector.acceptsDocsOutOfOrder(), &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;);
    &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (scorer != &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;) {
      scorer.score(collector);
    }
  }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;however, this prevents me from being able to take full advantage of per segment caching (which i really want to use for all my other caches)&lt;/p&gt;</comment>
                    <comment id="12744742" author="markrmiller@gmail.com" created="Tue, 18 Aug 2009 22:53:24 +0100"  >&lt;blockquote&gt;&lt;p&gt;breaking this out into a per segment cache would then require me to also store along with the int[] index array a String[] &lt;br/&gt;
this then gets really slow and nasty when trying to run through the required algorithms to use the cache for my application &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Why is this? You must be keying this to the multi reader now right? Why cannot you not do the same thing keyed to the sub reader?&lt;/p&gt;</comment>
                    <comment id="12744746" author="tsmith" created="Tue, 18 Aug 2009 22:58:43 +0100"  >&lt;p&gt;Thought about it a bit more&lt;/p&gt;

&lt;p&gt;all i really need is the following method added to Searcher&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
 &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; getIndexReaderBase(IndexReader);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;if this doesn&apos;t go into 2.9, i should be able to add this method to my subclass of IndexSearcher (so i wouldn&apos;t put this as blocker status for me anymore)&lt;/p&gt;

&lt;p&gt;as long as Searcher has getIndexReaderBase(), i can have my Weight hold onto the Searcher it was created with and when the scorer is created off it, i can lookup the base offset based on the IndexReader passed in&lt;/p&gt;

&lt;p&gt;i can work up a patch for this in the morning if desired&lt;/p&gt;

&lt;p&gt;also, there should be a big old warning in the change log about MultiReader based cache used in a Weight/Scorer being broken (not sure if there&apos;s a warning specifically about this in there)&lt;/p&gt;</comment>
                    <comment id="12744747" author="tsmith" created="Tue, 18 Aug 2009 23:01:58 +0100"  >&lt;p&gt;the int[] index array is used as a &quot;perfect&quot; hash function in order to be able to map all documents sharing the same value, so the int[] index from each sub reader can have different hash values for the same term (Thats why the string sorting in 2.9 is real nasty, because it can&apos;t just compare the ord values any more (unless you&apos;re lucky and you&apos;re comparing docs from the same reader)&lt;/p&gt;</comment>
                    <comment id="12744755" author="markrmiller@gmail.com" created="Tue, 18 Aug 2009 23:23:41 +0100"  >&lt;p&gt;I&apos;d go with your solution then - I&apos;d hate to expose doc base stuff on Searcher or in Scorer (I&apos;m not a big fan of gatherSubReaders being protected either).&lt;/p&gt;

&lt;p&gt;Also, &lt;b&gt;we&lt;/b&gt; can&apos;t put Searcher on a Weight - it needs to be serializable (though you can if you don&apos;t need to count on that).&lt;/p&gt;

&lt;p&gt;Where do you suggest we should place a warning that we havn&apos;t? I don&apos;t think we ever intended to support the use of external ids.&lt;/p&gt;

&lt;p&gt;I think your use case is fairly special, and I&apos;m not sure we should expose per segment stuff where we don&apos;t have to.&lt;/p&gt;

&lt;p&gt;It is an interesting use case though - maybe someone else will chime in.&lt;/p&gt;</comment>
                    <comment id="12744759" author="tsmith" created="Tue, 18 Aug 2009 23:35:33 +0100"  >&lt;p&gt;I&apos;ll prepare a patch in the morning (unless someone beats me to it) and look over the changelog then to suggest some more disclaimers (if what&apos;s there isn&apos;t sufficient)&lt;/p&gt;

&lt;p&gt;I don&apos;t hold the contract that Weight be serializable (so i&apos;m safe there)&lt;/p&gt;

&lt;p&gt;i agree that per-segment is the way to go in general and should be as tight as possible (as long as i can get my mits on the &quot;sub readers&quot;)&lt;/p&gt;

&lt;p&gt;but there are use cases that still require looking at the index as a whole as well&lt;br/&gt;
especially if you need to know the number of unique terms for a field, or otherwise need documents in one segment to be aware of documents in other segments (i could probably come up a bunch more use cases there)&lt;/p&gt;</comment>
                    <comment id="12744771" author="markrmiller@gmail.com" created="Wed, 19 Aug 2009 00:01:52 +0100"  >&lt;p&gt;You should pull that info from a top level reader and somehow pass it to your scorer through your query impl or something than.&lt;/p&gt;

&lt;p&gt;We are working very hard to make search per segment and discourage non per segment use - it doesn&apos;t seem like you want to be consulting the entire index on every call to scorer. Passing the base around does not help you with looking at the whole index either - just in terms of doc ids - which we don&apos;t support externally - and you are essentially caching them externally. The document ids should be purely internal - I don&apos;t think we want to support documents in one segment being aware of docs in another segment either.&lt;/p&gt;

&lt;p&gt;createWeight on Query gives you the opportunity to grab stuff your Scorer may need top level (off the Searcher).&lt;/p&gt;

&lt;p&gt;You could also make a new Query each time that takes a top level IndexReader and uses it.&lt;/p&gt;

&lt;p&gt;You can also override IndexSearcher (as you have) and work around things there.&lt;/p&gt;
</comment>
                    <comment id="12744793" author="tsmith" created="Wed, 19 Aug 2009 01:11:16 +0100"  >&lt;p&gt;My current plan of attack for this use case will be to:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;pull the cache using the MultiReader at createWeight() time (index into cache will be MultiReader docid)&lt;/li&gt;
	&lt;li&gt;pull the base offset for the IndexReader at scorer() creation time (will need to add the getIndexReaderBase() method to my searcher to do so)&lt;/li&gt;
	&lt;li&gt;when the scorer needs to hit the cache, it&apos;ll add the base to the scorer&apos;s docid to get the key for the cache lookup&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I should be able to do this easily enough with a customized IndexSearcher (subclass)&lt;/p&gt;

&lt;p&gt;there are use cases where documents from one segment need to be aware of documents from other segments&lt;br/&gt;
sorting is such a use case (this is just done at the Collector level, so there are more hooks to do the needed base offset stuff)&lt;br/&gt;
duplicate removal is another such use case (only return the first document for docs sharing a field value)&lt;/p&gt;

&lt;p&gt;both these use cases can be done at the Collector level, however Duplicate Removal could potentially be done at the Query level in order to perform duplicate removal at any location in the query matching&lt;br/&gt;
also, efficient duplicate removal for a String field would require the int[] ord index in order to reduce overall memory requirements&lt;br/&gt;
Using the int[] ord index allows using a BitSet for the hash set required to mark if a document for a specified value has been encountered (would need a HashSet&amp;lt;String&amp;gt; otherwise (ugh))&lt;/p&gt;

&lt;p&gt;my particular use case must be done at the query level in order to have full boolean query support, and the ability to layer multiple queries with all combinations of AND/OR/NOT, and any other query operators, and sadly i have yet to come up with any way to create a cache on a per segment level (without creating the cache at the MultiReader level)&lt;/p&gt;</comment>
                    <comment id="12744812" author="markrmiller@gmail.com" created="Wed, 19 Aug 2009 01:54:19 +0100"  >&lt;p&gt;Sorting is internal. To allow this switch to per segment we implemented a new HitCollector that can collect from multiple readers - sorting across multiple segments still needed to be supported, and custom comparators still needed to be supported. All of the ids are manged internally though - when I say internally, I mean within Lucene. If you implement a custom FieldComparator, you are still respecting Lucene&apos;s internal id usage. We map priority queue values so that they can be compared with the values in a different Reader, but again, all of the ids are managed internally. All caching and everything is still done per segment. All FieldCaches are per Reader and per segment.&lt;/p&gt;

&lt;p&gt;The goal is to move all caches to the segment level in Lucene - we don&apos;t want to encourage users to cache per multi-reader by providing API help to do so.&lt;/p&gt;

&lt;p&gt;If you need index wide stats, you use the Weight.&lt;/p&gt;

&lt;p&gt;You are trying to use the internal ids externally - you are caching from external id to ord - its really not something I think we intend to support. The fact that we don&apos;t support it is why we were able to make this change. The FieldCache is the caching mechanism that Lucene supports with internal ids - and it supports it per segment.&lt;/p&gt;</comment>
                    <comment id="12744824" author="hossman" created="Wed, 19 Aug 2009 02:10:07 +0100"  >&lt;blockquote&gt;&lt;p&gt;you are caching from external id to ord - its really not something I think we intend to support. The fact that we don&apos;t support it is why we were able to make this change. The FieldCache is the caching mechanism that Lucene supports with internal ids - and it supports it per segment.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think Tim&apos;s got a valid point though about wanting an ordinal value across the entire index ... he&apos;s not using external ids, he&apos;s using the internal lucene docIds, and wants to know the ordinal value of a field for each doc across the entire index &amp;#8211; as he said, he&apos;s essentially using a FieldCache.StringIndex he just doesn&apos;t care about the String[] part.&lt;/p&gt;

&lt;p&gt;Solr had/has the same problem with some of the function queries that wanted ordinal values (or the min/max field value for the whole index) that i think yonik just punted on and fetched the outermost field cache anyway ... we just weren&apos;t using it inside the Weight class, so we didn&apos;t encounter the specified problem Tim did.&lt;/p&gt;</comment>
                    <comment id="12744827" author="markrmiller@gmail.com" created="Wed, 19 Aug 2009 02:29:11 +0100"  >&lt;blockquote&gt;&lt;p&gt;I think Tim&apos;s got a valid point though about wanting an ordinal value across the entire index ...&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I don&apos;t disagree about wanting them at all. Hes using them for a neat purpose. &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;he&apos;s not using external ids, he&apos;s using the internal lucene docIds&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;If he were respecting the internal ids, you wouldn&apos;t need to calculate the multi-reader id. Hes essentially caching the multi-reader ids - thats the same as using a filter that always allows doc 0 to pass - its using the internal ids externally. To use the ids correctly, you get a reader and an id space that starts at 0 for that reader. If you want to use the whole reader, you should work with the multi-reader. You can use the multi-reader without breaking it apart here as well if you need to.&lt;/p&gt;

&lt;p&gt;I think its a slippery slope - we start having to support both the segment ids, plus the multi-reader ids. And as we work on real-time, we will have to count on users caching that way - I think its better to try and work all of our support towards per segment.&lt;/p&gt;

&lt;p&gt;I&apos;ll leave it for smarter people to discuss for now - but I don&apos;t think its the right path. He can essentially do what he needs without built in support, and personally I think thats the way to go. I think its great that right now, other than the sorting/hitcollector, things don&apos;t know about the sub reader breakout.&lt;/p&gt;</comment>
                    <comment id="12744833" author="tsmith" created="Wed, 19 Aug 2009 02:46:00 +0100"  >&lt;blockquote&gt;&lt;p&gt;The goal is to move all caches to the segment level in Lucene - we don&apos;t want to encourage users to cache per multi-reader by providing API help to do so.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I agree that this is the goal, and that using per segment caches should be the encouraged route for field caching needs. &lt;br/&gt;
I plan to update the vast majority of the caches i use to be loaded on a per segment basis once i switch to 2.9 to take advantage of this.&lt;br/&gt;
But it should still be possible for advanced users to do caching on the multireader level. This may require porting upon subsequent versions of lucene (as i&apos;m seeing i will have to for 2.9), however this should remain possible&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;If you need index wide stats, you use the Weight.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I&apos;m currently using weight to get this cache on the multireader level, however with 2.9 i will have to jump through some more hoops in order to be able to use this cache on each sub reader&apos;s scorer&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;You are trying to use the internal ids externally&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;All my usage of &quot;internal&quot; docids occurs inside Weight, Scorer, and HitCollector implementations. I don&apos;t see how this is really &quot;external&quot; as it is using published interfaces. Its just that the interpretation of these interfaces changed for 2.9 (i have no problem with this as long as i can port from 2.4 with minimal to moderate effort). The reason they were able to change was only because no implementations provided by vanilla lucene or in contrib required the &quot;whollistic&quot; view of the index&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;The FieldCache is the caching mechanism that Lucene supports with internal ids - and it supports it per segment.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;The FieldCache mechanism did not meet all my needs with regards to schema/retention policy/etc, so i have been doing caching in my own code base for quite some time. While the FieldCache usage should be encouraged, it should not be required of advanced users. It should be acceptable for advanced users to feel some pain on upgrading, but there should be a rather clear path for doing so (without a loss of functionality, and ideally without requiring custom patches on top of a released version of lucene)&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Sorting is internal.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;While sorting is provided by lucene APIs, there is nothing (and should be nothing) stopping someone from performing sorting on their own terms via the Collector interface and their own priority queues/API&lt;/p&gt;
</comment>
                    <comment id="12744847" author="markrmiller@gmail.com" created="Wed, 19 Aug 2009 03:25:51 +0100"  >&lt;p&gt;The &quot;internal&quot; vs &quot;external&quot; is kind of confusing made up terms - my fault really.&lt;/p&gt;

&lt;p&gt;When I think of using the ids &apos;internally&apos; I&apos;m thinking that you are taking the index reader and making no assumptions. You just use the single reader and its id space. You can use those ids to get values, and you can map from those ids to values.&lt;/p&gt;

&lt;p&gt;The assumption being made here is that you can load up ords for every doc and that these ords will be comparable in a way that every document id across the whole index maps to the same ord if it has the same value for a field. Nothing in the API promised that to my knowledge - it just happened to be a happy side effect. &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;While sorting is provided by lucene APIs, there is nothing (and should be nothing) stopping someone from performing sorting on their own terms via the Collector interface and their own priority queues/API&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Indeed - just like there is nothing stopping you from continuing to use a MultiReader for this functionality.&lt;/p&gt;

&lt;p&gt;What I mean by sorting is internal is that we specifically support comparing ords/values across readers. I think we would prefer that you don&apos;t count on ids coming from the top reader or a sub reader in other cases. We don&apos;t promise one way or another. We just give a reader and say work with this reader.&lt;/p&gt;

&lt;p&gt;Experts can generally jump around that if they need to - Solr does a bit of this - or you can choose to continue using Multi-Readers.&lt;/p&gt;

&lt;p&gt;I&apos;m not saying we should make it impossible for you to do this - but I don&apos;t think we should open a path for scorers to reconstruct multi-reader virtual ids. I don&apos;t think a Scorer should know or care why type of IndexReader it is working with.&lt;/p&gt;</comment>
                    <comment id="12744853" author="markrmiller@gmail.com" created="Wed, 19 Aug 2009 04:08:56 +0100"  >&lt;blockquote&gt;&lt;p&gt;he&apos;s not using external ids, he&apos;s using the internal lucene docIds&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Let me try a response to this one once more:&lt;/p&gt;

&lt;p&gt;If you try and make a filter that always matches docs 0-10, you could have made a filter that just sets bits 0-10. You are technically using &apos;internal&apos; lucene doc ids. &lt;/p&gt;

&lt;p&gt;With the new per segment search though, you will find that you match the first 10 docs in every segment, not just the first 10 docs in the multi-reader virtual id space. This is what I call using the internal doc ids externally. You are counting on a single id space covering the whole index for the reader. This was never promised though. So just like this type of filter was not &lt;b&gt;really&lt;/b&gt; supported and no longer works - this method of relying on the IndexReader to support one id space across the whole index no longer works as well. The Searcher supports the whole index, but a given IndexReader was never promised to do so. We could have passed base doc ids to the filters so that they could reconstruct the multi-reader virtual ids, and then just actually match docs 0-10 - but thats exactly the opposite of what we are trying to achieve. We switched to per segment to get away from that.&lt;/p&gt;</comment>
                    <comment id="12744854" author="tsmith" created="Wed, 19 Aug 2009 04:11:08 +0100"  >&lt;blockquote&gt;&lt;p&gt;I&apos;m not saying we should make it impossible for you to do this - but I don&apos;t think we should open a path for scorers to reconstruct multi-reader virtual ids. I don&apos;t think a Scorer should know or care why type of IndexReader it is working with.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;i disagree with that, i think the APIs should make it clear whether you are working with a sub reader or a top level reader&lt;br/&gt;
if a Scorer is given an IndexReader, it should have the same ability to reconstruct the &quot;client facing&quot; docid in the same manner as the Collector interface provides in order to provide a consistent interface between Collectors and Scorers&lt;br/&gt;
This reconstruction should be documented as &quot;advanced&quot;, however it should still be available&lt;/p&gt;

&lt;p&gt;Whereever an IndexReader is exposed in API calls, it should be possible to walk the IndexReader&apos;s parent IndexReaders until you get the top level reader in order to have the full context of that IndexReader. This walking should only be done at &quot;init&quot; time (Scorer construction/Collector setScorer(), and so on depending on need of the application, but it should be possible (ideally without doing nasty things))&lt;/p&gt;</comment>
                    <comment id="12744855" author="tsmith" created="Wed, 19 Aug 2009 04:17:17 +0100"  >&lt;p&gt;on &quot;internal lucene ids&quot;&lt;/p&gt;

&lt;p&gt;right now (2.4) i always know what docids map to what IndexReader (always the top level)&lt;br/&gt;
i don&apos;t have a problem breaking that assumption, as long as i have the context to map docids between spaces (sub reader to top level and back (mabye even a couple more levels in between))  &lt;/p&gt;

&lt;p&gt;I opened this ticket soley because in the Scorer API there is no presented way to actually do this mapping between spaces (short of the &quot;hacks&quot; i discussed way above)&lt;br/&gt;
I have no problem whatsoever in the changing of what &quot;space&quot; docids exist in, as long as i can get to and from the top level to this space&lt;br/&gt;
I also have no problem if how to do this mapping changes between releases (as long as its documented)&lt;/p&gt;</comment>
                    <comment id="12744857" author="markrmiller@gmail.com" created="Wed, 19 Aug 2009 04:30:05 +0100"  >&lt;p&gt;You may get furthers with others than me Tim, so don&apos;t get too, too caught up with me. McCandless is still on vacation for one, and he may have ideas other than mine (and certainly better ideas even if they are not other) - others may still jump in too. The two of us did the majority of the per segment work though. Yonik has also fought through a lot of this type of stuff with Solr - I&apos;m sure he has some stance on this stuff.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Whereever an IndexReader is exposed in API calls, it should be possible to walk the IndexReader&apos;s parent IndexReaders until you get the top level reader in order to have the full context of that IndexReader&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Solr now works this way to get around some of the per segment issues. I&apos;m not sure if it makes sense to support that fully in Lucene or not. Perhaps so. Too late for me to properly think though - time for bed.&lt;/p&gt;

&lt;p&gt;Any downsides I wonder ...&lt;/p&gt;

&lt;p&gt;I think my main issue is that it will encourage people to work per top level reader and it will force us to take that into account ...&lt;/p&gt;
</comment>
                    <comment id="12744970" author="mikemccand" created="Wed, 19 Aug 2009 10:45:24 +0100"  >
&lt;p&gt;BTW contrib/spatial has exactly this same problem.  It currently&lt;br/&gt;
builds up a cache, keyed on the &quot;top&quot; (MultiReader&apos;s) docID, of the&lt;br/&gt;
precise distance computed by its precise distance filters, to then be&lt;br/&gt;
used during sorting.  Right now it simply computes its own docBase and&lt;br/&gt;
increments it every time getDocIdSet() is called (which is messy).&lt;br/&gt;
Though I think it could (and should) switch to a per-segment cache.&lt;/p&gt;

&lt;p&gt;I am torn.  On the one hand we don&apos;t want to encourage apps to be&lt;br/&gt;
using &quot;top docIDs&quot; anywhere &quot;down low&quot; (eg Weight/Scorer).  We&apos;d like&lt;br/&gt;
all such per-segment swtiching to happen &quot;up high&quot;.&lt;/p&gt;

&lt;p&gt;But on the other hand, this is quite a sudden change, and most&lt;br/&gt;
advanced apps will be using the top docIDs by definition (since&lt;br/&gt;
per-segment docIDs only becomes an &lt;span class=&quot;error&quot;&gt;&amp;#91;easy&amp;#93;&lt;/span&gt; option in 2.9), so it&apos;d be&lt;br/&gt;
more friendly to offer up a cleaner migration path for such apps where&lt;br/&gt;
Weight/Scorer is told its docBase.&lt;/p&gt;

&lt;p&gt;And, having to migrate an ord index from &quot;top&quot; to &quot;sub&quot; docIDs is&lt;br/&gt;
truly a nightmare, having gone through that with Mark in getting&lt;br/&gt;
String sorting to work per segment!&lt;/p&gt;</comment>
                    <comment id="12745036" author="tsmith" created="Wed, 19 Aug 2009 13:46:32 +0100"  >&lt;p&gt;Concerning the changelog, i feel the below should be added to the &quot;Changes in runtime behavior&quot; section (it&apos;s kinda specified in &quot;New features&quot;, however&lt;br/&gt;
it is also a rather substantial change in the runtime behavior and should be called out explicitly there)&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
 13. LUCENE-1483: When searching over multiple segments, a &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; Scorer is created &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; each segment. 
        The Weight is created only once &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; the top level searcher. Each Scorer is passed the per-segment IndexReader.
        This will result in docids in the Scorer being internal to the per-segment IndexReader and there is currently no way
         to rebase these docids to the top level IndexReader. This results in any caches/filters that use docids over the top 
         IndexReader to be broken.
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                    <comment id="12745039" author="markrmiller@gmail.com" created="Wed, 19 Aug 2009 13:55:37 +0100"  >&lt;p&gt;I think thats a good idea. I think that last sentence needs a bit of work. Here is another attempt that I am still not quite happy with:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;13. LUCENE-1483: When searching over multiple segments, a &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; Scorer is created &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; each segment. 
        The Weight is created only once &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; the top level searcher. Each Scorer is passed the per-segment IndexReader.
        This will result in docids in the Scorer being internal to the per-segment IndexReader and there is currently no way
         to rebase these docids to the top level IndexReader. This will likely &lt;span class=&quot;code-keyword&quot;&gt;break&lt;/span&gt; any caches/filters in Scorers that rely on docids from the top 
         level IndexReader eg &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; you rely on the IndexReader to contain every doc id in the index.&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                    <comment id="12745041" author="tsmith" created="Wed, 19 Aug 2009 14:03:37 +0100"  >&lt;p&gt;One more pass&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
13. LUCENE-1483: When searching over multiple segments, a &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; Scorer is created &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; each segment. 
        The Weight is created only once &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; the top level searcher. Each Scorer is passed the per-segment IndexReader.
        This will result in docids in the Scorer being internal to the per-segment IndexReader. If a custom Scorer implementation 
         uses any caches/filters based on the top level IndexReader/Searcher, it will need to be updated to use caches/filters on a 
         per segment basis. There is currently no way provided to rebase the docids in the Scorer to the top level IndexReader.
         See LUCENE-1821 &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; discussion on workarounds &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                    <comment id="12745043" author="tsmith" created="Wed, 19 Aug 2009 14:07:40 +0100"  >&lt;p&gt;Here&apos;s a patch that adds getIndexReaderBase(IndexReader reader) to IndexSearcher&lt;/p&gt;

&lt;p&gt;sadly, this cannot be easily added to MultiSearcher as well as it uses &quot;Searchables&quot;, which would require adding this method to the Searchable interface&lt;br/&gt;
I could work up another patch that adds this method to the Searchable interface, however that has some back-compat concerns&lt;/p&gt;</comment>
                    <comment id="12745044" author="markrmiller@gmail.com" created="Wed, 19 Aug 2009 14:08:32 +0100"  >&lt;p&gt;Looks great!&lt;/p&gt;

&lt;p&gt;I still almost want to say rely on though:&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;uses any caches/filters based on the top level IndexReader/Searcher&lt;/p&gt;&lt;/blockquote&gt;

&lt;blockquote&gt;&lt;p&gt;uses any caches/filters that rely on being based on the top level IndexReader/Searcher&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;No? It seems like you could be based on a top level reader before, but not rely on the fact that it was a top level ...&lt;/p&gt;</comment>
                    <comment id="12745045" author="tsmith" created="Wed, 19 Aug 2009 14:10:38 +0100"  >&lt;p&gt;&quot;rely on&quot; it is&lt;/p&gt;</comment>
                    <comment id="12745367" author="mikemccand" created="Thu, 20 Aug 2009 10:57:40 +0100"  >&lt;p&gt;I think we should in fact add this API to 2.9?  It can ease the transition for users doing expert stuff w/ Lucene today.  The current patch looks reasonable, but we should mark it as expert and note that one should switch one&apos;s app logic to be per-segment whenever possible, instead of operating in the composite reader&apos;s docID space.  We should also note that it&apos;s O(N) CPU cost, ie, you should not call it for every hit (for example), but rather once per-segment and then hold onto that base.&lt;/p&gt;</comment>
                    <comment id="12745416" author="tsmith" created="Thu, 20 Aug 2009 13:05:17 +0100"  >&lt;p&gt;It would also be nice if the top level Searcher were pased in to Weight.scorer() (like in Weight.explain())&lt;br/&gt;
that way custom Weight implementations won&apos;t need to hold onto the Searcher at Weight creation time (thats a bigger patch though)&lt;/p&gt;</comment>
                    <comment id="12745418" author="markrmiller@gmail.com" created="Thu, 20 Aug 2009 13:08:29 +0100"  >&lt;blockquote&gt;&lt;p&gt;It can ease the transition for users doing expert stuff w/ Lucene today&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think it only helps if you counted on the reader having every doc id in it?&lt;/p&gt;

&lt;p&gt;It needs more than the current patch I think - we can&apos;t rely on people being able to plant a Searcher on the Weight ...&lt;/p&gt;</comment>
                    <comment id="12745423" author="tsmith" created="Thu, 20 Aug 2009 13:27:15 +0100"  >&lt;p&gt;I can work up another patch where the Searcher is passed into Weight.scorer() as well if that is an acceptable approach (this method was already changed alot in 2.9 anyway)&lt;/p&gt;</comment>
                    <comment id="12745430" author="markrmiller@gmail.com" created="Thu, 20 Aug 2009 13:52:54 +0100"  >&lt;p&gt;I&apos;m still not sold on this - these use cases don&apos;t work with MultiSearcher either right? I think thats part of this not being officially supported before either ...&lt;/p&gt;

&lt;p&gt;And passing the Searcher doesn&apos;t quite work right either - thats already kind of a bug with explain - you get the searcher with the doc - not a searcher that covers the whole multisearcher space.&lt;/p&gt;</comment>
                    <comment id="12745436" author="tsmith" created="Thu, 20 Aug 2009 14:06:03 +0100"  >&lt;p&gt;true, MultiSearcher does kink things up some (and the Searcher abstract class in general)&lt;/p&gt;

&lt;p&gt;personally, this is not a problem for me (don&apos;t use MultiSearcher (not yet at least)), and i&apos;m happy with being passed the IndexSearcher instance that directly contains the IndexReader i&apos;m being passed&lt;/p&gt;

&lt;p&gt;The contract could be marked that the Searcher provided is the direct container of the IndexReader also passed&lt;br/&gt;
at which point, both explain() and scorer() would be &quot;accurate&quot; in terms of this&lt;/p&gt;

&lt;p&gt;I would almost like to see something different passed in instead of a Searcher/IndexReader pair&lt;/p&gt;

&lt;p&gt;i would actually like to see a &quot;SearchContext&quot; sort of object passed in&lt;br/&gt;
this would represent the whole &quot;tree&quot; of Searchers/IndexReaders&lt;br/&gt;
this would allow access to the MultiSearcher, the direct IndexSearcher, and the sub IndexReader (which should actually be used for the scoring) (as well as any other Searcher&apos;s in the call stack) &lt;br/&gt;
this SearchContext could also pass in the &quot;topScorer/allowDocsInOrder&quot; flags (but that would be more difficult as scorers have subscorers that need to sometimes be created with different flags for these), but this SearchContext could be used to pass more information throughout the Scorer API in general from the top level (like - always use constant score queries where possible, use scoring algorithm X, Y, or Z, and so on)&lt;/p&gt;

&lt;p&gt;obviously this would impact the API of Searcher a good deal as it would have to maintain this stack as sub Searcher&apos;s search() methods are called)&lt;/p&gt;</comment>
                    <comment id="12745686" author="tsmith" created="Thu, 20 Aug 2009 22:56:52 +0100"  >&lt;p&gt;Marking as fix for 2.9 so this gets looked over real good prior to 2.9 going out (even if it is punted)&lt;/p&gt;

&lt;p&gt;i believe i can workaround this (still integrating 2.9 into my app, and i haven&apos;t got to per-field caching stuff yet)&lt;/p&gt;

&lt;p&gt;In order to get my app to work with 2.9 (without any major mods), i had to add the following to my IndexSearcher subclass:&lt;br/&gt;
NOTE: i never use Filter (thats why this skips it over)&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
  @Override
  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; void search(Weight weight, Filter filter, Collector collector) &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; IOException {
    &lt;span class=&quot;code-comment&quot;&gt;// Need to work on the top level reader &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; now
&lt;/span&gt;    collector.setNextReader(reader, 0);
    &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; Scorer scorer = weight.scorer(reader, !collector.acceptsDocsOutOfOrder(), &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;);
    &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (scorer != &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;) {
      scorer.score(collector);
    }
  }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                    <comment id="12745735" author="markrmiller@gmail.com" created="Fri, 21 Aug 2009 01:13:15 +0100"  >&lt;p&gt;I&apos;m still not a fan of giving access to the upper readers.&lt;/p&gt;

&lt;p&gt;I think I could go for having the offset available with the appropriate warnings.&lt;/p&gt;

&lt;p&gt;I tried this out, and after adjusting all scorer, explains to carry the offset as well, I ended up with one spot left:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; DocIdSet getDocIdSet(&lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; IndexReader reader) &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; IOException {
    &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; Weight weight = query.weight(&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; IndexSearcher(reader));
    &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; DocIdSet() {
      &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; DocIdSetIterator iterator() &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; IOException {
        &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; weight.scorer(reader, docBase?, &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;, &lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;);
      }
    };
  }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Trouble - in these cases, how do you pass the doc base? Its too much breakage to pass it with the reader &lt;b&gt;everywhere&lt;/b&gt;. You almost want a class that holds the reader ref and the docBase, but you still break apis all over. You could deprecate everything, but then you can&apos;t count on getting a good offset (would have to guess 0? ).&lt;/p&gt;</comment>
                    <comment id="12745754" author="tsmith" created="Fri, 21 Aug 2009 01:55:12 +0100"  >&lt;p&gt;in the case of the getDocIdSet() method, i would say you should pass &quot;0&quot; for the docBase&lt;br/&gt;
This is because in this case, you are asking for DocIdSet in the context of &lt;b&gt;reader&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;howevever, this method is actually also a bit broken now with per segment searching&lt;br/&gt;
what if &lt;b&gt;reader&lt;/b&gt; is a MultiReader&lt;br/&gt;
This could now incur the &quot;double ram usage&quot; penalty refered to in that &quot;explain()&quot; ticket i recall seeing last week&lt;br/&gt;
If  &lt;b&gt;query&lt;/b&gt; has any ValueSource based queries, it&apos;ll result in getting the ValueSource in the context of the MultiReader (if i&apos;m not mistaken)&lt;br/&gt;
So, this method in particular should probably be rewritten to return a DocIdSetIterator that will step through each segment in &quot;reader&quot; in turn&lt;/p&gt;</comment>
                    <comment id="12745756" author="markrmiller@gmail.com" created="Fri, 21 Aug 2009 02:03:43 +0100"  >&lt;blockquote&gt;
&lt;p&gt;howevever, this method is actually also a bit broken now with per segment searching&lt;br/&gt;
what if reader is a MultiReader&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Right - there are many places where this could be the case - your still free to use multi-readers, though we encourage you to switch. We provide a cool cache sanity checker to help you find these cases, and evaluate whether or not you can make the switch. &lt;b&gt;edit&lt;/b&gt; I know this doesn&apos;t help with filters - there was an issue that helped address that I think though - worked on by Hoss and Mike McCandless - not sure if that helps here or if this was overlooked or what though - I&apos;ll have to go skim that issue again. &lt;b&gt;edit&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;If you just pass 0, many times it will be wrong. Why shouldn&apos;t this have access to a doc id cache as well? We always ask for everything in the context of the Reader given. I think thats the issue. Lucene just never officially supported this use case - we can&apos;t with MultiSearcher, Searchable, Remote - the API doesn&apos;t work with the idea that you can count on all the doc ids from a Reader. You were taking advantage of the implementation and your limited use of the full API - but its never been part of the API IMHO.&lt;/p&gt;

&lt;p&gt;Perhaps we could one day change things - RMI hasn&apos;t really worked out in comparison to other methods large scale (supposedly very chatty - though I have been told very large installations have been built with it ) - we have already factored it into contrib. But this still doesn&apos;t fit the current model/API, and if we address it, it will take longer than 2.9 to do right IMO.&lt;/p&gt;</comment>
                    <comment id="12745911" author="mikemccand" created="Fri, 21 Aug 2009 12:41:40 +0100"  >&lt;p&gt;OK... pondering this some more, and on seeing just how much change&lt;br/&gt;
would be required, I&apos;m now nervous about making deep changes to&lt;br/&gt;
Lucene&apos;s scoring/filtering APIS (Weight.scorer, Filter.getDocIdSet) to&lt;br/&gt;
enable access to top readers and/or a sub-readers doc base.&lt;/p&gt;

&lt;p&gt;All of Lucene&apos;s core &amp;amp; contrib now operates &quot;context free&quot;&lt;br/&gt;
(per-segment), where each reader need not know its &quot;context&quot; in the&lt;br/&gt;
full searcher tree, and I think we should strongly encourage external&lt;br/&gt;
usage of these APIs to switch to context free as well.  Since there&lt;br/&gt;
are workarounds possible (accessing sub-readers via IndexSearcher),&lt;br/&gt;
external apps that have problems making the switch can use these&lt;br/&gt;
workarounds?&lt;/p&gt;</comment>
                    <comment id="12745941" author="tsmith" created="Fri, 21 Aug 2009 13:35:38 +0100"  >&lt;p&gt;I&apos;m OK with having to jump through some hoops in order to get back to the &quot;full index&quot; context&lt;/p&gt;

&lt;p&gt;It would be nice if this was more facilitated by lucene&apos;s API (IMO, this would be best handled by adding a Searcher as the first arg to Weight.scorer(), as then a Weight will not need to hold on to this (breaking serializable))&lt;/p&gt;

&lt;p&gt;There are definitely plenty of use cases that take advantage of the &quot;whole&quot; index (one created by IndexWriter), so this ability should not be removed&lt;br/&gt;
I have at least 3 in my application alone (and they are all very important)&lt;/p&gt;

&lt;p&gt;You get tradeoffs working &quot;Per-Segment&quot; vs &quot;Per-MultiReader&quot; when it comes to caching in general&lt;br/&gt;
going per-segment means caches load faster, and load less frequently, however this causes algorithms working with the caches to be slower (depending on algorithm and cache type)&lt;/p&gt;

&lt;p&gt;for static boosting from a field value (ValueSource), it makes no difference&lt;br/&gt;
for numeric sorting, it makes no difference &lt;/p&gt;

&lt;p&gt;for string sorting, it makes a big difference - you now have to do a bunch of String.equals() calls, where you didn&apos;t have to in 2.4 (just used the ord index)&lt;br/&gt;
Given this reason, you should really be able to do string sorting 2 ways&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;using per segment field cache (commit time/first query faster, sort time slower)&lt;/li&gt;
	&lt;li&gt;using multi-reader field cache (commit time/first query slower, sort time faster)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;This same argument also goes for features like faceting (not provided by lucene, but is provided by applications like solr, and my application). Using a per-segment cache will cause some significant performance loss when performing faceting, as it requires creating the facets for each segment, and then merging them (this results in a good deal of extra object overhead/memory overhead/more work where faceting on the multi-reader does not see this)&lt;/p&gt;

&lt;p&gt;In the end, it should be up to the application developer to choose what strategy works best for them, and their application (fast commits/fast cache loading may take a back seat to fast query execution)&lt;/p&gt;

&lt;p&gt;In general, i find there is a tradeoff between commit time and query time. The more you speed up commit time, the slower query time gets, and vice versa&lt;br/&gt;
I just want/need the ability to choose&lt;/p&gt;



</comment>
                    <comment id="12745948" author="markrmiller@gmail.com" created="Fri, 21 Aug 2009 13:48:08 +0100"  >&lt;blockquote&gt;&lt;p&gt;I&apos;m OK with having to jump through some hoops in order to get back to the &quot;full index&quot; context&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;You never officially had the full index context - only because you jettison a large part of the API did you have it.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;this would be best handled by adding a Searcher as the first arg to Weight.scorer()&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The current API would not support this without back compat breaks up the wazoo - the MultiSearcher can be on the client - its not available on the server. Passing just the local Searcher does not jive with the API.&lt;/p&gt;


&lt;blockquote&gt;&lt;p&gt;for string sorting, it makes a big difference - you now have to do a bunch of String.equals() calls, where you didn&apos;t have to in 2.4 (just used the ord index)&lt;br/&gt;
Given this reason, you should really be able to do string sorting 2 ways&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This is only valid for those short circuiting the API and ignoring MultiSearcher and its affects on the API. As a project, we can&apos;t and shouldn&apos;t support this type of thing unless we can make it work with MultiSearcher or eventually pull MultiSearcher.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;In the end, it should be up to the application developer to choose what strategy works best for them, and their application (fast commits/fast cache loading may take a back seat to fast query execution)&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;You can pick, but we have to be true to the API or change it (not easy with our back compat policies)&lt;/p&gt;</comment>
                    <comment id="12745951" author="markrmiller@gmail.com" created="Fri, 21 Aug 2009 13:55:55 +0100"  >&lt;p&gt;The Searcher being passed to explain is also really a break - I almost think we should pull it.&lt;/p&gt;

&lt;p&gt;We put it in because a break was already introduced when someone tried to add stats for the whole context in TermWeight - that wasn&apos;t legal though.&lt;/p&gt;

&lt;p&gt;To prevent further spread, I actually think we need to pull that searcher and that extra explain info in TermWeight.&lt;/p&gt;</comment>
                    <comment id="12745960" author="tsmith" created="Fri, 21 Aug 2009 14:15:32 +0100"  >&lt;blockquote&gt;&lt;p&gt;You never officially had the full index context&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Officially, i didn&apos;t &quot;not&quot; have the full index context either (it was undefined at best, but was clear from both lucene code and my use of the API that i did have the full index context)&lt;/p&gt;

&lt;p&gt;Whenever i do a search, i always explicitly know what context i&apos;m searching in (its always an IndexSearcher context)&lt;br/&gt;
further, whenever i pass an IndexReader to any method (to create a cache/etc), i explicitly know what context i&apos;m dealing with in order to know what the docids used mean&lt;br/&gt;
as the application developer, i have full control over what i pass into the lucene API and where, and know the context of passing that in (javadoc should just be fully clear on how what goes in is used (if not already) (i always have the option to not use a utility class/method provided by lucene if it does not have the proper context semantics i need (and can write my own that does)&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;The current API would not support this without back compat breaks up the wazoo&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;i kinda see what you mean here, but then how is it ok to pass an IndexReader to this method by the same right&lt;br/&gt;
it seems like it should be ok to pass the IndexSearcher (the direct context for the IndexReader) for the IndexReader in question to Weight.scorer() if its ok to pass the IndexReader (the scorer() method&apos;s interface was already changed between 2.4 and 2.9 (adding allowDocsInOrder and topScorer))&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;You can pick, but we have to be true to the API or change it (not easy with our back compat policies)&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;be fair, 2.9 has a lot of back compat breaks, both in API and runtime behavior (i had tons of compile errors when i dropped 2.9 in, as well as some other hacks i had to add in (at least temporarily) in order to get 2.9 to work due to run time changes (primarily this per segment search stuff))&lt;/p&gt;

&lt;p&gt;I have no problem with back compat breaks in general (only took me about a day to absorb 2.9 initially (still working on fully taking advantage of new features and getting rid of deprecated class use)) The only requirement i would put on a back compat break is that it have a workaround to get back the the previous versions behavior (in this case have it possible to remap the docids to the &quot;IndexSearcher&quot; context inside the scorer)&lt;/p&gt;
</comment>
                    <comment id="12745963" author="markrmiller@gmail.com" created="Fri, 21 Aug 2009 14:26:39 +0100"  >&lt;blockquote&gt;&lt;p&gt;(it was undefined at best, but was clear from both lucene code and my use of the API that i did have the full index context)&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;It was an implementation detail. If you look at MultiSearcher, Searchable, Searcher and how the API is put together, you can see we don&apos;t support that type of thing. I think its fairly clear after a little thought.&lt;/p&gt;

&lt;p&gt;You can limit your API&apos;s to handle just IndexSearchers, but as a project, we cannot.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;it seems like it should be ok to pass the IndexSearcher (the direct context for the IndexReader) &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;MultiSearcher and Searchable make this impossible IMO. We would be playing to those that don&apos;t fully use the API, and thats a mistake in my opinion. At best, we would have to shift the whole API.&lt;/p&gt;

&lt;p&gt;Its okay to pass the Reader because its a contextless Reader. There is no value in also passing a contextless Searcher IMO - especially when its an arbitrary different context. We have to live up to the current API - your throwing MultiSearcher, Searchable, Remote out the window.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;be fair, 2.9 has a lot of back compat breaks,&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Oh I&apos;m fair, I know that for sure - though I do like to argue way to much for my own good. All of these back compat breaks were painful to stomach &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/wink.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; But we reached each one under special circumstances - usually our own early incompetence &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; We technically are not allowed to just break things though. We break to fix what we already accidentally broke, or we break when we screwed up earlier and we are in between a rock and a hard place now - or we break when something else is broke anyway, so lets do more &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; This was the release of the break for sure. We don&apos;t necessarily want this to happen every release though, and its our responsibility to strive towards our back compat policy (listed on the wiki).&lt;/p&gt;

&lt;p&gt;I&apos;m not talking about a break in adding a Searcher - that would be fine - back compat is already broken there - but unless we can pass a MultiSearcher there over a remove RMI call, its a break of the whole API IMO.&lt;/p&gt;</comment>
                    <comment id="12745977" author="tsmith" created="Fri, 21 Aug 2009 14:58:19 +0100"  >&lt;blockquote&gt;
&lt;p&gt;It was an implementation detail. If you look at MultiSearcher, Searchable, Searcher and how the API is put together, you can see we don&apos;t support that type of thing. I think its fairly clear after a little thought.&lt;/p&gt;

&lt;p&gt;You can limit your API&apos;s to handle just IndexSearchers, but as a project, we cannot.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I totally understand your resistance here.  I get that i&apos;m really utilizing advanced lucene concepts at very low levels (and these are subject to some changes that i will have to absorb with new versions)&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Its okay to pass the Reader because its a contextless Reader. There is no value in also passing a contextless Searcher&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;well, when you pass the Searcher that contains Reader, the Reader is no longer contextless.&lt;br/&gt;
also, the context of the Searcher can be fairly well defined (its a &quot;leaf&quot; Searcher. the one that actually called Weight.scorer())&lt;/p&gt;

&lt;p&gt;Also, looking a bit more at MultiSearcher semantics, sorting requires this &quot;leaf&quot; Searcher context in order to work already&lt;br/&gt;
MultiSearcher just takes the top docs from each underlaying Searchable, adjusts the docids to the MultiSearcher Context, and sends them through another priority queue&lt;br/&gt;
So, this &quot;leaf&quot; Searcher context concept is required by sorting already. &lt;br/&gt;
I just want my Scorer to be given this &quot;leaf&quot; context as well&lt;/p&gt;

&lt;p&gt;Also, since it is a &quot;leaf&quot; context, the Weight.scorer() method could have the following interface:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
/**
 * @param searcher The IndexSearcher that contains reader.
 */
&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; Scorer scorer(IndexSearcher searcher, IndexReader reader, &lt;span class=&quot;code-object&quot;&gt;boolean&lt;/span&gt; allowDocsInOrder, &lt;span class=&quot;code-object&quot;&gt;boolean&lt;/span&gt; topScorer);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;then, with the patch i posted, i could call:&lt;br/&gt;
searcher.getIndexReaderBase(reader) &lt;br/&gt;
and i&apos;m all set&lt;/p&gt;
</comment>
                    <comment id="12745979" author="tsmith" created="Fri, 21 Aug 2009 15:00:04 +0100"  >&lt;p&gt;NOTE: if the leaf IndexSearcher were to be passed to scorer(), it would also have to be passed to explain()&lt;/p&gt;</comment>
                    <comment id="12745985" author="markrmiller@gmail.com" created="Fri, 21 Aug 2009 15:09:49 +0100"  >&lt;p&gt;I certainly think IndexSearcher makes a lot more sense than Searchable or Searcher there - it somewhat handles the whole API break thing - your clearly limiting to an IndexSearcher, so its compatible with the current API and can be clearly explained with javadoc - I still worry about pushing the API towards things that leave MultiSearcher/Remote out in the cold on features. They are technically first class citizens. I think some expert warnings could sell me anyway though.&lt;/p&gt;

&lt;p&gt;I still have a problem with this though:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; DocIdSet getDocIdSet(&lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; IndexReader reader) &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; IOException {
    &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; Weight weight = query.weight(&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; IndexSearcher(reader));
    &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; DocIdSet() {
      &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; DocIdSetIterator iterator() &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; IOException {
        &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; weight.scorer(reader, docBase?, &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;, &lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;);
      }
    };
  }

&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;That scorer call should get the right IndexSearcher and I don&apos;t see how it can without breaking back compat on this method and passing an IndexSearcher too.&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;edit *&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;And even if we fix this here, what about outside code doing the same thing? They won&apos;t get the right IndexSearcher.&lt;/p&gt;</comment>
                    <comment id="12745986" author="markrmiller@gmail.com" created="Fri, 21 Aug 2009 15:13:00 +0100"  >&lt;blockquote&gt;&lt;p&gt;clearly explained with javadoc&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;We need something along the lines of - you cannot count on this IndexSearcher to cover the whole index unless you control the use of your application to ensure that - its possible that this IndexSearcher will only correspond to one sub-index of multiple being used in the context of one large index.&lt;/p&gt;</comment>
                    <comment id="12745988" author="tsmith" created="Fri, 21 Aug 2009 15:15:22 +0100"  >&lt;p&gt;here&apos;s what you can do:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
  /** @deprecated use {@link getDocIdSet(IndexSearcher, IndexReader)} */
  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; DocIdSet getDocIdSet(&lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; IndexReader reader) &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; IOException {
    &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; getDocIdSet(&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; IndexSearcher(reader), reader);
  }

  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; DocIdSet getDocIdSet(&lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; IndexSearcher searcher, &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; IndexReader reader) {
    &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; Weight weight = query.weight(searcher);
    &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; DocIdSet() {
      &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; DocIdSetIterator iterator() &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; IOException {
        &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; weight.scorer(searcher, reader, &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;, &lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;);
      }
    };
  }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;and yeah, i&apos;m all for tons warnings in javadoc explicitly defining the contracts&lt;/p&gt;</comment>
                    <comment id="12745989" author="markrmiller@gmail.com" created="Fri, 21 Aug 2009 15:20:03 +0100"  >&lt;p&gt;I don&apos;t think thats fully back compat - though it covers a lot of ground.&lt;/p&gt;</comment>
                    <comment id="12745991" author="tsmith" created="Fri, 21 Aug 2009 15:28:44 +0100"  >&lt;p&gt;what class is this getDocIdSet method on (lacking the context of where its used)&lt;/p&gt;</comment>
                    <comment id="12745997" author="markrmiller@gmail.com" created="Fri, 21 Aug 2009 15:46:22 +0100"  >&lt;p&gt;Its org.apache.lucene.search.QueryWrapperFilter. And technically we have to account for subclasses and combinations and anything possible.&lt;/p&gt;

&lt;p&gt;This being the release of the break, who knows though. I can&apos;t reasonably see releasing without notes indicating that you &lt;b&gt;must&lt;/b&gt; recompile. And while we want to limit how much work must be done (we also consider the likely impact), this would be the time to skirt through.&lt;/p&gt;

&lt;p&gt;Pretty much depends on what McCandless weighs in with I guess - unless a new spectator pops up.&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
getDocIdSet(IndexReader) : DocIdSet - org.apache.lucene.search.QueryWrapperFilter
	bits(IndexReader) : BitSet - org.apache.lucene.search.BooleanFilterTest.getOldBitSetFilter(...).&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; Filter() {...}
	ConstantScorer(Similarity, IndexReader, Weight) - org.apache.lucene.search.ConstantScoreQuery.ConstantScorer
	explain(Searcher, IndexReader, &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt;) : Explanation - org.apache.lucene.search.FilteredQuery.createWeight(...).&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; Weight() {...}
	getDISI(ArrayList, &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt;, IndexReader) : DocIdSetIterator - org.apache.lucene.search.BooleanFilter
	getDocIdSet(IndexReader) : DocIdSet - org.apache.lucene.search.BooleanFilter (3 matches)
	getDocIdSet(IndexReader) : DocIdSet - org.apache.lucene.search.CachingWrapperFilter
	getDocIdSet(IndexReader) : DocIdSet - org.apache.lucene.search.CachingWrapperFilterHelper
	getDocIdSet(IndexReader) : DocIdSet - org.apache.lucene.search.RemoteCachingWrapperFilter
	getDocIdSet(IndexReader) : DocIdSet - org.apache.lucene.search.RemoteCachingWrapperFilterHelper
	scorer(IndexReader, &lt;span class=&quot;code-object&quot;&gt;boolean&lt;/span&gt;, &lt;span class=&quot;code-object&quot;&gt;boolean&lt;/span&gt;) : Scorer - org.apache.lucene.search.FilteredQuery.createWeight(...).&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; Weight() {...}
	searchWithFilter(IndexReader, Weight, Filter, Collector) : void - org.apache.lucene.search.IndexSearcher
	tstFilterCard(&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;, &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt;, Filter) : void - org.apache.lucene.search.BooleanFilterTest
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                    <comment id="12746004" author="tsmith" created="Fri, 21 Aug 2009 16:02:25 +0100"  >&lt;p&gt;Looks like Filter should have another method added getDocIdSet(IndexSearcher searcher, IndexReader reader) (deprecating getDocIdSet(IndexReader))&lt;/p&gt;

&lt;p&gt;new method would call old method by default (with little harm done in general)&lt;br/&gt;
IndexSearcher would call the new getDocIdSet() variant&lt;br/&gt;
QueryWrapperFilter would be updated to implement getDocIdSet(IndexSearcher, IndexReader) (with old method wrapping IndexReader with an IndexSearcher)&lt;br/&gt;
This would actually be cleaner for QueryWrapperFilter, as it wouldn&apos;t have to create a new IndexSearcher on every call&lt;/p&gt;

&lt;p&gt;i definitely see that this is potentially more painful than the changes to the scorer() method (question is how many people implement custom Filters?)&lt;/p&gt;

&lt;p&gt;Personally, i don&apos;t use Filter, so any changes here don&apos;t impact me, but to the best of my knowledge, i&apos;m not the only one using lucene &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                    <comment id="12746263" author="tsmith" created="Fri, 21 Aug 2009 22:59:13 +0100"  >&lt;p&gt;I started integrating the per-segment searching (removed my hack that was doing searching on MultiReader)&lt;/p&gt;

&lt;p&gt;In order to get my query implementations to work, i had to hold onto my Searcher in the Weight constructor and add getIndexReaderBase() method to my IndexSearcher implementation, and this seems to be working well&lt;/p&gt;

&lt;p&gt;I had 3 query implementations that were affected:&lt;br/&gt;
one used a cache that will be easy to create per segment (will have this use a per segment cache as soon as i can)&lt;br/&gt;
one used an int[] ord index (the underlaying cache cannot be made per segment)&lt;br/&gt;
one used a cached DocIdSet created over the top level MultiReader (should be able to have a DocIdSet per Segment reader here, but this will take some more thinking (source of the matching docids is from a separate index), will also need to know which sub docidset to use based on which IndexReader is passed to scorer() - shouldn&apos;t be any big deal)&lt;/p&gt;

&lt;p&gt;i&apos;m a bit concerned that i may not be testing &quot;multi-segment&quot; searching quite properly right now though since i think most of my indexes being tested only have one segment.&lt;br/&gt;
On that topic, if i create a subclass of LogByteSizeMergePolicy and return null from findMerges() and findMergesToExpungeDeletes() will this guarantee that segments will only be merged if i explicitly optimize? In which case, i can just pepper in some commits as i add documents to guarantee that i have more than 1 segment.&lt;/p&gt;

&lt;p&gt;Overall, i am really liking the per-segment stuff, and the Collector API in general &lt;br/&gt;
its already made it possible to optimize a good deal of things away (like calling Scorer.score() for docs that end up getting filtered away), however i hit some deoptimization due to some of the crazy stuff i had to do to make those 3 query implementations work, but this should only really be isolated to one of the implementations (and i can hopefully reoptimize those cases anyway)&lt;/p&gt;

&lt;p&gt;I would still like to see IndexSearcher passed to Weight.scorer(), and the getIndexReaderBase() method added to IndexSearcher&lt;br/&gt;
This will clean up my current &quot;hacks&quot; to map docids &lt;/p&gt;

</comment>
                    <comment id="12746278" author="yseeley@gmail.com" created="Fri, 21 Aug 2009 23:07:26 +0100"  >&lt;blockquote&gt;&lt;p&gt;i&apos;m a bit concerned that i may not be testing &quot;multi-segment&quot; searching quite properly right now though since i think most of my indexes being tested only have one segment.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yup - went through this in Solr.  I ended up changing the test config to use LogByteSizeMergePolicy  and maxBufferedDocs=10.&lt;/p&gt;</comment>
                    <comment id="12746291" author="markrmiller@gmail.com" created="Fri, 21 Aug 2009 23:18:33 +0100"  >&lt;p&gt;Lucene also use maxBufferedDocs to make sure there are multiple segments in some tests -&lt;/p&gt;

&lt;p&gt;Tim&apos;s idea here is nice too though - you still see some merging that way - with a mergepolicy that just returns null you can easily pick the exact number of segments by issuing commits mod whatever. (*edit you just have to set maxBufferedDocs to Integer.max)&lt;/p&gt;

&lt;p&gt;The mergepolicy impl is easy enough - anon class generated by eclipse will work out of the box.&lt;/p&gt;</comment>
                    <comment id="12746513" author="markrmiller@gmail.com" created="Sat, 22 Aug 2009 22:42:06 +0100"  >&lt;blockquote&gt;&lt;p&gt;Looks like Filter should have another method added getDocIdSet(IndexSearcher searcher, IndexReader reader) (deprecating getDocIdSet(IndexReader))&lt;br/&gt;
new method would call old method by default (with little harm done in general) &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Its the corner cases. Someone&apos;s class calls the deprecated method directly - someone is using that, plus a new class that overrides the none deprecated method - which never gets called, cause the other code is calling the dep method directly. Technically, everything has to be covered (Depending on how consensus goes anyway ... always depending ...). Its a pain in the butt just thinking about it. &lt;/p&gt;

&lt;p&gt;&lt;b&gt;edit&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;In your example deprecation this is actually the opposite - someone calls the new code directly, but other code you are using overrides the deprecated code. The override is now not called.&lt;/p&gt;</comment>
                    <comment id="12746600" author="tsmith" created="Sun, 23 Aug 2009 14:17:35 +0100"  >&lt;p&gt;well, you could go the route similar to the 2.4 TokenStream api (next() vs next(Token))&lt;/p&gt;

&lt;p&gt;have Filter.getDocIdSet(IndexSearcher, IndexReader) call Filter.getDocIdSet(IndexReader), and vice versa by default&lt;br/&gt;
one method or the other would be required to be overridden&lt;/p&gt;

&lt;p&gt;getDocIdSet(IndexReader) would be deprecated (and removed in 3.0)&lt;/p&gt;

&lt;p&gt;Since the deprecated method would be removed in 3.0, and since noone would probably be depending on these new semantics right away this should work&lt;/p&gt;

&lt;p&gt;Also, in general, QueryWrapperFilter performs a bit worse now in 2.9&lt;br/&gt;
this is because it creates an IndexSearcher for every query it wraps (which results in doing &quot;gatherSubReaders&quot; and creating the offsets anew each time getDocIdSet(IndexReader) is called&lt;br/&gt;
so, the new method with the IndexSearcher also passed in is much better for evaluating these Filters&lt;/p&gt;</comment>
                    <comment id="12746607" author="markrmiller@gmail.com" created="Sun, 23 Aug 2009 15:31:04 +0100"  >&lt;p&gt;You want to weigh in again Mike ? You still have the same stance as your last comment?&lt;/p&gt;</comment>
                    <comment id="12746608" author="markrmiller@gmail.com" created="Sun, 23 Aug 2009 15:34:30 +0100"  >&lt;blockquote&gt;&lt;p&gt;well, you could go the route similar to the 2.4 TokenStream api (next() vs next(Token))&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;thats a tough bunch of code to decide to spread ...&lt;/p&gt;</comment>
                    <comment id="12746613" author="tsmith" created="Sun, 23 Aug 2009 16:00:30 +0100"  >&lt;blockquote&gt;&lt;p&gt;thats a tough bunch of code to decide to spread ...&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;at least it&apos;ll be able to go away real soon with 3.0&lt;/p&gt;</comment>
                    <comment id="12746617" author="mikemccand" created="Sun, 23 Aug 2009 16:19:00 +0100"  >&lt;blockquote&gt;&lt;p&gt;You want to weigh in again Mike ? &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I do!  I&apos;m trying desperately to catch up over here &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                    <comment id="12746623" author="mikemccand" created="Sun, 23 Aug 2009 16:55:21 +0100"  >&lt;p&gt;Tim, one option might be to subclass DirectoryReader (though, it&apos;s package protected now, and, you&apos;d need to make your own &quot;open&quot; to return your subclass), and override getSequentialSubReaders to return null?  Then Lucene would treat it as an atomic reader.  Could that work?&lt;/p&gt;</comment>
                    <comment id="12746625" author="mikemccand" created="Sun, 23 Aug 2009 17:02:44 +0100"  >&lt;blockquote&gt;&lt;p&gt;for string sorting, it makes a big difference - you now have to do a bunch of String.equals() calls, where you didn&apos;t have to in 2.4 (just used the ord index)&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;We actually went through a number of iterations on this, on the first cutover to per-segment collection, and eventually arrived at a decent comparator (StringOrdValComparator) that operates per segment.  Have you tested performance of this comparator?&lt;/p&gt;</comment>
                    <comment id="12746630" author="yseeley@gmail.com" created="Sun, 23 Aug 2009 17:27:47 +0100"  >&lt;blockquote&gt;&lt;p&gt;Filter.getDocIdSet(IndexSearcher, IndexReader).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This suggests that one needs an IndexSearcher to get the ids matching a filter.&lt;/p&gt;</comment>
                    <comment id="12746633" author="mikemccand" created="Sun, 23 Aug 2009 17:48:08 +0100"  >&lt;blockquote&gt;&lt;p&gt;one used an int[] ord index (the underlaying cache cannot be made per segment)&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Could you compute the top-level ords, but then break it up&lt;br/&gt;
per-segment?  Ie, create your own map of IndexReader -&amp;gt; offset into&lt;br/&gt;
that large ord array?  This would make it &quot;virtually&quot; per-segment, but&lt;br/&gt;
allow you to continue computing at the top level.&lt;/p&gt;

&lt;p&gt;BTW another option is to simply accumulate your own docBase, by adding&lt;br/&gt;
up the maxDoc() every time an IndexReader is passed to your&lt;br/&gt;
Weight.scorer().  EG this is what contrib/spatial is now doing.&lt;/p&gt;

&lt;p&gt;This isn&apos;t a long-term solution, since the order in which Lucene&lt;br/&gt;
visits the readers isn&apos;t in general guaranteed, but it will work for&lt;br/&gt;
2.9 and buy time to figure out how to switch scoring to per-segment.&lt;/p&gt;</comment>
                    <comment id="12746634" author="mikemccand" created="Sun, 23 Aug 2009 17:52:14 +0100"  >&lt;blockquote&gt;&lt;p&gt;Using a per-segment cache will cause some significant performance loss when performing faceting, as it requires creating the facets for each segment, and then merging them (this results in a good deal of extra object overhead/memory overhead/more work where faceting on the multi-reader does not see this)&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This is a good point... Yonik, how &lt;span class=&quot;error&quot;&gt;&amp;#91;in general!&amp;#93;&lt;/span&gt; is Solr handling the cutover to per-segment, for faceting?&lt;/p&gt;</comment>
                    <comment id="12746635" author="mikemccand" created="Sun, 23 Aug 2009 17:57:18 +0100"  >&lt;blockquote&gt;&lt;p&gt;one used a cached DocIdSet created over the top level MultiReader (should be able to have a DocIdSet per Segment reader here, but this will take some more thinking (source of the matching docids is from a separate index), will also need to know which sub docidset to use based on which IndexReader is passed to scorer() - shouldn&apos;t be any big deal)&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think, similarly, you could continue to create the top-level&lt;br/&gt;
DocIdSet, but then make a new DocIdSet that presents one segment&apos;s&lt;br/&gt;
&quot;slice&quot; out of this top-level DocIdSet.  Then, pre-build the mapping&lt;br/&gt;
of IndexReader -&amp;gt; docBase like above, then when scorer() is called in&lt;br/&gt;
your custom query, just return the &quot;virtual&quot; per-segment DocIdSet.&lt;br/&gt;
Would this work?&lt;/p&gt;</comment>
                    <comment id="12746636" author="mikemccand" created="Sun, 23 Aug 2009 17:58:58 +0100"  >&lt;p&gt;Net/net, I&apos;m still nervous about pushing down &quot;full context&quot; plus&lt;br/&gt;
&quot;context free&quot; searcher/reader deep into Lucene&apos;s general searching&lt;br/&gt;
(scorer/filter) APIs.  I think these APIs should remain fully&lt;br/&gt;
context-free (even IndexSearcher still makes me nervous).&lt;/p&gt;

&lt;p&gt;In some sense, Multi/RemoteSearcher keep us honest, in that they force&lt;br/&gt;
us to clearly separate out &quot;stuff that has the luxury of full context&quot;&lt;br/&gt;
(to be done on construction of Weight) from &quot;the heavy lifting that&lt;br/&gt;
must be context free since it may not have access to the top searcher&quot;&lt;br/&gt;
(scorer(), getDocIdSet()).&lt;/p&gt;</comment>
                    <comment id="12746639" author="markrmiller@gmail.com" created="Sun, 23 Aug 2009 18:05:34 +0100"  >&lt;p&gt;Cool - I don&apos;t like it much either.&lt;/p&gt;

&lt;p&gt;I say we push this issue from 2.9 for now.&lt;/p&gt;</comment>
                    <comment id="12746643" author="tsmith" created="Sun, 23 Aug 2009 18:37:42 +0100"  >&lt;p&gt;Lot of new comments to respond to &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;br/&gt;
will try to cover them all&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;decent comparator (StringOrdValComparator) that operates per segment.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Still, the StringOrdValComparator will have to break down and call String.equals() whenever it compars docs in different IndexReaders&lt;br/&gt;
It also has to do more maintenance in general than would be needed for just a StringOrd comparator that would have a cache across all IndexReaders&lt;br/&gt;
While the StringOrdValComparator may be faster in 2.9 than string sorting in 2.4, its not as fast as it could be if the cache was created on the IndexSearcher level&lt;br/&gt;
I looked at the new string sorting stuff last week, and it looks pretty smart to reduce the number of String.equals() calls needed, but this adds extra complexity and will still be reduced to String.equals() calls, which will translate to slower sorting than could be possible&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;one option might be to subclass DirectoryReader &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The idea of this is to disable per segment searching?&lt;br/&gt;
I don&apos;t actually want to do that. I want to use per segment searching functionality to take advantage of caches on per segment basis where possible, and map docs to the IndexSearcher context when i can&apos;t do per segment caching.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Could you compute the top-level ords, but then break it up per-segment?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think i see what your getting at here, and i&apos;ve already thought of this as a potential solution. The cache will always need to be created at the top most level, but it will be pre-broken out into a per-segment cache whose context is the top level IndexSearcher/MultiReader. The biggest problem here is the complexity of actually creating such a cache, which i&apos;m sure will translate to this cache loading slower (hard to say how much slower without implementing)&lt;br/&gt;
I do plan to try this approach, but i expect this will be at least a week or two out from now.&lt;/p&gt;

&lt;p&gt;I&apos;ve currently updated my code for this to work per-segment by adding the docBase when performing the lookup into this cache (which is per-IndexSearcher)&lt;br/&gt;
I did this using my getIndexReaderBase() funciton i added to my subclass of IndexSearcher during Scorer construction time (I can live with this, however i would like to see getIndexReaderBase() added to IndexSearcher, and the IndexSearcher passed to Weight.scorer() so i don&apos;t need to hold onto my IndexSearcher subclass in my Weight implementation)&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;just return the &quot;virtual&quot; per-segment DocIdSet.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Thats what i&apos;m doing now. I use the docid base for the IndexReader, along with its maxDoc to have the Scorer represent a virtual slice for just the segment in question&lt;br/&gt;
The only real problem here is that during Scorer initialization for this i have to call fullDocIdSetIter.advance(docBase) in the Scorer constructor. If advance(int) for the DocIdSet in question is O(N), this adds an extra penalty per segment that did not exist before&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;his isn&apos;t a long-term solution, since the order in which Lucene visits the readers isn&apos;t in general guaranteed,&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;that&apos;s where IndexSearcher.getIndexReaderBase(IndexReader) comes into play. If you call this in your scorer to get the docBase, it doesn&apos;t matter what order the segments are searched in (as it&apos;ll always return the proper base (in the context of the IndexSearcher that is))&lt;/p&gt;


&lt;p&gt;Here&apos;s another potential thought (very rough, haven&apos;t consulted code to see how feasible this is):&lt;br/&gt;
what if Similarity had a method called getDocIdBase(IndexReader)&lt;br/&gt;
then, the searcher implementation could wrap the provided Similarity to provide the proper calculation&lt;br/&gt;
Similarity is always already passed through this chain of Weight creation and is passed into the Scorer&lt;br/&gt;
Obviously, a Query Implementation can completely drop the passing of the Searcher&apos;s similarity and drop in its own (but this would mean it doesn&apos;t care about getting these docid bases)&lt;br/&gt;
I think this approach would potentially resolve all MultiSearcher difficulties&lt;/p&gt;




</comment>
                    <comment id="12746644" author="yseeley@gmail.com" created="Sun, 23 Aug 2009 18:44:44 +0100"  >&lt;blockquote&gt;&lt;p&gt;This is a good point... Yonik, how &lt;span class=&quot;error&quot;&gt;&amp;#91;in general!&amp;#93;&lt;/span&gt; is Solr handling the cutover to per-segment, for faceting?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;It doesn&apos;t.  Faceting is not connected to searching in Solr, and is only done at the top level IndexReader.&lt;br/&gt;
We obviously want to enable per-segment faceting for more NRT in the future - with the expected disadvantage that it will be somewhat slower for some types of facets.  I imagine we will keep the top-level faceting as an option because there will be tradeoffs.&lt;/p&gt;</comment>
                    <comment id="12746645" author="yseeley@gmail.com" created="Sun, 23 Aug 2009 18:45:17 +0100"  >&lt;blockquote&gt;&lt;p&gt;I say we push this issue from 2.9 for now.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;+1 &lt;/p&gt;</comment>
                    <comment id="12746659" author="markrmiller@gmail.com" created="Sun, 23 Aug 2009 20:10:07 +0100"  >&lt;p&gt;I&apos;m going to push it out for now. Of course, feel free to argue for its re inclusion.&lt;/p&gt;</comment>
                    <comment id="12746662" author="tsmith" created="Sun, 23 Aug 2009 20:29:19 +0100"  >&lt;p&gt;can i at least argue for it being tagged for 3.0 or 3.1 (just so it gets looked at again prior to the next releases)&lt;/p&gt;

&lt;p&gt;I have workarounds for 2.9, so i&apos;m ok with it not getting in then (just want to make sure my use cases won&apos;t be made impossible in future releases)&lt;/p&gt;</comment>
                    <comment id="12746667" author="markrmiller@gmail.com" created="Sun, 23 Aug 2009 20:51:26 +0100"  >&lt;p&gt;Yeah, no problem - tag whatever you&apos;d like - I only went to nothing because it was the easiest default move.&lt;/p&gt;

&lt;p&gt;With the current plan (subject to change), the earliest it could be considered again is 3.1, so I&apos;ll move there.&lt;/p&gt;</comment>
                    <comment id="12746668" author="markrmiller@gmail.com" created="Sun, 23 Aug 2009 20:58:54 +0100"  >&lt;p&gt;whoops - try the right thing this time&lt;/p&gt;</comment>
                    <comment id="12746797" author="mikemccand" created="Mon, 24 Aug 2009 11:03:27 +0100"  >&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;decent comparator (StringOrdValComparator) that operates per segment.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Still, the StringOrdValComparator will have to break down and call String.equals() whenever it compars docs in different IndexReaders&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Agreed, it will be slower than a top-level ords cache, but I&apos;m&lt;br/&gt;
wondering in practice, in your case, what impact that turns out to be.&lt;br/&gt;
Also, since Lucene has already done this, maybe you could use its&lt;br/&gt;
StringOrdValComparator instead of having to cutover yours to&lt;br/&gt;
segment-based.&lt;/p&gt;

&lt;p&gt;Or, better, work up a patch for a &quot;forced&quot; top-level StringComparator&lt;br/&gt;
for apps that don&apos;t mind the slow commit time and possible risk of&lt;br/&gt;
burning memory, in exchange for faster sorting.&lt;/p&gt;

&lt;p&gt;Actually sorting (during collection) already gives you the docBase so&lt;br/&gt;
shouldn&apos;t your app already have the context needed for this?&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;The idea of this is to disable per segment searching?&lt;br/&gt;
I don&apos;t actually want to do that. I want to use per segment searching functionality to take advantage of caches on per segment basis where possible, and map docs to the IndexSearcher context when i can&apos;t do per segment caching.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;OK&lt;/p&gt;

&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;Could you compute the top-level ords, but then break it up per-segment?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think i see what your getting at here, and i&apos;ve already thought of this as a potential solution. The cache will always need to be created at the top most level, but it will be pre-broken out into a per-segment cache whose context is the top level IndexSearcher/MultiReader. The biggest problem here is the complexity of actually creating such a cache, which i&apos;m sure will translate to this cache loading slower (hard to say how much slower without implementing)&lt;br/&gt;
I do plan to try this approach, but i expect this will be at least a week or two out from now.&lt;/p&gt;

&lt;p&gt;I&apos;ve currently updated my code for this to work per-segment by adding the docBase when performing the lookup into this cache (which is per-IndexSearcher)&lt;br/&gt;
I did this using my getIndexReaderBase() funciton i added to my subclass of IndexSearcher during Scorer construction time (I can live with this, however i would like to see getIndexReaderBase() added to IndexSearcher, and the IndexSearcher passed to Weight.scorer() so i don&apos;t need to hold onto my IndexSearcher subclass in my Weight implementation)&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;OK sounds like an at least workable solution.&lt;/p&gt;

&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;just return the &quot;virtual&quot; per-segment DocIdSet.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Thats what i&apos;m doing now. I use the docid base for the IndexReader, along with its maxDoc to have the Scorer represent a virtual slice for just the segment in question&lt;br/&gt;
The only real problem here is that during Scorer initialization for this i have to call fullDocIdSetIter.advance(docBase) in the Scorer constructor. If advance(int) for the DocIdSet in question is O(N), this adds an extra penalty per segment that did not exist before&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Hmm... is advance in fact costly for your DocIdSets?&lt;/p&gt;

&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;This isn&apos;t a long-term solution, since the order in which Lucene visits the readers isn&apos;t in general guaranteed,&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;that&apos;s where IndexSearcher.getIndexReaderBase(IndexReader) comes into play. If you call this in your scorer to get the docBase, it doesn&apos;t matter what order the segments are searched in (as it&apos;ll always return the proper base (in the context of the IndexSearcher that is))&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think adding that one method for 2.9 would make sense?  (Marking it&lt;br/&gt;
expert, subject to change).  Because... assuming your app is OK w/&lt;br/&gt;
somehow (privately, external to Lucene) having access to the top&lt;br/&gt;
IndexSearcher via it&apos;s custom Weight, this one method would allow you&lt;br/&gt;
to not have to subclass IndexSearcher.&lt;/p&gt;</comment>
                    <comment id="12746809" author="tsmith" created="Mon, 24 Aug 2009 11:44:02 +0100"  >&lt;blockquote&gt;&lt;p&gt;Actually sorting (during collection) already gives you the docBase so shouldn&apos;t your app already have the context needed for this?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes, i get the docbase and all during collection, so doing sorting with a top level cache will be no problem.&lt;br/&gt;
I was mainly using sorting as an example of some of the pain caused by per-segment searching/caches (the Collector API makes it easy enough to do sorting&lt;br/&gt;
on the top level or per segment, so i&apos;m not concerned about integration here)&lt;/p&gt;

&lt;p&gt;For my app, i plan to allow sorting to be either &quot;per-segment&quot; or &quot;top-level&quot; in order to allow people to choose thier poison: faster commit/less memory vs faster sorting&lt;br/&gt;
I also plan to do faceting likewise&lt;br/&gt;
certain features will always require a top-level cache (but those are advanced features anyway and should be expected to have impacts on commit time/first search time)&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Hmm... is advance in fact costly for your DocIdSets?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Think how costly it would be to do advance for the SortedVInt DocIdSet (linear search over compressed values)&lt;br/&gt;
for a bitset, this is instantaneous, but to conserve memory, its better to use a sorted int[] (or the SortedVInt stuff 2.9 provides)&lt;/p&gt;

&lt;p&gt;in the end, i plan to bucketize the collected docs per segment, so in the end this should hopefully be less of an issue&lt;br/&gt;
nice thing about that approach is that i can have a bitset for one segment (lost of matches in this segment) and a very small int[] for a different segment based on the matches per segment. Biggest difficulty is doing the mapping to the per-segment &quot;DocIdSet&quot; (which will probably have to be slower)&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;this one method would allow you to not have to subclass IndexSearcher.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I already have to subclass index searcher (i do a lot of extra stuff)&lt;br/&gt;
however, the IndexSearcher doesn&apos;t provide any protected access to its sub readers and doc starts, so i have to do this myself in my subclass&apos;s constructor (in the same way IndexSearcher is doing this&lt;/p&gt;

&lt;p&gt;I would really like to see getIndexReaderBase() added to 2.9&apos;s IndexSearcher&lt;br/&gt;
I would also like to see the subreaders and docstarts either made protected or given protected accessor methods (so i don&apos;t have to recreate the same set of sub readers (and make sure i do this the same way for future versions of lucene)&lt;br/&gt;
Would also be nice to see a protected constructor on IndexSearcher like so:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
  &lt;span class=&quot;code-keyword&quot;&gt;protected&lt;/span&gt; IndexSearcher(IndexReader reader, IndexReader[] subReaders, &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt;[] docStarts) {
   ...
  }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This would allow creating &quot;temporary&quot; IndexSearchers much faster (don&apos;t need to gather sub readers)&lt;br/&gt;
This would allow:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;easily creating IndexSearcher that is &quot;top-level&quot; (subReaders[] would be length 1 and just contain reader)&lt;/li&gt;
	&lt;li&gt;create a &quot;temporary&quot; IndexSearcher off another IndexSearcher that contains some &quot;short lived&quot; context (i have this use case)&lt;/li&gt;
&lt;/ul&gt;


</comment>
                    <comment id="12746838" author="markrmiller@gmail.com" created="Mon, 24 Aug 2009 13:16:43 +0100"  >&lt;blockquote&gt;&lt;p&gt;faster commit/less memory vs faster sorting&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Have you done any benching here? I think we actually found that even most sorting cases were faster than in 2.4.1.&lt;/p&gt;

&lt;p&gt;Its a lot of poison to swallow top level - loading a field cache off a multi-segment index was dog slow. &lt;b&gt;edit&lt;/b&gt; (I can never remember if Yonik fix that or not - he fixed something related, or at least made it better)&lt;/p&gt;</comment>
                    <comment id="12746839" author="tsmith" created="Mon, 24 Aug 2009 13:25:31 +0100"  >&lt;blockquote&gt;&lt;p&gt;Have you done any benching here? I think we actually found that even most sorting cases were faster than in 2.4.1.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I haven&apos;t done any benchmarking. &lt;br/&gt;
I&apos;m not arguing that 2.9 string sorting is slower than 2.4 string sorting, it may well be faster for every case.&lt;br/&gt;
per segment searching and other improvements potentially added more gains in performance than the new string sorting added losses in performance.&lt;/p&gt;

&lt;p&gt;But, i can say rather confidently, that a large index with a bunch of segments will result in string sorting being slower when using a per segment string sort cache instead of a full index sort cache (think worst case using &amp;#42;:&amp;#42; query)&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;loading a field cache off a multi-segment index was dog slow&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;this is a trade off.&lt;br/&gt;
slower cache loading in order to get faster sorting&lt;br/&gt;
i plan to provide the ability to do both, and allow specific use cases to decide what is best for them&lt;/p&gt;</comment>
                    <comment id="12746840" author="markrmiller@gmail.com" created="Mon, 24 Aug 2009 13:29:05 +0100"  >&lt;blockquote&gt;
&lt;p&gt;this is a trade off.&lt;br/&gt;
slower cache loading in order to get faster sorting&lt;br/&gt;
i plan to provide the ability to do both, and allow specific use cases to decide what is best for them&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes a trade off &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; I meant it was bug slow though - as in it may take 80 seconds to load it when it should have taken 5. Unless the index was optimized. Could be fixed though - dropped off my radar now that we don&apos;t do it internally anymore.&lt;/p&gt;</comment>
                    <comment id="12746842" author="tsmith" created="Mon, 24 Aug 2009 13:35:09 +0100"  >&lt;p&gt;I allow caches to be loaded at commit time (if configured), and recommend that frequently used caches be configured to be loaded at this time&lt;br/&gt;
this can result in slower commit times, but responsive queries as soon as the commit is finished&lt;/p&gt;

&lt;p&gt;once i also add the option for per-segment caching for sorting and faceting (i&apos;ll probably put this on by default for sorting, faceting maybe not), this will allow full tunability for the end-user&lt;/p&gt;</comment>
                    <comment id="12757199" author="tsmith" created="Fri, 18 Sep 2009 16:19:09 +0100"  >&lt;p&gt;I&apos;ve been playing with per-segment caches for the last couple of weeks and have got everything working pretty well&lt;/p&gt;

&lt;p&gt;However, i have to end up doing a lot of mapping between an IndexReader instance, and the &quot;index into the IndexReader[]&quot; array of the IndexSearcher&lt;br/&gt;
this then allows me to easily get the proper document offset where needed, and/or get a handle on the proper per-segment cache/evaluation object/etc&lt;/p&gt;

&lt;p&gt;For my use cases, it would be much easier if the following methods were available:&lt;/p&gt;

&lt;p&gt;on Weight:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-comment&quot;&gt;// readerId is the &lt;span class=&quot;code-quote&quot;&gt;&quot;i&quot;&lt;/span&gt; in the &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; (&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; i = 0; i &amp;lt; readers.length; ++i) in IndexSearcher
&lt;/span&gt;&lt;span class=&quot;code-comment&quot;&gt;// NOTE: that readerId is at the IndexSearcher level, not the MultiSearcher level
&lt;/span&gt;&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; Scorer scorer(IndexReader reader, &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; readerId, &lt;span class=&quot;code-object&quot;&gt;boolean&lt;/span&gt; inOrder, &lt;span class=&quot;code-object&quot;&gt;boolean&lt;/span&gt; topLevel);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;on Collector:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; void setNextReader(IndexReader reader, &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; docBase, &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; readerId);
&lt;span class=&quot;code-comment&quot;&gt;// NOTE: &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; isn&apos;t extremely needed, as its easier to get the readerId from docBase (using a cached &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt;[] of docbases &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; the searcher)&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I suppose i could use the fact that these methods will always be called in order, keeping and incrementing counter, however the javadoc explicitly says that these methods may be called out of &quot;segment order&quot; to be more efficient in the future. It would therefore be very useful if these indexes were passed into these methods.&lt;/p&gt;

&lt;p&gt;To work around this, my searcher currently has a getReaderIdForReader() method very similar to my earlier proposed getIndexReaderBase() method&lt;/p&gt;

</comment>
                    <comment id="12849358" author="tsmith" created="Wed, 24 Mar 2010 19:01:42 +0000"  >&lt;p&gt;This would actually be solved by &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2345&quot; title=&quot;Make it possible to subclass SegmentReader&quot;&gt;LUCENE-2345&lt;/a&gt; for me as i would then be able to tag SegmentReaders with any additional accounting information i would need&lt;/p&gt;</comment>
                    <comment id="12981848" author="thetaphi" created="Fri, 14 Jan 2011 18:12:53 +0000"  >&lt;p&gt;This is resolved by adding AtomicReaderContext in 4.0 (&lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2831&quot; title=&quot;Revise Weight#scorer &amp;amp; Filter#getDocIdSet API to pass Readers context&quot;&gt;&lt;del&gt;LUCENE-2831&lt;/del&gt;&lt;/a&gt;).&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12417016" name="LUCENE-1821.patch" size="895" author="tsmith" created="Wed, 19 Aug 2009 14:07:40 +0100" />
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>1.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Tue, 18 Aug 2009 21:07:57 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>11942</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>25905</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                    <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                <customfieldname>Time in Status</customfieldname>
                <customfieldvalues>
                    
                </customfieldvalues>
            </customfield>
                            </customfields>
    </item>
</channel>
</rss>