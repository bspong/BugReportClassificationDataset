<!-- 
RSS generated by JIRA (5.2.8#851-sha1:3262fdc28b4bc8b23784e13eadc26a22399f5d88) at Tue Jul 16 13:18:33 UTC 2013

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/LUCENE-3767/LUCENE-3767.xml?field=key&field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>5.2.8</version>
        <build-number>851</build-number>
        <build-date>26-02-2013</build-date>
    </build-info>

<item>
            <title>[LUCENE-3767] Explore streaming Viterbi search in Kuromoji</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-3767</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;I&apos;ve been playing with the idea of changing the Kuromoji viterbi&lt;br/&gt;
search to be 2 passes (intersect, backtrace) instead of 4 passes&lt;br/&gt;
(break into sentences, intersect, score, backtrace)... this is very&lt;br/&gt;
much a work in progress, so I&apos;m just getting my current state up.&lt;br/&gt;
It&apos;s got tons of nocommits, doesn&apos;t properly handle the user dict nor&lt;br/&gt;
extended modes yet, etc.&lt;/p&gt;

&lt;p&gt;One thing I&apos;m playing with is to add a double backtrace for the long&lt;br/&gt;
compound tokens, ie, instead of penalizing these tokens so that&lt;br/&gt;
shorter tokens are picked, leave the scores unchanged but on backtrace&lt;br/&gt;
take that penalty and use it as a threshold for a 2nd best&lt;br/&gt;
segmentation...&lt;/p&gt;</description>
                <environment></environment>
            <key id="12542053">LUCENE-3767</key>
            <summary>Explore streaming Viterbi search in Kuromoji</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png">Closed</status>
                    <resolution id="1">Fixed</resolution>
                                <assignee username="cm">Christian Moen</assignee>
                                <reporter username="mikemccand">Michael McCandless</reporter>
                        <labels>
                    </labels>
                <created>Thu, 9 Feb 2012 23:39:00 +0000</created>
                <updated>Fri, 10 May 2013 11:43:29 +0100</updated>
                    <resolved>Sat, 10 Mar 2012 15:56:51 +0000</resolved>
                                            <fixVersion>3.6</fixVersion>
                <fixVersion>4.0-ALPHA</fixVersion>
                                <component>modules/analysis</component>
                        <due></due>
                    <votes>0</votes>
                        <watches>1</watches>
                                                    <comments>
                    <comment id="13205355" author="mikemccand" created="Fri, 10 Feb 2012 10:52:05 +0000"  >&lt;p&gt;New patch.&lt;/p&gt;

&lt;p&gt;I cleaned up some silly nocommits, got extended mode and user dict working, added a few more nocommits.&lt;/p&gt;

&lt;p&gt;All tests now pass if you set KuromojiTokenizer2.DO_OUTPUT_COMPOUND to false, which does the same score penalty logic as the current tokenizer.&lt;/p&gt;</comment>
                    <comment id="13206266" author="mikemccand" created="Sat, 11 Feb 2012 21:06:06 +0000"  >&lt;p&gt;I created a branch for this issue: &lt;a href=&quot;https://svn.apache.org/repos/asf/lucene/dev/branches/lucene3767&quot; class=&quot;external-link&quot;&gt;https://svn.apache.org/repos/asf/lucene/dev/branches/lucene3767&lt;/a&gt;&lt;/p&gt;</comment>
                    <comment id="13209540" author="mikemccand" created="Thu, 16 Feb 2012 17:52:49 +0000"  >&lt;p&gt;I think the branch is ready to land... I&apos;ll post an applyable patch&lt;br/&gt;
soon.&lt;/p&gt;

&lt;p&gt;In Mode.SEARCH the tokenizer produces the same tokens as current&lt;br/&gt;
trunk.&lt;/p&gt;

&lt;p&gt;The only real end-user visible change is the addition of&lt;br/&gt;
Mode.SEARCH_WITH_COMPOUNDS, which can produce two paths (compound&lt;br/&gt;
token + its segmentation).  This mode uses the new&lt;br/&gt;
PositionLengthAttribute to record how &quot;long&quot; the compound token is.&lt;/p&gt;

&lt;p&gt;In this mode, the Viterbi search first runs without penalties, but&lt;br/&gt;
then, if a too-long token (a token where the penalty would have been &amp;gt;&lt;br/&gt;
0) is in the best path, we effectively re-run the Viterbi under that&lt;br/&gt;
compound token, this time with penalties included.  If this results in&lt;br/&gt;
a different backtrace, we add that into the output tokens as well.&lt;/p&gt;

&lt;p&gt;Note that this will not produce congruent results as Mode.SEARCH,&lt;br/&gt;
because the 2nd segmentation runs &quot;in context&quot; of the best path,&lt;br/&gt;
meaning the chosen best wordID before and after the compound token are&lt;br/&gt;
&quot;enforced&quot; in the 2nd segmentation.  Sometimes this results in still&lt;br/&gt;
picking only the compound token where trunk today would have split it&lt;br/&gt;
up.  From TestQuality, the total number of edits was 4418 vs trunk&apos;s&lt;br/&gt;
4828.&lt;/p&gt;

&lt;p&gt;I didn&apos;t explore this, but, we may want to use harsher penalties in&lt;br/&gt;
SEARCH_WITH_COMPOUNDS mode, ie, since we&apos;re going to output the&lt;br/&gt;
compound as well we may as well &quot;try harder&quot; to produce the 2nd best&lt;br/&gt;
segmentation.&lt;/p&gt;

&lt;p&gt;I left the default mode as Mode.SEARCH... maybe if we can somehow&lt;br/&gt;
run some relevance tests we can make the default SEARCH_WITH_COMPOUNDS.&lt;br/&gt;
But it&apos;d also be tricky at query time...&lt;/p&gt;

&lt;p&gt;It looks like the rolling Viterbi is a bit faster (~16%: 1460&lt;br/&gt;
bytes/msec vs 1700 bytes/msec on TestQuality.testSingleText).&lt;/p&gt;</comment>
                    <comment id="13209579" author="mikemccand" created="Thu, 16 Feb 2012 18:25:11 +0000"  >&lt;p&gt;Applyable patch.&lt;/p&gt;</comment>
                    <comment id="13209586" author="mikemccand" created="Thu, 16 Feb 2012 18:34:39 +0000"  >&lt;p&gt;Attaching a file showing the segmentation change when you run with&lt;br/&gt;
SEARCH_WITH_COMPOUNDS vs SEARCH.&lt;/p&gt;

&lt;p&gt;Left is SEARCH and right is SEARCH_WITH_COMPOUNDS.&lt;/p&gt;

&lt;p&gt;I believe these diffs are due to the forced &quot;best path context&quot; that&lt;br/&gt;
we re-tokenize a given compound word in.  Ie, the left / right wordIDs&lt;br/&gt;
were picked because they had best path with this compound word, but&lt;br/&gt;
then we force the segmentation to also use these words.&lt;/p&gt;</comment>
                    <comment id="13212623" author="cm" created="Tue, 21 Feb 2012 14:41:14 +0000"  >&lt;p&gt;Mike,&lt;/p&gt;

&lt;p&gt;Thanks a lot for this.  I&apos;d meant to comment on this earlier and I&apos;d like to look further into the details, but I really like your idea of running the Viterbi in a streaming fashion.&lt;/p&gt;

&lt;p&gt;Kuromoji originally split input using two punctuation characters as this would be an articulation point in the lattice/graph in practice, but your idea is much more elegant and also faithful to the statistical model.&lt;/p&gt;

&lt;p&gt;As for dealing with compounds, the penalization is a crude hack as you know, but it turns to work quite well in practice as many of the &quot;decompounds&quot; are known to the statistical model.  However, in cases where not not all of them are known, we sometimes get wrong decomounds.  I&apos;ve done some analysis of these cases and it&apos;s possible to add more heuristics to deal with some that are obviouslt wrong, such a word starting with a long vowel sound in katakana.  This is a slippery slope that I&apos;m reluctant to pursue...&lt;/p&gt;

&lt;p&gt;Robert mentioned earlier that he believes IPADIC could have been annotated with compounds as the documentation mentions them, but they&apos;re not part of the IPADIC model we are using.  If it is possible to get the decompounds from the training data (Kyoto Corpus), a better overall approach is then to do regular segmentation (normal mode) and then provide the decompounds directly from the token info for the compounds.  We might need to retrain the model and preserving the decompounds in order for this to work, but I think it is worth investigating.&lt;/p&gt;</comment>
                    <comment id="13214148" author="rcmuir" created="Thu, 23 Feb 2012 00:59:08 +0000"  >&lt;blockquote&gt;
&lt;p&gt;Robert mentioned earlier that he believes IPADIC could have been annotated with compounds as the documentation mentions them, but they&apos;re not part of the IPADIC model we are using. If it is possible to get the decompounds from the training data (Kyoto Corpus), a better overall approach is then to do regular segmentation (normal mode) and then provide the decompounds directly from the token info for the compounds. We might need to retrain the model and preserving the decompounds in order for this to work, but I think it is worth investigating.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The dictionary documentation for the original ipadic has the ability to hold compound data (not in mecab-ipadic though, so maybe it was never implemented?!), &lt;br/&gt;
but I don&apos;t actually see it in any implementations. So yeah, we would need to find a corpus containing compound information (and of course extend the file format&lt;br/&gt;
and add support to kuromoji) to support that.&lt;/p&gt;

&lt;p&gt;However, would this really solve the total issue? Wouldn&apos;t that really only help for known kanji compounds... whereas most katakana compounds &lt;br/&gt;
(e.g. the software engineer example) are expected to be OOV anyway? So it seems like, even if we ensured the dictionary was annotated for &lt;br/&gt;
long kanji such that we always used decompounded forms, we need a &apos;heuristical&apos; decomposition like search-mode either way, at least for &lt;br/&gt;
the unknown katakana case?&lt;/p&gt;

&lt;p&gt;And I tend to like Mike&apos;s improvements from a relevance perspective for these reasons:&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;keeping the original compound term for improved precision&lt;/li&gt;
	&lt;li&gt;preventing compound decomposition from having any unrelated negative impact on the rest of the tokenization&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;So I think we should pursue this change, even if we want to separately train a dictionary in the future, because in that case, &lt;br/&gt;
we would just disable the kanji decomposition heuristic but keep the heuristic (obviously re-tuned!) for katakana?&lt;/p&gt;</comment>
                    <comment id="13214159" author="rcmuir" created="Thu, 23 Feb 2012 01:13:58 +0000"  >&lt;blockquote&gt;
&lt;p&gt;I&apos;ve done some analysis of these cases and it&apos;s possible to add more heuristics to deal with some that are obviouslt wrong, such a word starting with a long vowel sound in katakana.  This is a slippery slope that I&apos;m reluctant to pursue...&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I&apos;ve wondered about that also at least for the unknown katakana case: (though I don&apos;t know all the rules that could be applied).&lt;/p&gt;

&lt;p&gt;Adding such heuristics isn&apos;t really unprecedented, in a way its very similar to the compounds/ package (geared towards german, etc)&lt;br/&gt;
using TeX hyphenation rules to restrict word splits to hyphenation breaks; and similar to DictionaryBasedBreakIterators in&lt;br/&gt;
ICU/your JRE that use orthographic rules in combination with a dictionary to segment southeast asian languages like Thai, and &lt;br/&gt;
not too far from simple rules like &quot;don&apos;t separate a base character from any combining characters that follow it&quot;, or &quot;don&apos;t&lt;br/&gt;
separate a lead surrogate from a trail surrogate&quot; that you would generally use across all languages.&lt;/p&gt;</comment>
                    <comment id="13214196" author="cm" created="Thu, 23 Feb 2012 02:02:59 +0000"  >&lt;p&gt;I agree completely; we should definitely proceed with Mike&apos;s improvements.  A big +1 from me.&lt;/p&gt;

&lt;p&gt;I&apos;m sorry if this wasn&apos;t clear.  My comments on search mode are unrelated and I didn&apos;t mean to confuse these with the improvements made.  None of this is necessary for Mike&apos;s improvements to be used, of course.&lt;/p&gt;</comment>
                    <comment id="13214201" author="rcmuir" created="Thu, 23 Feb 2012 02:11:27 +0000"  >&lt;p&gt;Well my comments are only questions based on your previous JIRA comments,&lt;br/&gt;
I don&apos;t really know anything about decompounding Japanese and my suggestions&lt;br/&gt;
are only intuition, mostly based on patterns that have worked for other&lt;br/&gt;
languages... so just consider it thinking out loud.&lt;/p&gt;

&lt;p&gt;I think your points on search mode are actually related here: though we can&apos;t &lt;br/&gt;
really have a plan its good to think about possible paths we might take in the &lt;br/&gt;
future so that we don&apos;t lock ourselves out.&lt;/p&gt;</comment>
                    <comment id="13214217" author="cm" created="Thu, 23 Feb 2012 02:28:09 +0000"  >&lt;p&gt;Robert, some comments are below.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;And I tend to like Mike&apos;s improvements from a relevance perspective for these reasons:&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;keeping the original compound term for improved precision&lt;/li&gt;
	&lt;li&gt;preventing compound decomposition from having any unrelated negative impact on the rest of the tokenization&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;Very good points.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;So I think we should pursue this change, even if we want to separately train a dictionary in the future, because in that case, we would just disable the kanji decomposition heuristic but keep the heuristic (obviously re-tuned!) for katakana?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I agree completely.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;The dictionary documentation for the original ipadic has the ability to hold compound data (not in mecab-ipadic though, so maybe it was never implemented?!), but I don&apos;t actually see it in any implementations. So yeah, we would need to find a corpus containing compound information (and of course extend the file format and add support to kuromoji) to support that.&lt;/p&gt;

&lt;p&gt;However, would this really solve the total issue? Wouldn&apos;t that really only help for known kanji compounds... whereas most katakana compounds (e.g. the software engineer example) are expected to be OOV anyway? So it seems like, even if we ensured the dictionary was annotated for long kanji such that we always used decompounded forms, we need a &apos;heuristical&apos; decomposition like search-mode either way, at least for the unknown katakana case?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I&apos;ve made an inquiry to a friend who did his PhD work at Prof. Matsumoto&apos;s lab at NAIST (where ChaSen was made) and I&apos;ve made en inquiry regarding compound information and the Kyoto Corpus.&lt;/p&gt;


&lt;p&gt;You are perfectly right that this doesn&apos;t solve the complete problem as unknown words can actually be compounds &amp;#8211; unknown compounds.  The approach used today is basically adding all the potential decompounds the model knows about to the lattice and see if a short path can be found often in combination with an unknown word.&lt;/p&gt;

&lt;p&gt;We get errors such as &#12463;&#12452;&#12540;&#12531;&#12474;&#12467;&#12511;&#12483;&#12463;&#12473; (Queen&apos;s Comics) becoming &#12463;&#12452;&#12540;&#12531;&#12288;&#12474;&#12467;&#12511;&#12483;&#12463;&#12473; (Queen Zukuomikkusu) because &#12463;&#12452;&#12540;&#12531; (Queen) is known.&lt;/p&gt;

&lt;p&gt;I&apos;ll open up a separate JIRA for discussing search-mode improvements. &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                    <comment id="13214227" author="cm" created="Thu, 23 Feb 2012 02:34:26 +0000"  >&lt;blockquote&gt;
&lt;p&gt;I left the default mode as Mode.SEARCH... maybe if we can somehow run some relevance tests we can make the default SEARCH_WITH_COMPOUNDS.&lt;br/&gt;
But it&apos;d also be tricky at query time...&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Mike, could you share some details on any query-time trickiness here?  Are you thinking about composing a query with both the compound and its parts/decompounds?  Thanks.&lt;/p&gt;</comment>
                    <comment id="13214232" author="rcmuir" created="Thu, 23 Feb 2012 02:39:26 +0000"  >&lt;blockquote&gt;
&lt;p&gt;Mike, could you share some details on any query-time trickiness here? Are you thinking about composing a query with both the compound and its parts/decompounds? Thanks.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I don&apos;t understand Mike&apos;s comment there either. positionIncrement=0 is really going to be treated like synonyms by the QP,&lt;br/&gt;
so it shouldn&apos;t involve any trickiness.&lt;/p&gt;</comment>
                    <comment id="13215713" author="mikemccand" created="Fri, 24 Feb 2012 16:09:17 +0000"  >&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;I left the default mode as Mode.SEARCH... maybe if we can somehow run some relevance tests we can make the default SEARCH_WITH_COMPOUNDS. But it&apos;d also be tricky at query time...&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Mike, could you share some details on any query-time trickiness here? Are you thinking about composing a query with both the compound and its parts/decompounds? Thanks.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Sorry, I was wrong about this!  Apparently QueryParser works fine if&lt;br/&gt;
the analyzer produces a graph (it just creates a&lt;br/&gt;
somewhat-scary-yet-works MultiPhraseQuery).&lt;/p&gt;

&lt;p&gt;So I think we have no issue at query time... and I guess we should&lt;br/&gt;
default to SEARCH_WITH_COMPOUNDS.&lt;/p&gt;

&lt;p&gt;I agree we can explore better tweaking the decompounding in a new&lt;br/&gt;
issue.&lt;/p&gt;</comment>
                    <comment id="13217156" author="mikemccand" created="Mon, 27 Feb 2012 11:38:37 +0000"  >&lt;p&gt;New patch, making Mode.SEARCH always produce a graph output (ie no&lt;br/&gt;
more separate SEARCH_WITH_COMPOUNDS).  Mode.NORMAL and Mode.EXTENDED&lt;br/&gt;
are as they were before...&lt;/p&gt;

&lt;p&gt;I think it&apos;s ready!&lt;/p&gt;</comment>
                    <comment id="13218026" author="cm" created="Tue, 28 Feb 2012 09:40:09 +0000"  >&lt;p&gt;Mike,&lt;/p&gt;

&lt;p&gt;Thanks a lot for this.  I&apos;ve been taking this patch for a spin and I&apos;m seeing some exceptions being thrown when indexing some Wikipedia test documents.&lt;/p&gt;

&lt;p&gt;I haven&apos;t had a chance to analyze this in detail, but when indexing the attached &lt;tt&gt;SolrXml-5498.xml&lt;/tt&gt; I&apos;m getting:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
java.lang.ArrayIndexOutOfBoundsException: -69
	at org.apache.lucene.util.RollingCharBuffer.get(RollingCharBuffer.java:98)
	at org.apache.lucene.analysis.kuromoji.KuromojiTokenizer.computePenalty(KuromojiTokenizer.java:236)
	at org.apache.lucene.analysis.kuromoji.KuromojiTokenizer.computeSecondBestThreshold(KuromojiTokenizer.java:227)
	at org.apache.lucene.analysis.kuromoji.KuromojiTokenizer.backtrace(KuromojiTokenizer.java:910)
	at org.apache.lucene.analysis.kuromoji.KuromojiTokenizer.parse(KuromojiTokenizer.java:567)
	at org.apache.lucene.analysis.kuromoji.KuromojiTokenizer.incrementToken(KuromojiTokenizer.java:394)
	at org.apache.lucene.analysis.kuromoji.TestKuromojiTokenizer.doTestBocchan(TestKuromojiTokenizer.java:567)
	at org.apache.lucene.analysis.kuromoji.TestKuromojiTokenizer.testBocchan(TestKuromojiTokenizer.java:528)
	...
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I believe you can reproduce this by changing &lt;tt&gt;TestKuromojiTokenizer.doTestBocchan()&lt;/tt&gt; to read the attached &lt;tt&gt;SolrXml-5498.xml&lt;/tt&gt; instead of &lt;tt&gt;bocchan.utf-8&lt;/tt&gt;.&lt;/p&gt;
</comment>
                    <comment id="13218154" author="mikemccand" created="Tue, 28 Feb 2012 13:23:30 +0000"  >&lt;p&gt;Egads, I&apos;ll dig... thanks Christian.&lt;/p&gt;</comment>
                    <comment id="13218685" author="mikemccand" created="Tue, 28 Feb 2012 23:18:39 +0000"  >&lt;p&gt;New patch, fixing the exc Christian found: the 2nd best search was corrupting the bestIDX on the backtrace in the case where a compound wasn&apos;t selected.&lt;/p&gt;

&lt;p&gt;I also set a limit on the max UNK word length, and pulled the limits into static final private constants.&lt;/p&gt;

&lt;p&gt;I was able to parse the 2012/02/20 jaenwiki export with this (see commented out test case in TestKuromojiTokenizer).  I think it&apos;s ready (again!).&lt;/p&gt;</comment>
                    <comment id="13219015" author="cm" created="Wed, 29 Feb 2012 09:15:20 +0000"  >&lt;p&gt;Thanks, Mike.&lt;/p&gt;

&lt;p&gt;I&apos;ve tried the latest patch and things look fine here now.  I haven&apos;t had a chance to review the code changes yet, but I&apos;m happy to do that over the next couple of days and commit this &amp;#8211; unless you&apos;d like to do that yourself.  (This would be my first commit.)&lt;/p&gt;</comment>
                    <comment id="13219115" author="mikemccand" created="Wed, 29 Feb 2012 11:33:20 +0000"  >&lt;p&gt;Hi Christian, sure feel free to commit this!  Thanks &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                    <comment id="13221544" author="cm" created="Sat, 3 Mar 2012 09:03:04 +0000"  >&lt;p&gt;Thanks, Mike.  I&apos;ll commit this to &lt;tt&gt;trunk&lt;/tt&gt; tomorrow and backport to &lt;tt&gt;branch_3x&lt;/tt&gt; as well.&lt;/p&gt;</comment>
                    <comment id="13221599" author="mikemccand" created="Sat, 3 Mar 2012 14:50:51 +0000"  >&lt;p&gt;Thanks Christian!&lt;/p&gt;</comment>
                    <comment id="13221707" author="rcmuir" created="Sat, 3 Mar 2012 21:45:57 +0000"  >&lt;p&gt;+1&lt;/p&gt;</comment>
                    <comment id="13221903" author="cm" created="Sun, 4 Mar 2012 14:36:57 +0000"  >&lt;p&gt;Committed to &lt;tt&gt;trunk&lt;/tt&gt; with revision 1296805.&lt;/p&gt;

&lt;p&gt;Will backport to &lt;tt&gt;branch_3x&lt;/tt&gt; and then mark as fixed.&lt;/p&gt;</comment>
                    <comment id="13224025" author="cm" created="Wed, 7 Mar 2012 06:34:42 +0000"  >&lt;p&gt;I&apos;ve attached a patch for &lt;tt&gt;branch_3x&lt;/tt&gt; and I&apos;d be very thankful if Robert or Mike could have a quick look before I commit.&lt;/p&gt;

&lt;p&gt;These are some comments and questions:&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;I haven&apos;t backported the test for &lt;tt&gt;PositionLengthAttribute&lt;/tt&gt; since the structure seems very different in &lt;tt&gt;branch_3x&lt;/tt&gt;.  Perhaps this is something we could look to as we start using PositionLength in more of the other analyzers?&lt;/li&gt;
	&lt;li&gt;There will be some minor discrepancies between &lt;tt&gt;trunk&lt;/tt&gt; and &lt;tt&gt;branch_3x&lt;/tt&gt; with regards to &lt;tt&gt;Set&amp;lt;?&amp;gt;&lt;/tt&gt; and &lt;tt&gt;CharArraySet&lt;/tt&gt; types.  Perhaps we should use &lt;tt&gt;CharArraySet&lt;/tt&gt; on across the board here in the future?&lt;/li&gt;
	&lt;li&gt;Does the &lt;tt&gt;svn:mergeinfo&lt;/tt&gt; look okay in the patch?&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Tests pass on &lt;tt&gt;branch_3x&lt;/tt&gt; and I&apos;ve verified that the new feature seems fine in a Solr build as well.&lt;/p&gt;</comment>
                    <comment id="13224551" author="mikemccand" created="Wed, 7 Mar 2012 17:53:09 +0000"  >&lt;blockquote&gt;&lt;p&gt;I haven&apos;t backported the test for PositionLengthAttribute since the structure seems very different in branch_3x. Perhaps this is something we could look to as we start using PositionLength in more of the other analyzers?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Hmm you mean TestSimpleAttributeImpl?  I think that&apos;s fine.  We&apos;ll&lt;br/&gt;
improve testing of pos length as we make more tokenizers/filters&lt;br/&gt;
graphs...&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Does the svn:mergeinfo look okay in the patch?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Hmm looks spooky... there was a trick for this... (and I know it&apos;s&lt;br/&gt;
hard to merge back since the path changed in trunk).  Maybe do svn&lt;br/&gt;
propdel svn:mergeinfo on all affected files, and then at the top level&lt;br/&gt;
you can just do an &quot;svn merge --record-only&quot; to update toplevel&lt;br/&gt;
mergeinfo?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;There will be some minor discrepancies between trunk and branch_3x with regards to Set&amp;lt;?&amp;gt; and CharArraySet types. Perhaps we should use CharArraySet on across the board here in the future?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This was from &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-3765&quot; title=&quot;trappy ignoreCase behavior with StopFilter/ignoreCase&quot;&gt;&lt;del&gt;LUCENE-3765&lt;/del&gt;&lt;/a&gt;; I think it&apos;s OK to require&lt;br/&gt;
CharArraySet to Kuromoji...&lt;/p&gt;

&lt;p&gt;Looks like the added files are not included in the patch (eg&lt;br/&gt;
RollingCharBuffer)... if you&apos;re using svn &amp;gt;= 1.7, you can run &quot;svn&lt;br/&gt;
diff --show-copies-as-adds&quot; maybe?  Or, use&lt;br/&gt;
dev-tools/scripts/diffSources.py... but I don&apos;t think it&apos;s&lt;br/&gt;
really necessary before committing...&lt;/p&gt;

&lt;p&gt;Otherwise patch looks great!&lt;/p&gt;</comment>
                    <comment id="13226279" author="rcmuir" created="Fri, 9 Mar 2012 18:14:59 +0000"  >&lt;p&gt;Patch looks good: KuromojiAnalyzer can take CharArraySet because its not yet released, the only reason&lt;br/&gt;
its kinda funky in &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-3765&quot; title=&quot;trappy ignoreCase behavior with StopFilter/ignoreCase&quot;&gt;&lt;del&gt;LUCENE-3765&lt;/del&gt;&lt;/a&gt; is to avoid backwards compatibility breaks in 3.x.&lt;/p&gt;</comment>
                    <comment id="13226868" author="cm" created="Sat, 10 Mar 2012 14:38:43 +0000"  >&lt;p&gt;Thanks for the feedback.&lt;/p&gt;

&lt;p&gt;Mike, you are right that the added files aren&apos;t included in the patch, but I believe they will be added on check-in. (I&apos;ve verified that they&apos;re marked for addition, including &lt;tt&gt;RollingCharBuffer&lt;/tt&gt;. Sorry for the confusion.)&lt;/p&gt;

&lt;p&gt;I believe the patch is correct with regards to &lt;tt&gt;svn:mergeinfo&lt;/tt&gt; &amp;#8211; only the root directory, &lt;tt&gt;lucene&lt;/tt&gt; and &lt;tt&gt;solr&lt;/tt&gt; should have them.&lt;/p&gt;

&lt;p&gt;I&apos;ll commit to &lt;tt&gt;branch_3x&lt;/tt&gt; shortly.&lt;/p&gt;</comment>
                    <comment id="13226872" author="cm" created="Sat, 10 Mar 2012 14:55:31 +0000"  >&lt;p&gt;Committed revision 1299213 on &lt;tt&gt;branch_3x&lt;/tt&gt;&lt;/p&gt;</comment>
                    <comment id="13226881" author="cm" created="Sat, 10 Mar 2012 15:56:21 +0000"  >&lt;p&gt;Confirmed this working in a &lt;tt&gt;branch_3x&lt;/tt&gt; build.&lt;/p&gt;

&lt;p&gt;Thanks, Mike and Robert! &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                </comments>
                <issuelinks>
                        <issuelinktype id="10001">
                <name>dependent</name>
                                                <inwardlinks description="is depended upon by">
                            <issuelink>
            <issuekey id="12545062">LUCENE-3843</issuekey>
        </issuelink>
                    </inwardlinks>
                            </issuelinktype>
                    </issuelinks>
                <attachments>
                    <attachment id="12514836" name="compound_diffs.txt" size="49572" author="mikemccand" created="Thu, 16 Feb 2012 18:34:39 +0000" />
                    <attachment id="12517372" name="LUCENE-3767_branch_3x.patch" size="191069" author="cm" created="Wed, 7 Mar 2012 06:12:16 +0000" />
                    <attachment id="12516461" name="LUCENE-3767.patch" size="190492" author="mikemccand" created="Tue, 28 Feb 2012 23:18:37 +0000" />
                    <attachment id="12516156" name="LUCENE-3767.patch" size="188028" author="mikemccand" created="Mon, 27 Feb 2012 11:38:36 +0000" />
                    <attachment id="12514833" name="LUCENE-3767.patch" size="192343" author="mikemccand" created="Thu, 16 Feb 2012 18:25:10 +0000" />
                    <attachment id="12514096" name="LUCENE-3767.patch" size="69892" author="mikemccand" created="Fri, 10 Feb 2012 10:52:04 +0000" />
                    <attachment id="12514048" name="LUCENE-3767.patch" size="60185" author="mikemccand" created="Thu, 9 Feb 2012 23:41:45 +0000" />
                    <attachment id="12516300" name="SolrXml-5498.xml" size="31701" author="cm" created="Tue, 28 Feb 2012 09:41:22 +0000" />
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>8.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Tue, 21 Feb 2012 14:41:14 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>227341</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>23932</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                    <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                <customfieldname>Time in Status</customfieldname>
                <customfieldvalues>
                    
                </customfieldvalues>
            </customfield>
                            </customfields>
    </item>
</channel>
</rss>