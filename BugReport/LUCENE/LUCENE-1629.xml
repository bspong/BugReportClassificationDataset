<!-- 
RSS generated by JIRA (5.2.8#851-sha1:3262fdc28b4bc8b23784e13eadc26a22399f5d88) at Tue Jul 16 13:04:50 UTC 2013

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/LUCENE-1629/LUCENE-1629.xml?field=key&field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>5.2.8</version>
        <build-number>851</build-number>
        <build-date>26-02-2013</build-date>
    </build-info>

<item>
            <title>[LUCENE-1629] contrib intelligent Analyzer for Chinese</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-1629</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;I wrote a Analyzer for apache lucene for analyzing sentences in Chinese language. it&apos;s called &quot;imdict-chinese-analyzer&quot;, the project on google code is here: &lt;a href=&quot;http://code.google.com/p/imdict-chinese-analyzer/&quot; class=&quot;external-link&quot;&gt;http://code.google.com/p/imdict-chinese-analyzer/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;In Chinese, &quot;&#25105;&#26159;&#20013;&#22269;&#20154;&quot;(I am Chinese), should be tokenized as &quot;&#25105;&quot;(I)   &quot;&#26159;&quot;(am)   &quot;&#20013;&#22269;&#20154;&quot;(Chinese), not &quot;&#25105;&quot; &quot;&#26159;&#20013;&quot; &quot;&#22269;&#20154;&quot;. So the analyzer must handle each sentence properly, or there will be mis-understandings everywhere in the index constructed by Lucene, and the accuracy of the search engine will be affected seriously!&lt;/p&gt;

&lt;p&gt;Although there are two analyzer packages in apache repository which can handle Chinese: ChineseAnalyzer and CJKAnalyzer, they take each character or every two adjoining characters as a single word, this is obviously not true in reality, also this strategy will increase the index size and hurt the performance baddly.&lt;/p&gt;

&lt;p&gt;The algorithm of imdict-chinese-analyzer is based on Hidden Markov Model (HMM), so it can tokenize chinese sentence in a really intelligent way. Tokenizaion accuracy of this model is above 90% according to the paper &quot;HHMM-based Chinese Lexical analyzer ICTCLAL&quot; while other analyzer&apos;s is about 60%.&lt;/p&gt;

&lt;p&gt;As imdict-chinese-analyzer is a really fast and intelligent. I want to contribute it to the apache lucene repository.&lt;/p&gt;</description>
                <environment>&lt;p&gt;for java 1.5 or higher, lucene 2.4.1&lt;/p&gt;</environment>
            <key id="12424495">LUCENE-1629</key>
            <summary>contrib intelligent Analyzer for Chinese</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png">Closed</status>
                    <resolution id="1">Fixed</resolution>
                                <assignee username="mikemccand">Michael McCandless</assignee>
                                <reporter username="pinker">Xiaoping Gao</reporter>
                        <labels>
                    </labels>
                <created>Mon, 4 May 2009 05:51:17 +0100</created>
                <updated>Mon, 16 May 2011 19:15:36 +0100</updated>
                    <resolved>Thu, 14 May 2009 11:16:38 +0100</resolved>
                            <version>2.4.1</version>
                                <fixVersion>2.9</fixVersion>
                                <component>modules/analysis</component>
                        <due></due>
                    <votes>0</votes>
                        <watches>6</watches>
                                                    <comments>
                    <comment id="12706728" author="pinker" created="Thu, 7 May 2009 06:53:49 +0100"  >&lt;p&gt;Here is all the source code of intelligent analyzer for Chinese. About 2500 lines&lt;br/&gt;
The unit TestCase contains a main method, which needs lexical dictionary to run, so I will post the binary lexical dictionary soon.&lt;/p&gt;</comment>
                    <comment id="12706731" author="pinker" created="Thu, 7 May 2009 07:21:57 +0100"  >&lt;p&gt;Lexical dictionary files, unzip it to somewhere, run TestSmartChineseAnalyzer with this command:&lt;br/&gt;
java org.apache.lucene.analysis.cn.TestSmartChineseAnalyzer -Danalysis.data.dir=/path/to/analysis-data/&lt;/p&gt;</comment>
                    <comment id="12706782" author="mikemccand" created="Thu, 7 May 2009 10:45:49 +0100"  >&lt;p&gt;Patch looks good &amp;#8211; thanks Xiaoping!&lt;/p&gt;

&lt;p&gt;One problem is that contrib/analyzers is currently limited to Java 1.4, and I don&apos;t think we should change that at this point (though in 3.0, we will change it to 1.5).  How hard would it be to switch your sources to use only Java 1.4?&lt;/p&gt;

&lt;p&gt;A couple other issues:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;Each copyright header is missing the starting &apos;S&apos; in the sentence &apos;ee the License for the specific language governing permissions and&apos;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Can you remove the @author tags?  (Lucene sources don&apos;t include author tags anymore)&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="12706887" author="thetaphi" created="Thu, 7 May 2009 14:32:44 +0100"  >&lt;p&gt;Hi Xiaoping,&lt;/p&gt;

&lt;p&gt;looks good, but I have some suggestions:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Making the datafile only readable from a RandomAccessFile makes it hard to e.g. move the data file directly into the jar file. I would like to put the data file directly into the package directory  and load it with Class.getResourceAsStream(). In this case, the binary Lucene analyzer jar would be ready-to-use and the analyzer would run out of the box. Often configuring external files in e.g. web applications is complicated. An automatism to load the file from the JAR would be fine.&lt;/li&gt;
	&lt;li&gt;I have seen some singleton implementations, where the getInstance() static method is not synchronized. Without it there may be more than one instance, if different threads call getInstance() at the same time or close together.&lt;/li&gt;
	&lt;li&gt;Do we compile the source files with a fixed encoding of UTF-8 (build.xml?). If not, there may be problems, if the Java compiler uses another encoding (because platform default).&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="12706928" author="pinker" created="Thu, 7 May 2009 16:00:38 +0100"  >&lt;p&gt;to McCandless:&lt;br/&gt;
There is lots of code depending on Java 1.5, I use enum, generalization frequently. Because I saw these points on apache wiki:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;All core code to be included in 2.X releases should be compatible with Java 1.4.&lt;/li&gt;
	&lt;li&gt;All contrib code should be compatible with &lt;b&gt;either Java 5 or 1.4&lt;/b&gt;.&lt;br/&gt;
I have corrected the copyright header and @author tags, thank you.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;to Schindler:&lt;br/&gt;
1. This is really a good idea, I wanna to move the data file into jar in next develop cycle, but now I need to make some changes to the data files independently, can I just commit the codes now?&lt;br/&gt;
2. I have changed the getInstance() method to synchronized&lt;br/&gt;
3. All the source files are fixed encoded using UTF-8, and I had put a notice in package.html,  Should I do something else?&lt;/p&gt;

&lt;p&gt;Thank you all!&lt;/p&gt;</comment>
                    <comment id="12706933" author="pinker" created="Thu, 7 May 2009 16:09:57 +0100"  >&lt;p&gt;New patch in reply to Michael McCandless and Uwe Schindler &apos;s comments.&lt;/p&gt;</comment>
                    <comment id="12706948" author="rcmuir" created="Thu, 7 May 2009 16:49:02 +0100"  >&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;I see in the paper that lexical resources were also developed for Big5 (traditional chinese). Are you able to acquire these resources with BSD license as well?&lt;/p&gt;</comment>
                    <comment id="12707042" author="mikemccand" created="Thu, 7 May 2009 20:04:03 +0100"  >&lt;blockquote&gt;&lt;p&gt;There is lots of code depending on Java 1.5, I use enum, generalization frequently. Because I saw these points on apache wiki:&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Well... &quot;in general&quot; contrib packages can be 1.5, but the analyzers contrib package is widely used, and is not 1.5 now, so it&apos;s a biggish change to force it to 1.5 with this.  We should at least separate discuss in on java-dev if we want to consider allowing 1.5 code into contrib-analyzers.&lt;/p&gt;

&lt;p&gt;We could hold off on committing this until 3.0?&lt;/p&gt;</comment>
                    <comment id="12707235" author="pinker" created="Fri, 8 May 2009 06:05:12 +0100"  >&lt;p&gt;I have ported the code to Java1.4 today, fortunately there were not so much problems.&lt;/p&gt;

&lt;p&gt;&quot;Lucene-1629-java1.4.patch&quot;  is all the code working on Java 1.4, I have just changed it to fit Java1.4 code style.Data structures and algorithms are not modified. &lt;br/&gt;
It has been tested that it would produce the very same result, just with a slight affection on speed.&lt;/p&gt;</comment>
                    <comment id="12707236" author="pinker" created="Fri, 8 May 2009 06:05:50 +0100"  >&lt;p&gt;all the code working on java1.4&lt;/p&gt;</comment>
                    <comment id="12707278" author="mikemccand" created="Fri, 8 May 2009 09:47:07 +0100"  >&lt;blockquote&gt;&lt;p&gt;all the code working on java1.4&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Fabulous, thanks Xiaoping!&lt;/p&gt;</comment>
                    <comment id="12707280" author="mikemccand" created="Fri, 8 May 2009 09:52:38 +0100"  >&lt;p&gt;When I apply the patch and then run &quot;ant test&quot; in contrib/analyzers, I&apos;m hitting this compilation error:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
compile-core:
    [mkdir] Created dir: /lucene/src/cn.1629/build/contrib/analyzers/classes/java
    [javac] Compiling 88 source files to /lucene/src/cn.1629/build/contrib/analyzers/classes/java
    [javac] /lucene/src/cn.1629/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/AnalyzerProfile.java:98: load(java.io.InputStream) in java.util.Properties cannot be applied to (java.io.FileReader)
    [javac]       prop.load(reader);
    [javac]           ^
    [javac] Note: Some input files use or override a deprecated API.
    [javac] Note: Recompile with -Xlint:deprecation &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; details.
    [javac] 1 error
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                    <comment id="12707378" author="pinker" created="Fri, 8 May 2009 16:39:53 +0100"  >&lt;p&gt;new patch for java1.4, I have corrected the bug &quot;java.util.Property.load(Reader)&quot;.&lt;br/&gt;
The new code can now be compiled now.&lt;/p&gt;</comment>
                    <comment id="12707649" author="mikemccand" created="Sat, 9 May 2009 11:15:29 +0100"  >&lt;p&gt;Xiaoping, could you turn the TestSmartChineseAnalyzer into a real JUnit test case?  (Ie, invoke that sample method from the testChineseAnalyzer method)?&lt;/p&gt;

&lt;p&gt;Also, it looks like you didn&apos;t switch to Class.getResourceAsStream() (Uwe&apos;s suggestion above) &amp;#8211; are you planning on doing that?&lt;/p&gt;

&lt;p&gt;Finally, Robert asked a question above (about Big5) that maybe you missed?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Do we compile the source files with a fixed encoding of UTF-8 (build.xml?). If not, there may be problems, if the Java compiler uses another encoding (because platform default).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Lucene&apos;s common-build.xml already sets the encoding (for javac) to utf-8.  So I think we&apos;re good here...&lt;/p&gt;</comment>
                    <comment id="12707683" author="pinker" created="Sat, 9 May 2009 16:55:02 +0100"  >&lt;p&gt;to Robert Muir:&lt;br/&gt;
The dictionary only supports GB2312 encoding now, which has about 6800 characters, so I don&apos;t think it can support big5 encoding with this dictionary. &lt;br/&gt;
You can ask the author about the big5 issue. May be he has another lexical dictionary.&lt;/p&gt;

&lt;p&gt;Now I will switch to Class.getResourceAsStream() to load the dictionary, so the user don&apos;t have to download the dictionary independently.&lt;br/&gt;
After that I can write a real JUnit test case.&lt;/p&gt;
</comment>
                    <comment id="12707703" author="rcmuir" created="Sat, 9 May 2009 20:07:49 +0100"  >&lt;p&gt;Xiaoping, thanks. I see they didn&apos;t get great performance with big5 tests but just curious.&lt;/p&gt;

&lt;p&gt;Maybe mention somewhere in the javadocs that this analyzer is for simplified chinese text, just so its clear? &lt;/p&gt;</comment>
                    <comment id="12707886" author="pinker" created="Mon, 11 May 2009 06:11:54 +0100"  >&lt;p&gt;changes&lt;br/&gt;
1. Add two binary dictionary files into the java package: coredict.mem(1.6M) bigramdict.mem(4.7M), I&apos;ll post them after this&lt;br/&gt;
2. Using Class.getResourceAsStream() to load the dictionary, so users don&apos;t need to download dictionaries manually.&lt;br/&gt;
3. Switch TestSmartChineseAnalyzer into a real JUnit test case&lt;/p&gt;</comment>
                    <comment id="12707893" author="pinker" created="Mon, 11 May 2009 06:48:33 +0100"  >&lt;p&gt;two binary dictionary files, please put them into contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/hhmm/&lt;/p&gt;</comment>
                    <comment id="12707969" author="mikemccand" created="Mon, 11 May 2009 11:56:59 +0100"  >&lt;p&gt;When I run &quot;ant test&quot; in contrib/analyzers, SmartChineseAnalyzer is unable to locate the stopwords.txt:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
    [junit] Testcase: testChineseAnalyzer(org.apache.lucene.analysis.cn.TestSmartChineseAnalyzer):	Caused an ERROR
    [junit] &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;
    [junit] java.lang.NullPointerException
    [junit] 	at java.io.Reader.&amp;lt;init&amp;gt;(Reader.java:61)
    [junit] 	at java.io.InputStreamReader.&amp;lt;init&amp;gt;(InputStreamReader.java:80)
    [junit] 	at org.apache.lucene.analysis.cn.SmartChineseAnalyzer.loadStopWords(SmartChineseAnalyzer.java:112)
    [junit] 	at org.apache.lucene.analysis.cn.SmartChineseAnalyzer.&amp;lt;init&amp;gt;(SmartChineseAnalyzer.java:71)
    [junit] 	at org.apache.lucene.analysis.cn.TestSmartChineseAnalyzer.testChineseAnalyzer(TestSmartChineseAnalyzer.java:36)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                    <comment id="12708009" author="pinker" created="Mon, 11 May 2009 13:51:47 +0100"  >&lt;p&gt;On Mon, May 11, 2009 at 6:57 PM, Michael McCandless (JIRA)&lt;/p&gt;


&lt;p&gt;stopwords.txt should be in the same package as&lt;br/&gt;
org.apache.lucene.analysis.cn.SmartChineseAnalyzer , can you find it there?&lt;/p&gt;
</comment>
                    <comment id="12708032" author="mikemccand" created="Mon, 11 May 2009 14:46:06 +0100"  >&lt;p&gt;I do have the file, but at runtime the JRE cannot locate it using Class.getResourceAsStream().&lt;/p&gt;

&lt;p&gt;Are you able to run &quot;ant test -Dtestcase=TestSmartChineseAnalyzer&quot; from the command line in contrib/analzyers successfully?&lt;/p&gt;</comment>
                    <comment id="12708060" author="thetaphi" created="Mon, 11 May 2009 15:43:56 +0100"  >&lt;p&gt;Did the &amp;lt;jar&amp;gt; ANT task also adds the non *.class files? During compilation, the additional files must be copied to the build directory, this is normally done by an additional copy task (I do it in this way). The Packager then packs all files below build into the jar file. Maybe the build script must be modified?&lt;br/&gt;
I will try this out later.&lt;/p&gt;</comment>
                    <comment id="12708067" author="pinker" created="Mon, 11 May 2009 15:59:48 +0100"  >&lt;p&gt;I think Schindler should be right.&lt;br/&gt;
I modified the code to skip loading stopwords.txt, but NullPointerException&lt;br/&gt;
pop out again when loading coredict.mem file. When I run&lt;br/&gt;
TestSmartChineseAnalyzer using eclipse, it just run successfully.&lt;br/&gt;
So the problem might exist in the ant build script.&lt;/p&gt;

</comment>
                    <comment id="12708139" author="thetaphi" created="Mon, 11 May 2009 19:10:37 +0100"  >&lt;p&gt;I did some checks now, it is the problem of the ant script. Because of this, e.g. ArabicAnalyzer throws an IOException (but this is not tested, and so no test failures occur).&lt;br/&gt;
The ant script should copy all the data files to the build/classes directory after compiling and before jaring.&lt;/p&gt;

&lt;p&gt;I do not know, how to fix this correctly, because I do not fully understand all the parts of the build files and how maven and common-build.xml works together with contrib-build and so on.&lt;br/&gt;
The simpliest would be to customize the &quot;compile&quot; target for the analyzers package and list there all files that must be copied during the compilation step.&lt;/p&gt;

&lt;p&gt;Should I open an additional bug report for the ArabicAnalyzer, or should we fix the build.xml for analyzers with this case?&lt;/p&gt;</comment>
                    <comment id="12708181" author="mikemccand" created="Mon, 11 May 2009 21:05:32 +0100"  >&lt;blockquote&gt;&lt;p&gt;The simpliest would be to customize the &quot;compile&quot; target for the analyzers package and list there all files that must be copied during the compilation step.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Let&apos;s just do this fix, under this issue, for all contrib/analyzers that need to load a resource?&lt;/p&gt;</comment>
                    <comment id="12708236" author="thetaphi" created="Mon, 11 May 2009 22:39:50 +0100"  >&lt;p&gt;Hi Mike,&lt;/p&gt;

&lt;p&gt;here is a patch that adds a maven-like resources directory. It patches the build script in two ways:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;The junit test classpath is extended to include src/resources&lt;/li&gt;
	&lt;li&gt;The jarify macro is changed to also add src/resources to the jar file&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;So all resource files mut be put into the corresponding subdirectory under src/resources. The patch contains this for the stopword.txt file af the arabic analyzer. The data files should be removed from src/java.&lt;/p&gt;

&lt;p&gt;The cn analyzers stopwords must be put in the top-level cn directory, the mem files into cn/smart/hhmm (I took me some time to find this out).&lt;/p&gt;

&lt;p&gt;The patch also includes some src/resources directory additions. For the compilation to work, every src/ directory now needs at least an empty resources folder. I found no way to make the jarify macro work without this?&lt;/p&gt;

&lt;p&gt;If somebody has an idea, it would be good.&lt;/p&gt;</comment>
                    <comment id="12708320" author="pinker" created="Tue, 12 May 2009 05:51:46 +0100"  >&lt;p&gt;I think it is unacceptable to ask every package to have a resources folder,  &lt;br/&gt;
can we write the build script to test whether the resources file exists,  &lt;br/&gt;
like this:&lt;br/&gt;
&amp;lt;available property=&quot;resources.exists&quot; file=&quot;$&lt;/p&gt;
{resources.dir}
&lt;p&gt;&quot; type=&quot;dir&quot;/&amp;gt;&lt;br/&gt;
&amp;lt;target name=&quot;index&quot; depends=&quot;compile&quot; description=&quot;Build WordNet index&quot;&amp;gt;&lt;br/&gt;
&amp;lt;do_something if=&quot;sources.exists&quot;&amp;gt;&lt;br/&gt;
package the reources.&lt;br/&gt;
&amp;lt;/do_something&amp;gt;&lt;/p&gt;</comment>
                    <comment id="12708340" author="thetaphi" created="Tue, 12 May 2009 07:18:43 +0100"  >&lt;p&gt;I know this, the problem with th lucene build is that JAR ing is done using a macro called &amp;lt;jarify&amp;gt;. And here this is not possible. From ANT 1.7.1 on there is the possibility to specify a &quot;erroronmissingdir&quot; when using &amp;lt;fileset/&amp;gt;: &lt;a href=&quot;http://ant.apache.org/manual/CoreTypes/fileset.html&quot; class=&quot;external-link&quot;&gt;http://ant.apache.org/manual/CoreTypes/fileset.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I do not know what version of ant we require, but using it, the error can be avoided.&lt;/p&gt;</comment>
                    <comment id="12708878" author="mikemccand" created="Wed, 13 May 2009 11:40:16 +0100"  >&lt;p&gt;(Shooting in the dark, here, since I&apos;m no ant expert...)&lt;/p&gt;

&lt;p&gt;Lucene&apos;s common-build.xml has this:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&amp;lt;!-- Copy any data files present to the classpath --&amp;gt;
&amp;lt;copy todir=&lt;span class=&quot;code-quote&quot;&gt;&quot;@{destdir}&quot;&lt;/span&gt;&amp;gt;
  &amp;lt;fileset dir=&lt;span class=&quot;code-quote&quot;&gt;&quot;@{srcdir}&quot;&lt;/span&gt; excludes=&lt;span class=&quot;code-quote&quot;&gt;&quot;**/*.java&quot;&lt;/span&gt;/&amp;gt;
&amp;lt;/copy&amp;gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Which for all tests will copy any resources (any file that&apos;s not *.java) into the corresponding build/classes directory; eg contrib/xml-query-parser&apos;s tests rely on this.  This approach doesn&apos;t cause any errors when a given contrib module doesn&apos;t have resources.  Is there some way to use a similar approach here (and not bump up the minimum ant version required)?&lt;/p&gt;</comment>
                    <comment id="12708887" author="thetaphi" created="Wed, 13 May 2009 12:21:35 +0100"  >&lt;p&gt;I wonder, why this build fragment did not work for contrib? The only problem is, that this also copies the package. and overview javadoc files. They should also be excluded.&lt;/p&gt;</comment>
                    <comment id="12708889" author="mikemccand" created="Wed, 13 May 2009 12:48:17 +0100"  >&lt;p&gt;That fragment is under &quot;compile-test-macro&quot;, which is run only on src/test/*.  I agree, we should fix it to not copy package/javadoc files.&lt;/p&gt;</comment>
                    <comment id="12708892" author="thetaphi" created="Wed, 13 May 2009 12:59:53 +0100"  >&lt;p&gt;I will look into it this evening and provide a patch.&lt;/p&gt;

&lt;p&gt;Because of the file exclusion problematics, I thought, the approach to have a separate resources directory (like Maven does it), would be a great new invention. We could also do this for the tests. In my opinion, data files should be separated from source files. And by adding the resources folder to classpath during tests saves a lot of disk space during compilation and testing (ok, thats not important). By this compilation/test class path and building the jar files are separate tasks.&lt;br/&gt;
The problem with my current approach is only, that the JAR packager fails, when the directory is not available &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/sad.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; - Is it so bad to just add an empty resources folder to every compilation unit? This would be similar to Maven.&lt;/p&gt;</comment>
                    <comment id="12708894" author="mikemccand" created="Wed, 13 May 2009 13:08:00 +0100"  >&lt;p&gt;OK, I agree, separation of resources from source code is good.&lt;/p&gt;

&lt;p&gt;Can we limit the required addition of src/resources/org/apache/lucene/* to just contrib/analyzers?  Ie, somehow only override its jarify macro?&lt;/p&gt;</comment>
                    <comment id="12708909" author="thetaphi" created="Wed, 13 May 2009 13:50:20 +0100"  >&lt;p&gt;Its only needed to have the src/resources folder, no subfolders, I think it would be no problem to add this folder to every compilation unit (I added it to my svn in minutes). The good thing is, that future developments then know, where to put the resource files. But I agree, there should be a better way to automatically detect the resources folder before ANT 1.7.1.&lt;/p&gt;

&lt;p&gt;Maybe we should ask Erik Hatcher as the ANT specialist...!&lt;/p&gt;</comment>
                    <comment id="12708912" author="ehatcher" created="Wed, 13 May 2009 13:57:06 +0100"  >&lt;p&gt;My initial thought is to move the &amp;lt;copy&amp;gt; excluding &lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt; **/*.java and **/*.html&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;  to the &quot;compile&quot; macro.   In the ancient past, Ant actually used to do this automatically with &amp;lt;javac&amp;gt;.&lt;/p&gt;
</comment>
                    <comment id="12709292" author="thetaphi" created="Thu, 14 May 2009 08:05:26 +0100"  >&lt;p&gt;Here another try with Erik&apos;s suggestion:&lt;br/&gt;
I moved the &amp;lt;copy&amp;gt; task to the compile macro and extended the list of exclusions. With some work and verbose=true, I added all &quot;source&quot; files to the exclusion (also .jj and so on).&lt;/p&gt;

&lt;p&gt;Using this patch, you can compile Xiaoping Gao patch, add the resources to cn/ and cn/smart/hhmm/ and they appear in classpath for testing and the final jar file.&lt;/p&gt;

&lt;p&gt;My problem with this is the messy exclusion list. During reading ANT docs, I dound out that there is the possibility with the &amp;lt;copy&amp;gt; task to not stop on errors. The idea is now again to put the data files into a maven-like resources folder and just copy them to the classpath (if the folder does not exist, copy would simply do nothing).&lt;/p&gt;

&lt;p&gt;I post a patch/test later.&lt;/p&gt;</comment>
                    <comment id="12709326" author="thetaphi" created="Thu, 14 May 2009 10:07:34 +0100"  >&lt;p&gt;This is a second try, again with the resources folder. It is now optional, to have a src/resources folder, if it exists, all files from inside are copied to the build destination.&lt;/p&gt;

&lt;p&gt;The trick was, that the copy task can additionally use a globmapping, and by that, does the following:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;The source fileset of the copy task uses the src/ folder directly&lt;/li&gt;
	&lt;li&gt;The fileset only includes resources/**&lt;/li&gt;
	&lt;li&gt;Because then the target folder would get an additional sub-folder &quot;resources&quot; (because the base dir of the copy operation is &quot;src/&quot;), the filenames are replaced by a globmapping, stripping the &quot;resources/&quot; from the relative path&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;This patch also adds a simple test case, that shows, that ArabicAnalyzer does not start correctly, when the stopwords.txt file is not in the classpath. The test fails, if the stopwords.txt file stays at the original location and/or the copy operation is commented out.&lt;/p&gt;

&lt;p&gt;The patch does not contain the deletion of the arabic stopwords file from the sources folder (was binary), so remove it by hand or simply move it after aplying the patch.&lt;/p&gt;</comment>
                    <comment id="12709345" author="mikemccand" created="Thu, 14 May 2009 11:07:02 +0100"  >&lt;p&gt;Awesome!  I&apos;ve applied your patch, Uwe, and moved ArabicAnalyzer&apos;s stopwords.txt, as well as SmartChineseAnalyzer&apos;s stopwords.txt, bigramdict.mem, coredict.mem, under their respective subdirs under src/resources/*.  I confirmed TestArabicAnalyzer passes (and verified it really did instantiate ArabicAnalyzer).  All tests pass.&lt;/p&gt;

&lt;p&gt;I will commit shortly.&lt;/p&gt;

&lt;p&gt;This issue is a delightful example of the collaboration that makes open source development work so well.  Thanks Xiaoping, Uwe and Erik!&lt;/p&gt;</comment>
                    <comment id="12709346" author="mikemccand" created="Thu, 14 May 2009 11:16:38 +0100"  >&lt;p&gt;Thanks everyone!&lt;/p&gt;</comment>
                    <comment id="12709352" author="thetaphi" created="Thu, 14 May 2009 11:39:07 +0100"  >&lt;p&gt;Fine!&lt;br/&gt;
Should I commit the ArabicAnalyzer test, too? But I think the test is not really needed, as the new chinese analyzer already tests for the resources implicit.&lt;/p&gt;

&lt;p&gt;One thing: The change is in the main changes.txt, normally it should be in contrib&apos;s changes.txt, or not? If it should stay there, we should also add Spatial and TrieRange to main changes.txt.&lt;/p&gt;

&lt;p&gt;And one other thing: The analyzer (and many more) use the old TokenStream API at the moment, we should change this before 2.9 for all contrib analyzers, see &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-1460&quot; title=&quot;Change all contrib TokenStreams/Filters to use the new TokenStream API&quot;&gt;&lt;del&gt;LUCENE-1460&lt;/del&gt;&lt;/a&gt;?&lt;/p&gt;</comment>
                    <comment id="12709355" author="mikemccand" created="Thu, 14 May 2009 11:51:10 +0100"  >&lt;blockquote&gt;&lt;p&gt;Should I commit the ArabicAnalyzer test, too?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Woops, I missed it &amp;#8211; I&apos;ll commit it.  The more tests the better!&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;The change is in the main changes.txt, normally it should be in contrib&apos;s changes.txt, or not?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Woops &amp;#8211; you&apos;re right.  I&apos;ll move this to contrib&apos;s CHANGES.txt.&lt;/p&gt;</comment>
                    <comment id="12709357" author="mikemccand" created="Thu, 14 May 2009 11:51:51 +0100"  >&lt;blockquote&gt;&lt;p&gt;The analyzer (and many more) use the old TokenStream API at the moment, we should change this before 2.9 for all contrib analyzers, see &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-1460&quot; title=&quot;Change all contrib TokenStreams/Filters to use the new TokenStream API&quot;&gt;&lt;del&gt;LUCENE-1460&lt;/del&gt;&lt;/a&gt;?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes &amp;#8211; we need to resolve &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-1460&quot; title=&quot;Change all contrib TokenStreams/Filters to use the new TokenStream API&quot;&gt;&lt;del&gt;LUCENE-1460&lt;/del&gt;&lt;/a&gt; (and a great many more; the list keeps growing!) before 2.9.&lt;/p&gt;</comment>
                    <comment id="12709404" author="thetaphi" created="Thu, 14 May 2009 15:13:58 +0100"  >&lt;p&gt;Hi Mike,&lt;br/&gt;
a small patch: The HTML files generated by Javadoc do not contain the charset header and are displayed as ISO-8859-1. This breaks the docs for the chinese analyzer. The attached patch sets the output encoding correctly to UTF-8 using the &amp;lt;meta/&amp;gt; html tag.&lt;/p&gt;</comment>
                    <comment id="12709416" author="pinker" created="Thu, 14 May 2009 15:39:58 +0100"  >&lt;p&gt;Test successful on my laptop now! Thank all of you for your patience and hard work!&lt;br/&gt;
I will continue to maintain this analyzer and develop new features.&lt;/p&gt;

&lt;p&gt;Best Wishes! &lt;/p&gt;</comment>
                    <comment id="12709420" author="mikemccand" created="Thu, 14 May 2009 15:49:13 +0100"  >&lt;p&gt;OK, I just committed that fix (javadocs encoding == UTF-8) Uwe.  Thanks.&lt;/p&gt;</comment>
                    <comment id="12709425" author="thetaphi" created="Thu, 14 May 2009 16:04:01 +0100"  >&lt;p&gt;Hi Xiaoping,&lt;/p&gt;

&lt;p&gt;Thanks! The code is now committed.&lt;/p&gt;

&lt;p&gt;Only for the understanding (as I do not know chinese and cannot read some comments), some questions/comments:&lt;br/&gt;
The .mem files are serializations of the dictionaries. They are created by loading from the random access file (these dct files) and then serialized to the mem files. But for developers and further updates you need to have the dct files and rerun these steps (that are all these private methods).&lt;br/&gt;
An interesting addition would be to create a custom build step, that uses the dct files and builds the .mem files from it. How could I invoke that? So maybe you could extract the useless dct file loaders from the current classes and create a separate tool from it, that could be invoked from ant, that builds that mem files.&lt;/p&gt;

&lt;p&gt;Uwe&lt;/p&gt;

&lt;p&gt;P.S.: By the way: In these private conversation methods (that are never called from the library code) you have these default try-catch blocks, which is bad programming practice. So the proposed separate conversion tool should correctly handle the exceptions or better just not catch them at all and pass up (side note: I hate eclipse for generating these auto-catch blocks, better would be to auto-add throws-clauses to the method signatures!)&lt;/p&gt;</comment>
                    <comment id="12709796" author="mingfai" created="Fri, 15 May 2009 11:19:40 +0100"  >&lt;p&gt;hi Xiaoping,&lt;/p&gt;

&lt;p&gt;I&apos;m interested to get the Chinese analyzer work for Traditional Chinese (UTF-8/Big5).  Just wonder if your coredict.dct comes from ICTCLAS? (&lt;a href=&quot;http://ictclas.org/Down_share.html&quot; class=&quot;external-link&quot;&gt;http://ictclas.org/Down_share.html&lt;/a&gt;) if yes, is it 2009 or 2008?&lt;/p&gt;

&lt;p&gt;The ICTCLAS has traditional chinese edition for its 2008 release. But the distribution are not in .dct. I wonder if we have a simple specification for the .dct so I could find a way to convert the ICTCLAS&apos;s lexical dictionary to the .dct format to work with your library? &lt;/p&gt;</comment>
                    <comment id="12709866" author="pinker" created="Fri, 15 May 2009 16:15:46 +0100"  >&lt;p&gt;Hello Mingfai!&lt;/p&gt;

&lt;p&gt;coredict.mem is converted from coredict.dct which come from ICTCLAS1.0,  &lt;br/&gt;
neither 2008 nor 2009.&lt;br/&gt;
The author authorized me to release just the lexical dictionary from  &lt;br/&gt;
ICTCLAS1.0 under APLv2, but he didn&apos;t authorize the dictionary of  &lt;br/&gt;
ictclas2008~2009.&lt;br/&gt;
As far as I know, coredict.dct just contain GB2312 characters, so it cannot  &lt;br/&gt;
support Big5.&lt;/p&gt;

&lt;p&gt;I think we should find the proper big5 dictionary first, then I will help  &lt;br/&gt;
you to convert to dct file.&lt;/p&gt;


&lt;p&gt;On May 15, 2009 6:20pm, &quot;Mingfai Ma (JIRA)&quot; &amp;lt;jira@apache.org&amp;gt; wrote:&lt;/p&gt;















































</comment>
                    <comment id="12709867" author="pinker" created="Fri, 15 May 2009 16:15:47 +0100"  >&lt;p&gt;Hello Mingfai!&lt;/p&gt;

&lt;p&gt;coredict.mem is converted from coredict.dct which come from ICTCLAS1.0,  &lt;br/&gt;
neither 2008 nor 2009.&lt;br/&gt;
The author authorized me to release just the lexical dictionary from  &lt;br/&gt;
ICTCLAS1.0 under APLv2, but he didn&apos;t authorize the dictionary of  &lt;br/&gt;
ictclas2008~2009.&lt;br/&gt;
As far as I know, coredict.dct just contain GB2312 characters, so it cannot  &lt;br/&gt;
support Big5.&lt;/p&gt;

&lt;p&gt;I think we should find the proper big5 dictionary first, then I will help  &lt;br/&gt;
you to convert to dct file.&lt;/p&gt;


&lt;p&gt;On May 15, 2009 6:20pm, &quot;Mingfai Ma (JIRA)&quot; &amp;lt;jira@apache.org&amp;gt; wrote:&lt;/p&gt;















































</comment>
                    <comment id="12709880" author="rcmuir" created="Fri, 15 May 2009 16:25:19 +0100"  >&lt;p&gt;if you acquire the big5 resources, do you think it would be possible to create a single dictionary that works with both Simplified &amp;amp; Traditional?&lt;/p&gt;

&lt;p&gt;(i.e. merge the big5 resources with the gb resources)&lt;/p&gt;

&lt;p&gt;The reason I say this, is the existing chinese analyzers, although they tokenize in a less intelligent way, they are agnostic to Simplified/Traditional issues...&lt;/p&gt;</comment>
                    <comment id="12709885" author="rcmuir" created="Fri, 15 May 2009 16:33:30 +0100"  >&lt;p&gt;another potential issue with big5 i want to point out is that many of the big5 character sets such as HKSCS have characters that are mapped into regions of unicode outside of the BMP.&lt;/p&gt;

&lt;p&gt;just glancing at the code, some things will need to be modified for this to work correctly with surrogate pairs, various functions that take char will need to take codepoint (int), etc. &lt;/p&gt;</comment>
                    <comment id="12709974" author="mingfai" created="Fri, 15 May 2009 22:18:00 +0100"  >&lt;p&gt;could we use CC-CEDICT&apos;s dictionary instead? it is using Creative Commons Attribution-Share Alike 3.0 license&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.mdbg.net/chindict/chindict.php?page=cc-cedict&quot; class=&quot;external-link&quot;&gt;http://www.mdbg.net/chindict/chindict.php?page=cc-cedict&lt;/a&gt;&lt;/p&gt;</comment>
                    <comment id="12710035" author="koji" created="Sat, 16 May 2009 01:05:19 +0100"  >&lt;p&gt;Just an FYI. There have been a working for mapping between simplified and traditional chinese characters in Solr 1.4. (but you need to define mapping rules in mapping.txt)&lt;br/&gt;
See &lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-822&quot; title=&quot;CharFilter - normalize characters before tokenizer&quot;&gt;&lt;del&gt;SOLR-822&lt;/del&gt;&lt;/a&gt; and the attached JPG for chinese mapping sample.&lt;br/&gt;
I opened &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-1466&quot; title=&quot;CharFilter - normalize characters before tokenizer&quot;&gt;&lt;del&gt;LUCENE-1466&lt;/del&gt;&lt;/a&gt; for Lucene. &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                    <comment id="12710070" author="rcmuir" created="Sat, 16 May 2009 06:16:06 +0100"  >&lt;p&gt;koji, have you considered using icu transforms for this behavior?&lt;br/&gt;
Not only is the rule-based language very nice (you can define variables, use context, etc), but many transformations such as &quot;Traditional-Simplified&quot; are already defined.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://userguide.icu-project.org/transforms/general&quot; class=&quot;external-link&quot;&gt;http://userguide.icu-project.org/transforms/general&lt;/a&gt;&lt;/p&gt;</comment>
                    <comment id="12710118" author="koji" created="Sat, 16 May 2009 15:47:09 +0100"  >&lt;blockquote&gt;&lt;p&gt;koji, have you considered using icu transforms for this behavior?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Not yet.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Not only is the rule-based language very nice (you can define variables, use context, etc), but many transformations such as &quot;Traditional-Simplified&quot; are already defined. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Right. CharFilter framework that I introduced in &lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-822&quot; title=&quot;CharFilter - normalize characters before tokenizer&quot;&gt;&lt;del&gt;SOLR-822&lt;/del&gt;&lt;/a&gt; is not only for rule-based mapping, but it can use existing library like ICU to transform/normalize characters.&lt;/p&gt;</comment>
                    <comment id="12710147" author="mingfai" created="Sat, 16 May 2009 23:54:36 +0100"  >&lt;p&gt;i&apos;m not sure if the character mapping is a feasible approach. The CC-CEDICT dictionary has terms in both simplified and traditional chinese already and we need not to define any rule. how much effort is needed to define all mapping rules for the simplified and traditional chinese thesaurus?&lt;/p&gt;

&lt;p&gt;Besides, simplified and traditional Chinese conversion is not as simple as mapping the code. For the sample meaning, it may use different words in the SC and TC. &lt;/p&gt;

&lt;p&gt;if the approach accepted in this issue is ok, I just need to figure out how to convert CC-CEDICT to the dct / mem format, and i suppose it is doable.&lt;/p&gt;</comment>
                    <comment id="12710712" author="pinker" created="Tue, 19 May 2009 12:47:51 +0100"  >&lt;p&gt;The dictionary is loaded in to 2 classes:&lt;br/&gt;
BigramDictionary.java&lt;br/&gt;
WordDictionary.java&lt;/p&gt;

&lt;p&gt;so you can read from the loading section to get &quot;dct&quot; format&lt;/p&gt;

&lt;p&gt;mem files are just arrays that has been serialized to object files, you can get the format from the code and comments.&lt;/p&gt;</comment>
                    <comment id="12714285" author="otis" created="Fri, 29 May 2009 04:35:11 +0100"  >&lt;p&gt;I just got to look at this code and I only scanned it quickly.  Is all of the code really Chinese-specific? &lt;br/&gt;
Would any of it be applicable to other languages, say Japanese or Korean? (assuming we have dictionaries in suitable format)&lt;/p&gt;
</comment>
                    <comment id="12714293" author="pinker" created="Fri, 29 May 2009 05:09:42 +0100"  >&lt;p&gt;I think the algorithm of Hidden Markov Model is applicable,&lt;br/&gt;
but I doubt that Japanese and Korean may need some language specific&lt;br/&gt;
analysis, such as &quot;&#29255;&#20551;&#21517;&#12363;&#12383;&#12363;&#12394;.&#24179;&#20551;&#21517;&#12402;&#12425;&#12364;&#12394;&quot; in Japanese, as far as I know, the same&lt;br/&gt;
word can have different spelling. This may be a problem in the application?&lt;/p&gt;</comment>
                    <comment id="12714586" author="otis" created="Fri, 29 May 2009 21:38:46 +0100"  >&lt;p&gt;Hm, my Japanese is a little weak, so I&apos;m not sure what exactly that means and what exactly&lt;br/&gt;
different spelling means in this context... &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;Google says &quot;&#29255;&#20551;&#21517;&#12363;&#12383;&#12363;&#12394;.&#24179;&#20551;&#21517;&#12402;&#12425;&#12364;&#12394;&quot; means &quot;How do&#20551;&#21517;piece. Hiragana&#20551;&#21517;flat&quot;&lt;/p&gt;

&lt;p&gt;Which word in the above Japanese text is the same as another word, yet with a different spelling?  Is this a question of synonyms,&lt;br/&gt;
such as &quot;auto&quot;, &quot;automobile&quot;, and &quot;car&quot;, and even &quot;sedan&quot; in English?&lt;/p&gt;</comment>
                    <comment id="12714592" author="rcmuir" created="Fri, 29 May 2009 21:53:36 +0100"  >&lt;p&gt;otis if you are interested in japanese/korean you might find this link interesting:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://bugs.icu-project.org/trac/ticket/2229&quot; class=&quot;external-link&quot;&gt;http://bugs.icu-project.org/trac/ticket/2229&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;similar to the thai approach (in contrib) but with log probabilities.&lt;/p&gt;</comment>
                    <comment id="12714670" author="mingfai" created="Sat, 30 May 2009 06:54:55 +0100"  >&lt;p&gt;re. &#24179;&#20551;&#21517; and  &#29255;&#20551;&#21517; in Japanese&lt;br/&gt;
&lt;a href=&quot;http://en.wikipedia.org/wiki/Kana&quot; class=&quot;external-link&quot;&gt;http://en.wikipedia.org/wiki/Kana&lt;/a&gt;&lt;br/&gt;
&lt;a href=&quot;http://en.wikipedia.org/wiki/Hiragana&quot; class=&quot;external-link&quot;&gt;http://en.wikipedia.org/wiki/Hiragana&lt;/a&gt;&lt;br/&gt;
&lt;a href=&quot;http://en.wikipedia.org/wiki/Katakana&quot; class=&quot;external-link&quot;&gt;http://en.wikipedia.org/wiki/Katakana&lt;/a&gt;&lt;/p&gt;
</comment>
                    <comment id="12729373" author="tkurosaka" created="Thu, 9 Jul 2009 19:32:35 +0100"  >&lt;p&gt;WordTokenizer extends Tokenizer, but it&apos;s constructor takes a TokenStream rather than a Reader.  &lt;br/&gt;
Shouldn&apos;t WordTokenizer rather extends TokenFilter, and if so, shouldn&apos;t it be named WordTokenFilter?&lt;/p&gt;</comment>
                    <comment id="12729381" author="rcmuir" created="Thu, 9 Jul 2009 19:45:13 +0100"  >&lt;blockquote&gt;&lt;p&gt;Shouldn&apos;t WordTokenizer rather extends TokenFilter, and if so, shouldn&apos;t it be named WordTokenFilter? &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;yes, you are correct. see &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-1728&quot; title=&quot;Move SmartChineseAnalyzer &amp;amp; resources to own contrib project&quot;&gt;&lt;del&gt;LUCENE-1728&lt;/del&gt;&lt;/a&gt; where we have proposed correcting this.&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12407426" name="analysis-data.zip" size="2115255" author="pinker" created="Thu, 7 May 2009 07:21:57 +0100" />
                    <attachment id="12407741" name="bigramdict.mem" size="4825694" author="pinker" created="Mon, 11 May 2009 06:48:33 +0100" />
                    <attachment id="12408110" name="build-resources.patch" size="1120" author="thetaphi" created="Thu, 14 May 2009 08:05:26 +0100" />
                    <attachment id="12407821" name="build-resources.patch" size="7624" author="thetaphi" created="Mon, 11 May 2009 22:39:50 +0100" />
                    <attachment id="12408120" name="build-resources-with-folder.patch" size="8441" author="thetaphi" created="Thu, 14 May 2009 10:07:34 +0100" />
                    <attachment id="12407740" name="coredict.mem" size="1584870" author="pinker" created="Mon, 11 May 2009 06:48:33 +0100" />
                    <attachment id="12408132" name="LUCENE-1629-encoding-fix.patch" size="803" author="thetaphi" created="Thu, 14 May 2009 15:13:58 +0100" />
                    <attachment id="12407739" name="LUCENE-1629-java1.4.patch" size="142796" author="pinker" created="Mon, 11 May 2009 06:11:54 +0100" />
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>8.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Thu, 7 May 2009 09:45:49 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>12128</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    <customfieldvalue key="10120"><![CDATA[Patch Available]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>26099</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                    <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                <customfieldname>Time in Status</customfieldname>
                <customfieldvalues>
                    
                </customfieldvalues>
            </customfield>
                            </customfields>
    </item>
</channel>
</rss>