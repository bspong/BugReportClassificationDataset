<!-- 
RSS generated by JIRA (5.2.8#851-sha1:3262fdc28b4bc8b23784e13eadc26a22399f5d88) at Tue Jul 16 13:12:28 UTC 2013

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/LUCENE-2089/LUCENE-2089.xml?field=key&field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>5.2.8</version>
        <build-number>851</build-number>
        <build-date>26-02-2013</build-date>
    </build-info>

<item>
            <title>[LUCENE-2089] explore using automaton for fuzzyquery</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-2089</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;we can optimize fuzzyquery by using AutomatonTermsEnum. The idea is to speed up the core FuzzyQuery in similar fashion to Wildcard and Regex speedups, maintaining all backwards compatibility.&lt;/p&gt;

&lt;p&gt;The advantages are:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;we can seek to terms that are useful, instead of brute-forcing the entire terms dict&lt;/li&gt;
	&lt;li&gt;we can determine matches faster, as true/false from a DFA is array lookup, don&apos;t even need to run levenshtein.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;We build Levenshtein DFAs in linear time with respect to the length of the word: &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.16.652&quot; class=&quot;external-link&quot;&gt;http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.16.652&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;To implement support for &apos;prefix&apos; length, we simply concatenate two DFAs, which doesn&apos;t require us to do NFA-&amp;gt;DFA conversion, as the prefix portion is a singleton. the concatenation is also constant time with respect to the size of the fuzzy DFA, it only need examine its start state.&lt;/p&gt;

&lt;p&gt;with this algorithm, parametric tables are precomputed so that DFAs can be constructed very quickly.&lt;br/&gt;
if the required number of edits is too large (we don&apos;t have a table for it), we use &quot;dumb mode&quot; at first (no seeking, no DFA, just brute force like now).&lt;/p&gt;

&lt;p&gt;As the priority queue fills up during enumeration, the similarity score required to be a competitive term increases, so, the enum gets faster and faster as this happens. This is because terms in core FuzzyQuery are sorted by boost value, then by term (in lexicographic order).&lt;/p&gt;

&lt;p&gt;For a large term dictionary with a low minimal similarity, you will fill the pq very quickly since you will match many terms. &lt;br/&gt;
This not only provides a mechanism to switch to more efficient DFAs (edit distance of 2 -&amp;gt; edit distance of 1 -&amp;gt; edit distance of 0) during enumeration, but also to switch from &quot;dumb mode&quot; to &quot;smart mode&quot;.&lt;/p&gt;

&lt;p&gt;With this design, we can add more DFAs at any time by adding additional tables. The tradeoff is the tables get rather large, so for very high K, we would start to increase the size of Lucene&apos;s jar file. The idea is we don&apos;t have include large tables for very high K, by using the &apos;competitive boost&apos; attribute of the priority queue.&lt;/p&gt;

&lt;p&gt;For more information, see &lt;a href=&quot;http://en.wikipedia.org/wiki/Levenshtein_automaton&quot; class=&quot;external-link&quot;&gt;http://en.wikipedia.org/wiki/Levenshtein_automaton&lt;/a&gt;&lt;/p&gt;</description>
                <environment></environment>
            <key id="12441396">LUCENE-2089</key>
            <summary>explore using automaton for fuzzyquery</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png">Closed</status>
                    <resolution id="1">Fixed</resolution>
                                <assignee username="rcmuir">Robert Muir</assignee>
                                <reporter username="rcmuir">Robert Muir</reporter>
                        <labels>
                    </labels>
                <created>Sun, 22 Nov 2009 14:58:40 +0000</created>
                <updated>Fri, 10 May 2013 11:44:41 +0100</updated>
                    <resolved>Thu, 11 Mar 2010 12:18:42 +0000</resolved>
                            <version>4.0-ALPHA</version>
                                <fixVersion>4.0-ALPHA</fixVersion>
                                <component>core/search</component>
                        <due></due>
                    <votes>0</votes>
                        <watches>2</watches>
                                                    <comments>
                    <comment id="12781136" author="markrmiller@gmail.com" created="Sun, 22 Nov 2009 15:09:07 +0000"  >&lt;blockquote&gt;&lt;p&gt;(i will assign this to him, I know he is itching to write that nasty algorithm&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;ha - too much wine last night to laugh that hard this morning. Painful.&lt;/p&gt;</comment>
                    <comment id="12781137" author="rcmuir" created="Sun, 22 Nov 2009 15:12:05 +0000"  >&lt;p&gt;modify description to be readable, K is number of edits, N refers to length of word.&lt;/p&gt;</comment>
                    <comment id="12781138" author="thetaphi" created="Sun, 22 Nov 2009 15:14:05 +0000"  >&lt;blockquote&gt;&lt;p&gt;ha - too much wine last night to laugh that hard this morning. Painful.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I need more beer, after that 3.0.0 problem with the AttributeSource API &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/sad.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; (&lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2088&quot; title=&quot;AttributeSource.addAttribute should only accept interfaces, the missing test leads to problems with Token.TOKEN_ATTRIBUTE_FACTORY&quot;&gt;&lt;del&gt;LUCENE-2088&lt;/del&gt;&lt;/a&gt;).&lt;/p&gt;</comment>
                    <comment id="12781139" author="rcmuir" created="Sun, 22 Nov 2009 15:21:59 +0000"  >&lt;p&gt;by the way, the only open impl of this algorithm i could find is at &lt;a href=&quot;http://rrette.com/moman.html&quot; class=&quot;external-link&quot;&gt;http://rrette.com/moman.html&lt;/a&gt; (ZSpell) in python.&lt;br/&gt;
the download link for 0.2 is not available, and it appears from the website this is the version with the updated algorithm.&lt;/p&gt;</comment>
                    <comment id="12781140" author="rcmuir" created="Sun, 22 Nov 2009 15:26:00 +0000"  >&lt;p&gt;I hope its obvious from the benchmark why we shouldn&apos;t use the crappy prototype, and optimize K=1 (which is probably very common).&lt;br/&gt;
if someone has a small index, with very long terms (bio sequences, or something), then fuzzy would actually get slower.&lt;/p&gt;</comment>
                    <comment id="12781142" author="markrmiller@gmail.com" created="Sun, 22 Nov 2009 15:32:26 +0000"  >&lt;p&gt;I&apos;ll take a look anyway - too bad I can&apos;t find my stupid automata  text book - already bought the stupid thing twice in my life. Python translation sounds easier anyway &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                    <comment id="12781144" author="rcmuir" created="Sun, 22 Nov 2009 15:47:09 +0000"  >&lt;blockquote&gt;&lt;p&gt;I&apos;ll take a look anyway - too bad I can&apos;t find my stupid automata text book - already bought the stupid thing twice in my life. Python translation sounds easier anyway&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;you will need more wine. If it starts to catch, I&apos;ll be happy to offer help with use of the automaton API, etc.&lt;/p&gt;</comment>
                    <comment id="12781149" author="markrmiller@gmail.com" created="Sun, 22 Nov 2009 16:20:15 +0000"  >&lt;blockquote&gt;&lt;p&gt;we can precompute the tables with that algorithm up to some reasonable K, and then I think we are ok.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I wonder what the best way to handle this would be - these transition tables appear to get very large very fast. Looks like the python stuff is now calculating them on the fly. You wouldn&apos;t want to load them all up into RAM for those not using them. Perhaps some sort of lru cache load on demand or something - though can&apos;t really fully warm up that way.&lt;/p&gt;</comment>
                    <comment id="12781151" author="rcmuir" created="Sun, 22 Nov 2009 16:23:00 +0000"  >&lt;p&gt;Mark, they would get large fast, but i think we only need say 1,2,3.&lt;br/&gt;
once you go high K, it will be many disk seeks anyway, and &apos;linear&apos; mode like it does now will be probably faster.&lt;/p&gt;</comment>
                    <comment id="12781154" author="rcmuir" created="Sun, 22 Nov 2009 16:28:59 +0000"  >&lt;p&gt;Another twist, is that we have to support the &apos;constant prefix&apos; as well.&lt;br/&gt;
the easiest way would be that if the user asks for this, we intersect the automaton with a &apos;prefix automaton&apos; such as abc.*&lt;/p&gt;

&lt;p&gt;but the BasicOperations.intersection is quadratic in # of states, because it supports both NFA and DFA&lt;br/&gt;
we might need to implement a better DFA-only intersection alg for this constant prefix, so that supplying constant prefix doesnt actually make things slower &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                    <comment id="12781157" author="markrmiller@gmail.com" created="Sun, 22 Nov 2009 16:41:15 +0000"  >&lt;blockquote&gt;&lt;p&gt;Mark, they would get large fast, but i think we only need say 1,2,3.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Ah, right - you mention that above in the summary.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Another twist, is that we have to support the &apos;constant prefix&apos; as well.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Generally, if you have any kind of length to the prefix, linear wouldn&apos;t be so bad either though right? Guess it depends on your terms though.&lt;/p&gt;</comment>
                    <comment id="12781161" author="rcmuir" created="Sun, 22 Nov 2009 16:44:37 +0000"  >&lt;blockquote&gt;&lt;p&gt;Generally, if you have any kind of length to the prefix, linear wouldn&apos;t be so bad either though right? Guess it depends on your terms though.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;not so bad, but not so good either. see my wildcard results, for example (on the 10M equally distributed terms)&lt;/p&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Pattern&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Iter&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;AvgHits&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;AvgMS (old)&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;AvgMS (new)&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;N?N?N?N&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;10&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;1000.0&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;288.6&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;38.5&lt;/th&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
</comment>
                    <comment id="12781167" author="markrmiller@gmail.com" created="Sun, 22 Nov 2009 17:27:18 +0000"  >&lt;p&gt;Right, I wouldn&apos;t expect it to be great with a single char prefix - especially with the random terms your making. But I think thats a much worse case than a slightly longer prefix with a normal term distribution.&lt;/p&gt;</comment>
                    <comment id="12781170" author="rcmuir" created="Sun, 22 Nov 2009 17:34:02 +0000"  >&lt;p&gt;Mark maybe, though it also depends largely on the size of the &quot;alphabet&quot; in the term dictionary. &lt;/p&gt;

&lt;p&gt;for my test, the alphabet is pretty small size, &lt;span class=&quot;error&quot;&gt;&amp;#91;0-9&amp;#93;&lt;/span&gt;. With smaller alphabets, say &lt;span class=&quot;error&quot;&gt;&amp;#91;GACD&amp;#93;&lt;/span&gt;, the automaton enumeration becomes even more effective over any &quot;prefix&quot; mechanism&lt;/p&gt;

&lt;p&gt;so the performance is language-dependent.&lt;/p&gt;

&lt;p&gt;also mark, you said you want to &quot;scale&quot; fuzzy query right? Taking a linear algorithm and dividing it by a constant (alphabet size), which is what constant prefix does, does not improve the scalability. for english, it just makes your fuzzy query 26 times faster. so imo it would be better to improve the real scalability... the constant prefix is just an optimization/hack but does not really solve the problem&lt;/p&gt;</comment>
                    <comment id="12781173" author="markrmiller@gmail.com" created="Sun, 22 Nov 2009 17:52:42 +0000"  >&lt;blockquote&gt;&lt;p&gt;the constant prefix is just an optimization/hack&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Right - which is part of why I am not so worried about handling it with a new impl - its really only there now because the thing is so slow without it - if we really needed to support it, it wouldn&apos;t be so bad to fall back to as it is. Though if we can support it with the new impl fine - but I don&apos;t think I would go out of the way for it myself.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;for english, it just makes your fuzzy query 26 times faster&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;With a prefix of 1 again? Yeah - you really need to use a longer prefix to get much benifit - but I wouldnt think we care about a prefix of 1 with the new impl - just don&apos;t use a prefix. Its just a hack to somewhat get around the current scability issues.&lt;/p&gt;</comment>
                    <comment id="12781174" author="rcmuir" created="Sun, 22 Nov 2009 17:56:28 +0000"  >&lt;blockquote&gt;&lt;p&gt;With a prefix of 1 again? Yeah - you really need to use a longer prefix to get much benifit - but I wouldnt think we care about a prefix of 1 with the new impl - just don&apos;t use a prefix. Its just a hack to somewhat get around the current scability issues.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;you write that crazy algorithm from the paper, and I will do the easy part (fast DFA intersection) so we can use the constant prefix.&lt;br/&gt;
we must &quot;use&quot; the prefix, so the results are the same as old FuzzyQuery for back compat, might as well do it right.&lt;/p&gt;

&lt;p&gt;btw, I am implying here that we should never do a real levenshtein dynamic programming calculation if we do not have to.&lt;br/&gt;
for example, in my k=1 prototype, it is never used:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;float&lt;/span&gt; difference() {
      &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (currentTerm == &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt; || currentTerm.equals(term))
        &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; 1.0F;
      &lt;span class=&quot;code-keyword&quot;&gt;else&lt;/span&gt;
        &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; 1.0F - (1.0F / (&lt;span class=&quot;code-object&quot;&gt;Math&lt;/span&gt;.max(currentTerm.text().length(), term.text().length())));
    } 
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;instead, if up to k=3 is required, we should do these steps:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;equals(), then 1.0F&lt;/li&gt;
	&lt;li&gt;matches k=1 automaton?&lt;/li&gt;
	&lt;li&gt;matches k=2 automaton?&lt;/li&gt;
	&lt;li&gt;matches k=3 automaton?&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;even for k=3 i am sure this is likely faster on average than doing levenshtein, especially if the query matches many terms.&lt;br/&gt;
automaton matching is linear time, and just array access.&lt;/p&gt;
</comment>
                    <comment id="12781175" author="markrmiller@gmail.com" created="Sun, 22 Nov 2009 18:02:17 +0000"  >&lt;blockquote&gt;&lt;p&gt;we must &quot;use&quot; the prefix, so the results are the same as old FuzzyQuery for back compat&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I&apos;m not so worried about that myself. The prefix on the old Fuzzy is just to get around scalability - we could just deprecate and create a new Fuzzy without it. Why carry along the hack when we fix the root problem?&lt;/p&gt;

&lt;p&gt;&lt;b&gt;edit&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;Basically, what I&apos;m saying is the old FuzzyQuery is just garbage really - I&apos;ve even seen an email were Doug talked about just dropping it. He didn&apos;t like the non scalable queries. Even if this new impl didn&apos;t exactly match the results of the old, I&apos;d have no problem just deprecating the old and saying we are starting over with a &quot;real&quot; fuzzyquery - the old one goes away on the next major - anyone dying to stick with it can just pull the query into their own code.&lt;/p&gt;

&lt;p&gt;I think if we keep the prefix, it should be for a good reason in itself, rather than just back compat with the old crappy version.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;edit&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;Not that I can&apos;t see people wanting just the tail end to be fuzzy - in that case, fine - do your dirty work &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; I just don&apos;t know how much of a real req that is compared to a prefix query. Perhaps its more useful than I&apos;d guess myself. I think we are quite a ways from getting this alg implemented anyway &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; Though I am trying to track down the latest python code.&lt;/p&gt;</comment>
                    <comment id="12781179" author="rcmuir" created="Sun, 22 Nov 2009 18:09:49 +0000"  >&lt;blockquote&gt;&lt;p&gt;Basically, what I&apos;m saying is the old FuzzyQuery is just garbage really - I&apos;ve even seen an email were Doug talked about just dropping it.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I agree with garbage, but we cannot completely replace it anyway. for example what if someone supplies a term of length 54 and asks for distance of 0.5?&lt;br/&gt;
we should not use this algorithm for nonsense like that, in that case I think they should just use the garbage algorithm.&lt;/p&gt;

&lt;p&gt;Here is a quote from that moman page:&lt;br/&gt;
It means that in theory, you could ask for a Levenshtein distance of 27! Well, if you have a week ahead of you... &lt;/p&gt;

&lt;p&gt;we shouldnt burn cycles creating useless tables that will be huge arrays either in fuzzyquery, or whatever. we can&apos;t compute all the way up to infinity, this is why i think something like 1,2,3 is &quot;reasonable&quot; , if it requires more edits than that, go with the old brute force algorithm?&lt;/p&gt;</comment>
                    <comment id="12781180" author="markrmiller@gmail.com" created="Sun, 22 Nov 2009 18:15:20 +0000"  >&lt;blockquote&gt;&lt;p&gt;if it requires more edits than that, go with the old brute force algorithm?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Perhaps - but it should be an option. We don&apos;t want perf to just drop off a cliff (and thats an understatement on a large index), just because the input crossed some line. I&apos;d almost prefer the input just doesn&apos;t work - in my mind FuzzyQuery just doesn&apos;t work on large indecies. But far be it from me to take away the option ...&lt;/p&gt;

&lt;p&gt;I wouldnt really like it if it was default automatic - n=3 takes a few milliseconds, then n=4 takes an hour. Whoops.&lt;/p&gt;</comment>
                    <comment id="12781182" author="rcmuir" created="Sun, 22 Nov 2009 18:18:36 +0000"  >&lt;blockquote&gt;&lt;p&gt;I wouldnt really like it if it was default automatic - n=3 takes a few milliseconds, then n=4 takes an hour. Whoops.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;you are completely right, it should not drop off a cliff. though i think as n increases, it will get significantly worse, because of so much seeking.&lt;br/&gt;
we find the nice n where this is almost as bad as brute force, and cut her off there?&lt;/p&gt;

&lt;p&gt;this is basically what i did for the enum already, if you have a wildcard with leading * or a regex with .*, its faster to linear scan than to seek around.&lt;br/&gt;
Uwe and I are referring this as &quot;dumb&quot; mode versus &quot;smart&quot; mode. I&apos;m gonna add a constructor to the enum, so this can be specified rather than &quot;auto&quot; as well.&lt;/p&gt;</comment>
                    <comment id="12781204" author="markrmiller@gmail.com" created="Sun, 22 Nov 2009 19:58:29 +0000"  >&lt;blockquote&gt;&lt;p&gt;we find the nice n where this is almost as bad as brute force, and cut her off there?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;yeah, that sounds good if there is a nice transition at a relatively lower n.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;this is basically what i did for the enum already, if you have a wildcard with leading * or a regex with .*, its faster to linear scan than to seek around.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yeah, and I think this makes sense - but you will notice both the Lucene qp and Solr don&apos;t allow leading wildcard by default - the perf is generally bad enough on a large index that we assume you don&apos;t want to allow it and force users to &quot;turn on&quot; the option.&lt;/p&gt;</comment>
                    <comment id="12781205" author="rcmuir" created="Sun, 22 Nov 2009 20:01:21 +0000"  >&lt;blockquote&gt;&lt;p&gt;but you will notice both the Lucene qp and Solr don&apos;t allow leading wildcard by default&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;if we add this automaton stuff, I think we should reconsider this rule.&lt;br/&gt;
instead of don&apos;t allow leading * or ? by default,  just don&apos;t allow just leading * by default.&lt;/p&gt;

&lt;p&gt;i won&apos;t really argue for it though, because a wildcard query of &quot;?????????????abc&quot;, probably just as bad as &quot;*abc&quot;&lt;/p&gt;</comment>
                    <comment id="12781206" author="markrmiller@gmail.com" created="Sun, 22 Nov 2009 20:05:01 +0000"  >&lt;p&gt;I think it makes sense to allow leading ? - ?????????abc is prob rare enough that its worth it to allow by default. Or perhaps a separate knob to turn it on and leave leading * off.&lt;/p&gt;</comment>
                    <comment id="12781207" author="rcmuir" created="Sun, 22 Nov 2009 20:09:13 +0000"  >&lt;p&gt;mark, you are right.&lt;/p&gt;

&lt;p&gt;plus, the qp does not throw exceptions if you have a fuzzy query with no constant prefix, which is going to be actually worse than even leading *!!!&lt;/p&gt;</comment>
                    <comment id="12781208" author="markrmiller@gmail.com" created="Sun, 22 Nov 2009 20:15:03 +0000"  >&lt;p&gt;solr doesnt even allow for a constant prefix with fuzzy - its broken, broken, broken &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; DisMax to the rescue.&lt;/p&gt;</comment>
                    <comment id="12781226" author="markrmiller@gmail.com" created="Sun, 22 Nov 2009 21:44:11 +0000"  >&lt;p&gt;From Moman author:&lt;/p&gt;

&lt;p&gt;Absolutely. Sorry for the missing links. I had some problems with my&lt;br/&gt;
provider which backed up an old version of my website.&lt;br/&gt;
The library is MIT licensed, so feel free to take anything you want. I&lt;br/&gt;
didn&apos;t made the subversion available since I was working&lt;br/&gt;
on Daciuk&apos;s &quot;Incremental construction of minimal acyclic finite-state&lt;br/&gt;
automata&quot;, improving the memory footprint of the algorithm.&lt;br/&gt;
Since I want to be sure the new algorithm is right before publishing&lt;br/&gt;
it, the repository isn&apos;t available. Anyway, here&apos;s the source&lt;br/&gt;
code (attached). I must warn you that there&apos;s no comments at all,&lt;br/&gt;
which is unfortunate, since I was contacted by many people&lt;br/&gt;
lately, that had the same kind of request than yours.&lt;/p&gt;

&lt;p&gt;If you need anything, just don&apos;t hesitate to ask.&lt;/p&gt;</comment>
                    <comment id="12781230" author="rcmuir" created="Sun, 22 Nov 2009 21:47:50 +0000"  >&lt;p&gt;Mark, from his page it seemed like 0.2 was the version with the generalized edit distance?&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Moman 0.2 is out!
(2005-07-29) This version add the possibility to use a Levenshtein distance greater than 1. 
Before, the transitions tables were static, now we build them. 
It means that in theory, you could ask for a Levenshtein distance of 27! 
Well, if you have a week ahead of you... 
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                    <comment id="12781231" author="markrmiller@gmail.com" created="Sun, 22 Nov 2009 21:59:14 +0000"  >&lt;p&gt;Sorry - I attached the wrong file.&lt;/p&gt;</comment>
                    <comment id="12782458" author="markrmiller@gmail.com" created="Wed, 25 Nov 2009 15:03:27 +0000"  >&lt;p&gt;I might be on the trail of a java impl - get out the hounds!&lt;/p&gt;

&lt;p&gt;Perhaps I won&apos;t be looking over that paper for thanksgiving after all.&lt;/p&gt;</comment>
                    <comment id="12787882" author="rcmuir" created="Wed, 9 Dec 2009 02:42:15 +0000"  >&lt;p&gt;attached is a testcase that builds n=1 for some string the slow way.&lt;br/&gt;
you can use this to verify that some faster method is correct, via assertEquals&lt;/p&gt;</comment>
                    <comment id="12787989" author="thetaphi" created="Wed, 9 Dec 2009 09:13:13 +0000"  >&lt;p&gt;Cool! The code looks quite simple (but maybe this is because of n=1). But FuzzyQuery with n&amp;gt;1 are used seldom, or not? And how slow it is?&lt;/p&gt;</comment>
                    <comment id="12789466" author="rcmuir" created="Fri, 11 Dec 2009 19:09:51 +0000"  >&lt;p&gt;here is a patch that implements the crazy algorithm.&lt;/p&gt;

&lt;p&gt;i only include the tables for n=1, but the algorithm is general to all n, has tests, works, and is fast.&lt;/p&gt;

&lt;p&gt;the next steps imho are:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;look at how adding tables for n will help by looking at some dictionary statistics (assuming priority q of 1024)&lt;/li&gt;
	&lt;li&gt;add tables for those values of n&lt;/li&gt;
	&lt;li&gt;add the priority q attribute&lt;/li&gt;
	&lt;li&gt;write the enum (probably adding a little to automatontermsenum so it can be extended to do this)&lt;/li&gt;
	&lt;li&gt;maybe make the code better for this part.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="12789885" author="thetaphi" created="Sun, 13 Dec 2009 12:29:46 +0000"  >&lt;p&gt;You wrote that some parts of the code are &quot;automatically generated&quot;. Is there code available to do this for other n? Else I would move the class to the util.automaton package, as e.g. Regex parser are also there?&lt;/p&gt;

&lt;p&gt;I assume that I can use the generated automaton with AutomatonQuery and would hit all docs, but without scoring?&lt;/p&gt;</comment>
                    <comment id="12789900" author="rcmuir" created="Sun, 13 Dec 2009 15:17:51 +0000"  >&lt;blockquote&gt;&lt;p&gt;You wrote that some parts of the code are &quot;automatically generated&quot;. Is there code available to do this for other n? Else I would move the class to the util.automaton package, as e.g. Regex parser are also there? &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;it is not so simple... as n increases the amount of generated code gets very large. also the math to generate the code is very heavy.&lt;br/&gt;
the first &apos;table&apos; i simply keyed in manually. so it would be nice if these &apos;tables&apos; could be more compact.... this is one reason i uploaded this code.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I assume that I can use the generated automaton with AutomatonQuery and would hit all docs, but without scoring?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;yeah for n=1 with this code.  we could also &apos;key in&apos; the code for n=2, too, but its a lot more and it would be better to figure out the smallest representation in java and make a code generator.&lt;/p&gt;</comment>
                    <comment id="12790358" author="earwin" created="Mon, 14 Dec 2009 20:46:04 +0000"  >&lt;blockquote&gt;&lt;p&gt;by the way, the only open impl of this algorithm i could find is at &lt;a href=&quot;http://rrette.com/moman.html&quot; class=&quot;external-link&quot;&gt;http://rrette.com/moman.html&lt;/a&gt; (ZSpell) in python.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I recently stumbled upon a C++ey STLey impl -&amp;gt; &lt;a href=&quot;http://code.google.com/p/patl/&quot; class=&quot;external-link&quot;&gt;http://code.google.com/p/patl/&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I might be on the trail of a java impl - get out the hounds!&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;If you do take hold of it, do not hesitate to share &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; The original paper and C++ code likewise melt my brain, and I needed the algo in some other place.&lt;/p&gt;</comment>
                    <comment id="12790368" author="markrmiller@gmail.com" created="Mon, 14 Dec 2009 21:04:14 +0000"  >&lt;blockquote&gt;&lt;p&gt;If you do take hold of it, do not hesitate to share  The original paper and C++ code likewise melt my brain, and I needed the algo in some other place.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The java impl I was onto was about 75% complete according to the author, but I have not yet looked at the code. Robert was convinced it was a different less efficient algorithm last I heard though.&lt;/p&gt;

&lt;p&gt;We have cracked much of the paper - thats how Robert implemented n=1 here - thats from the paper. The next step is to work out how to construct the tables for n as Robert says above. And store those tables efficiently as they start getting quite large rather fast - though we might only use as high as n=3 or 4 in Lucene - Robert suspects term seeking will outweigh any gains at that point. I think we know how to do the majority of the work for the n case, but I don&apos;t really have much/any time for this, so it probably depends on if/when Robert gets to it. If he loses interest on finishing, I def plan to come back to it someday. I&apos;d like to complete my understanding of the paper and see a full n java impl of this in either case. The main piece left that I don&apos;t understand fully (computing all possible states for n), can be computed with just a brute force check (thats how the python impl is doing it), so there may not be much more to understand. I would like to know how the paper is getting &apos;i&apos; parametrized state generators though - thats much more efficient. The paper shows them for n=1 and n=2.&lt;/p&gt;</comment>
                    <comment id="12790392" author="rcmuir" created="Mon, 14 Dec 2009 21:52:30 +0000"  >&lt;p&gt;thanks for the pointer Earwin, its very useful to have something else to look at.&lt;/p&gt;

&lt;p&gt;Mark, definitely not losing interest on finishing, but mostly recovering from the brain damage that paper caused me.&lt;/p&gt;

&lt;p&gt;I will say i ran stats on various dictionaries, and I think the following:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;I think making the defaults for fuzzy (0.5 similarity with 1024 n-best expansions) is very challenging. I actually think the pq size (1024 n-best expansions) is the big kicker here, I mean it seems a little absurd to me... If this was smaller we could support lower n and even not worry about the 0.5 similarity so much, because we would fill up the pq quicker and n would drop fast. But I think adjusting both defaults might make sense.&lt;/li&gt;
	&lt;li&gt;I think we should try to find ways to figuring out what good defaults should be, and this shouldnt be based just on performance (i.e. ocr-damaged test collection, something like that).&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;So anyway, I&apos;m really happy with how far this is right now, because if you look at the performance numbers at the top, we were spending 300ms building DFAs for 56 character-long words, and now this is 5ms. just need to go to the next step.&lt;/p&gt;
</comment>
                    <comment id="12790586" author="earwin" created="Tue, 15 Dec 2009 06:28:39 +0000"  >&lt;blockquote&gt;&lt;p&gt;I would like to know how the paper is getting &apos;i&apos; parametrized state generators though - thats much more efficient.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Are you referring to the process of generating lev-automata, or to chapter 6, where they skip generating automata and compute its states on the go instead?&lt;/p&gt;</comment>
                    <comment id="12790712" author="rcmuir" created="Tue, 15 Dec 2009 12:03:27 +0000"  >&lt;blockquote&gt;&lt;p&gt;Are you referring to the process of generating lev-automata, or to chapter 6, where they skip generating automata and compute its states on the go instead?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;no, he refers to generating the parametric &apos;tables&apos; for any n&lt;/p&gt;</comment>
                    <comment id="12790748" author="markrmiller@gmail.com" created="Tue, 15 Dec 2009 14:14:02 +0000"  >&lt;p&gt;Sorry Earwin - to be clear, we don&apos;t actually use chapter 6 - AutomataQuery needs the automata.&lt;/p&gt;

&lt;p&gt;You can get all the states just by taking the power set of the subsumption triangle for every base position, and then removing from each set any position thats subsumed by another. Thats what I mean by brute force. But in the paper, they boil this down to nice little i param &quot;tables&quot;, extracting some sort of pattern from that process. They give no hint on how they do this, or whether it applicable to greater n&apos;s though. No big deal I guess - the computer can do the brute force method - but I wouldn&apos;t be surprised if it starts to bog down at much higher n&apos;s.&lt;/p&gt;</comment>
                    <comment id="12790756" author="rcmuir" created="Tue, 15 Dec 2009 14:24:45 +0000"  >&lt;blockquote&gt;&lt;p&gt;You can get all the states just by taking the power set of the subsumption triangle for every base position, and then removing from each set any position thats subsumed by another. Thats what I mean by brute force. But in the paper, they boil this down to nice little i param &quot;tables&quot;, extracting some sort of pattern from that process. They give no hint on how they do this, or whether it applicable to greater n&apos;s though.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;these parameterized states are based on minimal boundary, and it helps me a bit to start thinking in those terms (instead of E_2 think of 2#1, 3#1, 4#1). Then the parameterized tables from n=2 make a little more sense... (i&apos;m guessing you already know this Mark, just in case)&lt;/p&gt;</comment>
                    <comment id="12832033" author="funtick" created="Wed, 10 Feb 2010 15:47:38 +0000"  >&lt;p&gt;Downloadable article (PDF):&lt;br/&gt;
&lt;a href=&quot;http://www.mitpressjournals.org/doi/pdf/10.1162/0891201042544938?cookieSet=1&quot; class=&quot;external-link&quot;&gt;http://www.mitpressjournals.org/doi/pdf/10.1162/0891201042544938?cookieSet=1&lt;/a&gt;&lt;/p&gt;</comment>
                    <comment id="12832038" author="rcmuir" created="Wed, 10 Feb 2010 16:01:33 +0000"  >&lt;p&gt;Fuad, this is the wrong paper, it will not work for lucene...this issue uses &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.16.652&quot; class=&quot;external-link&quot;&gt;http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.16.652&lt;/a&gt; (linked in the summary).&lt;/p&gt;</comment>
                    <comment id="12832049" author="funtick" created="Wed, 10 Feb 2010 16:25:33 +0000"  >&lt;p&gt;Ok; I am trying to study DFA&amp;amp;NFA and to compare with &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2230&quot; title=&quot;Lucene Fuzzy Search: BK-Tree can improve performance 3-20 times.&quot;&gt;LUCENE-2230&lt;/a&gt; (BKTree size is fixed without dependency on distance, but we need to hard-cache it...). What I found is that classic Levenshtein algo is eating 75% CPU, and classic brute-force TermEnum 25%...&lt;br/&gt;
Distance (submitted by end user) must be integer...&lt;/p&gt;


&lt;p&gt;Edited: BKTree memory requirements don&apos;t have dependency on distance threshold etc.; but BKTree can help only if threshold is small, otherwise it is similar to full-scan.&lt;/p&gt;</comment>
                    <comment id="12832058" author="rcmuir" created="Wed, 10 Feb 2010 16:36:44 +0000"  >&lt;p&gt;Fuad yes, this is the problem with the paper you listed. in my opinion the paper you mentioned is actually better/simpler in some ways than what we are using, but the problem is the term dictionary would have to be restructured for this, or we would have to keep all terms in RAM or something worse.&lt;/p&gt;

&lt;p&gt;the advantage of the (more complex) 2002 paper is that we can run it on lucene&apos;s existing term dictionary, and it will seek to the right spots (i.e. you can take the code i attached here, and pass it to automatonquery and it executes efficiently). &lt;/p&gt;

&lt;p&gt;its also extremely fast algorithm, the construction of the DFA itself is O&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/thumbs_down.gif&quot; height=&quot;19&quot; width=&quot;19&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; where n is the length of the word.&lt;/p&gt;

&lt;p&gt;i tried to make a visual description of how Automaton works for regexp, wildcard, and fuzzy here if you are interested: &lt;a href=&quot;http://rcmuir.wordpress.com/2009/12/04/finite-state-queries-for-lucene/&quot; class=&quot;external-link&quot;&gt;http://rcmuir.wordpress.com/2009/12/04/finite-state-queries-for-lucene/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;in short, AutomatonQuery can be viewed as a simplified form of the DFA intersection presented in the paper you linked to (section 5.1). The lucene term dictionary itself is a trie of terms in sorted order, therefore it is also a DFA in the mathematical sense.&lt;/p&gt;</comment>
                    <comment id="12832060" author="rcmuir" created="Wed, 10 Feb 2010 16:41:26 +0000"  >&lt;blockquote&gt;&lt;p&gt;Distance (submitted by end user) must be integer&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;this is not true for this issue. the plan is to keep all apis and results the same, we can just convert the minimum distance into an integer N to determine which DFA needs to be executing. &lt;/p&gt;

&lt;p&gt;another difference here is that the minimum distance is not fixed, but changes during enumeration, if the priority queue fills up then there is no point wasting time seeking to terms that will only be dropped, see &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2140&quot; title=&quot;TopTermsScoringBooleanQueryRewrite minscore&quot;&gt;&lt;del&gt;LUCENE-2140&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;</comment>
                    <comment id="12832119" author="rcmuir" created="Wed, 10 Feb 2010 18:45:06 +0000"  >&lt;blockquote&gt;&lt;p&gt;but BKTree can help only if threshold is small, otherwise it is similar to full-scan.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;right, this is one aspect that makes the DFA approach attractive, even if there is a high threshold, so high that its not even worth seeking around and brute-force iteration is faster, we can still compute the levenstein distance in O&amp;#40;n&amp;#41; time with a DFA, instead of the expensive dynamic programming algorithm.&lt;/p&gt;

&lt;p&gt;i found this out for wildcard as well, as even when just doing brute-force enumeration, the compiled RunAutomaton eats the old hand-coded wildcardEquals() for breakfast, because its a faster matcher.&lt;/p&gt;

&lt;p&gt;this is in line with some tests done by jflex: &lt;a href=&quot;http://jflex.de/manual.html#PerformanceHandwritten&quot; class=&quot;external-link&quot;&gt;http://jflex.de/manual.html#PerformanceHandwritten&lt;/a&gt; . I have to agree that in this day and age, writing hand-coded stuff to parse or match is simply a waste of time, as chances are a DFA will be considerably more efficient.&lt;/p&gt;</comment>
                    <comment id="12832130" author="funtick" created="Wed, 10 Feb 2010 19:06:44 +0000"  >&lt;p&gt;What about this,&lt;br/&gt;
&lt;a href=&quot;http://www.catalysoft.com/articles/StrikeAMatch.html&quot; class=&quot;external-link&quot;&gt;http://www.catalysoft.com/articles/StrikeAMatch.html&lt;/a&gt;&lt;br/&gt;
it seems logically more appropriate to (human-entered) text objects than Levenshtein distance, and it is (in theory) extremely fast; is DFA-distance faster?&lt;/p&gt;</comment>
                    <comment id="12832136" author="rcmuir" created="Wed, 10 Feb 2010 19:19:46 +0000"  >&lt;p&gt;Fuad as far as edit distances go, I am implementing Levenshtein only out of interest of speeding up core FuzzyQuery (and keeping backwards compatibility). &lt;/p&gt;

&lt;p&gt;its entirely possible you could write an algorithm to create a DFA that accepts any word within &lt;em&gt;n&lt;/em&gt; strikeAMatch distances if you wanted to. you could use AutomatonQuery then to run it efficiently.&lt;/p&gt;

&lt;p&gt;The only reason i select levenshtein is because thats what FuzzyQuery does already, not because I think its particularly &quot;good&quot;, although I do feel its the measure that FuzzyQuery should stick with, as it is the most general-purpose.&lt;/p&gt;

&lt;p&gt;The distance measure you mentioned might make more sense for some languages and purposes, but for others maybe not.&lt;/p&gt;


</comment>
                    <comment id="12832143" author="funtick" created="Wed, 10 Feb 2010 19:38:39 +0000"  >&lt;p&gt;Hi Robert,&lt;/p&gt;

&lt;p&gt;Yes, I agree; we need to stick with Levenshtein distance also to isolate performance comparisons: same distance, but FuzzyTermEnum with full-scan vs. DFA-based approach, and we need to be able to compare old relevance with new one (with integer distance threshold it is not the same as with classic float-point...) thanks for the link to your article! &lt;/p&gt;

&lt;p&gt;What if we can store some precounted values in the index... such as storing &quot;similar terms&quot; in additional field... or some pieces of DFA (which I still need to learn...)&lt;/p&gt;</comment>
                    <comment id="12832150" author="rcmuir" created="Wed, 10 Feb 2010 19:49:57 +0000"  >&lt;blockquote&gt;&lt;p&gt;and we need to be able to compare old relevance with new one (with integer distance threshold it is not the same as with classic float-point...)&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;this is not true, we can determine for any word, and any floating point value, the maximum integer distance that equates.&lt;/p&gt;

&lt;p&gt;This query will be completely backwards compatible, same results, same order, only faster.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;What if we can store some precounted values in the index... such as storing &quot;similar terms&quot; in additional field... or some pieces of DFA (which I still need to learn...)&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This is not necessary for AutomatonQuery, and imposes too much of a space tradeoff. You can see an example of this in &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-1513&quot; title=&quot;fastss fuzzyquery&quot;&gt;&lt;del&gt;LUCENE-1513&lt;/del&gt;&lt;/a&gt;, where deletion neighborhoods are indexed. &lt;/p&gt;

&lt;p&gt;However, given the speedups to AutomatonQuery itself done in &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-1606&quot; title=&quot;Automaton Query/Filter (scalable regex)&quot;&gt;&lt;del&gt;LUCENE-1606&lt;/del&gt;&lt;/a&gt;, it will actually surpass this approach, impose no index or storage requirements, scale to higher N, and allow for full back compat.  &lt;/p&gt;

&lt;p&gt;The only &quot;tradeoff&quot; in this approach is the headache caused by the mathematics, and the majority of this is now solved... unlike approaches that require additional disk or memory or indexing changes, users are unaffected if we stay up too late staring at funky math.&lt;/p&gt;</comment>
                    <comment id="12832173" author="funtick" created="Wed, 10 Feb 2010 20:19:30 +0000"  >&lt;p&gt;For &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2230&quot; title=&quot;Lucene Fuzzy Search: BK-Tree can improve performance 3-20 times.&quot;&gt;LUCENE-2230&lt;/a&gt; I did a lot of long-run load-stress tests (against SOLR), but before doing that I created baseline for static admin screen in SOLR: 1500TPS. And I reached 220TPS with Fuzzy Search... what I am trying to say is this: can DFA with Levenshtein reach 250TPS (in real-world multi-tier web environment)? Baseline for static page is 1500. Also, is DFA mostly CPU-bound? Can we improve it by making (some) I/O-bound unload?&lt;br/&gt;
Just joking &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/wink.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;I used explicitly distance threshold=2, that&apos;s why 220TPS... with threshold=5 it would be 50TPS or may be less...&lt;/p&gt;

&lt;p&gt;If DFA doesn&apos;t have dependency on threshold, it is the way to go.&lt;/p&gt;</comment>
                    <comment id="12832182" author="rcmuir" created="Wed, 10 Feb 2010 20:33:28 +0000"  >&lt;blockquote&gt;&lt;p&gt;what I am trying to say is this: can DFA with Levenshtein reach 250TPS (in real-world multi-tier web environment)? &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;It depends a lot on what is in your index! for now i use a benchmark of generating 10 million unique terms, and generating random fuzzy queries against them. There is no reason to involve Solr, I don&apos;t understand what it has to do with benchmarking this, and its still stuck on Lucene 2.9 so it wouldnt be practical in any case.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Also, is DFA mostly CPU-bound? Can we improve it by making (some) I/O-bound unload?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;not really, I dont think the DFA intersection can really be any faster than it is. for example on the 10 million terms benchmark, a Lev(1) DFA must seek to 64 terms. This averages right under 20ms last i checked (it is currently averaging over 3 seconds in trunk and flex).&lt;/p&gt;</comment>
                    <comment id="12832194" author="funtick" created="Wed, 10 Feb 2010 21:09:45 +0000"  >&lt;p&gt;Ok, now I understand what AutomatonQuery is... frankly, I had this idea, to create small dictionary of &quot;similar words&quot;, to create terms from those words, and to execute query &amp;lt;Word1&amp;gt; OR &amp;lt;Word2&amp;gt; OR ... instead of scanning whole term dictionary, but how &quot;small&quot; will be such dictionary in case, for instance, &quot;????dogs&quot;... is size of dictionary (in case of ASCII-characters) 26*26*26*26? Or, 65536*65536*65536*65536 in case of Unicode?&lt;br/&gt;
Simple.&lt;br/&gt;
Is it so simple?&lt;/p&gt;

&lt;p&gt;Even with Unicode, we can precount set of characters for a specific field instance; it can be 36 characters; and query like &amp;lt;aaaadogs&amp;gt; OR &amp;lt;aaabdogs&amp;gt; OR ... OR &amp;lt;9999dogs&amp;gt;&lt;/p&gt;

&lt;p&gt;and, if we can quickly find intersection of custom dictionary with terms dictionary, then build the query... am I on correct path with understanding?&lt;/p&gt;</comment>
                    <comment id="12832205" author="rcmuir" created="Wed, 10 Feb 2010 21:20:55 +0000"  >&lt;p&gt;fuad it does not expand the query to OR&apos;ed terms.&lt;/p&gt;

&lt;p&gt;here is a description from the 2002 paper that describes how automaton works (for regex, wildcard, fuzzy, whatever the case)&lt;br/&gt;
I edited the description to substitute components with their implementation... and yes it works for all Unicode (all over 1 million codepoints not just the BMP)&lt;/p&gt;

&lt;p&gt;For more details, see AutomatonTermsEnum in the flex branch svn.&lt;/p&gt;

&lt;p&gt;The set of all dictionary words is treated as a regular language over the alphabet of letters. At each step, the prefix of all letters that are consumed on the path from the initial state to the current state is maintained. A variant of the Wagner-Fisher algorithm is used to control the &lt;del&gt;walk through the automaton&lt;/del&gt; (enumeration of Lucene&apos;s term dictionary) in such a way that only &lt;del&gt;prefixes are generated&lt;/del&gt; (FilteredTermsEnums only seeks to terms) that potentially lead to a correction candidate V where the Levenshtein distance between V and W does not exceed a fixed bound n.&lt;/p&gt;</comment>
                    <comment id="12832360" author="funtick" created="Thu, 11 Feb 2010 02:12:10 +0000"  >&lt;p&gt;Levenshtein Distance is good for &quot;Spelling Corrections&quot; use case (even terminology is the same: insert, delete, replace...) &lt;br/&gt;
But is is not good for more generic similarity: distance between RunAutomation and AutomationRun is huge (6!). But it is two-word combination indeed,and I don&apos;t know good one-(human)-word use case where Levenshtein Distance is not good (or natural). From other viewpoint, I can&apos;t see any use case where StrikeAMatch (counts of 2-char similarities) is bad, although it is not &quot;spelling corrections&quot;. And, from third viewpoint, if we totally forget that it is indeed human-generated-input and implement Levenshtein distance on raw bitsets instead of unicode characters (end-user clicks on keyboard)... we will get totally non-acceptable results... &lt;/p&gt;

&lt;p&gt;I believe such &quot;distance&quot; algos were initially designed many years ago, before Internet (and Search), to allow auto-recovery during data transmission (first astronauts...) - but autorecovery was based on fact that (acceptable) mistaken code has one and only one closest match from the dictionary; so it was extremely fast (50 years ago). And now, we are using old algo designed for completely different use case (fixed-size bitset transmissions) for &quot;spelling corrections&quot;... &lt;/p&gt;

&lt;p&gt;What if we will focus on a keyboard (101 keys?) instead of Unicode... &quot;spelling corrections&quot;...&lt;/p&gt;

&lt;p&gt;20ms is not good, it is 50TPS only (on a single core)...&lt;/p&gt;</comment>
                    <comment id="12832368" author="funtick" created="Thu, 11 Feb 2010 02:52:54 +0000"  >&lt;p&gt;Another idea (similar to 50-years-old &quot;auto-recovery&quot;), it doesn&apos;t allow me to sleep &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;br/&gt;
What if we do all distance calculations (and other types of calculations) at indexing time instead of at query time? For instance, we may have index structure like &lt;/p&gt;
{Term, List[MisspelledTerm, Distance]}
&lt;p&gt;, and we can query this structure by &lt;/p&gt;
{MisspelledTerm, Distance}
&lt;p&gt;? We mentioned it here already, &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-1513&quot; title=&quot;fastss fuzzyquery&quot;&gt;&lt;del&gt;LUCENE-1513&lt;/del&gt;&lt;/a&gt;, but our use case is very specific... and why to allow 5 spelling mistakes in Unicode if user&apos;s input contains 3 characters only in Latin1? We should hardcode some constraints.&lt;/p&gt;

&lt;p&gt;Yes, memory requirements... in case of &quot;????dogs&quot; it can be at least few millions of additional misspelled-terms for this specific &quot;dogs&quot; term only... but it doesn&apos;t grow linearly... and we can limit such structure for distance=2, and use additional query-time processing if we need distance=3.&lt;/p&gt;

&lt;p&gt;It&apos;s just (naive) idea: to precalculate &quot;similar terms&quot; at indexing time...&lt;/p&gt;</comment>
                    <comment id="12832424" author="eksdev" created="Thu, 11 Feb 2010 09:01:26 +0000"  >&lt;blockquote&gt;
&lt;p&gt; What about this,&lt;br/&gt;
&lt;a href=&quot;http://www.catalysoft.com/articles/StrikeAMatch.html&quot; class=&quot;external-link&quot;&gt;http://www.catalysoft.com/articles/StrikeAMatch.html&lt;/a&gt;&lt;br/&gt;
it seems logically more appropriate to (human-entered) text objects than Levenshtein distance, and it is (in theory) extremely fast; is DFA-distance faster? &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Is that only me who sees plain, vanilla bigram distance here? What is new or better in StrikeAMatch compared to the first phase of the current SpellCehcker (feeding PriorityQueue with candidates)? &lt;/p&gt;

&lt;p&gt;If you need too use this, nothing simpler, you do not even need pair comparison (aka traversal), just Index terms split into bigrams and search with standard Query. &lt;/p&gt;


&lt;p&gt;Autmaton trick is a neat one. Imo,  the only thing that would work better is to make term dictionary real trie (ternary, n-ary, dfa, makes no big diff). Making TerrmDict some sort of trie/dfa would permit smart beam-search,  even without compiling query DFA. Beam search also makes implementation of better distances possible (Weighted Edit distance without &quot;metric constraint&quot; ). I guess this is going to be possible with Flex, Mike was allready talking about DFA Dictionary &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;It took a while to figure out the trick Robert pooled here, treating term dictionary as another DFA due to the sortedness, nice. &lt;/p&gt;</comment>
                    <comment id="12832687" author="rcmuir" created="Thu, 11 Feb 2010 21:27:03 +0000"  >&lt;blockquote&gt;&lt;p&gt;Imo, the only thing that would work better is to make term dictionary real trie (ternary, n-ary, dfa, makes no big diff). &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;yeah I think a real DFA would be more efficient, its too much to think about. I guess this is why we leave this kind of thing to Mike &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;I haven&apos;t grasped how this beam search would work with a DFA dictionary, although it definitely sounds cool. &lt;br/&gt;
I assume you mean by weighted edit distance that the transitions in the state machine would have costs?&lt;br/&gt;
If this is the case couldn&apos;t we even define standard levenshtein very easily (instead of nasty math), and would the beam search technique enumerate efficiently for us?&lt;/p&gt;</comment>
                    <comment id="12832741" author="eksdev" created="Thu, 11 Feb 2010 23:46:57 +0000"  >&lt;blockquote&gt;
&lt;p&gt;I assume you mean by weighted edit distance that the transitions in the state machine would have costs?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes, kind of, not embedded in the trie, just defined externally.&lt;/p&gt;

&lt;p&gt;What I am talking about is a part of the noisy channel approach, modeling only channel distribution. Have a look at the &lt;a href=&quot;http://norvig.com/spell-correct.html&quot; class=&quot;external-link&quot;&gt;http://norvig.com/spell-correct.html&lt;/a&gt; for basic theory. I am suggesting almost the same,  just applied at character level and without language model part. It is rather easy once you have your dictionary in some sort of tree structure.&lt;/p&gt;


&lt;p&gt;You guide your trie traversal over the trie by  iterating on each char in your search term accumulating   log probabilities of single transformations (recycling prefix part). When you hit a leaf insert into PriorityQueue of appropriate depth. What I mean by &quot;probabilities of single transformations&quot; are defined as:&lt;br/&gt;
insertion(character a)//map char-&amp;gt;log probability (think of it as kind of &quot;cost of inserting this particular character)&lt;br/&gt;
deletion(character)//map char-&amp;gt;log probability...&lt;br/&gt;
transposition(char a, char b)&lt;br/&gt;
replacement(char a, char b)//2D matrix &amp;lt;char,char&amp;gt;-&amp;gt;probability (cost)&lt;br/&gt;
if you wish , you could even add some positional information, boosting match on start/end of the string&lt;/p&gt;

&lt;p&gt;I avoided tricky mechanicson traversal, insertion, deletion, but on trie you can do it by following different paths... &lt;/p&gt;

&lt;p&gt;the only good implementation (in memory) around there I know of is in LingPipe spell checker (they implement full Noisy Channel, with Language model  driving traversal)... has huge educational value, Bob is really great at explaining things. The code itself is proprietary. &lt;br/&gt;
I would suggest you to peek into this code to see this 2-Minute rumbling I wrote here properly explained &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; Just ignore the language model part and assume you have NULL language model (all chars in language are equally probable) , doing full traversal over the trie. &lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;If this is the case couldn&apos;t we even define standard levenshtein very easily (instead of nasty math), and would the beam search technique enumerate efficiently for us?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt; Standard Lev. is trivially configured once you have this, it is just setting all these costs to 1 (delete, insert... in log domain)... But who would use standard distance with such a beast, reducing impact of inserting/deleting silent &quot;h&quot; as in &quot;Thomas&quot; &quot;Tomas&quot;... &lt;br/&gt;
Enumeration is trie traversal, practically calculating distance against all terms at the same time and collectiong N best along the way. The place where you save your time is recycling prefix part in this calculation. Enumeration is optimal as this trie there contains only the terms from termDict, you are not trying all possible alphabet characters and you can implement &quot;early path abandoning&quot; easily ether by cost (log probability) or/and by limiting the number  of successive insertions&lt;/p&gt;

&lt;p&gt;If interested in really in depth things, look at &lt;a href=&quot;http://www.amazon.com/Algorithms-Strings-Trees-Sequences-Computational/dp/0521585198&quot; class=&quot;external-link&quot;&gt;http://www.amazon.com/Algorithms-Strings-Trees-Sequences-Computational/dp/0521585198&lt;/a&gt;&lt;br/&gt;
Great book, (another great tip from  Bob@LingPipe). A bit strange with terminology (at least to me), but once you get used to it, is really worth the time you spend trying to grasp it.&lt;/p&gt;




</comment>
                    <comment id="12832773" author="amccurry" created="Fri, 12 Feb 2010 01:02:39 +0000"  >&lt;p&gt;I have written a levenstein generator today that seems to operate similarly to what is being discussed here.  It generates all the possible matches to levenstein algorithm given a term and a character set, it then creates a booleanquery from it.  For a given term with edit distance of 1 or 2 it is extremely fast.  I tested it on my dev data that has about 8 billion documents with 20 shards, each shard has about 170,000,000 terms in the field that I&apos;m testing.  The normal fuzzy query with a term length of 8 and and edit distance of 2 took about 110 seconds to complete, and the auto generated query took around a 1.5 seconds complete.  However this method will probably only work with edits distances in the 1 and 2 range, because once I hit 3 it spiked the memory usage and slowed way down (to be expected).  Not sure if you all want to take a look at my code or not, but if you want me to post it I will.&lt;/p&gt;</comment>
                    <comment id="12832785" author="rcmuir" created="Fri, 12 Feb 2010 01:51:09 +0000"  >&lt;p&gt;Aaron i think generation may pose a problem for a full unicode alphabet.&lt;/p&gt;

&lt;p&gt;e.g. edit distance of 1 on &quot;otter&quot; is 720,891 strings... this is a large boolean query!&lt;/p&gt;

&lt;p&gt;so this is where the ping-ponging of automatonquery really helps, we dont have to do 720,891 term lookups because the term dictionary is in sorted order, so its treated like dfa intersection.&lt;/p&gt;

&lt;p&gt;btw, you can test this patch with your 170M terms by applying this patch to flex branch, and using the following code.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
LevenshteinAutomata a = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; LevenshteinAutomata(&lt;span class=&quot;code-quote&quot;&gt;&quot;otter&quot;&lt;/span&gt;); &lt;span class=&quot;code-comment&quot;&gt;// searching on otter
&lt;/span&gt;Automaton lev1 = a.toAutomaton(1); &lt;span class=&quot;code-comment&quot;&gt;// edit distance of 1
&lt;/span&gt;Query query = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; AutomatonQuery(&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; Term(&lt;span class=&quot;code-quote&quot;&gt;&quot;field&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;bogus&quot;&lt;/span&gt;), lev1);
...
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                    <comment id="12832789" author="rcmuir" created="Fri, 12 Feb 2010 02:00:49 +0000"  >&lt;blockquote&gt;&lt;p&gt;But who would use standard distance with such a beast, reducing impact of inserting/deleting silent &quot;h&quot; as in &quot;Thomas&quot; &quot;Tomas&quot;...&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;hehe, well my only goal with this issue (and really automaton in general) is to speed up in a backwards compatible way, so I am stuck with standard distance for that purpose.&lt;/p&gt;

&lt;p&gt;but the more intelligent stuff you speak of could be really cool esp. for spellchecking, sure you dont want to rewrite our spellchecker?&lt;/p&gt;

&lt;p&gt;btw its not clear to me yet, could you implement that stuff on top of &quot;ghetto DFA&quot; (the sorted terms dict we have now) or is something more sophisticated needed? its a lot easier to write this stuff now with the flex MTQ apis &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                    <comment id="12832794" author="amccurry" created="Fri, 12 Feb 2010 02:13:59 +0000"  >&lt;p&gt;Excuse my ignorance, but what is a DFA?&lt;/p&gt;</comment>
                    <comment id="12832798" author="amccurry" created="Fri, 12 Feb 2010 02:21:48 +0000"  >&lt;blockquote&gt;
&lt;p&gt;Aaron i think generation may pose a problem for a full unicode alphabet.&lt;br/&gt;
e.g. edit distance of 1 on &quot;otter&quot; is 720,891 strings... this is a large boolean query!&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;So I have to ask, who has the entire unicode alphabet indexed in a single field?  I don&apos;t, and I am willing to make tradeoffs in my system for performance.  So for instance, I am willing to code a mapping file or scan my index on startup to know the character set (a-z 0-9 or whatever) to reduce the possibilities.  This is a simple know your data problem, at least in IMHO.&lt;/p&gt;

&lt;p&gt;Aaron&lt;/p&gt;</comment>
                    <comment id="12832814" author="rcmuir" created="Fri, 12 Feb 2010 02:48:31 +0000"  >&lt;blockquote&gt;&lt;p&gt;So I have to ask, who has the entire unicode alphabet indexed in a single field? I don&apos;t, and I am willing to make tradeoffs in my system for performance.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Aaron, would you mind testing this patch with the flex branch (with the sample code i listed before)? There is no tradeoff to support unicode.&lt;br/&gt;
If you don&apos;t have chinese chars in your index, the enum will not seek to them, but skip past them, as they do not exist, and Term(s)Enum always returns things in sorted order.&lt;/p&gt;

&lt;p&gt;For more details, see AutomatonTermsEnum in flex (this is the code that actually does DFA &amp;lt;-&amp;gt; DFA intersection of any Automaton with lucene&apos;s term dictionary). &lt;/p&gt;</comment>
                    <comment id="12832817" author="rcmuir" created="Fri, 12 Feb 2010 02:54:02 +0000"  >&lt;blockquote&gt;&lt;p&gt;Excuse my ignorance, but what is a DFA? &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;here is wikipedia page with a description: &lt;a href=&quot;http://en.wikipedia.org/wiki/Deterministic_finite-state_machine&quot; class=&quot;external-link&quot;&gt;http://en.wikipedia.org/wiki/Deterministic_finite-state_machine&lt;/a&gt;&lt;/p&gt;</comment>
                    <comment id="12832819" author="amccurry" created="Fri, 12 Feb 2010 02:56:37 +0000"  >&lt;p&gt;Sure I will give it a try, still wrapping my head around the flex branch.&lt;/p&gt;</comment>
                    <comment id="12832822" author="rcmuir" created="Fri, 12 Feb 2010 03:12:53 +0000"  >&lt;p&gt;thanks, this will give you something to play with wrt DFA queries. &lt;br/&gt;
in flex, Regex and wildcard are already implemented this way.&lt;/p&gt;

&lt;p&gt;one additional complexity here that this code will not do (until some other issues are resolved and i update the patch), is the &quot;pq trick&quot;.&lt;/p&gt;

&lt;p&gt;This is the fact that FuzzyQuery (or really anything else that uses TopTermsRewrite) is really an n-best list, and we shouldn&apos;t seek to terms that will simply be overflowed off the priority queue anyway, as its just a waste of time.&lt;/p&gt;

&lt;p&gt;this is the part i am working on now, most of it already one under &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2140&quot; title=&quot;TopTermsScoringBooleanQueryRewrite minscore&quot;&gt;&lt;del&gt;LUCENE-2140&lt;/del&gt;&lt;/a&gt; and some setup now under &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2261&quot; title=&quot;configurable MultiTermQuery TopTermsScoringBooleanRewrite pq size&quot;&gt;&lt;del&gt;LUCENE-2261&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;</comment>
                    <comment id="12832911" author="eksdev" created="Fri, 12 Feb 2010 09:31:09 +0000"  >&lt;blockquote&gt;
&lt;p&gt;...Aaron i think generation may pose a problem for a full unicode alphabet...&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I wouldn&apos;t discount Aron&apos;s approach so quickly! There is one &lt;b&gt;really smart&lt;/b&gt;  way to aproach generation of the distance negborhood. Have a look at FastSS &lt;a href=&quot;http://fastss.csg.uzh.ch/&quot; class=&quot;external-link&quot;&gt;http://fastss.csg.uzh.ch/&lt;/a&gt;  The trick is to delete, not to genarate variations over complete  alphabet! They call it &quot;deletion negborhood&quot;.  Also, generates much less variation Terms, reducing pressure on binary search in TermDict!&lt;/p&gt;

&lt;p&gt;You do not get all these goodies from Weighted  distance implementation, but the solution is much simpler. Would work similary to the  current spellchecker (just lookup on &quot;variations&quot;), only  faster. They have even some exemple code to see how they generate &quot;deletions&quot; (&lt;a href=&quot;http://fastss.csg.uzh.ch/FastSimilarSearch.java&quot; class=&quot;external-link&quot;&gt;http://fastss.csg.uzh.ch/FastSimilarSearch.java&lt;/a&gt;).&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;but the more intelligent stuff you speak of could be really cool esp. for spellchecking, sure you dont want to rewrite our spellchecker?&lt;/p&gt;

&lt;p&gt;btw its not clear to me yet, could you implement that stuff on top of &quot;ghetto DFA&quot; (the sorted terms dict we have now) or is something more sophisticated needed? its a lot easier to write this stuff now with the flex MTQ apis &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I really  would love to, but I was paid before to work on this. &lt;/p&gt;

&lt;p&gt;I guess  &quot;gheto dfa&quot; would not work, at least not fast enough (I didn&apos;t think about it really). Practically you would need to know which characters extend current character in you dictionary, or in DFA parlance, all outgoing transitions from the current state. &quot;gheto dfa&quot; cannot do it efficiently?&lt;/p&gt;

&lt;p&gt;What would be an idea with flex is to implement this stuff with an in memory trie (full trie or TST), befor jumping into noisy channel (this is easy to add later) and persistent trie-dictionary.  The traversal part is identical,  and  would make a nice contrib with a usefull use case as the majority of folks have  enogh memory to slurp complete termDict into memory... Would serve as a proof of concept for flex and fuzzyQ,  help you understand the magic of calculating edit distance against Trie structures. Once you have trie structure, the sky is the limit, prefix, regex... If I remeber corectly, there were some trie implmentations floating around, with it you need just one extra traversal method to find all terms at distance N. You can have a look at &quot;http://jaspell.sourceforge.net/&quot; TST implmentation, class TernarySearchTrie.matchAlmost(...) methods. Just for an ilustration what is going there, it is simple recursive traversal of all terms at max distance of N.&lt;br/&gt;
Later we could tweak memory demand, switch to some more compact trie... and at the and add weighted distance and convince Mike to make blasing fast persisten trie &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;... in meantime, the folks with enogh memory would have really really fast fuzzy, prefix... better distance... &lt;/p&gt;



&lt;p&gt;So the theory &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; I hope you find these comments usful, even without patches&lt;/p&gt;



</comment>
                    <comment id="12832920" author="mikemccand" created="Fri, 12 Feb 2010 09:58:50 +0000"  >&lt;blockquote&gt;&lt;p&gt;and convince Mike to make blasing fast persisten trie&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Mike is looking forward to that!&lt;/p&gt;

&lt;p&gt;The terms dict should really be stored as transducer (DFA, where the edges can have outputs).  This way both prefixes (trie) &amp;amp; suffixes (trie, in reverse) are shared.  If this uses too much RAM, I wonder if we could do it only with the terms index, and then somehow automata query would efficiently identify parts (of 128 terms in a row) of the on-disk terms dict to scan based on the index (might be tricky...).&lt;/p&gt;

&lt;p&gt;This sudden burst of ideas &amp;amp; interest in fast fuzzy multi term queries is very exciting...&lt;/p&gt;</comment>
                    <comment id="12832986" author="rcmuir" created="Fri, 12 Feb 2010 12:54:29 +0000"  >&lt;p&gt;Eks, i actually linked FastSS to this issue (&lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-1513&quot; title=&quot;fastss fuzzyquery&quot;&gt;&lt;del&gt;LUCENE-1513&lt;/del&gt;&lt;/a&gt;). I used to use it, but prefer automaton as i dont want to have to build/keep track of that massive deletion neighborhood. its not like it has better computational complexity or anything, and as # terms increases, these additional data structures become horrendous.&lt;/p&gt;

&lt;p&gt;for larger n, those deletion neighborhoods just get worse and worse (insanely large). just as query expansions do. and its a static structure, so its problematic for NRT and other use cases.&lt;/p&gt;</comment>
                    <comment id="12833014" author="rcmuir" created="Fri, 12 Feb 2010 14:39:52 +0000"  >&lt;blockquote&gt;&lt;p&gt;I guess &quot;gheto dfa&quot; would not work, at least not fast enough (I didn&apos;t think about it really). Practically you would need to know which characters extend current character in you dictionary, or in DFA parlance, all outgoing transitions from the current state. &quot;gheto dfa&quot; cannot do it efficiently?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;i think it does it efficiently enough for any finite language (such as what you would use for fuzzy), the real problem with ghetto DFA relates more to infinite languages (* operator in wildcard/regex).&lt;/p&gt;

&lt;p&gt;its very easy to see the worst case and very difficult to see the average case, see: &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.133.2155&amp;amp;rep=rep1&amp;amp;type=pdf&quot; class=&quot;external-link&quot;&gt;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.133.2155&amp;amp;rep=rep1&amp;amp;type=pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;if you have an idea you should try it on ghetto DFA (lucene&apos;s existing term dictionary), unless you can easily consume that average-case analysis presented in the paper (i can&apos;t)... &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; &lt;/p&gt;

&lt;p&gt;eg its surprising to me in practice.  Don&apos;t be afraid of the average complexity presented in the paper of O&amp;#40;sqrt&amp;#40;n&amp;#41;&amp;#41; where n is number of terms..., this is for &quot;average regular expression&quot;, for things like levenstein automata it is much less as they are finite (e.g. 64 inspections for Lev1 of my 10 million terms benchmark, not 3,000 inspections.&lt;/p&gt;</comment>
                    <comment id="12834111" author="rcmuir" created="Tue, 16 Feb 2010 07:33:38 +0000"  >&lt;p&gt;here is a more fleshed out patch (rough but all tests pass):&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;FuzzyTermsEnum is a proxy enum, it delegates behavior to either LinearFuzzyTermsEnum (the old code) or AutomatonFuzzyTermsEnum, and can switch during enumeration.&lt;/li&gt;
	&lt;li&gt;TODO is to handle minBoostChanged, this is the event where the priority queue is full and the minimum score to be competitive has changed (the opportunity to switch to a more efficient enum).&lt;/li&gt;
	&lt;li&gt;code is generalized for all N, but i only include the table for N=0,1 for now.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="12834127" author="rcmuir" created="Tue, 16 Feb 2010 09:06:44 +0000"  >&lt;p&gt;updated patch:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;the proxy is a TermsEnum not a FilteredTermsEnum, this makes more sense&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="12834625" author="rcmuir" created="Wed, 17 Feb 2010 03:32:56 +0000"  >&lt;p&gt;updated patch:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;when the user supplies a constant prefix, it is actually used for speeding up the automaton enum (less seeks). it is simply DFA-concatenated with the Lev DFA for the non-prefix part.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;i will upload a separate patch that improves BasicOperations.concatenate to allow this without creating an intermediate NFA.&lt;/p&gt;</comment>
                    <comment id="12834626" author="rcmuir" created="Wed, 17 Feb 2010 03:36:14 +0000"  >&lt;p&gt;this is the patch to improve BasicOperations.concatenate. If the left side is a singleton automaton, then it has only one final state with no outgoing transitions. applying epsilon transitions with the NFA concatenation algorithm when the right side is a DFA always produces a resulting DFA, so mark it as such.&lt;/p&gt;

&lt;p&gt;if no one objects i will commit this small concatenate patch soon to the flex branch, as it also improves all other Automaton queries, and speeds up the rewrite-to-PrefixQuery check in AutomatonQuery.getEnum()&lt;/p&gt;</comment>
                    <comment id="12834676" author="thetaphi" created="Wed, 17 Feb 2010 07:43:08 +0000"  >&lt;blockquote&gt;&lt;p&gt;this is the patch to improve BasicOperations.concatenate. If the left side is a singleton automaton, then it has only one final state with no outgoing transitions. applying epsilon transitions with the NFA concatenation algorithm when the right side is a DFA always produces a resulting DFA, so mark it as such. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Strange that the automaton author did not add this? Have you reported upstream?&lt;/p&gt;</comment>
                    <comment id="12834695" author="thetaphi" created="Wed, 17 Feb 2010 09:02:06 +0000"  >&lt;p&gt;Hi Robert,&lt;br/&gt;
I reviewed you latest patch and was a little bit irritated, but then everything explained when also looking into AutomatonTermsEnum and understanding what happes. But there is still some &quot;code duplication&quot; (not really duplication, but functionality duplication):&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;If a constant prefix is used, the generated Automatons are using a constant prefix + a Levenshtein Automaton (using concat)&lt;/li&gt;
	&lt;li&gt;If you run such an automaton agains the TermIndex using the superclass, it will seek first to the prefix term (or some term starting with the prefix), thats ok. As soon as the prefix is no longer valid, the AutomatonTermsEnum stops processing (if running such an automaton using the standard AutomatonTermsEnum).&lt;/li&gt;
	&lt;li&gt;The AutomatonFuzzyTermsEnum checks if the term starts with prefix and if not it exists ENDs &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/warning.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; the automaton. The reason why this works is because nextString() in superclass returns automatically a string starting with the prefix, but this was the main fact that irritated me.&lt;/li&gt;
	&lt;li&gt;The question is now, is this extra prefix check really needed? Running the automaton against the current term in accept would simply return no match and nextString() would stop further processing? Or is this because the accept method should not iterate through all distances once the prefix is not matched?&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Maybe you should add some comments to the AutomatonFuzzyTermsEnum or some asserts to show whats happening.&lt;/p&gt;</comment>
                    <comment id="12834732" author="rcmuir" created="Wed, 17 Feb 2010 11:05:39 +0000"  >&lt;blockquote&gt;&lt;p&gt;The question is now, is this extra prefix check really needed?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;probably not, as you are right, nextString() will generate null, there is no need to explicitly handle the prefix. AutomatonTermsEnum does not do this either, for the same reason.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Maybe you should add some comments to the AutomatonFuzzyTermsEnum&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;yeah, i know it needs this... (the patch is not yet ready for committing, but it is functional). &lt;/p&gt;</comment>
                    <comment id="12834943" author="rcmuir" created="Wed, 17 Feb 2010 19:18:58 +0000"  >&lt;blockquote&gt;&lt;p&gt;Strange that the automaton author did not add this? Have you reported upstream?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;in my opinion, the optimization is incomplete. I think it can be generalized more further as this:&lt;br/&gt;
the NFA concatenation of DFA1 and DFA2 always results in a DFA, if SfDFA1 and SiDFA2 have no intersection, where SfDFA1 is the set of transitions from the final states of DFA1, and SiDFA2 is the set of transitions from the initial state of DFA2.&lt;/p&gt;

&lt;p&gt;i doubt the usefulness of this opto in general, but the very specific case where DFA1 is a singleton (and has no outgoing transitions so the intersection is null by definition) is extremely important to Lucene, as it prevents expensive NFA-DFA conversion for backwards compatibility with these &quot;prefix&quot; options (Automaton.getEnum and Fuzzy prefix), especially for the fuzzy case, DFA2 is very large. &lt;/p&gt;

&lt;p&gt;if a user supplies a prefix, it should make the query faster, not slower &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;also, you will note i didnt optimize concatenate(List) but only concatenate(A1, A2). obviously a proper patch would optimization the List case, too.&lt;/p&gt;

&lt;p&gt;i feel the very specific case we care about is proved by induction in the junit cases i supplied, but i would think as this is a math-oriented library they would want the general opto and a proof... if you can find one let me know &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                    <comment id="12835787" author="rcmuir" created="Fri, 19 Feb 2010 15:46:22 +0000"  >&lt;ul&gt;
	&lt;li&gt;implement the pq algorithm, when the value at the bottom of the pq changes (BoostAttribute maxCompetitiveBoost), the enum adjusts itself by decreasing edit distance, and swapping in more efficient code.&lt;/li&gt;
	&lt;li&gt;remove the wasted prefix checks in automatonfuzzytermsenum, as Uwe noticed, because its not necessary and handled as part of the DFA itself (it will never seek to such terms).&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;here is a patch, which is complete... needs code beautification/tests/docs but it has all functionality.&lt;/p&gt;

&lt;p&gt;we should also add a table for n=2, maybe n=3 also, but these can be separate issues.&lt;/p&gt;</comment>
                    <comment id="12835912" author="rcmuir" created="Fri, 19 Feb 2010 19:34:26 +0000"  >&lt;p&gt;attached is a &apos;contrived fuzzy benchmark&apos; derived from my wildcard benchmark (randomly generated 7-digit terms)&lt;/p&gt;

&lt;p&gt;for the benchmark, i ran results for various combinations of minimum similarity, prefix length, and pq size for the test index of 10million terms.&lt;/p&gt;

&lt;p&gt;Avg MS old is the current flex branch. Avg MS new is with the patch.&lt;/p&gt;

&lt;p&gt;Notes:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;only the table for distance n=1 is implemented yet!&lt;/li&gt;
	&lt;li&gt;n=1 is fast.&lt;/li&gt;
	&lt;li&gt;Use of the PQ boost attribute speeds up fuzzy queries for higher n slightly, too.&lt;/li&gt;
	&lt;li&gt;adding a table for n=2 should be extremely helpful, and maybe even enough for the default PQ size of 1024 (BQ.maxClauseCount), to make all fuzzy queries reasonable.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;tt&gt;Minimum Sim = 0.73f (edit distance of 1)&lt;/tt&gt; &lt;/p&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Prefix Length&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;PQ Size&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Avg MS (old)&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Avg MS (new)&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;0&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1024&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;3286.0&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;10.6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;0&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;64&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;3320.4&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;7.2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1024&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;316.8&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;5.3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;64&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;314.3&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;5.3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1024&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;31.8&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;4.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;64&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;31.9&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;4.2&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;


&lt;p&gt;&lt;tt&gt;Minimum Sim = 0.58f (edit distance of 2)&lt;/tt&gt;&lt;/p&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Prefix Length&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;PQ Size&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Avg MS (old)&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Avg MS (new)&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;0&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1024&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;4223.3&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1341.6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;0&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;64&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;4199.7&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;501.9&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1024&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;430.1&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;304.1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;64&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;392.8&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;44.7&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1024&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;82.5&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;70.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;64&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;38.4&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;7.7&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;



&lt;p&gt;&lt;tt&gt;Minimum Sim = 0.43f (edit distance of 3)&lt;/tt&gt;&lt;/p&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Prefix Length&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;PQ Size&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Avg MS (old)&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Avg MS (new)&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;0&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1024&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;5299.9&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2617.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;0&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;64&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;5231.8&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;476.4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1024&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;522.9&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;318.9&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;64&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;480.9&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;73.9&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1024&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;89.0&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;83.9&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;64&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;46.3&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;8.6&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;



&lt;p&gt;&lt;tt&gt;Minimum Sim = 0.29f (edit distance of 4)&lt;/tt&gt;&lt;/p&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Prefix Length&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;PQ Size&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Avg MS (old)&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Avg MS (new)&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;0&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1024&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;6258.1&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;3114.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;0&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;64&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;6247.6&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;684.6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1024&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;609.9&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;380.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;64&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;567.1&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;69.3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1024&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;98.6&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;93.8&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;64&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;55.6&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;11.4&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
</comment>
                    <comment id="12835949" author="rcmuir" created="Fri, 19 Feb 2010 20:57:41 +0000"  >&lt;p&gt;edit the description, to hopefully be simpler.&lt;/p&gt;</comment>
                    <comment id="12836367" author="mikemccand" created="Sun, 21 Feb 2010 13:15:14 +0000"  >&lt;p&gt;Attached gen.py, to generate the java code for the parametric DFA.&lt;/p&gt;

&lt;p&gt;I also attached the resulting code fragment for the N=2 case (6544 lines, 212.4 KB!).&lt;/p&gt;

&lt;p&gt;This is only for the transition function, and it&apos;s not complete &amp;#8211; we need to fixup how state/offset are encoded into a single outer &quot;int state&quot;.  But I think it&apos;s close...&lt;/p&gt;</comment>
                    <comment id="12836372" author="rcmuir" created="Sun, 21 Feb 2010 13:59:04 +0000"  >&lt;p&gt;Mike, this is awesome! &lt;/p&gt;

&lt;p&gt;We can use the junit test case to test N=1 once we get to a nice place with this.&lt;br/&gt;
The way it works is, it builds an NFA for N=1, and compares it with the results of this with Automaton.equals, which ensures they accept the same language.&lt;br/&gt;
The test already checks this for all possible characteristic vectors, so if you believe the paper, and the tests pass, then its correct for all strings.&lt;/p&gt;

&lt;p&gt;Testing the correctness of N=2 is harder, we can use the same principles I think, but no automaton.equals as I don&apos;t know how to generate an NFA for N=2, even slowly.&lt;br/&gt;
instead I think we will have to verify against the actual levenshtein distance formula, but i think that verifying for all permutations of an alphabet of size 2n+1, for a string of length at least 2n+1 should be sufficient.&lt;/p&gt;

&lt;p&gt;(i plan to also randomly brute-force test the new query against the old fuzzy query at some point, in any case)&lt;/p&gt;

&lt;p&gt;in my opinion we should take the lessons learned from N=2 and if successful, regenerate N=1 too, as the way I &quot;keyed it in&quot; is likely not the best or most compact.&lt;/p&gt;</comment>
                    <comment id="12836381" author="mikemccand" created="Sun, 21 Feb 2010 15:10:01 +0000"  >&lt;p&gt;New gen.py and Lev2 attached &amp;#8211; adds the size() and getPosition() methods.&lt;/p&gt;

&lt;p&gt;The resulting java code at least compiles, but I have no idea if it&apos;s working!&lt;/p&gt;</comment>
                    <comment id="12836414" author="mikemccand" created="Sun, 21 Feb 2010 18:08:40 +0000"  >&lt;p&gt;New patch attached, implemented isAccept.  It compiles but I&apos;m not sure it&apos;s right!&lt;/p&gt;</comment>
                    <comment id="12836418" author="mikemccand" created="Sun, 21 Feb 2010 18:27:26 +0000"  >&lt;p&gt;New gen.py, just wires the init state to state 0.&lt;/p&gt;</comment>
                    <comment id="12836481" author="rcmuir" created="Mon, 22 Feb 2010 01:23:18 +0000"  >&lt;p&gt;Mike, i made some modifications (mostly places where i misled you), and i have your generator producing correct DFA for n=1, (the tests pass) which is a great sign. &lt;/p&gt;

&lt;p&gt;will try to upload a new python script tonight and explain what changed.&lt;/p&gt;</comment>
                    <comment id="12836491" author="rcmuir" created="Mon, 22 Feb 2010 02:31:06 +0000"  >&lt;p&gt;based on the work mike provided here (slightly modified), N=2 appears to work correctly (all correct results so far). I will figure out a way to exhaustively test this beast &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;For now, here are the updated benchmarks, again same 10M terms, on my regular old HDD, you can scroll up to see how big of a difference N=2 makes, compared to only having N=1 available!&lt;/p&gt;

&lt;p&gt;I think with N=2 implemented, its also clear, that PQ size can actually be more important than using a prefix. For instance, I&apos;m very happy with the performance here with a smaller PQ size of 64.&lt;/p&gt;

&lt;p&gt;&lt;tt&gt;Minimum Sim = 0.73f (edit distance of 1)&lt;/tt&gt; &lt;/p&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Prefix Length&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;PQ Size&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Avg MS (old)&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Avg MS (new)&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;0&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1024&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;3286.0&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;7.8&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;0&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;64&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;3320.4&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;7.6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1024&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;316.8&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;5.6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;64&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;314.3&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;5.6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1024&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;31.8&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;3.8&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;64&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;31.9&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;3.7&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;


&lt;p&gt;&lt;tt&gt;Minimum Sim = 0.58f (edit distance of 2)&lt;/tt&gt;&lt;/p&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Prefix Length&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;PQ Size&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Avg MS (old)&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Avg MS (new)&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;0&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1024&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;4223.3&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;87.7&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;0&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;64&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;4199.7&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;12.6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1024&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;430.1&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;56.4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;64&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;392.8&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;9.3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1024&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;82.5&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;45.5&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;64&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;38.4&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;6.2&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;



&lt;p&gt;&lt;tt&gt;Minimum Sim = 0.43f (edit distance of 3)&lt;/tt&gt;&lt;/p&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Prefix Length&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;PQ Size&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Avg MS (old)&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Avg MS (new)&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;0&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1024&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;5299.9&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;424.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;0&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;64&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;5231.8&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;54.1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1024&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;522.9&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;103.6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;64&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;480.9&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;14.5&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1024&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;89.0&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;67.9&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;64&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;46.3&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;6.8&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;



&lt;p&gt;&lt;tt&gt;Minimum Sim = 0.29f (edit distance of 4)&lt;/tt&gt;&lt;/p&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Prefix Length&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;PQ Size&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Avg MS (old)&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Avg MS (new)&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;0&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1024&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;6258.1&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;363.7&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;0&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;64&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;6247.6&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;75.6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1024&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;609.9&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;108.3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;64&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;567.1&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;13.3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1024&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;98.6&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;66.6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;64&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;55.6&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;6.8&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
</comment>
                    <comment id="12836562" author="mikemccand" created="Mon, 22 Feb 2010 09:45:39 +0000"  >&lt;p&gt;Great progress!  These speedups are incredible, Robert.&lt;/p&gt;

&lt;p&gt;We should also change the default fuzzy rewrite with this (eg reduce default max # terms from 1024)?&lt;/p&gt;</comment>
                    <comment id="12836654" author="rcmuir" created="Mon, 22 Feb 2010 15:01:02 +0000"  >&lt;p&gt;Mike, attached is a first crack at modified version of gen.py&lt;br/&gt;
This is producing Lev(1,2) code that passes all tests (I also added random tests to junit for n=1 so far)&lt;/p&gt;

&lt;p&gt;Changes:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;@override, license note, stuff like that&lt;/li&gt;
	&lt;li&gt;getPosition() should return the index in the word that the state is associated with, this is the offset&lt;/li&gt;
	&lt;li&gt;there are w + 1, not w absolute states per parametric state.&lt;/li&gt;
	&lt;li&gt;in the ctor, the number of states to initialize per parametric state depends on the number of positions this parametric state represents (the length of the python list). So the inner loop here is modified to:
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt;(&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; j=0;j&amp;lt;((w+1)*stateSizes[i]);j++)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;if you get a chance, can you review? I don&apos;t think its the final version but we can iterate slowly.&lt;/p&gt;</comment>
                    <comment id="12836655" author="rcmuir" created="Mon, 22 Feb 2010 15:09:19 +0000"  >&lt;p&gt;attached is a monster patch for flex, with autogenerated Lev1 and Lev2 from the gen.py i just uploaded.&lt;/p&gt;</comment>
                    <comment id="12836691" author="rcmuir" created="Mon, 22 Feb 2010 16:08:18 +0000"  >&lt;blockquote&gt;&lt;p&gt;We should also change the default fuzzy rewrite with this (eg reduce default max # terms from 1024)?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I thought that once we hammer out this issue, with no backwards breaks, we could then separately discuss ways to improve defaults.&lt;br/&gt;
One easy way would be to use Version in QueryParser to produce better defaults for the 3.1 release, which wouldnt break any backwards compatibility.&lt;/p&gt;</comment>
                    <comment id="12836711" author="mikemccand" created="Mon, 22 Feb 2010 16:45:41 +0000"  >&lt;p&gt;Collapse switch cases that have the same action (just fall through).  Also gen directly to the right location, and handle when Moman package is &quot;installed&quot;.  This brings byte code size from lev2 down from  ~43 KB to ~25 KB.&lt;/p&gt;</comment>
                    <comment id="12836759" author="rcmuir" created="Mon, 22 Feb 2010 18:08:08 +0000"  >&lt;p&gt;attached is an updated (smaller) patch. the generated files are re-generated with Mike&apos;s fallthru optimization.&lt;/p&gt;</comment>
                    <comment id="12839264" author="mikemccand" created="Sat, 27 Feb 2010 15:52:14 +0000"  >&lt;p&gt;New rev of gen.py, that uses packed arrays for the states/offsets.&lt;/p&gt;

&lt;p&gt;It&apos;s much more compact &amp;#8211; Lev1 is now 5KB, Lev2 is 11KB, Lev3 is 160KB.  And Lev3 compiles!  (Robert now you need a test case for Lev3 &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/wink.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; ).  The class files are OK too: Lev1 3.9KB, Lev2 is 7.3KB, Lev3 is 102KB.&lt;/p&gt;</comment>
                    <comment id="12839272" author="mikemccand" created="Sat, 27 Feb 2010 17:46:17 +0000"  >&lt;p&gt;New patch... just fixes a few small things Uwe noticed (moved unpack method &amp;amp; MASKS up to super class; use newlines to make the massive tables a bit more friendly to look at).&lt;/p&gt;</comment>
                    <comment id="12839307" author="rcmuir" created="Sat, 27 Feb 2010 20:50:32 +0000"  >&lt;p&gt;Mike, this is awesome. I ran benchmarks: we are just as fast as before (with only Lev1 and Lev2 enabled), but with smaller generated code.&lt;br/&gt;
When i turn on Lev3, it speeds up the worst-case ones (no prefix, pq=1024, fuzzy of n=3, n=4), but slows down some of the &quot;better-case&quot; n=3/n=4 cases where there is a prefix or PQ.&lt;/p&gt;

&lt;p&gt;I think this is because the benchmark is contrived, but realistically n=3 (with seeking!) should be a win for users. A less-contrived benchmark (a &apos;typical&apos; massive term dictionary) would help for tuning.&lt;/p&gt;

&lt;p&gt;separately, I think we can add heuristics: e.g. for n &amp;gt; 3 WITH a prefix, use the DFA in &quot;linear mode&quot; until you drop to n=2, as you already have a nice prefix anyway, stuff like that. But if the user doesn&apos;t supply a prefix, i think seeking is always a win.&lt;/p&gt;

&lt;p&gt;Here are the results anyway: I ran it many times and its consistent (obviously differences of just a few MS are not significant). I bolded the ones i think illustrate the differences I am talking about.&lt;/p&gt;

&lt;p&gt;Its cool to be at the point where we are actually able to measure these kinds of tradeoffs!&lt;/p&gt;

&lt;p&gt;&lt;tt&gt;Minimum Sim = 0.73f (edit distance of 1)&lt;/tt&gt; &lt;/p&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Prefix Length&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;PQ Size&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Avg MS (flex trunk)&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Avg MS (1,2)&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Avg MS (1,2,3)&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;0&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1024&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;3286.0&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;7.8&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;7.6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;0&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;64&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;3320.4&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;7.6&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;8.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1024&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;316.8&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;5.6&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;5.3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;64&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;314.3&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;5.6&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;5.2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1024&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;31.8&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;3.8&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;4.2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;64&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;31.9&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;3.7&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;4.5&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;


&lt;p&gt;&lt;tt&gt;Minimum Sim = 0.58f (edit distance of 2)&lt;/tt&gt;&lt;/p&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Prefix Length&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;PQ Size&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Avg MS (flex trunk)&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Avg MS (1,2)&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Avg MS (1,2,3)&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;0&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1024&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;4223.3&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;87.7&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;91.2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;0&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;64&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;4199.7&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;12.6&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;13.2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1024&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;430.1&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;56.4&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;62.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;64&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;392.8&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;9.3&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;8.5&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1024&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;82.5&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;45.5&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;48.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;64&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;38.4&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;6.2&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;6.3&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;



&lt;p&gt;&lt;tt&gt;Minimum Sim = 0.43f (edit distance of 3)&lt;/tt&gt;&lt;/p&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Prefix Length&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;PQ Size&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Avg MS (flex trunk)&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Avg MS (1,2)&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Avg MS (1,2,3)&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;0&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1024&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;5299.9&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;424.0&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;&lt;b&gt;199.8&lt;/b&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;0&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;64&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;5231.8&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;54.1&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;&lt;b&gt;93.2&lt;/b&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1024&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;522.9&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;103.6&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;107.9&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;64&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;480.9&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;14.5&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;&lt;b&gt;49.3&lt;/b&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1024&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;89.0&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;67.9&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;70.8&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;64&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;46.3&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;6.8&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;&lt;b&gt;19.7&lt;/b&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;



&lt;p&gt;&lt;tt&gt;Minimum Sim = 0.29f (edit distance of 4)&lt;/tt&gt;&lt;/p&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Prefix Length&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;PQ Size&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Avg MS (flex trunk)&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Avg MS (1,2)&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Avg MS (1,2,3)&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;0&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1024&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;6258.1&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;363.7&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;&lt;b&gt;206.5&lt;/b&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;0&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;64&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;6247.6&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;75.6&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;78.8&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1024&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;609.9&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;108.3&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;110.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;64&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;567.1&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;13.3&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;&lt;b&gt;45.5&lt;/b&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1024&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;98.6&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;66.6&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;73.8&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;64&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;55.6&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;6.8&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;&lt;b&gt;22.3&lt;/b&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
</comment>
                    <comment id="12839428" author="rcmuir" created="Sun, 28 Feb 2010 14:53:17 +0000"  >&lt;p&gt;attached is an update patch, i left the Lev3Parametric out for simplicity.&lt;/p&gt;

&lt;p&gt;i added junit testing to exhaustively test all possible characteristic vectors.&lt;br/&gt;
our n=1 is fine, there is a bug in our n=2 generation.&lt;/p&gt;</comment>
                    <comment id="12839743" author="mikemccand" created="Mon, 1 Mar 2010 16:35:51 +0000"  >&lt;p&gt;New patch w/ fixes to not overallocate states.&lt;/p&gt;</comment>
                    <comment id="12839783" author="rcmuir" created="Mon, 1 Mar 2010 18:00:37 +0000"  >&lt;p&gt;this is mike&apos;s patch, except with the additional tests (that fail for lev2/lev3 due to the off-by-one bug).&lt;/p&gt;

&lt;p&gt;edit: found the bug, it is in moman, the author is taking a look. the problem was i ran tests against moman with oflazer, (a slower alternative algorithm), and the bug in moman doesn&apos;t happen there.&lt;/p&gt;</comment>
                    <comment id="12840344" author="rcmuir" created="Tue, 2 Mar 2010 21:58:16 +0000"  >&lt;p&gt;the author responded with a bugfix to moman.&lt;br/&gt;
additionally, he made his code public at &lt;a href=&quot;http://bitbucket.org/jpbarrette/moman/overview/&quot; class=&quot;external-link&quot;&gt;http://bitbucket.org/jpbarrette/moman/overview/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;attached is a patch, regen&apos;ed code from his bugfix.&lt;br/&gt;
all Lev1, Lev2, and Lev3 tests pass.&lt;/p&gt;</comment>
                    <comment id="12840345" author="rcmuir" created="Tue, 2 Mar 2010 21:59:47 +0000"  >&lt;p&gt;attached is the patch the author provided to the moman source code that fixes the bug.&lt;/p&gt;</comment>
                    <comment id="12840557" author="mikemccand" created="Wed, 3 Mar 2010 09:55:09 +0000"  >&lt;p&gt;This is awesome progress Robert!!&lt;/p&gt;

&lt;p&gt;I also applied the patch and see the tests now passing.&lt;/p&gt;

&lt;p&gt;I don&apos;t see the &lt;span class=&quot;error&quot;&gt;&amp;#91;MIT&amp;#93;&lt;/span&gt; license on that public URL?&lt;/p&gt;</comment>
                    <comment id="12840585" author="rcmuir" created="Wed, 3 Mar 2010 11:22:41 +0000"  >&lt;blockquote&gt;&lt;p&gt;I don&apos;t see the &lt;span class=&quot;error&quot;&gt;&amp;#91;MIT&amp;#93;&lt;/span&gt; license on that public URL?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I do not even know Mercurial enough to know if such a thing is possible?&lt;/p&gt;</comment>
                    <comment id="12840658" author="mikemccand" created="Wed, 3 Mar 2010 13:42:48 +0000"  >&lt;p&gt;Mercurial (hg) is just the source control system they use.  It competes with git, but, it&apos;s (hg&apos;s) written in Python &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/wink.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;So eg maybe they could check in a LICENSE file somewhere?&lt;/p&gt;</comment>
                    <comment id="12840718" author="rcmuir" created="Wed, 3 Mar 2010 16:29:09 +0000"  >&lt;blockquote&gt;&lt;p&gt;So eg maybe they could check in a LICENSE file somewhere?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;fyi, the MIT LICENSE.TXT was just committed to the src repo: &lt;br/&gt;
&lt;a href=&quot;http://bitbucket.org/jpbarrette/moman/changeset/ef6f18799428/raw/moman-ef6f18799428.diff&quot; class=&quot;external-link&quot;&gt;http://bitbucket.org/jpbarrette/moman/changeset/ef6f18799428/raw/moman-ef6f18799428.diff&lt;/a&gt;&lt;/p&gt;
</comment>
                    <comment id="12840728" author="rcmuir" created="Wed, 3 Mar 2010 16:57:19 +0000"  >&lt;p&gt;first shot at a committable patch. please review.&lt;/p&gt;</comment>
                    <comment id="12840792" author="mikemccand" created="Wed, 3 Mar 2010 18:40:36 +0000"  >&lt;p&gt;Added usage line, fixed missing space in license header, added URLs referencing moman/finenight package, always append L in the long[] static values.&lt;/p&gt;</comment>
                    <comment id="12840836" author="rcmuir" created="Wed, 3 Mar 2010 19:46:59 +0000"  >&lt;p&gt;Thanks Mike, I&apos;ll regen tonight and pass thru the code another time looking for more cleanups, and upload a new patch.&lt;/p&gt;</comment>
                    <comment id="12842659" author="rcmuir" created="Mon, 8 Mar 2010 13:56:02 +0000"  >&lt;p&gt;updated patch, regenerated with mike&apos;s update to the python script.&lt;br/&gt;
I also added an optional ant task createLevAutomata to build.xml.&lt;br/&gt;
This does a mercurial clone/pull of rev 115 and regenerates the tables.&lt;/p&gt;</comment>
                    <comment id="12843236" author="rcmuir" created="Tue, 9 Mar 2010 18:45:17 +0000"  >&lt;p&gt;Updated patch:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;optimizations for small indexes&lt;/li&gt;
	&lt;li&gt;improved tests&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;the improved test reads in an input file of queries. &lt;br/&gt;
it more-or-less brute forces all fuzzy parameters for binary strings of 2^n&lt;/p&gt;

&lt;p&gt;the test data included was generated from trunk (its only for 2^3).&lt;br/&gt;
I ran much larger n against both trunk and this patch to ensure correctness. &lt;br/&gt;
the code to generate the file is included in the test, in case we need to change it.&lt;/p&gt;</comment>
                    <comment id="12843327" author="rcmuir" created="Tue, 9 Mar 2010 22:17:41 +0000"  >&lt;p&gt;I think this patch is ready.&lt;/p&gt;

&lt;p&gt;Will commit in a few days (to the heavy committing branch) if no one objects.&lt;/p&gt;</comment>
                    <comment id="12843331" author="markrmiller@gmail.com" created="Tue, 9 Mar 2010 22:23:50 +0000"  >&lt;p&gt;Sweet!&lt;/p&gt;</comment>
                    <comment id="12843337" author="thetaphi" created="Tue, 9 Mar 2010 22:44:36 +0000"  >&lt;p&gt;Phantastic!&lt;/p&gt;</comment>
                    <comment id="12843354" author="mikemccand" created="Tue, 9 Mar 2010 23:16:18 +0000"  >&lt;p&gt;YEAH!&lt;/p&gt;</comment>
                    <comment id="12844041" author="rcmuir" created="Thu, 11 Mar 2010 12:18:42 +0000"  >&lt;p&gt;Committed revision 921820.&lt;/p&gt;</comment>
                    <comment id="12844064" author="mikemccand" created="Thu, 11 Mar 2010 13:16:23 +0000"  >&lt;p&gt;Yay &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                </comments>
                <issuelinks>
                        <issuelinktype id="10030">
                <name>Reference</name>
                                                <inwardlinks description="is related to">
                            <issuelink>
            <issuekey id="12411782">LUCENE-1513</issuekey>
        </issuelink>
                    </inwardlinks>
                            </issuelinktype>
                        <issuelinktype id="10001">
                <name>dependent</name>
                                <outwardlinks description="depends upon">
                            <issuelink>
            <issuekey id="12442589">LUCENE-2123</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12442932">LUCENE-2140</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12456030">LUCENE-2261</issuekey>
        </issuelink>
                    </outwardlinks>
                                            </issuelinktype>
                    </issuelinks>
                <attachments>
                    <attachment id="12436358" name="ContrivedFuzzyBenchmark.java" size="5112" author="rcmuir" created="Fri, 19 Feb 2010 19:34:26 +0000" />
                    <attachment id="12437765" name="createLevAutomata.py" size="14187" author="mikemccand" created="Wed, 3 Mar 2010 18:40:34 +0000" />
                    <attachment id="12436593" name="gen.py" size="6805" author="mikemccand" created="Mon, 22 Feb 2010 16:45:41 +0000" />
                    <attachment id="12436577" name="gen.py" size="5576" author="rcmuir" created="Mon, 22 Feb 2010 15:01:02 +0000" />
                    <attachment id="12436511" name="gen.py" size="5177" author="mikemccand" created="Sun, 21 Feb 2010 18:27:26 +0000" />
                    <attachment id="12436509" name="gen.py" size="5128" author="mikemccand" created="Sun, 21 Feb 2010 18:08:39 +0000" />
                    <attachment id="12436496" name="gen.py" size="4610" author="mikemccand" created="Sun, 21 Feb 2010 15:10:01 +0000" />
                    <attachment id="12436489" name="gen.py" size="3412" author="mikemccand" created="Sun, 21 Feb 2010 13:15:14 +0000" />
                    <attachment id="12436512" name="Lev2ParametricDescription.java" size="218632" author="mikemccand" created="Sun, 21 Feb 2010 18:27:26 +0000" />
                    <attachment id="12436510" name="Lev2ParametricDescription.java" size="218632" author="mikemccand" created="Sun, 21 Feb 2010 18:08:40 +0000" />
                    <attachment id="12436497" name="Lev2ParametricDescription.java" size="218231" author="mikemccand" created="Sun, 21 Feb 2010 15:10:01 +0000" />
                    <attachment id="12436490" name="Lev2ParametricDescription.java" size="217456" author="mikemccand" created="Sun, 21 Feb 2010 13:15:14 +0000" />
                    <attachment id="12436080" name="LUCENE-2089_concat.patch" size="1106" author="rcmuir" created="Wed, 17 Feb 2010 03:36:14 +0000" />
                    <attachment id="12438307" name="LUCENE-2089.patch" size="110617" author="rcmuir" created="Tue, 9 Mar 2010 18:45:17 +0000" />
                    <attachment id="12438181" name="LUCENE-2089.patch" size="67327" author="rcmuir" created="Mon, 8 Mar 2010 13:56:02 +0000" />
                    <attachment id="12437750" name="LUCENE-2089.patch" size="64365" author="rcmuir" created="Wed, 3 Mar 2010 16:57:19 +0000" />
                    <attachment id="12437650" name="LUCENE-2089.patch" size="229685" author="rcmuir" created="Tue, 2 Mar 2010 21:58:16 +0000" />
                    <attachment id="12437497" name="LUCENE-2089.patch" size="229563" author="rcmuir" created="Mon, 1 Mar 2010 18:00:33 +0000" />
                    <attachment id="12437491" name="LUCENE-2089.patch" size="226802" author="mikemccand" created="Mon, 1 Mar 2010 16:35:51 +0000" />
                    <attachment id="12437382" name="LUCENE-2089.patch" size="59202" author="rcmuir" created="Sun, 28 Feb 2010 14:53:17 +0000" />
                    <attachment id="12437345" name="LUCENE-2089.patch" size="230097" author="mikemccand" created="Sat, 27 Feb 2010 17:46:17 +0000" />
                    <attachment id="12437344" name="LUCENE-2089.patch" size="220061" author="mikemccand" created="Sat, 27 Feb 2010 15:52:14 +0000" />
                    <attachment id="12436599" name="LUCENE-2089.patch" size="183824" author="rcmuir" created="Mon, 22 Feb 2010 18:08:08 +0000" />
                    <attachment id="12436578" name="LUCENE-2089.patch" size="263380" author="rcmuir" created="Mon, 22 Feb 2010 15:09:19 +0000" />
                    <attachment id="12436331" name="LUCENE-2089.patch" size="30801" author="rcmuir" created="Fri, 19 Feb 2010 15:46:21 +0000" />
                    <attachment id="12436079" name="LUCENE-2089.patch" size="30021" author="rcmuir" created="Wed, 17 Feb 2010 03:32:56 +0000" />
                    <attachment id="12435959" name="LUCENE-2089.patch" size="29502" author="rcmuir" created="Tue, 16 Feb 2010 09:06:44 +0000" />
                    <attachment id="12435949" name="LUCENE-2089.patch" size="28295" author="rcmuir" created="Tue, 16 Feb 2010 07:33:38 +0000" />
                    <attachment id="12427753" name="LUCENE-2089.patch" size="21308" author="rcmuir" created="Fri, 11 Dec 2009 19:09:51 +0000" />
                    <attachment id="12425774" name="Moman-0.2.1.tar.gz" size="18262" author="markrmiller@gmail.com" created="Sun, 22 Nov 2009 21:59:14 +0000" />
                    <attachment id="12437651" name="moman-57f5dc9dd0e7.diff" size="3192" author="rcmuir" created="Tue, 2 Mar 2010 21:59:47 +0000" />
                    <attachment id="12427417" name="TestFuzzy.java" size="2930" author="rcmuir" created="Wed, 9 Dec 2009 02:42:15 +0000" />
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>32.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Sun, 22 Nov 2009 15:09:07 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>11690</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    <customfieldvalue key="10120"><![CDATA[Patch Available]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>25636</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                    <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                <customfieldname>Time in Status</customfieldname>
                <customfieldvalues>
                    
                </customfieldvalues>
            </customfield>
                            </customfields>
    </item>
</channel>
</rss>