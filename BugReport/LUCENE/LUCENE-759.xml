<!-- 
RSS generated by JIRA (5.2.8#851-sha1:3262fdc28b4bc8b23784e13eadc26a22399f5d88) at Tue Jul 16 13:07:33 UTC 2013

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/LUCENE-759/LUCENE-759.xml?field=key&field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>5.2.8</version>
        <build-number>851</build-number>
        <build-date>26-02-2013</build-date>
    </build-info>

<item>
            <title>[LUCENE-759] Add n-gram tokenizers to contrib/analyzers</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-759</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;It would be nice to have some n-gram-capable tokenizers in contrib/analyzers.  Patch coming shortly.&lt;/p&gt;</description>
                <environment></environment>
            <key id="12359323">LUCENE-759</key>
            <summary>Add n-gram tokenizers to contrib/analyzers</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png">Resolved</status>
                    <resolution id="1">Fixed</resolution>
                                <assignee username="otis">Otis Gospodnetic</assignee>
                                <reporter username="otis">Otis Gospodnetic</reporter>
                        <labels>
                    </labels>
                <created>Fri, 22 Dec 2006 23:22:58 +0000</created>
                <updated>Mon, 14 Jan 2008 18:39:00 +0000</updated>
                    <resolved>Fri, 13 Jul 2007 14:54:03 +0100</resolved>
                                                            <component>modules/analysis</component>
                        <due></due>
                    <votes>0</votes>
                        <watches>0</watches>
                                                    <comments>
                    <comment id="12460613" author="otis" created="Fri, 22 Dec 2006 23:43:04 +0000"  >&lt;p&gt;Included:&lt;br/&gt;
  NGramTokenizer&lt;br/&gt;
  NGramTokenizerTest&lt;br/&gt;
  EdgeNGramTokenizer&lt;br/&gt;
  EdgeNGramTokenizerTest&lt;/p&gt;</comment>
                    <comment id="12460618" author="otis" created="Fri, 22 Dec 2006 23:44:20 +0000"  >&lt;p&gt;Unit tests pass, committed.&lt;/p&gt;</comment>
                    <comment id="12473810" author="otis" created="Fri, 16 Feb 2007 20:01:59 +0000"  >&lt;p&gt;Reopening, because I&apos;m bringing in Adam Hiatt&apos;s modifications that he uploaded in a patch for &lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-81&quot; title=&quot;Add Query Spellchecker functionality&quot;&gt;&lt;del&gt;SOLR-81&lt;/del&gt;&lt;/a&gt;.  Adam&apos;s changes allow this tokenizer to create n-grams whose sizes are specified as a min-max range.&lt;/p&gt;

&lt;p&gt;This patch fixes a bug in Adam&apos;s code, but has another bug that I don&apos;t know how to fix now.&lt;br/&gt;
Adam&apos;s bug:&lt;br/&gt;
  input: abcde&lt;br/&gt;
  minGram: 1&lt;br/&gt;
  maxGram: 3&lt;br/&gt;
  output: a ab abc  &amp;#8211; and this is where tokenizing stopped, which was wrong, it should have continued: b bc bcd c cd cde d de e&lt;/p&gt;

&lt;p&gt;Otis&apos; bug:&lt;br/&gt;
  input: abcde&lt;br/&gt;
  minGeam: 1&lt;br/&gt;
  maxGram: 3&lt;br/&gt;
  output: e de cde d cd bcd c bc abc b ab &amp;#8211; and this is where tokenizing stops, which is wrong, it should generate one more n-gram: a&lt;/p&gt;

&lt;p&gt;This bug won&apos;t hurt &lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-81&quot; title=&quot;Add Query Spellchecker functionality&quot;&gt;&lt;del&gt;SOLR-81&lt;/del&gt;&lt;/a&gt;, but it should be fixed.&lt;/p&gt;</comment>
                    <comment id="12473812" author="otis" created="Fri, 16 Feb 2007 20:04:20 +0000"  >&lt;p&gt;The modified tokenizer and the extended unit test.&lt;/p&gt;</comment>
                    <comment id="12473851" author="adamh" created="Fri, 16 Feb 2007 23:10:04 +0000"  >&lt;p&gt;Otis: this really isn&apos;t a bug. The min/max gram code I added only applied to the EdgeNGramTokenizer.  I only want to generate &lt;em&gt;edge&lt;/em&gt; n-grams between the range of sizes provided.&lt;/p&gt;

&lt;p&gt;For example, with the EdgeNGramTokenizer&lt;br/&gt;
 input: abcde&lt;br/&gt;
  minGram: 1&lt;br/&gt;
  maxGram: 3 &lt;/p&gt;

&lt;p&gt;&apos;a ab abc&apos; is in fact what I intended to produce.&lt;/p&gt;

&lt;p&gt;I think it makes more sense for the functionality to which you referred to be located in NGramTokenizer.&lt;/p&gt;</comment>
                    <comment id="12473891" author="otis" created="Sat, 17 Feb 2007 08:10:45 +0000"  >&lt;p&gt;Damn, I think you are right! &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;  Once again, I&apos;m making late night mistakes.  When will I learn!?&lt;br/&gt;
But I could take my code to NGramTokenizer then, at least.&lt;br/&gt;
My bug remains, though..... got an idea for a fix?&lt;/p&gt;
</comment>
                    <comment id="12474712" author="otis" created="Wed, 21 Feb 2007 14:23:07 +0000"  >&lt;p&gt;Here is the proper version.  This one is essentially the Lucene-n-gram-analyzer-specific Adam&apos;s patch from &lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-81&quot; title=&quot;Add Query Spellchecker functionality&quot;&gt;&lt;del&gt;SOLR-81&lt;/del&gt;&lt;/a&gt; + some passing unit tests I wrote to exercise the new n-gram range functionality.&lt;/p&gt;

&lt;p&gt;I&apos;ll commit this by the end of the week unless Adam spots a bug.&lt;/p&gt;</comment>
                    <comment id="12476951" author="otis" created="Thu, 1 Mar 2007 14:23:23 +0000"  >&lt;p&gt;In SVN.&lt;/p&gt;</comment>
                    <comment id="12477115" author="doronc" created="Thu, 1 Mar 2007 22:49:44 +0000"  >&lt;p&gt;I have two comments/questions on the n-gram tokenizers:&lt;/p&gt;

&lt;p&gt;(1) Seems that only the first 1024 characters of the input are handled, and the rest is ignored (and I think as result the input stream would remain dangling open). &lt;/p&gt;

&lt;p&gt;If you add this test case:&lt;/p&gt;

&lt;p&gt;    /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Test that no ngrams are lost, even for really long inputs&lt;/li&gt;
	&lt;li&gt;@throws EXception&lt;br/&gt;
     */&lt;br/&gt;
    public void testLongerInput() throws Exception {&lt;br/&gt;
      int expectedNumTokens = 1024;&lt;br/&gt;
      int ngramLength = 2;&lt;br/&gt;
      // prepare long string&lt;br/&gt;
      StringBuffer sb = new StringBuffer();&lt;br/&gt;
      while (sb.length()&amp;lt;expectedNumTokens+ngramLength-1) &lt;br/&gt;
        sb.append(&apos;a&apos;);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;      StringReader longStringReader = new StringReader (sb.toString());&lt;br/&gt;
      NGramTokenizer tokenizer = new NGramTokenizer(longStringReader, ngramLength, ngramLength);&lt;br/&gt;
      int numTokens = 0;&lt;br/&gt;
      Token token;&lt;br/&gt;
      while ((token = tokenizer.next())!=null) &lt;/p&gt;
{
        numTokens++;
        assertEquals(&quot;aa&quot;,token.termText());
      }
&lt;p&gt;      assertEquals(&quot;wrong number of tokens&quot;,expectedNumTokens,numTokens);&lt;br/&gt;
    }&lt;/p&gt;

&lt;p&gt;With expectedNumTokens = 1023 it would pass, but any larger number would fail. &lt;/p&gt;

&lt;p&gt;(2) It seems safer to read the characters like this&lt;br/&gt;
            int n = input.read(chars);&lt;br/&gt;
            inStr = new String(chars, 0, n);&lt;br/&gt;
(This way not counting on String.trim(), which does work, but worries me).&lt;/p&gt;
</comment>
                    <comment id="12477393" author="otis" created="Fri, 2 Mar 2007 18:13:49 +0000"  >&lt;p&gt;More goodies coming.&lt;/p&gt;</comment>
                    <comment id="12477394" author="otis" created="Fri, 2 Mar 2007 18:17:16 +0000"  >&lt;p&gt;N-gram-producting TokenFilters for Karl&apos;s mom.&lt;/p&gt;</comment>
                    <comment id="12477593" author="doronc" created="Sat, 3 Mar 2007 04:01:42 +0000"  >&lt;p&gt;Hi Otis, &lt;/p&gt;

&lt;p&gt;&amp;gt;  (and I think as result the input stream would remain dangling open)&lt;/p&gt;

&lt;p&gt;I take this part back - closing tokenStream would close the reader, and at least for the case that I thought of, invertDocument, the tokenStream is properly closed. &lt;/p&gt;

&lt;p&gt;Can you comment on the input length: is it correct to handle only the first 1024 characters?&lt;/p&gt;

&lt;p&gt;Thanks,&lt;br/&gt;
Doron&lt;/p&gt;</comment>
                    <comment id="12477649" author="otis" created="Sat, 3 Mar 2007 16:44:38 +0000"  >&lt;p&gt;Ah, didn&apos;t see your comments here earlier, Doron.  Yes, I think you are correct about the 1024 limit  - when I wrote that Tokenizer I was thinking TokenFilter, and thus I was thinking that that input Reader represents a Token, which was wrong.  So, I thought, &quot;oh, 1024 chars/token, that will be enough&quot;.  I ended up needing TokenFilters for &lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-81&quot; title=&quot;Add Query Spellchecker functionality&quot;&gt;&lt;del&gt;SOLR-81&lt;/del&gt;&lt;/a&gt;, so that&apos;s what I checked in.  Those operate on tokens and don&apos;t have the 1024 limitation.&lt;/p&gt;

&lt;p&gt;Anyhow, feel free to slap your test + the fix in and thanks for checking!&lt;/p&gt;</comment>
                    <comment id="12479221" author="patrek" created="Thu, 8 Mar 2007 04:35:08 +0000"  >&lt;p&gt;Is it just me or are the NGramTokenFilter and EdgeNGramTokenFilter class not committed to SVN and not in the patch either?&lt;/p&gt;

&lt;p&gt;NGramTokenFilterTest and EdgeNGramTokenFilterTest are referring to them, but I can not seem to find them.&lt;/p&gt;

&lt;p&gt;Thanks and keep the good work.&lt;/p&gt;

&lt;p&gt;Patrick&lt;/p&gt;</comment>
                    <comment id="12479378" author="hossman" created="Thu, 8 Mar 2007 18:14:11 +0000"  >&lt;p&gt;Otis&apos;s most recent attachment contains only tests .. but previous attachemnts had implementations.&lt;/p&gt;

&lt;p&gt;all of which have been commited under contrib/analyzers&lt;/p&gt;

&lt;p&gt;(tip: if you click &quot;All&quot; at the top of the list of comments in Jira, you see every modification related to this issue, including subversion commits that Jira detects are related to the issue based on the commit message)&lt;/p&gt;</comment>
                    <comment id="12479441" author="hossman" created="Thu, 8 Mar 2007 21:43:37 +0000"  >&lt;p&gt;Ack! ... i&apos;m sorry i completely missread Patrick&apos;s question.&lt;/p&gt;

&lt;p&gt;ngram &lt;b&gt;Tokenizers&lt;/b&gt; have been commited &amp;#8211; but there are no ngram TokenFilters ... there are tests for TokenFilters Otis commited on March2, but those tests do&apos;t currently compile/run without the TokenFilter&apos;s themselves.&lt;/p&gt;

&lt;p&gt;Otis: do you have these TokenFilter&apos;s in your working directory that you perhaps forgot to svn add before committing?&lt;/p&gt;</comment>
                    <comment id="12480236" author="otis" created="Tue, 13 Mar 2007 00:31:10 +0000"  >&lt;p&gt;Oh, look at that!&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;otis@localhost contrib&amp;#93;&lt;/span&gt;$ svn st&lt;br/&gt;
A      analyzers/src/java/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilter.java&lt;br/&gt;
A      analyzers/src/java/org/apache/lucene/analysis/ngram/NGramTokenFilter.java&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;br/&gt;
It&apos;s in the repo now.  Sorry about that!&lt;/p&gt;</comment>
                    <comment id="12500811" author="lucenebugs@danielnaber.de" created="Fri, 1 Jun 2007 19:53:51 +0100"  >&lt;p&gt;Can this issue be closed or is there anything still open?&lt;/p&gt;</comment>
                    <comment id="12512486" author="otis" created="Fri, 13 Jul 2007 14:54:03 +0100"  >&lt;p&gt;This should have been marked Fixed a while back.&lt;/p&gt;</comment>
                </comments>
                <issuelinks>
                        <issuelinktype id="10030">
                <name>Reference</name>
                                                <inwardlinks description="is related to">
                            <issuelink>
            <issuekey id="12358370">SOLR-81</issuekey>
        </issuelink>
                    </inwardlinks>
                            </issuelinktype>
                    </issuelinks>
                <attachments>
                    <attachment id="12352469" name="LUCENE-759-filters.patch" size="19720" author="otis" created="Fri, 2 Mar 2007 18:17:16 +0000" />
                    <attachment id="12351682" name="LUCENE-759.patch" size="10762" author="otis" created="Wed, 21 Feb 2007 14:23:06 +0000" />
                    <attachment id="12351398" name="LUCENE-759.patch" size="11823" author="otis" created="Fri, 16 Feb 2007 20:04:20 +0000" />
                    <attachment id="12347781" name="LUCENE-759.patch" size="15802" author="otis" created="Fri, 22 Dec 2006 23:43:04 +0000" />
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>4.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Fri, 16 Feb 2007 23:10:04 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>12993</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    <customfieldvalue key="10120"><![CDATA[Patch Available]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>26971</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                    <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                <customfieldname>Time in Status</customfieldname>
                <customfieldvalues>
                    
                </customfieldvalues>
            </customfield>
                            </customfields>
    </item>
</channel>
</rss>