<!-- 
RSS generated by JIRA (5.2.8#851-sha1:3262fdc28b4bc8b23784e13eadc26a22399f5d88) at Tue Jul 16 13:34:40 UTC 2013

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/LUCENE-2167/LUCENE-2167.xml?field=key&field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>5.2.8</version>
        <build-number>851</build-number>
        <build-date>26-02-2013</build-date>
    </build-info>

<item>
            <title>[LUCENE-2167] Implement StandardTokenizer with the UAX#29 Standard</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-2167</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;It would be really nice for StandardTokenizer to adhere straight to the standard as much as we can with jflex. Then its name would actually make sense.&lt;/p&gt;

&lt;p&gt;Such a transition would involve renaming the old StandardTokenizer to EuropeanTokenizer, as its javadoc claims:&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;This should be a good tokenizer for most European-language documents&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The new StandardTokenizer could then say&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;This should be a good tokenizer for most languages.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;All the english/euro-centric stuff like the acronym/company/apostrophe stuff can stay with that EuropeanTokenizer, and it could be used by the european analyzers.&lt;/p&gt;</description>
                <environment></environment>
            <key id="12443564">LUCENE-2167</key>
            <summary>Implement StandardTokenizer with the UAX#29 Standard</summary>
                <type id="2" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/newfeature.png">New Feature</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png">Closed</status>
                    <resolution id="1">Fixed</resolution>
                                <assignee username="steve_rowe">Steve Rowe</assignee>
                                <reporter username="shyamal">Shyamal Prasad</reporter>
                        <labels>
                    </labels>
                <created>Wed, 16 Dec 2009 18:48:23 +0000</created>
                <updated>Mon, 16 May 2011 19:15:48 +0100</updated>
                    <resolved>Mon, 15 Nov 2010 18:27:30 +0000</resolved>
                            <version>3.1</version>
                <version>4.0-ALPHA</version>
                                <fixVersion>3.1</fixVersion>
                <fixVersion>4.0-ALPHA</fixVersion>
                                <component>modules/analysis</component>
                        <due></due>
                    <votes>0</votes>
                        <watches>2</watches>
                          <timeoriginalestimate seconds="1800">0.5h</timeoriginalestimate>
                    <timeestimate seconds="1800">0.5h</timeestimate>
                                  <comments>
                    <comment id="12791522" author="shyamal" created="Wed, 16 Dec 2009 18:55:05 +0000"  >&lt;p&gt;Patch fixes Javadoc with suggested text, adds test cases to motivate change.&lt;/p&gt;</comment>
                    <comment id="12792271" author="rcmuir" created="Fri, 18 Dec 2009 01:19:51 +0000"  >&lt;p&gt;Hi Shyamal, I am not sure we should document this behavior, but instead improve standard analyzer.&lt;/p&gt;

&lt;p&gt;Like you said it is hard to make everyone happy, but we now have a mechanism to improve things, that is based on that Version constant you provide.&lt;br/&gt;
For example, in a future release we hope to be able to use Jflex 1.5, which has greatly improved unicode support.&lt;/p&gt;

&lt;p&gt;you can try your examples against unicode segmentation standards here to get a preview of what this might look like: &lt;a href=&quot;http://unicode.org/cldr/utility/breaks.jsp&quot; class=&quot;external-link&quot;&gt;http://unicode.org/cldr/utility/breaks.jsp&lt;/a&gt;&lt;/p&gt;</comment>
                    <comment id="12792726" author="shyamal" created="Sat, 19 Dec 2009 00:43:05 +0000"  >&lt;p&gt;Hi Robert, I presume that when you say we should &quot;instead improve standard analyzer&quot; you mean the code should work more like the original Javadoc states it should? Or are you suggesting that moving to Jflex 1.5 &lt;/p&gt;

&lt;p&gt;The problem I observed was that the current JFlex rules don&apos;t implement what the Javadoc says is the  behavior of the tokenizer. I&apos;d be happy to spend some time on this if I could get some direction on where I should focus.&lt;/p&gt;</comment>
                    <comment id="12792737" author="rcmuir" created="Sat, 19 Dec 2009 01:07:26 +0000"  >&lt;blockquote&gt;&lt;p&gt;Hi Robert, I presume that when you say we should &quot;instead improve standard analyzer&quot; you mean the code should work more like the original Javadoc states it should?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Shyamal I guess what I am saying is I would prefer the javadoc of StandardTokenizer to be a little vague as to exactly what it does.&lt;br/&gt;
I would actually prefer it have less details than it currently has: in my opinion it starts getting into nitty-gritty details of what could be considered Version-specific.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I&apos;d be happy to spend some time on this if I could get some direction on where I should focus.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;If you have fixes to the grammar, I would prefer this over &apos;documenting buggy behavior&apos;. &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2074&quot; title=&quot;Use a separate JFlex generated Unicode 4 by Java 5 compatible StandardTokenizer&quot;&gt;&lt;del&gt;LUCENE-2074&lt;/del&gt;&lt;/a&gt; gives us the capability to fix bugs without breaking backwards compatibility.&lt;/p&gt;</comment>
                    <comment id="12837579" author="shyamal" created="Wed, 24 Feb 2010 02:13:56 +0000"  >&lt;p&gt;Hi Robert,&lt;/p&gt;

&lt;p&gt;It&apos;s been a while but  I finally got around to working on the grammar. Clearly, much of this is an opinion, so I finally stuck to the one minor change that I believe is arguably an improvement. Previously comma separated fields containing digits would be mistaken for numbers and combined into a single token. I believe this is a mistake because part numbers etc. are rarely comma separated, and regular text that is comma separated is not uncommon. This is also the problem I ran into in real life when using Lucene &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;This patch stops treating comma separated tokens as numbers when they contain digits.&lt;/p&gt;

&lt;p&gt;I did not included the patched Java file since I don&apos;t know what  JFlex version I should use to create it  (I used JFlex 1.4.3, and test-tag passes with JDK 1.5/1.6; I presume the Java 1.4 compatibility comment in the generated file is now history?).&lt;/p&gt;

&lt;p&gt;Let me know if this is headed in a useful direction.&lt;/p&gt;

&lt;p&gt;Cheers!&lt;br/&gt;
Shyamal&lt;/p&gt;</comment>
                    <comment id="12837603" author="rcmuir" created="Wed, 24 Feb 2010 03:39:29 +0000"  >&lt;blockquote&gt;&lt;p&gt;Clearly, much of this is an opinion, so I finally stuck to the one minor change that I believe is arguably an improvement. Previously comma separated fields containing digits would be mistaken for numbers and combined into a single token. I believe this is a mistake because part numbers etc. are rarely comma separated, and regular text that is comma separated is not uncommon.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I don&apos;t think it really has to be, i actually am of the opinion StandardTokenizer should follow unicode standard tokenization. then we can throw subjective decisions away, and stick with a standard.&lt;/p&gt;

&lt;p&gt;In this example, i think the change would be bad, as the comma is treated differently depending upon context, as it is a decimal separator and thousands separator in many languages, including English. so, the treatment of the comma depends upon the previous character.&lt;/p&gt;

&lt;p&gt;this is why in unicode, the comma has the Mid_Num Word_Break property.&lt;/p&gt;</comment>
                    <comment id="12838060" author="shyamal" created="Wed, 24 Feb 2010 22:58:57 +0000"  >&lt;blockquote&gt;
&lt;p&gt;I don&apos;t think it really has to be, i actually am of the opinion StandardTokenizer should follow unicode standard tokenization. then we can throw subjective decisions away, and stick with a standard.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yep, I see I am going for the wrong ambition level and only tweaking the existing grammar. I&apos;ll take a crack at understanding unicode standard tokenization, as you&apos;d suggested originally,  and try and produce something as soon as I get a chance. I see your point.&lt;/p&gt;

&lt;p&gt;Cheers!&lt;br/&gt;
Shyamal&lt;/p&gt;</comment>
                    <comment id="12838068" author="rcmuir" created="Wed, 24 Feb 2010 23:07:12 +0000"  >&lt;blockquote&gt;&lt;p&gt;I&apos;ll take a crack at understanding unicode standard tokenization, as you&apos;d suggested originally, and try and produce something as soon as I get a chance.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I would love it if you could produce a grammar that implemented UAX#29!&lt;/p&gt;

&lt;p&gt;If so, in my opinion it should become the StandardAnalyzer for the next lucene version. If I thought I could do it correctly, I would have already done it, as the support for the unicode properties needed to do this is now in the trunk of Jflex!&lt;/p&gt;

&lt;p&gt;here are some references that might help: &lt;br/&gt;
The standard itself: &lt;a href=&quot;http://unicode.org/reports/tr29/&quot; class=&quot;external-link&quot;&gt;http://unicode.org/reports/tr29/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;particularly the &quot;Testing&quot; portion: &lt;a href=&quot;http://unicode.org/reports/tr41/tr41-5.html#Tests29&quot; class=&quot;external-link&quot;&gt;http://unicode.org/reports/tr41/tr41-5.html#Tests29&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Unicode provides a WordBreakTest.txt file, that we could use from Junit, to help verify correctness: &lt;a href=&quot;http://www.unicode.org/Public/UNIDATA/auxiliary/WordBreakTest.txt&quot; class=&quot;external-link&quot;&gt;http://www.unicode.org/Public/UNIDATA/auxiliary/WordBreakTest.txt&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I&apos;ll warn you I think it might be hard, but perhaps its not that bad. In particular the standard is defined in terms of &quot;chained&quot; rules, and Jflex doesnt support rule chaining, but I am not convinced we need rule chaining to implement WordBreak (maybe LineBreak, but maybe WordBreak can be done easily without it?) &lt;/p&gt;

&lt;p&gt;Steven Rowe is the expert on this stuff, maybe he has some ideas.&lt;/p&gt;</comment>
                    <comment id="12838073" author="rcmuir" created="Wed, 24 Feb 2010 23:15:12 +0000"  >&lt;p&gt;btw, here is some statement that seems to confirm my suspicions, from the standard:&lt;/p&gt;

&lt;p&gt;In section 6.3, there is an example of the grapheme cluster boundaries converted into a simple regex (the kind we could do easily in jflex now that it has the properties available).&lt;/p&gt;

&lt;p&gt;They make this statement: Such a regular expression can also be turned into a fast, deterministic finite-state machine. Similar regular expressions are possible for Word boundaries. Line and Sentence boundaries are more complicated, and more difficult to represent with regular expressions.&lt;/p&gt;</comment>
                    <comment id="12838081" author="steve_rowe" created="Wed, 24 Feb 2010 23:24:09 +0000"  >&lt;p&gt;I wrote word break rules grammar specifications for JFlex 1.5.0-SNAPSHOT and both Unicode versions 5.1 and 5.2 - you can see the files here:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://jflex.svn.sourceforge.net/viewvc/jflex/trunk/testsuite/testcases/src/test/cases/unicode-word-break/&quot; class=&quot;external-link&quot;&gt;http://jflex.svn.sourceforge.net/viewvc/jflex/trunk/testsuite/testcases/src/test/cases/unicode-word-break/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The files are &lt;tt&gt;UnicodeWordBreakRules_5_&amp;#42;.&amp;#42;&lt;/tt&gt; - these are written to: parse the Unicode test files; run the generated scanner against each composed test string; output the break opportunities/prohibitions in the same format as the test files; and then finally compare the output against the test file itself, looking for a match.  (These tests currently pass.)&lt;/p&gt;

&lt;p&gt;The .flex files would need to be significantly changed to be used as a StandardTokenizer replacement, but you can get an idea from them how to implement the Unicode word break rules in (as yet unreleased version 1.5.0) JFlex syntax.&lt;/p&gt;</comment>
                    <comment id="12838094" author="rcmuir" created="Wed, 24 Feb 2010 23:46:03 +0000"  >&lt;p&gt;Steven, thanks for providing the link.&lt;/p&gt;

&lt;p&gt;I guess this is the point where I also say, I think it would be really nice for StandardTokenizer to adhere straight to the standard as much as we can with jflex (I realize in 1.5, we won&apos;t have &amp;gt; 0xffff support). Then its name would actually make sense.&lt;/p&gt;

&lt;p&gt;In my opinion, such a transition would involve something like renaming the old StandardTokenizer to EuropeanTokenizer, as its javadoc claims:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
This should be a good tokenizer &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; most European-language documents
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The new StandardTokenizer could then say&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
This should be a good tokenizer &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; most languages.
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;All the english/euro-centric stuff like the acronym/company/apostrophe stuff could stay with that &quot;EuropeanTokenizer&quot; or whatever its called, and it could be used by the european analyzers.&lt;/p&gt;

&lt;p&gt;but if we implement the Unicode rules, I think we should drop all this english/euro-centric stuff for StandardTokenizer. Otherwise it should be called &lt;b&gt;StandardishTokenizer&lt;/b&gt;.&lt;/p&gt;

&lt;p&gt;we can obviously preserve the backwards compat with Version, as Uwe has created a way to use a different grammar for a different Version.&lt;/p&gt;

&lt;p&gt;I expect some -1 to this, waiting comments &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                    <comment id="12838574" author="shyamal" created="Thu, 25 Feb 2010 22:43:41 +0000"  >&lt;p&gt;Robert Muir wrote:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;I would love it if you could produce a grammar that implemented UAX#29!&lt;/p&gt;

&lt;p&gt;If so, in my opinion it should become the StandardAnalyzer for the next lucene version. If I thought I could do it correctly, I would have already done it, as the support for the unicode properties needed to do this is now in the trunk of Jflex!&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I&apos;m not smart enough to know if I should even try to do it at all (leave alone correctly), but am always willing to learn! Thanks for the references, I will certainly give it an honest try.&lt;/p&gt;

&lt;p&gt;/Shyamal&lt;/p&gt;</comment>
                    <comment id="12862905" author="steve_rowe" created="Fri, 30 Apr 2010 23:18:03 +0100"  >&lt;p&gt;(stole Robert&apos;s comment to change the issue description)&lt;/p&gt;</comment>
                    <comment id="12864650" author="steve_rowe" created="Thu, 6 May 2010 06:01:02 +0100"  >&lt;p&gt;Patch implementing a UAX#29 tokenizer, along with most of Robert&apos;s TestICUTokenizer tests (left out tests for Thai, Lao, and breaking at 4K chars, none of which are features of this tokenizer) - I re-upcased the downcased expected terms, and un-normalized the trailing greek lowercase sigma one of the expected terms in testGreek().&lt;/p&gt;</comment>
                    <comment id="12864651" author="steve_rowe" created="Thu, 6 May 2010 06:07:22 +0100"  >&lt;p&gt;I want to test performance relative to StandardTokenizer and ICUTokenizer, and also consider switching from lookahead chaining to single regular expression per term type to improve performance.&lt;/p&gt;</comment>
                    <comment id="12865170" author="steve_rowe" created="Fri, 7 May 2010 15:30:46 +0100"  >&lt;p&gt;I ran contrib/benchmark over 10k Reuters docs with tokenization-only analyzers; Sun JDK 1.6, Windows Vista/Cygwin; best of five:&lt;/p&gt;

&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Operation&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;recsPerRun&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;rec/s&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;elapsedSec&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;StandardTokenizer&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1262799&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;655,318.62&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1.93&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;ICUTokenizer&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1268451&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;536,116.25&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2.37&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;UAX29Tokenizer&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1268451&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;524,586.88&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2.42&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;


&lt;p&gt;I think UAX29Tokenizer is slower than StandardTokenizer because it does the lookahead/chaining thing.  Still, decent performance.&lt;/p&gt;</comment>
                    <comment id="12865200" author="rcmuir" created="Fri, 7 May 2010 16:22:56 +0100"  >&lt;p&gt;Hi Steve, this is great progress!&lt;/p&gt;

&lt;p&gt;Looking at the code/perf, is there anyway to avoid the CharBuffer.wrap calls in updateAttributes()?&lt;/p&gt;

&lt;p&gt;It seems since you are just appending, it might be better to use some &quot;append&quot; like:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;int newLength = termAtt.length() + &amp;lt;length of text you are appending from zzBuffer&amp;gt;)
char bufferWithRoom[] = termAtt.resizeBuffer(newLength);
System.arrayCopy(from zzBuffer into bufferWithRoom, starting at termAtt.length());
termAtt.setLength(newLength);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
</comment>
                    <comment id="12865481" author="steve_rowe" created="Sat, 8 May 2010 17:51:36 +0100"  >&lt;p&gt;I added your change removing CharBuffer.wrap(), Robert, and it appears to have sped it up, though not as much as I would like:&lt;/p&gt;

&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Operation&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;recsPerRun&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;rec/s&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;elapsedSec&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;StandardTokenizer&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1262799&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;647,589.23&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1.95&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;ICUTokenizer&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1268451&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;526,328.22&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2.41&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;UAX29Tokenizer&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1268451&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;558,788.99&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2.27&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;


&lt;p&gt;I plan on attempting to rewrite the grammar to eliminate chaining/lookahead this weekend.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;edit&lt;/b&gt;: fixed the rec/s, which were from the worst of five instead of the best of five - the elapsedSec, however, were correct.&lt;/p&gt;</comment>
                    <comment id="12865541" author="steve_rowe" created="Sun, 9 May 2010 06:07:20 +0100"  >&lt;p&gt;Attached a patch that removes lookahead/chaining.  All tests pass.&lt;/p&gt;

&lt;p&gt;UAX29Tokenizer is now in the same ballpark performance-wise as StandardTokenizer:&lt;/p&gt;

&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Operation&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;recsPerRun&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;rec/s&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;elapsedSec&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;StandardTokenizer&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1262799&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;658,737.06&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1.92&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;ICUTokenizer&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1268451&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;542,768.94&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2.34&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;UAX29Tokenizer&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1268451&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;668,661.56&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1.90&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
</comment>
                    <comment id="12865743" author="rcmuir" created="Mon, 10 May 2010 13:06:34 +0100"  >&lt;p&gt;Hi Steven: this is impressive progress!&lt;/p&gt;

&lt;p&gt;What do you think the next steps should be?&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;should we look at any tailorings to this? The first thing that comes to mind is full-width forms, which have no WordBreak property&lt;/li&gt;
	&lt;li&gt;is it simple, or would it be messy, to apply this to the existing grammar (English/EuroTokenizer)? Another way to say it, is it possible for&lt;br/&gt;
  English/EuroTokenizer (StandardTokenizer today) to instead be a tailoring to UAX#29, for companies,acronym, etc, such that if it encounters&lt;br/&gt;
  say some hindi or thai text it will behave better?&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="12865757" author="steve_rowe" created="Mon, 10 May 2010 14:24:58 +0100"  >&lt;blockquote&gt;&lt;p&gt;should we look at any tailorings to this? The first thing that comes to mind is full-width forms, which have no WordBreak property&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Looks like Latin full-width letters are included (from &lt;a href=&quot;http://www.unicode.org/Public/5.2.0/ucd/auxiliary/WordBreakProperty.txt):&quot; class=&quot;external-link&quot;&gt;http://www.unicode.org/Public/5.2.0/ucd/auxiliary/WordBreakProperty.txt):&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;FF21..FF3A    ; ALetter # L&amp;amp;  &lt;span class=&quot;error&quot;&gt;&amp;#91;26&amp;#93;&lt;/span&gt; FULLWIDTH LATIN CAPITAL LETTER A..FULLWIDTH LATIN CAPITAL LETTER Z&lt;br/&gt;
FF41..FF5A    ; ALetter # L&amp;amp;  &lt;span class=&quot;error&quot;&gt;&amp;#91;26&amp;#93;&lt;/span&gt; FULLWIDTH LATIN SMALL LETTER A..FULLWIDTH LATIN SMALL LETTER Z&lt;/p&gt;

&lt;p&gt;But as you mention in a code comment in TestICUTokenizer, there are no full-width WordBreak:Numeric characters, so we could just add these to the &lt;/p&gt;
{NumericEx}
&lt;p&gt; macro, I think.&lt;/p&gt;

&lt;p&gt;Was there anything else you were thinking of?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;is it simple, or would it be messy, to apply this to the existing grammar (English/EuroTokenizer)? Another way to say it, is it possible for English/EuroTokenizer (StandardTokenizer today) to instead be a tailoring to UAX#29, for companies,acronym, etc, such that if it encounters say some hindi or thai text it will behave better?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Not sure about difficulty level, but it should be possible.&lt;/p&gt;

&lt;p&gt;Naming will require some thought, though - I don&apos;t like EnglishTokenizer or EuropeanTokenizer - both seem to exclude valid constituencies.&lt;/p&gt;

&lt;p&gt;What do you think about adding tailorings for Thai, Lao, Myanmar, Chinese, and Japanese?  (Are there others like these that aren&apos;t well served by UAX#29 without customizations?)&lt;/p&gt;

&lt;p&gt;I&apos;m thinking of leaving UAX29Tokenizer as-is, and adding tailorings as separate classes - what do you think?&lt;/p&gt;</comment>
                    <comment id="12865759" author="steve_rowe" created="Mon, 10 May 2010 14:30:40 +0100"  >&lt;p&gt;One other thing, Robert: what do you think of adding URL tokenization? &lt;/p&gt;

&lt;p&gt;I&apos;m not sure whether it&apos;s more useful to have the domain and path components separately tokenized.  But maybe if someone wants that, they could add a filter to decompose?  &lt;/p&gt;

&lt;p&gt;It would be impossible to do post-tokenization composition to get back the original URL, however, so I&apos;m leaning toward adding URL tokenization.&lt;/p&gt;</comment>
                    <comment id="12865763" author="rcmuir" created="Mon, 10 May 2010 14:54:12 +0100"  >&lt;blockquote&gt;
&lt;p&gt;But as you mention in a code comment in TestICUTokenizer, there are no full-width WordBreak:Numeric characters, so we could just add these to the &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {NumericEx}&lt;/span&gt; &lt;/div&gt;
&lt;p&gt; macro, I think.&lt;/p&gt;

&lt;p&gt;Was there anything else you were thinking of?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;No, that&apos;s it!&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Naming will require some thought, though - I don&apos;t like EnglishTokenizer or EuropeanTokenizer - both seem to exclude valid constituencies.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;What valid constituencies do you refer to? In general the acronym,company,possessive stuff here are very english/euro-specific.&lt;br/&gt;
Bugs in JIRA get opened if it doesn&apos;t do this stuff right on english, but it doesn&apos;t even work at all for a lot of languages.&lt;br/&gt;
Personally I think its great to rip this stuff out of what should be a &quot;default&quot; language-independent tokenizer based on &lt;br/&gt;
standards (StandardTokenizer), and put it into the language-specific package that it belongs. Otherwise we have to &lt;br/&gt;
worry about these sort of things overriding and screwing up UAX#29 rules for words in real languages.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;What do you think about adding tailorings for Thai, Lao, Myanmar, Chinese, and Japanese? (Are there others like these that aren&apos;t well served by UAX#29 without customizations?)&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;It gets a little tricky: we should be careful about how we interpret what is &quot;reasonable&quot; for a language-independent default tokenizer. &lt;br/&gt;
I think its &quot;enough&quot; to output the best indexing unit that is possible and relatively unambiguous to identify. I think this is a shortcut&lt;br/&gt;
we can make, because we are trying to tokenize things for information retrieval, not for other purposes. The approach for Lao, &lt;br/&gt;
Myanmar, Khmer, CJK, etc in ICUTokenizer is to just output syllables as indexing unit, since words are ambiguous. Thai is based &lt;br/&gt;
on words, not syllables, in ICUTokenizer, which is inconsistent from this, but we get this for free, so its just a laziness thing.&lt;/p&gt;

&lt;p&gt;By the way: none of those syllable-grammars in ICUTokenizer used chained rules, so you are welcome to steal what you want!&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I&apos;m thinking of leaving UAX29Tokenizer as-is, and adding tailorings as separate classes - what do you think?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Well, either way I again strongly feel this logic should be tied into &quot;Standard&quot; tokenizer, so that it has better unicode behavior. I think&lt;br/&gt;
it makes sense for us to have a reasonable, language-independent, standards-based tokenizer that works well for most languages.&lt;br/&gt;
 I think it also makes sense to have English/Euro-centric stuff thats language-specific, sitting in the analysis.en package just like we&lt;br/&gt;
 do with other languages.&lt;/p&gt;</comment>
                    <comment id="12865765" author="rcmuir" created="Mon, 10 May 2010 14:58:24 +0100"  >&lt;blockquote&gt;&lt;p&gt;One other thing, Robert: what do you think of adding URL tokenization?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think I would lean towards not doing this, only because of how complex a URL can be these days. It also&lt;br/&gt;
starts to get a little ambiguous and will likely interfere with other rules (generating a lot of false positives).&lt;/p&gt;

&lt;p&gt;I guess I don&apos;t care much either way, if its strict and standards-based, it probably won&apos;t cause any harm.&lt;br/&gt;
But if you start allowing things like http urls without the http:// being present, its gonna cause some problems.&lt;/p&gt;</comment>
                    <comment id="12865815" author="steve_rowe" created="Mon, 10 May 2010 17:35:38 +0100"  >&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;Naming will require some thought, though - I don&apos;t like EnglishTokenizer or EuropeanTokenizer - both seem to exclude valid constituencies.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;What valid constituencies do you refer to?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Well, we can&apos;t call it English/EuropeanTokenizer (maybe EnglishAndEuropeanAnalyzer?  seems too long), and calling it either only English or only European seems to leave the other out.  Americans, e.g., don&apos;t consider themselves European, maybe not even linguistically (however incorrect that might be).&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;In general the acronym,company,possessive stuff here are very english/euro-specific.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Right, I agree.  I&apos;m just looking for a name that covers the languages of interest unambiguously.  WesternTokenizer?  (but &quot;I live east of the Rockies - can I use WesternTokenizer?&quot;...)  Maybe EuropeanLanguagesTokenizer?  The difficulty as I see it is the messy intersection between political, geographic, and linguistic boundaries.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Bugs in JIRA get opened if it doesn&apos;t do this stuff right on english, but it doesn&apos;t even work at all for a lot of languages.  Personally I think its great to rip this stuff out of what should be a &quot;default&quot; language-independent tokenizer based on standards (StandardTokenizer), and put it into the language-specific package that it belongs. Otherwise we have to worry about these sort of things overriding and screwing up UAX#29 rules for words in real languages.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I assume you don&apos;t mean to say that English and European languages are not real languages &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; .&lt;/p&gt;

&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;What do you think about adding tailorings for Thai, Lao, Myanmar, Chinese, and Japanese? (Are there others like these that aren&apos;t well served by UAX#29 without customizations?)&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;It gets a little tricky: we should be careful about how we interpret what is &quot;reasonable&quot; for a language-independent default tokenizer. I think its &quot;enough&quot; to output the best indexing unit that is possible and relatively unambiguous to identify. I think this is a shortcut we can make, because we are trying to tokenize things for information retrieval, not for other purposes. The approach for Lao, Myanmar, Khmer, CJK, etc in ICUTokenizer is to just output syllables as indexing unit, since words are ambiguous. Thai is based on words, not syllables, in ICUTokenizer, which is inconsistent from this, but we get this for free, so its just a laziness thing.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think that StandardTokenizer should contain tailorings for CJK, Thai, Lao, Myanmar, and Khmer, then - it should be able to do reasonable things for all languages/scripts, to the greatest extent possible.&lt;/p&gt;

&lt;p&gt;The English/European tokenizer can then extend StandardTokenizer (conceptually, not in the Java sense).&lt;/p&gt;

&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;I&apos;m thinking of leaving UAX29Tokenizer as-is, and adding tailorings as separate classes - what do you think?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Well, either way I again strongly feel this logic should be tied into &quot;Standard&quot; tokenizer, so that it has better unicode behavior. I think it makes sense for us to have a reasonable, language-independent, standards-based tokenizer that works well for most languages. I think it also makes sense to have English/Euro-centric stuff thats language-specific, sitting in the analysis.en package just like we&lt;br/&gt;
do with other languages.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I agree that stuff like giving &quot;O&apos;Reilly&apos;s&quot; the &amp;lt;APOSTROPHE&amp;gt; type, to enable so-called StandardFilter to strip out the trailing /&apos;s/, is stupid for all non-English languages.&lt;/p&gt;

&lt;p&gt;It might be confusing, though, for a (e.g.) Greek user to have to go look at the analysis.en package to get reasonable performance for her language.&lt;/p&gt;

&lt;p&gt;Maybe an EnglishTokenizer, and separately a EuropeanAnalyzer?  Is that what you&apos;ve been driving at all along??? (Silly me....  Sigh.)&lt;/p&gt;</comment>
                    <comment id="12865817" author="steve_rowe" created="Mon, 10 May 2010 17:45:36 +0100"  >&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;What do you think about adding tailorings for Thai, Lao, Myanmar, Chinese, and Japanese?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;By the way: none of those syllable-grammars in ICUTokenizer used chained rules, so you are welcome to steal what you want!&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Thanks, I will!  Of course now that you&apos;ve given permission, it won&apos;t be as much fun...&lt;/p&gt;</comment>
                    <comment id="12865818" author="steve_rowe" created="Mon, 10 May 2010 17:49:14 +0100"  >&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;One other thing, Robert: what do you think of adding URL tokenization?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think I would lean towards not doing this, only because of how complex a URL can be these days. It also starts to get a little ambiguous and will likely interfere with other rules (generating a lot of false positives).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I have written standards-based URL tokenization routines in the past.  I agree it&apos;s very complex, but I know it&apos;s do-able.&lt;/p&gt;

&lt;p&gt;Do you have some examples of false positives?  I&apos;d like to add tests for them.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I guess I don&apos;t care much either way, if its strict and standards-based, it probably won&apos;t cause any harm.  But if you start allowing things like http urls without the http:// being present, its gonna cause some problems.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yup, I would only accept strictly correct URLs.&lt;/p&gt;

&lt;p&gt;Now that international TLDs are a reality, it would be cool to be able to identify them.&lt;/p&gt;</comment>
                    <comment id="12865819" author="rcmuir" created="Mon, 10 May 2010 17:51:26 +0100"  >&lt;blockquote&gt;
&lt;p&gt;I assume you don&apos;t mean to say that English and European languages are not real languages  .&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think the heuristics I am talking about that are in StandardTokenizer today, that don&apos;t really even work*,&lt;br/&gt;
shouldn&apos;t have a negative effect on other languages, thats all. &lt;/p&gt;


&lt;blockquote&gt;
&lt;p&gt;I agree that stuff like giving &quot;O&apos;Reilly&apos;s&quot; the &amp;lt;APOSTROPHE&amp;gt; type, to enable so-called StandardFilter to strip out the trailing /&apos;s/, is stupid for all non-English languages.&lt;/p&gt;

&lt;p&gt;It might be confusing, though, for a (e.g.) Greek user to have to go look at the analysis.en package to get reasonable performance for her language.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;fyi, GreekAnalyzer didn&apos;t even use this stuff until 3.1 (it omitted StandardFilter).&lt;/p&gt;

&lt;p&gt;But I don&apos;t think it matters where we put the &quot;western&quot; tokenizer, as long as its not StandardTokenizer.&lt;br/&gt;
I don&apos;t really even care too much about the stuff it does honestly, I don&apos;t consider it very important, nor very&lt;br/&gt;
accurate, only the source of many jira bugs* and hassle and confusion (invalidAcronym etc). &lt;br/&gt;
Just seems to be more trouble than its worth.&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-1438&quot; title=&quot;StandardTokenizer splits host names with hyphens into multiple tokens&quot;&gt;&lt;del&gt;LUCENE-1438&lt;/del&gt;&lt;/a&gt;&lt;/li&gt;
	&lt;li&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2244&quot; title=&quot;Improve StandardTokenizer&amp;#39;s understanding of non ASCII punctuation and quotes&quot;&gt;&lt;del&gt;LUCENE-2244&lt;/del&gt;&lt;/a&gt;&lt;/li&gt;
	&lt;li&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-1787&quot; title=&quot;Standard Tokenizer doesn&amp;#39;t recognise I.B.M as Acronym, it requires it ends with a dot i.e I.B.M.&quot;&gt;&lt;del&gt;LUCENE-1787&lt;/del&gt;&lt;/a&gt;&lt;/li&gt;
	&lt;li&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-1403&quot; title=&quot;StandardTokenizer - Improper Hostname Recognition&quot;&gt;&lt;del&gt;LUCENE-1403&lt;/del&gt;&lt;/a&gt;&lt;/li&gt;
	&lt;li&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-1100&quot; title=&quot;StandardTokenizer incorrectly types certain values&quot;&gt;&lt;del&gt;LUCENE-1100&lt;/del&gt;&lt;/a&gt;&lt;/li&gt;
	&lt;li&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-1556&quot; title=&quot;some valid email address characters not correctly recognized&quot;&gt;&lt;del&gt;LUCENE-1556&lt;/del&gt;&lt;/a&gt;&lt;/li&gt;
	&lt;li&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-571&quot; title=&quot;StandardTokenizer parses decimal number as &amp;lt;HOST&amp;gt;&quot;&gt;&lt;del&gt;LUCENE-571&lt;/del&gt;&lt;/a&gt;&lt;/li&gt;
	&lt;li&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-34&quot; title=&quot;e-mail token in StandardTokenizer.jj does not match valid e-mail addresses&quot;&gt;&lt;del&gt;LUCENE-34&lt;/del&gt;&lt;/a&gt;&lt;/li&gt;
	&lt;li&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-1068&quot; title=&quot;Invalid behavior of StandardTokenizerImpl&quot;&gt;&lt;del&gt;LUCENE-1068&lt;/del&gt;&lt;/a&gt;&lt;/li&gt;
	&lt;li&gt;i stopped at this point, i think this is enough examples&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="12865822" author="rcmuir" created="Mon, 10 May 2010 17:53:55 +0100"  >&lt;blockquote&gt;
&lt;p&gt;Yup, I would only accept strictly correct URLs.&lt;/p&gt;

&lt;p&gt;Now that international TLDs are a reality, it would be cool to be able to identify them.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;+1. This is in my opinion, the way such things in &lt;b&gt;Standard&lt;/b&gt; Tokenizer should work. &lt;br/&gt;
Perhaps too strict for some folks tastes, but correct!&lt;/p&gt;</comment>
                    <comment id="12865857" author="creamyg" created="Mon, 10 May 2010 19:37:48 +0100"  >&lt;p&gt;I find that it works well to parse URLs as multiple tokens, so long as the&lt;br/&gt;
query parser tokenizes them as phrases rather than individual terms.  That&lt;br/&gt;
allows you to hit on URL substrings, so e.g. a document containing&lt;br/&gt;
&apos;http://www.example.com/index.html&apos; is a hit for &apos;example.com&apos;.&lt;/p&gt;

&lt;p&gt;Happily, no special treatment for URLs also makes for a simpler parser.&lt;/p&gt;</comment>
                    <comment id="12865863" author="steve_rowe" created="Mon, 10 May 2010 19:52:42 +0100"  >&lt;p&gt;Good point, Marvin - indexing URLs makes no sense without query support for them.  (Is this a stupid can of worms for me to have opened?)  I have used Lucene tokenizers for other things than retrieval (e.g. term vectors as input to other processes), and I suspect I&apos;m not alone. The ability to extract URLs would be very nice.&lt;/p&gt;

&lt;p&gt;Ideally, URL analysis would produce both the full URL as a single token, and as overlapping tokens the hostname, path components, etc.  However, I don&apos;t think it&apos;s a good idea for the tokenizer to output overlapping tokens - I suspect this would break more than a few things.&lt;/p&gt;

&lt;p&gt;A filter that breaks URL type tokens into their components, and then adds them as overlapping tokens, or replaces the full URL with the components, should be easy to write, though.&lt;/p&gt;</comment>
                    <comment id="12865879" author="rcmuir" created="Mon, 10 May 2010 20:26:21 +0100"  >&lt;blockquote&gt;&lt;p&gt;A filter that breaks URL type tokens into their components, and then adds them as overlapping tokens, or replaces the full URL with the components, should be easy to write, though.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Not sure, for this to really work for non-english, it should recognize and normalize punycode representations of international domain names, etc.&lt;/p&gt;

&lt;p&gt;So while its a good idea, maybe it is a can of worms, and better to leave it alone for now?&lt;/p&gt;</comment>
                    <comment id="12865921" author="steve_rowe" created="Mon, 10 May 2010 21:59:04 +0100"  >&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;A filter that breaks URL type tokens into their components, and then adds them as overlapping tokens, or replaces the full URL with the components, should be easy to write, though.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Not sure, for this to really work for non-english, it should recognize and normalize punycode representations of international domain names, etc.&lt;/p&gt;

&lt;p&gt;So while its a good idea, maybe it is a can of worms, and better to leave it alone for now?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Do you mean URL-as-token should not be attempted now?  Or just this URL-breaking filter?&lt;/p&gt;</comment>
                    <comment id="12867690" author="rcmuir" created="Fri, 14 May 2010 22:40:30 +0100"  >&lt;blockquote&gt;&lt;p&gt;Do you mean URL-as-token should not be attempted now? Or just this URL-breaking filter?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;We can always add tailorings later, as Uwe has implemented Version-based support.&lt;/p&gt;

&lt;p&gt;Personally I see no problems with this patch, and I think we should look at tying this in as-is as the new StandardTokenizer, still backwards compatible thanks to Version support (we can just invoke EnglishTokenizerImpl in that case).&lt;/p&gt;

&lt;p&gt;I still want to rip StandardTokenizer out of lucene core and into modules. I think thats not too far away and its probably better to do this afterwards?, but we can do it before that time if you want, doesn&apos;t matter to me.&lt;/p&gt;

&lt;p&gt;It will be great to have StandardTokenizer working for non-European languages out of box!&lt;/p&gt;</comment>
                    <comment id="12867697" author="steve_rowe" created="Fri, 14 May 2010 22:58:26 +0100"  >&lt;p&gt;I think UAX29Tokenizer should remain as-is, except that I think there are some valid letter chars (Lao/Myanmar, I think) that are being dropped rather than returned as singletons, as CJ chars are now.  I need to augment the tests and make sure that valid word/number chars are not being dropped.  Also, I want to add full-width numeric chars to the &lt;/p&gt;
{NumericEx}
&lt;p&gt; macro.&lt;/p&gt;

&lt;p&gt;A separate replacement StandardTokenizer class should have standards-based email and url tokenization - the current StandardTokenizer gets part of the way there, but doesn&apos;t support some valid emails, and while it recognizes host/domain names, it doesn&apos;t recognize full URLs.  I want to get this done before anything in this issue is committed.&lt;/p&gt;

&lt;p&gt;Then (after this issue is committed), in separate issues, we can add EnglishTokenizer (for things like acronyms and maybe removing posessives (current StandardFilter), and then as needed, other language-specific tokenizers.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I still want to rip StandardTokenizer out of lucene core and into modules. I think thats not too far away and its probably better to do this afterwards?, but we can do it before that time if you want, doesn&apos;t matter to me.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I&apos;ll finish the UAX29Tokenizer fixes this weekend, but I think it&apos;ll take me a week or so to get the URL/email tokenization in place.&lt;/p&gt;</comment>
                    <comment id="12867699" author="steve_rowe" created="Fri, 14 May 2010 23:01:15 +0100"  >&lt;p&gt;Currently in StandardTokenizer there is a hack to allow contiguous Thai chars to be sent in a block to the ThaiWordFilter, which then uses the JDK BreakIterator to generate words.  &lt;/p&gt;

&lt;p&gt;Robert, were you thinking of not supporting that in the StandardTokenizer replacement in the short term?&lt;/p&gt;</comment>
                    <comment id="12867702" author="rcmuir" created="Fri, 14 May 2010 23:24:37 +0100"  >&lt;blockquote&gt;
&lt;p&gt;Currently in StandardTokenizer there is a hack to allow contiguous Thai chars to be sent in a block to the ThaiWordFilter, which then uses the JDK BreakIterator to generate words.&lt;br/&gt;
Robert, were you thinking of not supporting that in the StandardTokenizer replacement in the short term?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;You don&apos;t need any special support.&lt;/p&gt;

&lt;p&gt;I don&apos;t know how this hack founds its way in, but from a Thai tokenization perspective the only thing it is doing is preventing StandardTokenizer from splitting thai on non-spacing marks (like it does wrongly for other languages).&lt;/p&gt;

&lt;p&gt;So UAX#29 itself is the fix...&lt;/p&gt;</comment>
                    <comment id="12867703" author="rcmuir" created="Fri, 14 May 2010 23:26:10 +0100"  >&lt;blockquote&gt;&lt;p&gt;except that I think there are some valid letter chars (Lao/Myanmar, I think) that are being dropped rather than returned as singletons&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Do you have any examples?&lt;/p&gt;</comment>
                    <comment id="12867705" author="steve_rowe" created="Fri, 14 May 2010 23:28:41 +0100"  >&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;except that I think there are some valid letter chars (Lao/Myanmar, I think) that are being dropped rather than returned as singletons&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Do you have any examples?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I imported your tests from TestICUTokenizer, but I left out Lao, Myanmar and Thai because I didn&apos;t plan on adding tailorings like those you put in for ICUTokenizer.  However, I think Lao had zero tokens output, so if you just import the Lao test from TestICUTokenizer you should see the issue.&lt;/p&gt;</comment>
                    <comment id="12867708" author="steve_rowe" created="Fri, 14 May 2010 23:31:20 +0100"  >&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;Currently in StandardTokenizer there is a hack to allow contiguous Thai chars to be sent in a block to the ThaiWordFilter, which then uses the JDK BreakIterator to generate words.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Robert, were you thinking of not supporting that in the StandardTokenizer replacement in the short term?&lt;/p&gt;

&lt;p&gt;I don&apos;t know how this hack founds its way in, but from a Thai tokenization perspective the only thing it is doing is preventing StandardTokenizer from splitting thai on non-spacing marks (like it does wrongly for other languages).&lt;/p&gt;

&lt;p&gt;So UAX#29 itself is the fix...&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;AFAICT, UAX#29 would output individual Thai chars, just like CJ.  Is that appropriate?&lt;/p&gt;</comment>
                    <comment id="12867709" author="rcmuir" created="Fri, 14 May 2010 23:32:51 +0100"  >&lt;blockquote&gt;&lt;p&gt;However, I think Lao had zero tokens output, so if you just import the Lao test from TestICUTokenizer you should see the issue.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Ok, I will take a look. The algorithm there has some handling for incorrectly ordered unicode, for example combining characters before the base form when they should be after... so it might be no problem at all&lt;/p&gt;</comment>
                    <comment id="12867712" author="rcmuir" created="Fri, 14 May 2010 23:36:31 +0100"  >&lt;blockquote&gt;&lt;p&gt;AFAICT, UAX#29 would output individual Thai chars, just like CJ. Is that appropriate?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;What is a Thai character? &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;. According to the standard, it should be outputting phrases as there is nothing to delimit them... you can see this by pasting some text into &lt;a href=&quot;http://unicode.org/cldr/utility/breaks.jsp&quot; class=&quot;external-link&quot;&gt;http://unicode.org/cldr/utility/breaks.jsp&lt;/a&gt;&lt;/p&gt;</comment>
                    <comment id="12867715" author="steve_rowe" created="Fri, 14 May 2010 23:40:49 +0100"  >&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;AFAICT, UAX#29 would output individual Thai chars, just like CJ. Is that appropriate?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;What is a Thai character? . According to the standard, it should be outputting phrases as there is nothing to delimit them... you can see this by pasting some text into &lt;a href=&quot;http://unicode.org/cldr/utility/breaks.jsp&quot; class=&quot;external-link&quot;&gt;http://unicode.org/cldr/utility/breaks.jsp&lt;/a&gt;&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yeah, your Thai text &quot;&#3585;&#3634;&#3619;&#3607;&#3637;&#3656;&#3652;&#3604;&#3657;&#3605;&#3657;&#3629;&#3591;&#3649;&#3626;&#3604;&#3591;&#3623;&#3656;&#3634;&#3591;&#3634;&#3609;&#3604;&#3637;. &#3649;&#3621;&#3657;&#3623;&#3648;&#3608;&#3629;&#3592;&#3632;&#3652;&#3611;&#3652;&#3627;&#3609;? &#3665;&#3666;&#3667;&#3668;&quot; breaks at space and punctuation and nowhere else.  This test should be put back into TestUAX29Tokenizer with the appropriate expected output.&lt;/p&gt;</comment>
                    <comment id="12867721" author="rcmuir" created="Fri, 14 May 2010 23:48:15 +0100"  >&lt;p&gt;Hmm i ran some tests, I think i see your problem.&lt;/p&gt;

&lt;p&gt;I tried this:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; void testThai() &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; Exception {
    assertAnalyzesTo(a, &lt;span class=&quot;code-quote&quot;&gt;&quot;&#3616;&#3634;&#3625;&#3634;&#3652;&#3607;&#3618;&quot;&lt;/span&gt;, &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;[] { &lt;span class=&quot;code-quote&quot;&gt;&quot;&#3616;&#3634;&#3625;&#3634;&#3652;&#3607;&#3618;&quot;&lt;/span&gt; });
  }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The reason you get something different than the unicode site, is because recently? these have &lt;span class=&quot;error&quot;&gt;&amp;#91;:WordBreak=Other:&amp;#93;&lt;/span&gt;&lt;br/&gt;
Instead anything that needs a dictionary or whatever is identified by &lt;span class=&quot;error&quot;&gt;&amp;#91;:Line_Break=Complex_Context:&amp;#93;&lt;/span&gt;&lt;br/&gt;
You can see this mentioned in the standard:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;In particular, the characters with the Line_Break property values of Contingent_Break (CB), 
Complex_Context (SA/South East Asian), and XX (Unknown) are assigned word boundary property 
values based on criteria outside of the scope of this annex. 
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In ICU, i noticed the default rules do this:&lt;br/&gt;
$dictionary   = &lt;span class=&quot;error&quot;&gt;&amp;#91;:LineBreak = Complex_Context:&amp;#93;&lt;/span&gt;;&lt;br/&gt;
$dictionary $dictionary&lt;/p&gt;

&lt;p&gt;(so they just stick together with this chained rule)&lt;/p&gt;</comment>
                    <comment id="12867722" author="rcmuir" created="Fri, 14 May 2010 23:49:42 +0100"  >&lt;blockquote&gt;&lt;p&gt;Yeah, your Thai text &quot;&#3585;&#3634;&#3619;&#3607;&#3637;&#3656;&#3652;&#3604;&#3657;&#3605;&#3657;&#3629;&#3591;&#3649;&#3626;&#3604;&#3591;&#3623;&#3656;&#3634;&#3591;&#3634;&#3609;&#3604;&#3637;. &#3649;&#3621;&#3657;&#3623;&#3648;&#3608;&#3629;&#3592;&#3632;&#3652;&#3611;&#3652;&#3627;&#3609;? &#3665;&#3666;&#3667;&#3668;&quot; breaks at space and punctuation and nowhere else. This test should be put back into TestUAX29Tokenizer with the appropriate expected output.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;But why does it fail for my test (listed above) with only a single thai phrase (nothing is output)? &lt;br/&gt;
Do you think it is because of Complex_Context or is there an off-by-one bug somehow?&lt;/p&gt;</comment>
                    <comment id="12867737" author="steve_rowe" created="Sat, 15 May 2010 00:40:05 +0100"  >&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;Yeah, your Thai text &quot;&#3585;&#3634;&#3619;&#3607;&#3637;&#3656;&#3652;&#3604;&#3657;&#3605;&#3657;&#3629;&#3591;&#3649;&#3626;&#3604;&#3591;&#3623;&#3656;&#3634;&#3591;&#3634;&#3609;&#3604;&#3637;. &#3649;&#3621;&#3657;&#3623;&#3648;&#3608;&#3629;&#3592;&#3632;&#3652;&#3611;&#3652;&#3627;&#3609;? &#3665;&#3666;&#3667;&#3668;&quot; breaks at space and punctuation and nowhere else. This test should be put back into TestUAX29Tokenizer with the appropriate expected output.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;But why does it fail for my test (listed above) with only a single thai phrase (nothing is output)?&lt;br/&gt;
Do you think it is because of Complex_Context or is there an off-by-one bug somehow?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Definitely Complex_Content.  I&apos;ll add that in, and this should address Thai, Myanmar, Khmer, Tai Le, etc.&lt;/p&gt;</comment>
                    <comment id="12867889" author="steve_rowe" created="Sat, 15 May 2010 17:12:38 +0100"  >&lt;p&gt;New patch addressing the following issues:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;On #lucene-dev, Uwe mentioned that methods in the generated scanner should be (package) private, since unlike the current StandardTokenizer, UAX29Tokenizer is not hidden behind a facade class. I added JFlex&apos;s %apiprivate option to fix this issue.&lt;/li&gt;
	&lt;li&gt;Thai, Lao, Khmer, Myanmar and other scripts&apos; characters are now kept together, like the ICU UAX#29 implementation, using rule &lt;span class=&quot;error&quot;&gt;&amp;#91;:Line_Break = Complex_Context:&amp;#93;&lt;/span&gt;+.&lt;/li&gt;
	&lt;li&gt;Added the Thai test back from Robert&apos;s TestICUTokenizer.&lt;/li&gt;
	&lt;li&gt;Added full-width numeric characters to the 
{NumericEx}
&lt;p&gt; macro, so that they can be appropriately tokenized, just like full-width alpha characters are now.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I couldn&apos;t find any suitable Lao test text (mostly because I don&apos;t know Lao at all), so I left out the Lao test in TestICUTokenizer, because Robert mentioned on #lucene that its characters are not in logical order.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;edit&lt;/b&gt; Complex_Content --&amp;gt; Complex_Context&lt;br/&gt;
&lt;b&gt;edit #2&lt;/b&gt; Added bullet about full-width numerics issue&lt;/p&gt;</comment>
                    <comment id="12867891" author="rcmuir" created="Sat, 15 May 2010 17:20:19 +0100"  >&lt;blockquote&gt;&lt;p&gt;I couldn&apos;t find any suitable Lao test text (mostly because I don&apos;t know Lao at all), so I left out the Lao test in TestICUTokenizer, because Robert mentioned on #lucene that its characters are not in logical order.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Only some of my icu tests contain &quot;screwed up lao&quot;.&lt;/p&gt;

&lt;p&gt;But you should be able to use &quot;good text&quot; and it should do the right thing.&lt;br/&gt;
Here&apos;s a test&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
assertAnalyzesTo(a, &lt;span class=&quot;code-quote&quot;&gt;&quot;&#3754;&#3762;&#3735;&#3762;&#3749;&#3760;&#3737;&#3760;&#3749;&#3761;&#3732; &#3739;&#3760;&#3722;&#3762;&#3735;&#3764;&#3739;&#3760;&#3780;&#3733; &#3739;&#3760;&#3722;&#3762;&#3722;&#3771;&#3737;&#3749;&#3762;&#3751;&quot;&lt;/span&gt;, 
&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;[] { &lt;span class=&quot;code-quote&quot;&gt;&quot;&#3754;&#3762;&#3735;&#3762;&#3749;&#3760;&#3737;&#3760;&#3749;&#3761;&#3732;&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;&#3739;&#3760;&#3722;&#3762;&#3735;&#3764;&#3739;&#3760;&#3780;&#3733;&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;&#3739;&#3760;&#3722;&#3762;&#3722;&#3771;&#3737;&#3749;&#3762;&#3751;&quot;&lt;/span&gt; });
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                    <comment id="12867927" author="steve_rowe" created="Sat, 15 May 2010 23:52:07 +0100"  >&lt;p&gt;New patch:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;added Robert&apos;s Lao test (thanks, Robert).&lt;/li&gt;
	&lt;li&gt;added a javadoc comment about UAX29Tokenizer not handling supplementary characters (thanks to Uwe for bringing this up on #lucene), with a pointer to Robert&apos;s ICUTokenizer.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="12867940" author="steve_rowe" created="Sun, 16 May 2010 02:13:30 +0100"  >&lt;p&gt;This patch contains the benchmarking implementation I&apos;ve been using.  I&apos;m  pretty sure we don&apos;t want this stuff in Lucene, so I&apos;m including it here only for reproducibility by others.  I have hardcoded absolute paths to the ICU4J jar and the contrib/icu jar in the script I use to run the benchmark (&lt;tt&gt;lucene/contrib/benchmark/scripts/compare.uax29.analyzers.sh&lt;/tt&gt;), so if anybody tries to run this stuff, they will have to first modify that script.&lt;/p&gt;

&lt;p&gt;On #lucene, Robert suggested comparing the performance of the straight ICU4J RBBI against UAX29Tokenizer, so I took his ICUTokenizer and associated classes, stripped out the script-detection logic, and made something I named RBBITokenizer, which is included in this patch.&lt;/p&gt;

&lt;p&gt;To run the benchmark, you have to first run &quot;ant jar&quot; in &lt;tt&gt;lucene/&lt;/tt&gt; to produce the lucene core jar, and then again in &lt;tt&gt;lucene/contrib/icu/&lt;/tt&gt;.  Then in &lt;tt&gt;contrib/benchmark/&lt;/tt&gt;, run &lt;tt&gt;scripts/compare.uax29.analyzers.sh&lt;/tt&gt;.&lt;/p&gt;

&lt;p&gt;Here are the results on my machine (Sun JDK 1.6.0_13; Windows Vista/Cygwin; best of five):&lt;/p&gt;

&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Operation&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;recsPerRun&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;rec/s&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;elapsedSec&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;ICUTokenizer&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1268451&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;548,638.00&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2.31&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;RBBITokenizer&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1268451&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;568,047.94&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2.23&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;StandardTokenizer&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1262799&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;644,614.06&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1.96&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;UAX29Tokenizer&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1268451&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;640,631.81&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1.98&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

</comment>
                    <comment id="12867954" author="rcmuir" created="Sun, 16 May 2010 06:39:51 +0100"  >&lt;blockquote&gt;&lt;p&gt;Here are the results on my machine (Sun JDK 1.6.0_13; Windows Vista/Cygwin; best of five):&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This is really cool, I think its a great benchmark to know, I played with it and saw similar results.&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;For Lucene Tokenizer-ish (forward-iteration) purposes, JFlex is quite a bit faster than RBBI for unicode segmentation.&lt;/li&gt;
	&lt;li&gt;Supporting unicode segmentation in StandardTokenizer doesn&apos;t slow it down in comparison to the current implementation.&lt;/li&gt;
	&lt;li&gt;The script detection/delegation in ICU doesn&apos;t really cost that tokenizer much; though, the benchmark is reuters, and it cheats for Latin-1 (see bottom of ScriptIterator.java).&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="12868005" author="steve_rowe" created="Sun, 16 May 2010 17:08:37 +0100"  >&lt;p&gt;Robert, what do you think of &amp;lt;SOUTHEAST_ASIAN&amp;gt; for the token type for Thai, Khmer, Lao, etc. Complex_Context runs?&lt;/p&gt;</comment>
                    <comment id="12868058" author="steve_rowe" created="Mon, 17 May 2010 01:22:28 +0100"  >&lt;p&gt;Sequences of South East Asian scripts are now assigned term type &amp;lt;SOUTHEAST_ASIAN&amp;gt; by UAX29Tokenizer.  I think UAX29Tokenizer is now a complete untailored UAX#29 implementation.  &lt;/p&gt;

&lt;p&gt;For the future StandardTokenizer replacement, I plan on making a copy of the UAX29Tokenizer grammar and adding email/URL tokenization, and maybe Southeast Asian tailorings converted from those in ICUTokenizer.&lt;/p&gt;</comment>
                    <comment id="12868086" author="steve_rowe" created="Mon, 17 May 2010 06:49:09 +0100"  >&lt;p&gt;As of r591, JFlex now has code in the generated yyreset() method to resize the internal scan buffer (zzBuffer) back down to its initial size if it has grown.  This is exactly the same workaround code in the reset() method in the UAX29Tokenizer grammar.&lt;/p&gt;

&lt;p&gt;This patch just removes the scan buffer size check and reallocation code from reset() in the .jflex file, as well as the .java file generated with r591 JFlex.&lt;/p&gt;</comment>
                    <comment id="12868184" author="rcmuir" created="Mon, 17 May 2010 13:01:23 +0100"  >&lt;blockquote&gt;&lt;p&gt;This patch just removes the scan buffer size check and reallocation code from reset() in the .jflex file, as well as the .java file generated with r591 JFlex.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;We have this code in our existing StandardTokenizer .jflex files, should we open an issue and fix these (we would have to ensure that we use a jflex &amp;gt; r591 for generation?) &lt;/p&gt;

&lt;p&gt;Additionally shouldn&apos;t we regen WikipediaTokenizer etc too, I noticed it doesnt even have the hack in its .jflex file.&lt;/p&gt;</comment>
                    <comment id="12868189" author="thetaphi" created="Mon, 17 May 2010 13:08:15 +0100"  >&lt;p&gt;Yeah we should regen all jflex files when pathing this (ant jflex does this automatically, so we dont need to care). Removing the hack from StandardTokenizers jflex file should be done in an issue, but it also does not hurt if the hack stays in code.&lt;/p&gt;

&lt;p&gt;Checking the jflex version is hard to do, i think about it, maybe there is an ANT trick. Is the version noted somewhere in a class file as constant?&lt;/p&gt;

&lt;p&gt;I think we should simply reopen &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2384&quot; title=&quot;Reset zzBuffer in StandardTokenizerImpl* when lexer is reset.&quot;&gt;&lt;del&gt;LUCENE-2384&lt;/del&gt;&lt;/a&gt; (its part of 3x and trunk)&lt;/p&gt;</comment>
                    <comment id="12868194" author="steve_rowe" created="Mon, 17 May 2010 13:15:11 +0100"  >&lt;blockquote&gt;&lt;p&gt;Yeah we should regen all jflex files when pathing this (ant jflex does this automatically, so we dont need to care). Removing the hack from StandardTokenizers jflex file should be done in an issue, but it also does not hurt if the hack stays in code.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Agreed.  I was thinking since Robert is moving StandardTokenizer that the regen could wait until afterward.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Checking the jflex version is hard to do, i think about it, maybe there is an ANT trick. Is the version noted somewhere in a class file as constant?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Release version is, I think, but we&apos;re using an unreleased version ATM.  Hmm, for the SVN checkout, maybe the .svn/entries file could be checked or something?  If we go that route (and I think it&apos;s probably not a good idea), we should instead maybe be &quot;svn up&quot;ing the checkout?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I think we should simply reopen &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2384&quot; title=&quot;Reset zzBuffer in StandardTokenizerImpl* when lexer is reset.&quot;&gt;&lt;del&gt;LUCENE-2384&lt;/del&gt;&lt;/a&gt; (its part of 3x and trunk)&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;+1&lt;/p&gt;</comment>
                    <comment id="12868230" author="dmsmith" created="Mon, 17 May 2010 15:10:54 +0100"  >&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;Naming will require some thought, though - I don&apos;t like EnglishTokenizer or EuropeanTokenizer - both seem to exclude valid constituencies.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;What valid constituencies do you refer to?&lt;/p&gt;&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Well, we can&apos;t call it English/EuropeanTokenizer (maybe EnglishAndEuropeanAnalyzer? seems too long), and calling it either only English or only European seems to leave the other out. Americans, e.g., don&apos;t consider themselves European, maybe not even linguistically (however incorrect that might be).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Tongue in cheek:&lt;br/&gt;
By and large, these are Romance languages (i.e. latin derivatives). And the constructs that are being considered for special processing for the most part are fairly recent additions to the languages. So how about &lt;b&gt;ModernRomanceAnalyzer&lt;/b&gt;?&lt;/p&gt;</comment>
                    <comment id="12868418" author="steve_rowe" created="Mon, 17 May 2010 23:06:57 +0100"  >&lt;p&gt;My daughter likes the Lady Gaga song &quot;Bad Romance&quot; - why not &lt;b&gt;BadRomanceAnalyzer&lt;/b&gt;?  Advertizing slogans: &quot;It slices your text when it&apos;s supposed to dice it, but it always apologizes afterward - how can you stay mad?&quot;; &quot;Who knew that analysis could have such catchy lyrics?&quot; &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                    <comment id="12872108" author="steve_rowe" created="Thu, 27 May 2010 07:01:01 +0100"  >&lt;p&gt;Updated to trunk.  Tests pass.&lt;/p&gt;

&lt;p&gt;This patch removes the jflex-* target dependencies on init, since init builds Lucene, which isn&apos;t a necessity prior running JFlex.&lt;/p&gt;</comment>
                    <comment id="12872117" author="steve_rowe" created="Thu, 27 May 2010 07:23:06 +0100"  >&lt;p&gt;Maven plugin including a mojo that generates a file containing a JFlex macro that accepts all valid ASCII top-level domains (TLDs), by downloading the IANA Root Zone Database, parsing the HTML file, and outputting ASCIITLD.jflex-macro into the analysis/common/src/java/org/apache/lucene/analysis/standard/ source directory; this file is also included in the patch.&lt;/p&gt;

&lt;p&gt;To run the Maven plugin, first run &quot;mvn install&quot; from the lucene-buildhelper-maven-plugin/ directory, then from the &lt;tt&gt;src/java/org/apache/lucene/analysis/standard/&lt;/tt&gt; directory, run the following command:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
mvn org.apache.lucene:lucene-buildhelper-maven-plugin:generate-jflex-tld-macros
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Execution is not yet hooked into build.xml, but this goal should run before JFlex runs.&lt;/p&gt;</comment>
                    <comment id="12872124" author="thetaphi" created="Thu, 27 May 2010 08:18:33 +0100"  >&lt;p&gt;Hi Steven,&lt;/p&gt;

&lt;p&gt;looks cool, I have some suggestions:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Must it be a maven plugin? From what I see, the same code could be done as a simple Java Class with main() like Roberts ICU converter. The external dependency to httpclient can be replaces by simply java.net.HttpUrlConnection and the URL itsself (you can even set the no-cache directives). Its much easier from ant to invoke a java method as a build step. So why not refactor a little bit to use a main() method that acceps the target directory.&lt;/li&gt;
	&lt;li&gt;You use the HTML root zone database from IANA. The format of this file is hard to parse and may change suddenly. BIND administrators know, that there is also the root zone file available for BIND in the standardized named-format @ &lt;a href=&quot;http://www.internic.net/zones/root.zone&quot; class=&quot;external-link&quot;&gt;http://www.internic.net/zones/root.zone&lt;/a&gt; (ASCII only, as DNS is ASCII only). You just have to use all rows that are not comments and contain &quot;NS&quot; as second token. The nameservers behind are not used, just use the DNS name before. This should be much easier to do. A python script may also work well.&lt;/li&gt;
	&lt;li&gt;You can write the Last-Modified-Header of the HTTP-date (HttpURLConnection.getLastModified()) also into the generated file.&lt;/li&gt;
	&lt;li&gt;The database only contains the punycode enabled DNS names. But users use the non-encoded variants, so you should decode punycode, too &lt;span class=&quot;error&quot;&gt;&amp;#91;we need ICU for that :( &amp;#93;&lt;/span&gt; and create patterns for that, too.&lt;/li&gt;
	&lt;li&gt;About changes in analyzer syntax because of regeneration: This should not be a problem, as the IANA only &lt;b&gt;adds&lt;/b&gt; new zones to the file and very seldom removes some (like old yugoslavian zones). As eMails and Webadresses should &lt;b&gt;not&lt;/b&gt; appear in tokenized text &lt;b&gt;before&lt;/b&gt; they are in the zone file, its no problem that they suddenly later are marked as &quot;URL/eMail&quot; (as they cannot appear before). So in my opinion we can update the zone database even in minor Lucene releases without breaking analyzers.&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;Fine idea!&lt;/p&gt;</comment>
                    <comment id="12872221" author="steve_rowe" created="Thu, 27 May 2010 14:53:09 +0100"  >&lt;blockquote&gt;&lt;p&gt;Must it be a maven plugin? &lt;span class=&quot;error&quot;&gt;&amp;#91;...&amp;#93;&lt;/span&gt; Its much easier from ant to invoke a java method as a build step.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Lucene&apos;s build could be converted to Maven, though, and this could be a place for build-related stuff.&lt;/p&gt;

&lt;p&gt;Maven Ant Tasks allows for Ant to call full Maven builds without a Maven installation: &lt;a href=&quot;http://maven.apache.org/ant-tasks/examples/mvn.html&quot; class=&quot;external-link&quot;&gt;http://maven.apache.org/ant-tasks/examples/mvn.html&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;From what I see, the same code could be done as a simple Java Class with main() like Roberts ICU converter. &lt;span class=&quot;error&quot;&gt;&amp;#91;snip&amp;#93;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I hadn&apos;t seen Robert&apos;s ICU converter - I&apos;ll take a look.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;A python script may also work well.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Perl is my scripting language of choice, not Python, but yes, a script would likely do the trick, assuming there are no external (Java) dependencies.  (And as you pointed out, HttpComponents, the only dependency of the Maven plugin, does not need to be a dependency.)&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;You use the HTML root zone database from IANA. The format of this file is hard to parse and may change suddenly. BIND administrators know, that there is also the root zone file available for BIND in the standardized named-format @ &lt;a href=&quot;http://www.internic.net/zones/root.zone&quot; class=&quot;external-link&quot;&gt;http://www.internic.net/zones/root.zone&lt;/a&gt; (ASCII only, as DNS is ASCII only).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think I&apos;ll stick with the HTML version for now - there are no decoded versions of the internationalized TLDs and no descriptive information in the named-format version.  I agree the HTML format is not ideal, but it took me just a little while to put together the regexes to parse it; when the format changes, the effort to fix will likely be similarly small.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;You can write the Last-Modified-Header of the HTTP-date (HttpURLConnection.getLastModified()) also into the generated file.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Excellent idea, I searched the HTML page source for this kind of information but it wasn&apos;t there.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;The database only contains the punycode enabled DNS names. But users use the non-encoded variants, so you should decode punycode, too &lt;span class=&quot;error&quot;&gt;&amp;#91;we need ICU for that :( &amp;#93;&lt;/span&gt; and create patterns for that, too.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I agree.  However, I looked into what&apos;s required to do internationalized domain names properly, and it&apos;s quite complicated.  I plan on doing what you suggest eventually, both for TLDs and all other domain labels, but I&apos;d rather finish the ASCII implementation and deal with IRIs in a separate follow-on issue.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;About changes in analyzer syntax because of regeneration: This should not be a problem, as the IANA only adds new zones to the file and very seldom removes some (like old yugoslavian zones). As eMails and Webadresses should not appear in tokenized text before they are in the zone file, its no problem that they suddenly later are marked as &quot;URL/eMail&quot; (as they cannot appear before). So in my opinion we can update the zone database even in minor Lucene releases without breaking analyzers.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;+1&lt;/p&gt;</comment>
                    <comment id="12872252" author="thetaphi" created="Thu, 27 May 2010 16:19:01 +0100"  >&lt;p&gt;Here my patch with the TLD-macro generator:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Uses zone database from DNS (downloaded)&lt;/li&gt;
	&lt;li&gt;Outputs correct platform dependent newlines, else commits with SVN fail&lt;/li&gt;
	&lt;li&gt;Has no comments &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/sad.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/li&gt;
	&lt;li&gt;Is included into build.xml. Run ant gen-tlds in modules/analysis/common&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;The resulting macro is almost identical, 4 TLDs are missing, but the file on internic.net is actual (see last mod date). The comments are not available, of course.&lt;/p&gt;</comment>
                    <comment id="12872260" author="thetaphi" created="Thu, 27 May 2010 16:47:18 +0100"  >&lt;p&gt;Small update (dont output lastMod date if internic.net gave none)&lt;/p&gt;</comment>
                    <comment id="12873917" author="thetaphi" created="Tue, 1 Jun 2010 09:10:23 +0100"  >&lt;p&gt;Updated patch.&lt;/p&gt;

&lt;p&gt;I had not seen that the previous jflex generator version had a bug in missing locale in String.toUpperCase (turkish i!). This version uses Character.toUpperCase() &lt;span class=&quot;error&quot;&gt;&amp;#91;non-locale-aware&amp;#93;&lt;/span&gt; and also only iterates over tld.charAt() &lt;span class=&quot;error&quot;&gt;&amp;#91;what was the reason for the strange substring stuff?&amp;#93;&lt;/span&gt;. This is fine, as the TLDs only contain &lt;span class=&quot;error&quot;&gt;&amp;#91;\-A-Za-z0-9&amp;#93;&lt;/span&gt; (Standard for domain names and the regex enforces this, so no supplementary chars.&lt;/p&gt;

&lt;p&gt;This patch also creates correct macro (single escaping).&lt;/p&gt;</comment>
                    <comment id="12874290" author="steve_rowe" created="Wed, 2 Jun 2010 00:01:06 +0100"  >&lt;blockquote&gt;&lt;p&gt;This version uses Character.toUpperCase() &lt;span class=&quot;error&quot;&gt;&amp;#91;non-locale-aware&amp;#93;&lt;/span&gt; and also only iterates over tld.charAt() &lt;span class=&quot;error&quot;&gt;&amp;#91;what was the reason for the strange substring stuff?&amp;#93;&lt;/span&gt;.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I looked for Character.toUpperCase(), didn&apos;t find it (no idea why), so went with the strange substring stuff to use the String version instead ...&lt;/p&gt;

&lt;p&gt;I plan on integrating your patch with mine, to make a single one, including a definition for a StandardTokenizer replacement.  I have implemented URL, Email and Host rules, just gotta write some tests now.&lt;/p&gt;</comment>
                    <comment id="12876154" author="steve_rowe" created="Mon, 7 Jun 2010 09:00:48 +0100"  >&lt;p&gt;New patch incorporating Uwe&apos;s JFlex TLD macro generation patch (with a few small adjustments), and also including a jflex grammar for a new class: NewStandardTokenizer.  This grammar adds recognition of URLs, e-mail addresses, and host names and IP addresses (both v4 and v6) to the UAX29Tokenizer grammar.  &lt;/p&gt;

&lt;p&gt;This is a work in progress &amp;#8211; testing for http: scheme URLs and e-mail addresses is included, but there is no testing yet for the &lt;a href=&quot;file:&quot; class=&quot;external-link&quot;&gt;file:&lt;/a&gt;, https:, or ftp: schemes.&lt;/p&gt;

&lt;p&gt;I have dropped the idea of recognizing mailto: URIs, because these seem more complicated than they are worth (mailto: URIs can include multiple email addresses, comments, full email bodies, etc.).  E-mail addresses within mailto: URIs should still be recognized.&lt;/p&gt;

&lt;p&gt;WARNING: I had to invoke Ant with a 900MB heap (&lt;tt&gt;ANT_OPTS=-Xmx900m ant jflex&lt;/tt&gt; on Windows Vista, 64 bit Sun JDK 1.5.0_22) in order to allow the JFlex generation process to complete for NewStandardTokenizer; the process also took a minute or two to finish.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;edit&lt;/b&gt;: Sun 1. -&amp;gt; Sun JDK 1.5.0_22&lt;/p&gt;</comment>
                    <comment id="12877315" author="steve_rowe" created="Thu, 10 Jun 2010 05:27:06 +0100"  >&lt;p&gt;URL testing for NewStandardTokenizer is now complete.&lt;/p&gt;

&lt;p&gt;I have dropped the &amp;lt;HOST&amp;gt; token type, since it seems to me that, e.g., both of the following strings should be interpretable as URLs, given that they effectively refer to the same resource (when interpreted in the context of the HTTP URI scheme), and the first is clearly not just a host name:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
example.com/

example.com
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Both of the above are now marked by NewStandardTokenizer with type &amp;lt;URL&amp;gt;.&lt;/p&gt;

&lt;p&gt;NewStandardTokenizer is not quite finished; I plan on stealing Robert&apos;s Southeast Asian (Lao, Myanmar, Khmer) syllabification routines from ICUTokenizer and incorporating them into NewStandardTokenizer.  Once that&apos;s done, I think we can make NewStandardTokenizer the new StandardTokenizer.&lt;/p&gt;</comment>
                    <comment id="12877846" author="rcmuir" created="Fri, 11 Jun 2010 17:28:49 +0100"  >&lt;blockquote&gt;&lt;p&gt;NewStandardTokenizer is not quite finished; I plan on stealing Robert&apos;s Southeast Asian (Lao, Myanmar, Khmer) syllabification routine&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Curious, what is your plan here? Do you plan to somehow &quot;jflex-#include&quot; these into the grammar so that these are longest-matched instead of the Complex_Context rule? &lt;/p&gt;

&lt;p&gt;How to handle the cases where the grammar cannot be forward-only deterministic matching? (at least i don&apos;t see how it could be, but maybe). e.g. the lao cases where some backtracking is needed... and the combining class reordering needed for real-world text?&lt;/p&gt;

&lt;p&gt;Curious what would you plan to index for Thai, words? a grammar for TCC?&lt;/p&gt;

&lt;p&gt;Also, some of these syllable techniques are probably not very good for search without doing a &quot;shingle&quot; later... in some cases it may perform OK like single ideographs or tibetan syllables do with the grammar you have. For others (Khmer, etc) I think the shingling is likely mandatory since they are really only a bit better than indexing grapheme clusters.&lt;/p&gt;

&lt;p&gt;As far as needing punctuation for shingling, the similar problem already exists. For example, after tokenizing, some discarding of information (punctuation) has been lost and its too late to do a nice shingle. practical cheating/workarounds exist for CJK (you could look at the offset or something and cheat, to figure out that they were adjacent), but for something like Tibetan the type of punctuation itself is important: the tsheg being unambiguous syllable separator, but ambiguous word separator, but the shad or whitespace being both. &lt;/p&gt;

&lt;p&gt;Here is the paper I brought up at ehatcher&apos;s house recently when we were discussing tibetan, that recommends this syllable bigram technique, where the shingling is dependent on the original punctuation: &lt;a href=&quot;http://terpconnect.umd.edu/~oard/pdf/iral00b.pdf&quot; class=&quot;external-link&quot;&gt;http://terpconnect.umd.edu/~oard/pdf/iral00b.pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;One alternative for the short term would be to make a tokenfilter that hooks into the ICUTokenizer logic but looks for Complex_Context, or similar. I definitely agree it would be best if standardtokenizer worked the best out of box without doing something like this.&lt;/p&gt;

&lt;p&gt;Finally, I think its worth considering a lot of this as a special case of a larger problem that affects even english. For a lot of users, punctuation such as the hyphen in english might have some special meaning and they might want to shingle or something else in that case too. Its a general problem with tokenstreams that the tokenizer often discards this information and the filters are left with only a partial picture. Some ideas to improve it would be to make use of properties like &lt;span class=&quot;error&quot;&gt;&amp;#91;:Terminal_Punctuation=Yes:&amp;#93;&lt;/span&gt; somehow, or to try to integrate Sentence segmentation.&lt;/p&gt;</comment>
                    <comment id="12878266" author="steve_rowe" created="Sat, 12 Jun 2010 14:56:49 +0100"  >&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;NewStandardTokenizer is not quite finished; I plan on stealing Robert&apos;s Southeast Asian (Lao, Myanmar, Khmer) syllabification routine&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Curious, what is your plan here? Do you plan to somehow &quot;jflex-#include&quot; these into the grammar so that these are longest-matched instead of the Complex_Context rule?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Sorry, I haven&apos;t looked at the details yet, but roughly, yes, what you said.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;How to handle the cases where the grammar cannot be forward-only deterministic matching? (at least i don&apos;t see how it could be, but maybe). e.g. the lao cases where some backtracking is needed... and the combining class reordering needed for real-world text?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I was thinking of trying to make regex versions of all of these, and failing that, recognize chunks that need special handling, and do that outside of matching in methods in the tokenizer class.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Curious what would you plan to index for Thai, words? a grammar for TCC?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;You had mentioned wanting to make a Thai syllabification routine - I was thinking that either you or I would do this.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Also, some of these syllable techniques are probably not very good for search without doing a &quot;shingle&quot; later... in some cases it may perform OK like single ideographs or tibetan syllables do with the grammar you have. For others (Khmer, etc) I think the shingling is likely mandatory since they are really only a bit better than indexing grapheme clusters.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I&apos;m thinking of leaving shingling for later, using the conditional branching filter idea (&lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2470&quot; title=&quot;Add conditional braching/merging to Lucene&amp;#39;s analysis pipeline&quot;&gt;LUCENE-2470&lt;/a&gt;) based on token type.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;As far as needing punctuation for shingling, the similar problem already exists. For example, after tokenizing, some discarding of information (punctuation) has been lost and its too late to do a nice shingle. practical cheating/workarounds exist for CJK (you could look at the offset or something and cheat, to figure out that they were adjacent), but for something like Tibetan the type of punctuation itself is important: the tsheg being unambiguous syllable separator, but ambiguous word separator, but the shad or whitespace being both.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;You&apos;re arguing either for in-tokenizer shingling or passing non-tokenized data out of the tokenizer in addition to the tokens.  Hmm.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Here is the paper I brought up at ehatcher&apos;s house recently when we were discussing tibetan, that recommends this syllable bigram technique, where the shingling is dependent on the original punctuation: &lt;a href=&quot;http://terpconnect.umd.edu/~oard/pdf/iral00b.pdf&quot; class=&quot;external-link&quot;&gt;http://terpconnect.umd.edu/~oard/pdf/iral00b.pdf&lt;/a&gt;&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Interesting paper. With syllable n-grams (in Tibetan anyway), you trade off (quadrupled) index size for word segmentation, but otherwise, these work equally well.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;One alternative for the short term would be to make a tokenfilter that hooks into the ICUTokenizer logic but looks for Complex_Context, or similar. I definitely agree it would be best if standardtokenizer worked the best out of box without doing something like this.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yeah, I&apos;d rather build it into the new StandardTokenizer.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Finally, I think its worth considering a lot of this as a special case of a larger problem that affects even english. For a lot of users, punctuation such as the hyphen in english might have some special meaning and they might want to shingle or something else in that case too. Its a general problem with tokenstreams that the tokenizer often discards this information and the filters are left with only a partial picture. Some ideas to improve it would be to make use of properties like &lt;span class=&quot;error&quot;&gt;&amp;#91;:Terminal_Punctuation=Yes:&amp;#93;&lt;/span&gt; somehow, or to try to integrate Sentence segmentation.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I don&apos;t understand how Sentence segmentation could help?&lt;/p&gt;

&lt;p&gt;One other possibility is to return &lt;b&gt;everything&lt;/b&gt; from the tokenizer, marking the non-tokens with an appropriate type, similar to how the ICU tokenizer works.  This has the unfortunate side effect of &lt;b&gt;requiring&lt;/b&gt; post-tokenization filtering to discard non-tokens.&lt;/p&gt;</comment>
                    <comment id="12878268" author="rcmuir" created="Sat, 12 Jun 2010 15:15:23 +0100"  >&lt;blockquote&gt;&lt;p&gt;You had mentioned wanting to make a Thai syllabification routine - I was thinking that either you or I would do this.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;OK, this makes sense. &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;You&apos;re arguing either for in-tokenizer shingling or passing non-tokenized data out of the tokenizer in addition to the tokens. Hmm.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Or attributes that mark sentence boundaries. or bumped position increments for sentence boundaries (that also prevent phrase searches across sentences). or maybe other ideas.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Interesting paper. With syllable n-grams (in Tibetan anyway), you trade off (quadrupled) index size for word segmentation, but otherwise, these work equally well.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Careful, the way they did the measurement only tells us that neither one is absolute shit, but i dont think its clear yet they are equal.&lt;br/&gt;
either way, the argument in the paper is for bigrams (n=2)... how is this quadrupled index size? its just like CJKTokenizer...&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;I don&apos;t understand how Sentence segmentation could help?&lt;/p&gt;

&lt;p&gt;One other possibility is to return everything from the tokenizer, marking the non-tokens with an appropriate type, similar to how the ICU tokenizer works. This has the unfortunate side effect of requiring post-tokenization filtering to discard non-tokens.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Right, but it could be attributes or position increments for sentence boundaries too. then you just wouldnt shingle across missing position increments, and phrase queries wouldnt match across sentence boundaries either.&lt;/p&gt;

&lt;p&gt;In my opinion, I think the patch here already solves a lot of problems on its own, and I suggest we explore these ideas later (including thai etc) in a separate issue. With the patch as-is now, people can use the ThaiWordFilter. If they need support for the other languages, they have ICUTokenizer as a workaround. We could think about how to do the more complex stuff in more general ways (sentence seg., conditional branching, etc).&lt;/p&gt;

&lt;p&gt;In general i&apos;d like to think that UAX#29 sentence segmentation, implemented nicely, would be a cool feature that could help with some of these problems, and maybe other problems too. Perhaps it could be re-used by highlighting etc as well.&lt;/p&gt;
</comment>
                    <comment id="12878274" author="steve_rowe" created="Sat, 12 Jun 2010 16:22:31 +0100"  >&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;Interesting paper. With syllable n-grams (in Tibetan anyway), you trade off (quadrupled) index size for word segmentation, but otherwise, these work equally well.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Careful, the way they did the measurement only tells us that neither one is absolute shit, but i dont think its clear yet they are equal.&lt;br/&gt;
either way, the argument in the paper is for bigrams (n=2)... &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes, you&apos;re right - fine-grained performance comparisons are inappropriate here.  You&apos;ve said for other language(s?) that unigram/bigram combo works best - too bad they didn&apos;t test that here.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;how is this quadrupled index size? its just like CJKTokenizer...&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;From the paper:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;As has been observed in other languages &lt;span class=&quot;error&quot;&gt;&amp;#91;Miller et al., 2000&amp;#93;&lt;/span&gt;, ngram indexing resulted in explosive growth in the number of terms with increasing n. The index size for word-based indexing was less than one quarter of that of syllable bigrams.&lt;/p&gt;&lt;/blockquote&gt;

&lt;blockquote&gt;&lt;p&gt;In general i&apos;d like to think that UAX#29 sentence segmentation, implemented nicely, would be a cool feature that could help with some of these problems, and maybe other problems too.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;You mentioned it would be useful to eliminate phrase matches across sentence boundaries - what other problems would it solve?&lt;/p&gt;</comment>
                    <comment id="12878275" author="rcmuir" created="Sat, 12 Jun 2010 16:32:15 +0100"  >&lt;blockquote&gt;&lt;p&gt;Yes, you&apos;re right - fine-grained performance comparisons are inappropriate here. You&apos;ve said for other language(s?) that unigram/bigram combo works best - too bad they didn&apos;t test that here.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;agreed!&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;You mentioned it would be useful to eliminate phrase matches across sentence boundaries - what other problems would it solve?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;in addition to inhibiting phrase matches, the sentence boundaries themselves (however we would represent them) could be used by later filters: such as inhibiting shingle generation, inhibiting multi-word synonym generation, ... I am sure there are some other ways too that don&apos;t immediately come to mind. &lt;/p&gt;

&lt;p&gt;at the moment the cleanest way I can think of doing this would be to bump the position increment, but who knows. there doesnt&apos; seem to be a de-facto way of doing this, since nothing in lucene out of box implements or uses sentence boundaries really, which is sad!&lt;/p&gt;</comment>
                    <comment id="12878276" author="rcmuir" created="Sat, 12 Jun 2010 16:43:00 +0100"  >&lt;p&gt;by the way Steven, one alternative idea i had before for this was to have a jflex or rbbi-powered charfilter for sentences.&lt;/p&gt;

&lt;p&gt;you could provide it with string constants in the ctor to replace sentence boundaries, to add position increments just add these to your stopfilter.&lt;/p&gt;

&lt;p&gt;the advantage to this would be that you could use it with other tokenizers by using this special token (i guess just be careful which one you use!).&lt;/p&gt;

&lt;p&gt;sorry to stray off topic a bit with this, but i think its sorta a missing piece thats relevant and becomes more important with ComplexContext &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                    <comment id="12878277" author="steve_rowe" created="Sat, 12 Jun 2010 16:53:53 +0100"  >&lt;p&gt;I&apos;m looking at UAX#29 sentence breaking rules, and this one looks suspicious to me:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Break after paragraph separators.&lt;br/&gt;
SB4. 	Sep | CR | LF 	&#247; 	 &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Lots of text I look at includes newlines that don&apos;t indicate paragraph boundaries.  In the implementations of sentence breaking that I&apos;ve done, I always use double newlines for this purpose.  Thoughts?&lt;/p&gt;

&lt;p&gt;I&apos;m thinking that it would be difficult to (correctly) incorporate sentence-boundary rules directly into the existing word-boundary rules.  Maybe a two-pass arrangement, where the sentence-boundary detector passes sentences as complete inputs to a word-boundary detector?&lt;/p&gt;</comment>
                    <comment id="12878279" author="rcmuir" created="Sat, 12 Jun 2010 17:01:10 +0100"  >&lt;blockquote&gt;&lt;p&gt;Lots of text I look at includes newlines that don&apos;t indicate paragraph boundaries.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;What is this text? Some manually-wrapped text? &lt;/p&gt;

&lt;p&gt;I mean, i guess the whole point is a reasonable default, yet tailorable with a grammar.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Maybe a two-pass arrangement, where the sentence-boundary detector passes sentences as complete inputs to a word-boundary detector?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Well this is why i liked the charfilter idea. then its separate and optional, and you can do what you want with the sentence boundary indicator strings.&lt;/p&gt;</comment>
                    <comment id="12878282" author="steve_rowe" created="Sat, 12 Jun 2010 17:16:41 +0100"  >&lt;blockquote&gt;&lt;p&gt;by the way Steven, one alternative idea i had before for this was to have a jflex or rbbi-powered charfilter for sentences.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;nice idea - composition becomes simpler.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;you could provide it with string constants in the ctor to replace sentence boundaries, to add position increments just add these to your stopfilter.&lt;/p&gt;

&lt;p&gt;the advantage to this would be that you could use it with other tokenizers by using this special token (i guess just be careful which one you use!).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Why not just insert &lt;tt&gt;U+2029 PARAGRAPH SEPARATOR (PS)&lt;/tt&gt;?  Then it will also trigger word boundaries, and tokenizers that care about appropriately responding to it can specialize for just this one, instead of having to also be aware of whatever it was that the user specified in the ctor to the charfilter.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;sorry to stray off topic a bit with this, but i think its sorta a missing piece thats relevant and becomes more important with ComplexContext&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I like where this is going - toward a solid general solution.&lt;/p&gt;

&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;Lots of text I look at includes newlines that don&apos;t indicate paragraph boundaries.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;What is this text? Some manually-wrapped text?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Email.  Source code.  TREC collections (I think - don&apos;t have any right here with me).  And yes, manually generated and wrapped text.  Isn&apos;t most text manually generated? &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                    <comment id="12878283" author="rcmuir" created="Sat, 12 Jun 2010 17:25:21 +0100"  >&lt;blockquote&gt;&lt;p&gt;Why not just insert U+2029 PARAGRAPH SEPARATOR (PS)?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I would argue because its a sentence boundary, not a paragraph boundary &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;But i thought it would be best to just allow the user to specify the replacement string (which could be just U+2029 if you want).&lt;br/&gt;
They could also use &quot;&amp;lt;boundary/&amp;gt;&quot; or something entirely different.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;and tokenizers that care about appropriately responding to it can specialize for just this one, instead of having to also be aware of whatever it was that the user specified in the ctor to the charfilter.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;well, by default these filters could just work with position increments appropriately, and you add whatever string you use to a stopword filter to create these position increments.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I like where this is going - toward a solid general solution.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Good, if we get some sorta plan we should open a new JIRA issue i think.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Email. Source code. TREC collections (I think - don&apos;t have any right here with me). And yes, manually generated and wrapped text. Isn&apos;t most text manually generated?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Right, but unicode encodes character &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; So things like text wrapping in my opinion belongs in the display component, and not in a character encoding model... most modern text in webpages etc isnt manually wrapped like this.&lt;/p&gt;

&lt;p&gt;I think our default implementation should be for unicode text. for the non-unicode text you speak of, you can just tailor the default rules.&lt;/p&gt;


</comment>
                    <comment id="12878307" author="steve_rowe" created="Sat, 12 Jun 2010 21:12:54 +0100"  >&lt;p&gt;Ok, so for sentence boundaries, we&apos;re talking about a separate composable implementation.&lt;/p&gt;

&lt;p&gt;What, then, will the replacement for StandardAnalyzer be?  This issue needs to include a substitute definition when replacing StandardTokenizer.&lt;/p&gt;

&lt;p&gt; Which of these should be included, in addition to NewStandardTokenizer?:&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;SentenceBoundaryCharFilter (clunky name, but descriptive)&lt;/li&gt;
	&lt;li&gt;LowerCaseFilter&lt;/li&gt;
	&lt;li&gt;StopFilter&lt;/li&gt;
&lt;/ol&gt;
</comment>
                    <comment id="12878309" author="rcmuir" created="Sat, 12 Jun 2010 21:30:29 +0100"  >&lt;blockquote&gt;&lt;p&gt;Which of these should be included, in addition to NewStandardTokenizer?:&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I would say only lowercase + stop.&lt;/p&gt;

&lt;p&gt;the charfilter would just be another optional charfilter, like html-stripping.&lt;br/&gt;
I don&apos;t think it should be enabled by standardanalyzer by default, especially for performance reasons.&lt;/p&gt;</comment>
                    <comment id="12878312" author="rcmuir" created="Sat, 12 Jun 2010 21:39:44 +0100"  >&lt;p&gt;OK I created &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2498&quot; title=&quot;add sentence boundary charfilter&quot;&gt;LUCENE-2498&lt;/a&gt; for the sentence boundary charfilter idea. &lt;/p&gt;

&lt;p&gt;I think this is really unrelated to standardtokenizer except that its also using the same jflex functionality &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                    <comment id="12878342" author="steve_rowe" created="Sun, 13 Jun 2010 05:08:08 +0100"  >&lt;p&gt;This is the benchmarking patch brought up-to-date with trunk, and with NewStandardTokenizer added in to the list of tested tokenizers.&lt;/p&gt;

&lt;p&gt;Here are the results on my machine (Sun JDK 1.6.0_13; Windows Vista/Cygwin; best of five):&lt;/p&gt;

&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Operation&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;recsPerRun&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;rec/s&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;elapsedSec&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;NewStandardTokenizer&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1268450&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;654,852.88&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1.94&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;UAX29Tokenizer&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1268451&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;679,042.31&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1.87&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;StandardTokenizer&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1262799&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;680,021.00&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1.86&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;RBBITokenizer&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1268451&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;575,261.25&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2.20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;ICUTokenizer&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1268451&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;557,315.88&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2.28&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;


&lt;p&gt;NewStandardTokenizer is consistently slower than UAX29Tokenizer and StandardTokenizer, but still faster than the ICU implementation; it appears that URL and Email tokenization have slowed things down a little bit.  IMHO, recognizing them is worth taking a small hit in throughput.&lt;/p&gt;</comment>
                    <comment id="12879589" author="steve_rowe" created="Thu, 17 Jun 2010 01:00:18 +0100"  >&lt;p&gt;After a discussion with Robert on #lucene, I think this issue is complete - we can add more stuff later in a separate issue.&lt;/p&gt;</comment>
                    <comment id="12883594" author="rcmuir" created="Tue, 29 Jun 2010 17:03:38 +0100"  >&lt;p&gt;zip file of my current integration progress.&lt;/p&gt;

&lt;p&gt;the zip file is relevant to modules/analysis/common.&lt;/p&gt;

&lt;p&gt;not all the tests pass as we have to figure a few things out...&lt;br/&gt;
the first thing to figure out is TestEmails/Urls in TestStandardAnalyzer (currently commented out)&lt;/p&gt;

&lt;p&gt;the problem is how to get the bracketed rules to work without actually including the brackets in the tokens, while using StandardTokenizerInterface.&lt;/p&gt;</comment>
                    <comment id="12883659" author="steve_rowe" created="Tue, 29 Jun 2010 20:14:52 +0100"  >&lt;p&gt;Robert,&lt;/p&gt;

&lt;p&gt;Special handling for bracketed URLs makes no sense - that rule can be dropped.&lt;/p&gt;

&lt;p&gt;Bracketed emails are useful, though, since the domain in the host portion doesn&apos;t need to be a registerable domain.  I think this could be handled with two changes to the bracketed email rule.  Here it is in the form you wrote:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-quote&quot;&gt;&quot;&amp;lt;&quot;&lt;/span&gt; {EMAILaddressLoose} &lt;span class=&quot;code-quote&quot;&gt;&quot;&amp;gt;&quot;&lt;/span&gt; { &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; EMAIL_TYPE; }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Here is my suggestion:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-quote&quot;&gt;&quot;&amp;lt;&quot;&lt;/span&gt; {EMAILaddressLoose} / &lt;span class=&quot;code-quote&quot;&gt;&quot;&amp;gt;&quot;&lt;/span&gt; { ++zzStartRead; &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; EMAIL_TYPE; }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This combines incrementing the start of the matched region (&lt;tt&gt;++zzStartRead;&lt;/tt&gt;) and lookahead for the trailing angle bracket (&lt;tt&gt;/ &quot;&amp;gt;&quot;&lt;/tt&gt;).  AFAICT, directly modifying &lt;tt&gt;zzStartRead&lt;/tt&gt; shouldn&apos;t cause any problems.  After this rule completes, the trailing angle bracket will be at the beginning of the remaining text to be matched.&lt;/p&gt;</comment>
                    <comment id="12883935" author="rcmuir" created="Wed, 30 Jun 2010 15:21:30 +0100"  >&lt;p&gt;ok here is a patch file. before applying it, you have to run these commands:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;# original grammar -&amp;gt; ClassicTokenizerImpl
svn move modules/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardTokenizerImplOrig.java modules/analysis/common/src/java/org/apache/lucene/analysis/standard/ClassicTokenizerImpl.java
svn move modules/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardTokenizerImplOrig.jflex modules/analysis/common/src/java/org/apache/lucene/analysis/standard/ClassicTokenizerImpl.jflex
# this one is not needed, this patch becomes the new grammar
svn delete modules/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardTokenizerImpl31.java
svn delete modules/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardTokenizerImpl31.jflex
# expose the old tokenizer, not just via Version, but also as ClassicAnalyzer/Tokenizer/Filter
svn copy modules/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardAnalyzer.java modules/analysis/common/src/java/org/apache/lucene/analysis/standard/ClassicAnalyzer.java
svn copy modules/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardTokenizer.java modules/analysis/common/src/java/org/apache/lucene/analysis/standard/ClassicTokenizer.java
svn copy modules/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardFilter.java modules/analysis/common/src/java/org/apache/lucene/analysis/standard/ClassicFilter.java
svn copy modules/analysis/common/src/test/org/apache/lucene/analysis/core/TestStandardAnalyzer.java modules/analysis/common/src/test/org/apache/lucene/analysis/core/TestClassicAnalyzer.java
# temporarily edit solr/src/java/org/apache/solr/analysis/StandardFilterFactory.java (change the $Id hossman.... to just $Id$)
# apply the patch.
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;if you want to iterate on the patch, make your changes and generate a patch with &apos;svn --no-diff-deleted&apos;.&lt;/p&gt;

&lt;p&gt;some notes:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;patch is against 4.0, but i think we can do this in 3.1. all the back compat is preserved, etc. we just gotta figure a few things out. all the tests pass though.&lt;/li&gt;
	&lt;li&gt;The patch is large mainly because of the DFA size. I have some concerns about this... the email/url stuff seems to be the culprit, as the UAX#29 generated class is only 12KB, about the same size as our existing standardtokenizer.&lt;/li&gt;
	&lt;li&gt;I gave backwards compat (you get the old behavior) with Version, but also setup ClassicAnalyzer/Tokenizer/Filter for those that want the...not so international-friendly old version, for its company Identification, etc.&lt;/li&gt;
	&lt;li&gt;I modified token types for icu to be more consistent with this.&lt;/li&gt;
	&lt;li&gt;StandardFilter is currently a no-op for the new grammar. In my opinion this is a place to implement the &apos;more sophisticated&apos; logic that the standard mentions for certain scripts. We can use token types (IDEOGRAPHIC, SOUTHEAST_ASIAN) to drive this. This way the standardanalyzer is a reasonable tokenizer for most languages.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;So, not completely sure this is the best approach, but it is one... the patch is still rough around the edges but at least now we can iterate more easily on it.&lt;/p&gt;</comment>
                    <comment id="12884103" author="rcmuir" created="Thu, 1 Jul 2010 01:02:52 +0100"  >&lt;p&gt;attached is an updated patch.&lt;/p&gt;

&lt;p&gt;Steven and I debugged the large DFA size and reduced it somewhat (.class file drops from 167,945 bytes to 52,399 bytes).&lt;/p&gt;</comment>
                    <comment id="12887226" author="steve_rowe" created="Sun, 11 Jul 2010 21:41:25 +0100"  >&lt;p&gt;Attaching benchmark patch brought up-to-date with Robert&apos;s last patch.&lt;/p&gt;

&lt;p&gt;Here are the current results on my machine:&lt;/p&gt;

&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Operation&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;recsPerRun&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;rec/s&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;elapsedSec&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;ClassicTokenizer&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1262799&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;644,943.31&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1.96&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;ICUTokenizer&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1268451&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;546,040.06&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2.32&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;RBBITokenizer&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1268451&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;570,090.31&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2.22&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;StandardTokenizer&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1268450&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;659,963.56&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1.92&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;UAX29Tokenizer&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1268451&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;643,883.75&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1.97&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
</comment>
                    <comment id="12887354" author="rcmuir" created="Mon, 12 Jul 2010 13:02:49 +0100"  >&lt;p&gt;Thanks Steven! Looks to me like we have resolved the perf problem?!&lt;/p&gt;</comment>
                    <comment id="12887366" author="steve_rowe" created="Mon, 12 Jul 2010 14:29:06 +0100"  >&lt;blockquote&gt;&lt;p&gt;Thanks Steven! Looks to me like we have resolved the perf problem?! &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I don&apos;t know... I&apos;ll run it a few more times tonight and see if it&apos;s consistent.&lt;/p&gt;</comment>
                    <comment id="12887741" author="steve_rowe" created="Tue, 13 Jul 2010 13:54:41 +0100"  >&lt;p&gt;I ran it three more times, and it appears that the difference between ClassicTokenizer, UAX29Tokenizer, and the new StandardTokenizer is in the noise:&lt;/p&gt;

&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Operation&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;recsPerRun&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;rec/s&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;elapsedSec&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;ClassicTokenizer&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1262799&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;665,682.12&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1.90&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;ICUTokenizer&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1268451&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;553,666.94&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2.29&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;RBBITokenizer&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1268451&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;575,261.25&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2.20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;StandardTokenizer&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1268450&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;658,935.06&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1.92&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;UAX29Tokenizer&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1268451&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;642,579.00&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1.97&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Operation&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;recsPerRun&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;rec/s&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;elapsedSec&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;ClassicTokenizer&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1262799&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;668,501.31&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1.89&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;ICUTokenizer&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1268451&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;546,275.19&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2.32&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;RBBITokenizer&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1268451&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;563,255.31&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2.25&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;StandardTokenizer&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1268450&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;651,824.25&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1.95&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;UAX29Tokenizer&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1268451&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;664,806.62&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1.91&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Operation&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;recsPerRun&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;rec/s&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;elapsedSec&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;ClassicTokenizer&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1262799&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;674,932.69&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1.87&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;ICUTokenizer&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1268451&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;541,841.50&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2.34&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;RBBITokenizer&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1268451&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;586,431.38&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2.16&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;StandardTokenizer&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1268450&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;635,814.56&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2.00&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;UAX29Tokenizer&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1268451&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;650,487.69&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1.95&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
</comment>
                    <comment id="12888373" author="steve_rowe" created="Wed, 14 Jul 2010 16:05:33 +0100"  >&lt;p&gt;I tried increasing the number of documents in the benchmark alg from 10k to 50k, but apparently 50k docs was too much to fit into my OS FS cache, because it thrashed the whole time, and performance was more than an order of magnitude worse.&lt;/p&gt;

&lt;p&gt;I increased the number of rounds from 5 to 25, and increased the number of documents from 10k to 20k - below are three runs with these settings:&lt;/p&gt;

&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Operation&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;recsPerRun&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;rec/s&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;elapsedSec&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;ClassicTokenizer&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2467769&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;669,134.75&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;3.69&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;ICUTokenizer&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2481688&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;548,924.56&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;4.52&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;RBBITokenizer&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2481688&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;573,270.50&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;4.33&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;StandardTokenizer&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2481687&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;656,704.69&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;3.78&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;UAX29Tokenizer&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2481688&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;661,254.44&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;3.75&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Operation&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;recsPerRun&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;rec/s&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;elapsedSec&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;ClassicTokenizer&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2467769&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;667,867.12&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;3.69&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;ICUTokenizer&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2481688&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;546,025.94&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;4.54&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;RBBITokenizer&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2481688&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;576,466.44&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;4.30&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;StandardTokenizer&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2481687&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;656,878.50&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;3.78&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;UAX29Tokenizer&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2481688&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;665,510.31&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;3.73&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Operation&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;recsPerRun&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;rec/s&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;elapsedSec&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;ClassicTokenizer&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2467769&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;664,092.81&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;3.72&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;ICUTokenizer&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2481688&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;551,486.25&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;4.50&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;RBBITokenizer&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2481688&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;581,191.56&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;4.27&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;StandardTokenizer&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2481687&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;655,317.38&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;3.79&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;UAX29Tokenizer&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2481688&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;663,021.12&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;3.74&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;


&lt;p&gt;These are more consistent.  I think the ~3% performance hit for the new StandardTokenizer over ClassicTokenizer is acceptable.&lt;/p&gt;</comment>
                    <comment id="12888382" author="rcmuir" created="Wed, 14 Jul 2010 16:28:35 +0100"  >&lt;p&gt;Steven, thanks for all these benchmarks.&lt;/p&gt;

&lt;p&gt;I think any perf issues are resolved, I also think the DFA size is more manageable from our previous changes, and arguably ok now (ill defer to your judgement on whether we need to attack this more though).&lt;/p&gt;

&lt;p&gt;I have a few more questions:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Are there still ipv6 issues you wanted to address? I cant remember (lost in the std documents) but I think you found grammar improvements?&lt;/li&gt;
	&lt;li&gt;What about standardfilter with the new scheme? The previous impl does some &apos;cleanup&apos; on the tokenizer, in the latest patch its a TODO/no-op for Version &amp;gt;= 3.1. Are there any email/url/other things we need to do here? on the unicode side, i think if we want to do anything here, it should be the &quot;more sophisticated mechanism&quot; for the SE asian (as then its name Standard would also make sense)... leave as a no-op for now with Version &amp;gt;= 3.1?&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="12888490" author="steve_rowe" created="Wed, 14 Jul 2010 20:05:09 +0100"  >&lt;blockquote&gt;&lt;p&gt;I think any perf issues are resolved, I also think the DFA size is more manageable from our previous changes, and arguably ok now (ill defer to your judgement on whether we need to attack this more though).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;To address the DFA size I want to try your previous suggestion of a simpler IPv6 regex in the JFlex grammar, then full validation in the action via a java.util.regex NFA.   You&apos;ve previously said that you thought returning a new type like INVALID_URL would be fine, but I&apos;d prefer not to do that - I want to try backing out and trying an alternate path if this action-based validation fails.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;What about standardfilter with the new scheme?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I don&apos;t have an opinion on this one, except that it seems a little weird to have a no-op filter in the standard analyzer chain.&lt;/p&gt;
</comment>
                    <comment id="12889760" author="steve_rowe" created="Mon, 19 Jul 2010 06:18:32 +0100"  >&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;I think any perf issues are resolved, I also think the DFA size is more manageable from our previous changes, and arguably ok now (ill defer to your judgement on whether we need to attack this more though).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;To address the DFA size I want to try your previous suggestion of a simpler IPv6 regex in the JFlex grammar, then full validation in the action via a java.util.regex NFA. You&apos;ve previously said that you thought returning a new type like INVALID_URL would be fine, but I&apos;d prefer not to do that - I want to try backing out and trying an alternate path if this action-based validation fails.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The attached &lt;tt&gt;StandardTokenizerImpl.jflex&lt;/tt&gt; is the result of my attempt, which appears to be successful - tests all pass.&lt;/p&gt;

&lt;p&gt;However, the resultant .class file size is even larger than before: 67,947 bytes.&lt;/p&gt;

&lt;p&gt;I give up: I think we should go with the full-blown IPv6 regex as part of the DFA.&lt;/p&gt;</comment>
                    <comment id="12889769" author="steve_rowe" created="Mon, 19 Jul 2010 07:44:29 +0100"  >&lt;p&gt;This patch contains 3 modifications: &lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;The &lt;tt&gt;IPv6Address&lt;/tt&gt; macro in &lt;tt&gt;StandardTokenizerImpl.jflex&lt;/tt&gt; now makes everything in front of the double colon optional, so that e.g. &quot;::&quot; alone is a valid address.&lt;/li&gt;
	&lt;li&gt;The &lt;tt&gt;EMAILbracketedHost&lt;/tt&gt; macro in &lt;tt&gt;StandardTokenizerImpl.jflex&lt;/tt&gt; now contains IPv6 and IPv4 addresses, along with a comment about how DFA minimization keeps the size of the resulting DFA in check.&lt;/li&gt;
	&lt;li&gt;Renamed the &lt;tt&gt;EMAILaddressStrict&lt;/tt&gt; macro to &lt;tt&gt;EMAIL&lt;/tt&gt; in &lt;tt&gt;StandardTokenizerImpl.jflex&lt;/tt&gt;.&lt;/li&gt;
	&lt;li&gt;The &lt;tt&gt;root.zone&lt;/tt&gt; file format has changed (hunh? why? I don&apos;t know anything about DNS...), so I modified &lt;tt&gt;GenerateJflexTLDMacros.java&lt;/tt&gt; to parse the current format in addition to the previous format.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;This version looks roughly the same in terms of performance - below are the numbers for the 25 round, 20k doc benchmark:&lt;/p&gt;

&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Operation&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;recsPerRun&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;rec/s&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;elapsedSec&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;ClassicTokenizer&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2467769&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;661,245.69&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;3.73&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;ICUTokenizer&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2481688&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;544,827.25&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;4.55&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;RBBITokenizer&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2481688&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;571,817.50&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;4.34&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;StandardTokenizer&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2481687&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;650,848.94&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;3.81&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;UAX29Tokenizer&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2481688&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;655,317.69&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;3.79&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;


&lt;p&gt;For some reason, the size of the .class file for &lt;tt&gt;StandardTokenizerImpl.jflex&lt;/tt&gt; is smaller: 51,798 bytes.&lt;/p&gt;</comment>
                    <comment id="12892191" author="steve_rowe" created="Mon, 26 Jul 2010 07:18:22 +0100"  >&lt;p&gt;Attached patch includes a Perl script to generate a test based on Unicode.org&apos;s WordBreakTest.txt UAX#29 test sequences, along with the java source generated by the Perl script.  Both UAX29Tokenizer and StandardTokenizerImpl are tested, and all Lucene and Solr tests pass.  I added a note to modules/analyzer/NOTICE.txt about the Unicode.org data files used in creating the test class.&lt;/p&gt;

&lt;p&gt;This test suite turned up a problem in both tested grammars: the WORD_TYPE rule could match zero characters, and so was in certain cases involving underscores returning a zero-length token instead of end-of-stream.  I fixed the issue by changing the rule in both grammars to require at least one character for a match to succeed.  All test sequences are now successfully tokenized.&lt;/p&gt;

&lt;p&gt;I attempted to also test ICUAnalyzer, but since it downcases, the expected tokens are incorrect in some cases.  I didn&apos;t pursue it further.&lt;/p&gt;

&lt;p&gt;I ran the best-of-25-rounds/20k docs benchmark, and the grammar change has not noticeably affected the results.&lt;/p&gt;</comment>
                    <comment id="12892196" author="steve_rowe" created="Mon, 26 Jul 2010 07:27:42 +0100"  >&lt;p&gt;Removed unnecessarily re-generated WikipediaTokenizerImpl.java in the previous patch from this patch.&lt;/p&gt;</comment>
                    <comment id="12904586" author="simonw" created="Tue, 31 Aug 2010 11:21:01 +0100"  >&lt;p&gt;Last update is a month ago - any idea how far away this is from being committable?&lt;/p&gt;</comment>
                    <comment id="12904616" author="steve_rowe" created="Tue, 31 Aug 2010 13:52:17 +0100"  >&lt;blockquote&gt;&lt;p&gt;Last update is a month ago - any idea how far away this is from being committable? &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Trunk version functionality is complete.  Needs docs and backporting to 3.x branch.  &lt;/p&gt;

&lt;p&gt;I&apos;m finishing up &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2611&quot; title=&quot;IntelliJ IDEA and Eclipse setup&quot;&gt;&lt;del&gt;LUCENE-2611&lt;/del&gt;&lt;/a&gt; (to make backporting a little less painful), and then I&apos;ll get back to this issue.&lt;/p&gt;

&lt;p&gt;Rough completion estimate for this issue: 2010-09-13 @ 02:37 GMT-5.&lt;/p&gt;</comment>
                    <comment id="12904662" author="simonw" created="Tue, 31 Aug 2010 16:44:54 +0100"  >&lt;blockquote&gt;&lt;p&gt;Rough completion estimate for this issue: 2010-09-13 @ 02:37 GMT-5.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Awesome!&lt;/p&gt;</comment>
                    <comment id="12904837" author="rcmuir" created="Wed, 1 Sep 2010 00:56:16 +0100"  >&lt;blockquote&gt;&lt;p&gt;Trunk version functionality is complete. Needs docs and backporting to 3.x branch.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I agree, the testing is now very nice. &lt;br/&gt;
For example, when we want to bump to Unicode 6.0 we can autogenerate a test class from the 6.0 data files with the perl script.&lt;br/&gt;
Great work.&lt;/p&gt;</comment>
                    <comment id="12904856" author="rcmuir" created="Wed, 1 Sep 2010 02:04:30 +0100"  >&lt;blockquote&gt;&lt;p&gt;I&apos;m finishing up &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2611&quot; title=&quot;IntelliJ IDEA and Eclipse setup&quot;&gt;&lt;del&gt;LUCENE-2611&lt;/del&gt;&lt;/a&gt; (to make backporting a little less painful), and then I&apos;ll get back to this issue.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;By the way, I dont think you need to produce an explicit 3.x patch?&lt;br/&gt;
we should be able to svn merge without much trouble I think.&lt;/p&gt;</comment>
                    <comment id="12905055" author="steve_rowe" created="Wed, 1 Sep 2010 16:40:59 +0100"  >&lt;blockquote&gt;
&lt;p&gt;By the way, I dont think you need to produce an explicit 3.x patch?&lt;br/&gt;
we should be able to svn merge without much trouble I think.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Great, for some reason I thought you had said that backporting would require lots of decisions, so I assumed it would require a separate patch.&lt;/p&gt;

&lt;p&gt;That leaves documentation.  I think I need a MIGRATE.txt entry, some package-level documentation, and notes cross-referencing from ClassicTokenizer/Analyzer to StandardTokenizer/Analyzer and vice-versa.  Anything else?&lt;/p&gt;</comment>
                    <comment id="12905062" author="rcmuir" created="Wed, 1 Sep 2010 16:43:53 +0100"  >&lt;blockquote&gt;&lt;p&gt;That leaves documentation. I think I need a MIGRATE.txt entry, some package-level documentation, and notes cross-referencing from ClassicTokenizer/Analyzer to StandardTokenizer/Analyzer and vice-versa. Anything else?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Agreed, though the change is completely backwards compatible, so I don&apos;t know if we need a MIGRATE.txt entry?&lt;/p&gt;

&lt;p&gt;(separately I realize its a big change, but there is no back compat issue)&lt;/p&gt;</comment>
                    <comment id="12909647" author="steve_rowe" created="Wed, 15 Sep 2010 09:07:54 +0100"  >&lt;p&gt;Updated to trunk.  All tests pass.  Documentation improved at package and class level.  modules/analysis/CHANGES.txt entry included.&lt;/p&gt;

&lt;p&gt;I think this is ready to commit.&lt;/p&gt;</comment>
                    <comment id="12909759" author="rcmuir" created="Wed, 15 Sep 2010 16:02:24 +0100"  >&lt;blockquote&gt;&lt;p&gt;I think this is ready to commit.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think so too, i applied the svn moves and the patch and all tests pass.&lt;/p&gt;

&lt;p&gt;One last question, it might be reasonable to move ClassicTokenizer and friends to .classic package?&lt;br/&gt;
There is nothing standards-based about them at all and it makes the .standard directory a little confusing.&lt;/p&gt;

&lt;p&gt;To do this i would have to make StandardTokenizerInterface public, but it could marked @lucene.internal.&lt;/p&gt;</comment>
                    <comment id="12909760" author="rcmuir" created="Wed, 15 Sep 2010 16:08:32 +0100"  >&lt;blockquote&gt;&lt;p&gt;One last question, it might be reasonable to move ClassicTokenizer and friends to .classic package?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;by the way, if we decide this is best, i would like to open a new issue for it.&lt;br/&gt;
we don&apos;t have to do everything in one step, and currently this patch cleanly applies with the svn move instructions.&lt;/p&gt;

&lt;p&gt;so I would like to commit this patch in a few days as-is if they are no objections.&lt;/p&gt;

&lt;p&gt;if we want to improve packaging lets open a followup-issue.&lt;/p&gt;</comment>
                    <comment id="12909770" author="steve_rowe" created="Wed, 15 Sep 2010 16:27:30 +0100"  >&lt;blockquote&gt;&lt;p&gt;One last question, it might be reasonable to move ClassicTokenizer and friends to .classic package?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I agree with your arguments about moving to .classic package.  I think new users won&apos;t care about what StandardTokenizer/Analyzer used to be.&lt;/p&gt;

&lt;p&gt;My only concern here is existing users&apos; upgrade experience - users should be able to continue using the ClassicTokenizer if they want to keep current behavior.  Right now, they can do that by setting Version to 3.0 in the constructor to StandardTokenizer/Analyzer.  I think this should remain the case until Lucene version 5.0.&lt;/p&gt;</comment>
                    <comment id="12909772" author="rcmuir" created="Wed, 15 Sep 2010 16:32:51 +0100"  >&lt;blockquote&gt;
&lt;p&gt;My only concern here is existing users&apos; upgrade experience - users should be able to continue using the ClassicTokenizer if they want to keep current behavior. Right now, they can do that by setting Version to 3.0 in the constructor to StandardTokenizer/Analyzer. I think this should remain the case until Lucene version 5.0.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I agree completely, i think we can do this though with the Classic stuff in a separate package? (like we can have both)&lt;/p&gt;</comment>
                    <comment id="12909776" author="steve_rowe" created="Wed, 15 Sep 2010 16:39:25 +0100"  >&lt;blockquote&gt;&lt;p&gt;I agree completely, i think we can do this though with the Classic stuff in a separate package? (like we can have both)&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Right, I didn&apos;t mean to say that moving the Classic stuff out of .standard was antithetical to preserving Classic functionality in StandardTokenizer - I just wanted to make sure that we agreed that that move doesn&apos;t mean complete separation (yet).  Sounds like we agree.&lt;/p&gt;</comment>
                    <comment id="12915616" author="simonw" created="Tue, 28 Sep 2010 06:21:47 +0100"  >&lt;blockquote&gt;&lt;p&gt;Assignee: Steven Rowe  (was: Robert Muir)&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Yay! &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                    <comment id="12915632" author="steve_rowe" created="Tue, 28 Sep 2010 07:06:48 +0100"  >&lt;p&gt;Sync&apos;d to trunk (TestThaiAnalyzer.java had conflicts).  All tests pass.  Committing shortly.&lt;/p&gt;</comment>
                    <comment id="12915636" author="steve_rowe" created="Tue, 28 Sep 2010 07:19:26 +0100"  >&lt;p&gt;Committed to trunk r1002032.&lt;/p&gt;

&lt;p&gt;I&apos;ll work on merging to the 3.X branch tomorrow.&lt;/p&gt;</comment>
                    <comment id="12916012" author="steve_rowe" created="Wed, 29 Sep 2010 05:46:52 +0100"  >&lt;p&gt;Backported to 3.x branch revision 1002468&lt;/p&gt;</comment>
                    <comment id="12929325" author="rcmuir" created="Sun, 7 Nov 2010 10:48:10 +0000"  >&lt;p&gt;I&apos;d like to re-open this issue.&lt;/p&gt;

&lt;p&gt;I think that full urls as tokens is not a good default for StandardTokenizer, because i don&apos;t think users ever search&lt;br/&gt;
on full URLS. its also dangerous, many apps that upgrade will find themselves with huge terms dictionaries, &lt;br/&gt;
and different performance characteristics.&lt;/p&gt;

&lt;p&gt;i think it would be better if standardtokenizer just implemented the uax#29 algorithm. the url identification we could&lt;br/&gt;
keep as a separate tokenizer for people that want that.&lt;/p&gt;</comment>
                    <comment id="12929327" author="mikemccand" created="Sun, 7 Nov 2010 10:53:15 +0000"  >&lt;p&gt;+1&lt;/p&gt;

&lt;p&gt;When I indexed Wikipedia w/ StandardAnalyzer I saw a huge number of full-url tokens, which is just silly as a default.  Inserting WordDelimiterFilter fixed it, but, I don&apos;t think StandardTokenizer should produce whole URLs as tokens, to begin with.&lt;/p&gt;</comment>
                    <comment id="12929359" author="steve_rowe" created="Sun, 7 Nov 2010 16:08:27 +0000"  >&lt;blockquote&gt;
&lt;p&gt;I think that full urls as tokens is not a good default for StandardTokenizer, because i don&apos;t think users ever search&lt;br/&gt;
on full URLS.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Probably true, but this is a chicken and egg issue, no?  Maybe people never search on full URLs because it doesn&apos;t work, because there is no tokenization support for it?&lt;/p&gt;

&lt;p&gt;My preferred solution here, as I &lt;a href=&quot;#action_12865759&quot;&gt;said earlier in this issue&lt;/a&gt;, is to use a decomposing filter, because when people want full URLs, they can&apos;t be reassembled after the separator chars are thrown away by the tokenizer.&lt;/p&gt;

&lt;p&gt;Robert, when I mentioned the decomposition filter, you &lt;a href=&quot;#action_12865879&quot;&gt;said&lt;/a&gt; you didn&apos;t like that idea.  Do you still feel the same?&lt;/p&gt;

&lt;p&gt;I&apos;m really reluctant to drop the ability to recognize full URLs.  I agree, though, that as a default it&apos;s not good.&lt;/p&gt;</comment>
                    <comment id="12929361" author="steve_rowe" created="Sun, 7 Nov 2010 16:12:09 +0000"  >&lt;blockquote&gt;&lt;p&gt;I don&apos;t think StandardTokenizer should produce whole URLs as tokens, to begin with.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think Standard &lt;b&gt;Analyzer&lt;/b&gt; should not by default produce whole URLs as tokens.  But (yay repetition!) if the tokenizer throws away the separator chars, URLs can&apos;t be reassembled from their parts.&lt;/p&gt;

&lt;p&gt;Would a URL decomposition filter, with full URL emission turned off by default, work here?&lt;/p&gt;</comment>
                    <comment id="12929363" author="rcmuir" created="Sun, 7 Nov 2010 16:22:38 +0000"  >&lt;blockquote&gt;&lt;p&gt;because when people want full URLs, they can&apos;t be reassembled after the separator chars are thrown away by the tokenizer.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Well, i dont much like this argument, because its true about anything.&lt;br/&gt;
Indexing text for search is a lossy thing by definition.&lt;/p&gt;

&lt;p&gt;yeah, when you tokenize this stuff, you lose paragraphs, sentences, all kinds of things.&lt;br/&gt;
should we output whole paragraphs as tokens so its not lost? &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Robert, when I mentioned the decomposition filter, you said you didn&apos;t like that idea. Do you still feel the same?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Well, i said it was a can of worms, i still feel that it is complicated, yes.&lt;br/&gt;
But i mean we do have a ghetto decomposition filter (WordDelimiterFilter) already.&lt;br/&gt;
And someone can use this with the UAX#29+URLRecognizingTokenizer to index these urls in a variety of ways, including preserving the original full url too.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Would a URL decomposition filter, with full URL emission turned off by default, work here?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;It works in theory, but its confusing that its &apos;required&apos; to not get absymal tokens.&lt;br/&gt;
i would prefer we switch the situation around: make UAX#29 &apos;standardtokenizer&apos; and give the uax#29+url+email+ip+... a different name.&lt;/p&gt;

&lt;p&gt;because to me, uax#29 handles urls in nice ways, e.g. my user types &apos;facebook&apos; and they get back facebook.com&lt;br/&gt;
its certainly simple and won&apos;t blow up terms dictionaries...&lt;/p&gt;

&lt;p&gt;otherwise, creating lots of long, unique tokens (urls) by default is a serious performance trap, particularly for lucene 3.x&lt;/p&gt;
</comment>
                    <comment id="12929366" author="steve_rowe" created="Sun, 7 Nov 2010 16:38:55 +0000"  >&lt;blockquote&gt;&lt;p&gt;i would prefer we switch the situation around: make UAX#29 &apos;standardtokenizer&apos; and give the uax#29+url+email+ip+... a different name.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;UAX29Tokenizer does not have email or hostname recognition.  StandardTokenizer has long had these capabilities (though not standard-based).  Removing them would be bad.&lt;/p&gt;</comment>
                    <comment id="12929368" author="mikemccand" created="Sun, 7 Nov 2010 16:50:46 +0000"  >&lt;p&gt;Would it somehow be possible to allow multiple Tokenizers to work together?&lt;/p&gt;

&lt;p&gt;Today we only allow one (and then any number of subsequent TokenFilters) in the chain, so if your Tokenizer destroys information (eg erases the . from the host name) it&apos;s hard for subsequent TokenFilters to put them back.&lt;/p&gt;

&lt;p&gt;But if, say, we had a Tokenizer that recognizes hostnames/URLs, one that recognizes email addresses, one for proper names/places/date/time, other app dependent stuff like detecting part numbers and what not, then ideally one could simply cascade/compose these tokenizers at will to build up whatever &quot;initial&quot; tokenizer you need for you chain?&lt;/p&gt;

&lt;p&gt;I think our current lack of composability of the initial tokenizer (&quot;there can be only one&quot;) makes cases like this hard...&lt;/p&gt;</comment>
                    <comment id="12929369" author="rcmuir" created="Sun, 7 Nov 2010 16:51:28 +0000"  >&lt;blockquote&gt;&lt;p&gt;UAX29Tokenizer does not have email or hostname recognition. StandardTokenizer has long had these capabilities (though not standard-based). Removing them would be bad.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Thats true, so maybe something in the &quot;middle&quot; / &quot;compromise&quot; is better as a default.&lt;/p&gt;

&lt;p&gt;I just tend to really like plain old &quot;uax#29&quot; as a default, since its consistent with how &quot;tokenization&quot; works elsewhere in people&apos;s wordprocessors, browsers, etc&lt;br/&gt;
(e.g. control-F find, that sort of thing), where they dont know specifics of content and want to just have a reasonable default.&lt;/p&gt;

&lt;p&gt;but there might be something else we can do, too.&lt;/p&gt;</comment>
                    <comment id="12929370" author="rcmuir" created="Sun, 7 Nov 2010 16:55:58 +0000"  >&lt;blockquote&gt;
&lt;p&gt;But if, say, we had a Tokenizer that recognizes hostnames/URLs, one that recognizes email addresses, one for proper names/places/date/time, other app dependent stuff like detecting part numbers and what not, then ideally one could simply cascade/compose these tokenizers at will to build up whatever &quot;initial&quot; tokenizer you need for you chain?&lt;/p&gt;

&lt;p&gt;I think our current lack of composability of the initial tokenizer (&quot;there can be only one&quot;) makes cases like this hard...&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I agree that sounds like a &quot;cool&quot; idea to have, but at the same time, we should try to not make analysis the &quot;wonder-do-it-all&quot; machine.&lt;br/&gt;
I mean some stuff belongs in the app, and i think that includes a lot of things you mentioned... e.g. the app can do &quot;NER&quot; and pull&lt;br/&gt;
out proper names/places/dates and put them in separate fields.&lt;/p&gt;

&lt;p&gt;I don&apos;t think the analysis chain is the easiest or best place to do this, i would prefer if we tried to keep the complexity down and recognize&lt;br/&gt;
that some things (really a lot of this &quot;recognizer&quot; stuff) might be better implemented in the app.&lt;/p&gt;

</comment>
                    <comment id="12929372" author="steve_rowe" created="Sun, 7 Nov 2010 17:11:39 +0000"  >&lt;blockquote&gt;&lt;p&gt;I just tend to really like plain old &quot;uax#29&quot; as a default &lt;span class=&quot;error&quot;&gt;&amp;#91;...&amp;#93;&lt;/span&gt; i would prefer if we tried to keep the complexity down&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;So we&apos;re talking about two separate issues here: a) Lucene&apos;s default behavior; and b) Lucene&apos;s capabilities. &lt;/p&gt;

&lt;p&gt;For a), you&apos;ll have a lot of &apos;splaining to do if you drop existing functionality (e.g. email and hostname &quot;recognition&quot; &amp;#8211; where quotes indicate &quot;bad&quot; things, right? &quot;Cool&quot;!)&lt;/p&gt;

&lt;p&gt;For b), you appear to agree with Marvin Humphries about keeping the product lean and mean: complexity (a.k.a. functionality beyond the default) is bad because it creates maintenance problems.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;we should try to not make analysis the &quot;wonder-do-it-all&quot; machine.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Why not?  Why shouldn&apos;t Lucene be a catch-&lt;b&gt;all&lt;/b&gt; for &quot;cool&quot; linguistic stuff?&lt;/p&gt;</comment>
                    <comment id="12929376" author="rcmuir" created="Sun, 7 Nov 2010 17:19:46 +0000"  >&lt;blockquote&gt;&lt;p&gt;So we&apos;re talking about two separate issues here: a) Lucene&apos;s default behavior; and b) Lucene&apos;s capabilities.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;agreed!&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;For a), you&apos;ll have a lot of &apos;splaining to do if you drop existing functionality (e.g. email and hostname &quot;recognition&quot; - where quotes indicate &quot;bad&quot; things, right? &quot;Cool&quot;!)&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;to me recognizing hostnames is specific to what one application might want.&lt;br/&gt;
if you recognize www.facebook.com but my app wants to find this with a query of &apos;facebook&apos;, it cant.&lt;br/&gt;
yet if just stick to uax#29, if a user queries on www.facebook.com, and they are unsatisfied with the results,&lt;br/&gt;
that user can always &quot;refine&quot; their query by searching on &quot;www.facebook.com&quot; and they get a phrasequery.&lt;br/&gt;
I think this is pretty intuitive and users are used to this... again this is just for general defaults...&lt;/p&gt;

&lt;p&gt;and again, hostnames are just an example, why do we recognize them and not filenames?&lt;br/&gt;
yet a lot of people are happy being able to do &apos;partial filename&apos; matching and not the whole path...&lt;br/&gt;
users that are unhappy with this &apos;default&apos; behavior can use double quotes to refine their results.&lt;/p&gt;

&lt;p&gt;and in both cases, apps that need something more specific can use a custom tokenizer.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Why not? Why shouldn&apos;t Lucene be a catch-all for &quot;cool&quot; linguistic stuff?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;In this case I think analysis won&apos;t meet their needs anyway. a lot of people wanting to recognize full urls or proper names (mike&apos;s example)&lt;br/&gt;
actually want to do this in the &apos;document build&apos; and dump the extracted entities into a separate field, so they can do things like&lt;br/&gt;
facet on this field, or find other documents that refer to the same person. This is because they are trying to &apos;find structure in the unstructured&apos;,&lt;br/&gt;
but it starts to get complicated if we mix this problem with &apos;feature extraction&apos; which is what i think analysis should be.&lt;/p&gt;


</comment>
                    <comment id="12929377" author="steve_rowe" created="Sun, 7 Nov 2010 17:24:27 +0000"  >&lt;blockquote&gt;&lt;p&gt;Would it somehow be possible to allow multiple Tokenizers to work together? &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The only thing I can think of right now is a new kind of component that feeds raw text (or post-char-filter text) to a configurable set of tokenizers/recognizers, then melds their results using some (hopefully configurable) strategy, like &quot;longest-match-wins&quot; or &quot;create-overlapping-tokens&quot;, etc.  This would slow things down, of course, since analysis has to be performed multiple times over the same chunk of input text...&lt;/p&gt;</comment>
                    <comment id="12929379" author="steve_rowe" created="Sun, 7 Nov 2010 17:33:04 +0000"  >&lt;blockquote&gt;&lt;p&gt;if you recognize www.facebook.com but my app wants to find this with a query of &apos;facebook&apos;, it cant. yet if just stick to uax#29, if a user queries on www.facebook.com, and they are unsatisfied with the results, that  &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;&quot;www.facebook.com&quot; is way non-intuitive.  My guess is the average user would never go there: how is something a phrase, and in need of bounding quotes, if it has no spaces in it?&lt;/p&gt;</comment>
                    <comment id="12929380" author="rcmuir" created="Sun, 7 Nov 2010 17:44:16 +0000"  >&lt;blockquote&gt;&lt;p&gt;&quot;www.facebook.com&quot; is way non-intuitive&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;well, i&apos;m just saying that the &quot;UAX#29&quot; behavior i describe, people are used to:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;google and twitter search engines find and highlight say &apos;cnn&apos; in urls such as &apos;http:/www.cnn.com/x/y&apos;&lt;/li&gt;
	&lt;li&gt;this is how &quot;find&quot; in apps such as browsers, word processors, even windows notepad work.&lt;/li&gt;
	&lt;li&gt;the idea of putting quotes around things to be &quot;more exact&quot; is pretty general, e.g. in google i refine queries like &quot;documents&quot; with quotes to prevent stemming: try it.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;So i think its just intuitive and becoming rather universal to put quotes around things to get a &quot;more exact search&quot;.&lt;/p&gt;

&lt;p&gt;Like i said, i&apos;m not too picky how we solve the problem, but i think UAX#29 is a great default... its used everywhere else...&lt;/p&gt;</comment>
                    <comment id="12929382" author="steve_rowe" created="Sun, 7 Nov 2010 18:30:56 +0000"  >&lt;blockquote&gt;&lt;p&gt;So i think its just intuitive and becoming rather universal to put quotes around things to get a &quot;more exact search&quot;.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;You&apos;ve convinced me, though I don&apos;t think this idea has been around long enough to qualify as intiutive.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;hostnames are just an example, why do we recognize them and not filenames?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Although following precedent is important (principle of least surprise), we have to be able to revisit these decisions.  My philosophy tends toward kitchen-sinkness, while allowing people to ignore the stuff they don&apos;t want (today).  So, yeah, I think we &lt;b&gt;should&lt;/b&gt; (be able to) recognize filenames, at least as part of a URL-decomposing filter:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;http://www.example.com/path/file%20name.html?param=value#fragment&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;=&amp;gt; &lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;http://www.example.com/path/file%20name.html?param=value#fragment&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt; &amp;lt;URL&amp;gt;&lt;br/&gt;
www.example.com &amp;lt;HOSTNAME&amp;gt;&lt;br/&gt;
example.com &amp;lt;HOSTNAME&amp;gt;&lt;br/&gt;
example &amp;lt;HOSTNAME&amp;gt;&lt;br/&gt;
com &amp;lt;HOSTNAME&amp;gt;&lt;br/&gt;
path &amp;lt;URL_PATH_ELEMENT&amp;gt;&lt;br/&gt;
file name.html &amp;lt;URL_FILENAME&amp;gt;&lt;br/&gt;
file name &amp;lt;URL_FILENAME&amp;gt;&lt;br/&gt;
file &amp;lt;URL_FILENAME&amp;gt;&lt;br/&gt;
name &amp;lt;URL_FILENAME&amp;gt;&lt;br/&gt;
html &amp;lt;URL_FILENAME&amp;gt;&lt;br/&gt;
param &amp;lt;URL_PARAMETER&amp;gt;&lt;br/&gt;
value &amp;lt;URL_PARAMETER_VALUE&amp;gt;&lt;br/&gt;
fragment &amp;lt;URL_FRAGMENT&amp;gt;&lt;/p&gt;

&lt;p&gt;Output of each token type could be optional in a URL decomposition filter.  The URL decomposition filter could serve as a place to handle punycode, too.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;i&apos;m not too picky how we solve the problem, but i think UAX#29 is a great default... its used everywhere else...&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think if we remove EMAIL/HOSTNAME recognition, we need to have an alternative that provides the same thing.  So we would have UAX#29 tokenizer as default; a UAX29+EMAIL+HOSTNAME tokenizer as the equivalent to the pre-3.1 StandardTokenizer; and a UAX29+URL+EMAIL tokenizer (current StandardTokenizer).  Or maybe the last two could be combined: a UAX29+URL+EMAIL tokenizer that provides a configurable feature to not output URLs, but instead HOSTNAMEs and URL component tokens?&lt;/p&gt;</comment>
                    <comment id="12929385" author="rcmuir" created="Sun, 7 Nov 2010 19:17:20 +0000"  >&lt;blockquote&gt;
&lt;p&gt;You&apos;ve convinced me, though I don&apos;t think this idea has been around long enough to qualify as intiutive.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Well obviously i dont have hard references to this stuff, but from my interaction with my own users, most of them&lt;br/&gt;
dont even think of double quotes as doing phrases, nor are they technical enough to even know what a phrase&lt;br/&gt;
is or what that means for a search... they just think of it as more exact.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;I think if we remove EMAIL/HOSTNAME recognition, we need to have an alternative that provides the same thing. So we would have UAX#29 tokenizer as default; a UAX29+EMAIL+HOSTNAME tokenizer as the equivalent to the pre-3.1 StandardTokenizer; and a UAX29+URL+EMAIL tokenizer (current StandardTokenizer). Or maybe the last two could be combined: a UAX29+URL+EMAIL tokenizer that provides a configurable feature to not output URLs, but instead HOSTNAMEs and URL component tokens?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Well, like i said, i&apos;m not particularly picky, especially since someone can always use ClassicTokenizer to get the old behavior,&lt;br/&gt;
which, no one could ever agree on and there was constantly issues about not recognizing my company&apos;s name etc etc.&lt;/p&gt;

&lt;p&gt;To some extent, i like UAX#29 because there&apos;s someone else making and standardizing the decisions and validating&lt;br/&gt;
its not gonna annoy users of major languages, and making sure it works well by default: like its not gonna be the most &lt;br/&gt;
full-featured tokenizer but theres little chance it will be really annoying: i think this is great for &quot;defaults&quot;.&lt;/p&gt;

&lt;p&gt;as for all the other &quot;bonus&quot; stuff we can always make options, especially if its some pluggable thing somehow (sorry not sure about how this could work in jflex)&lt;br/&gt;
where you could have options as to what you want to do.&lt;/p&gt;

&lt;p&gt;but again, i think UAX#29 itself is more than sufficient by default, and even hostname etc is pretty dangerous &lt;b&gt;by default&lt;/b&gt; &lt;br/&gt;
(again my example of searching partial hostnames being flexible to the end-user and not baked-in, by letting them using quotes).&lt;/p&gt;</comment>
                    <comment id="12929407" author="earwin" created="Sun, 7 Nov 2010 23:00:19 +0000"  >&lt;blockquote&gt;&lt;p&gt;Would it somehow be possible to allow multiple Tokenizers to work together?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;The fact that Tokenizers now are not TokenFilters bugs me somewhat.&lt;br/&gt;
In theory, you should just feed initial text as a single monster token from hell into analysis chain, and then you only have TokenFilters, none/one/some of which might split this token.&lt;br/&gt;
If there are no TokenFilters at all, you get a NOT_ANALYZED case without extra flags, yahoo!&lt;/p&gt;

&lt;p&gt;The only problem here is the need for ability to wrap arbitrary Reader in a TermAttribute :/&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;But (yay repetition!) if the tokenizer throws away the separator chars, URLs can&apos;t be reassembled from their parts.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Why don&apos;t we teach StandardTokenizer to produce tokens for separator chars?&lt;br/&gt;
A special filter at the end of the chain will drop them, so they won&apos;t get into index.&lt;br/&gt;
And in the midst of the filter chain you are free to do whatever you want with them - detect emails/urls/sentences/whatever.&lt;/p&gt;</comment>
                    <comment id="12929408" author="steve_rowe" created="Sun, 7 Nov 2010 23:25:07 +0000"  >&lt;blockquote&gt;&lt;p&gt;Why don&apos;t we teach StandardTokenizer to produce tokens for separator chars?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I&apos;ve been thinking about this - the word break rules in UAX#29 are intended for use in break iterators, and tokenizers take that one step further by discarding stuff between some breaks.&lt;/p&gt;

&lt;p&gt;StandardTokenizer is faster, though, since it doesn&apos;t have to tokenize the stuff between tokens, so if we go down this route, I think it should go somewhere else: UAX29WordBreakSegmenter or something like that.&lt;/p&gt;

&lt;p&gt;I&apos;d like to have (nestable) SentenceSegmenter, ParagraphSegmenter, etc., the output from which could be the input to tokenizers.&lt;/p&gt;</comment>
                    <comment id="12929560" author="rcmuir" created="Mon, 8 Nov 2010 13:03:17 +0000"  >&lt;blockquote&gt;
&lt;p&gt;In theory, you should just feed initial text as a single monster token from hell into analysis chain, and then you only have TokenFilters, none/one/some of which might split this token.&lt;br/&gt;
If there are no TokenFilters at all, you get a NOT_ANALYZED case without extra flags, yahoo!&lt;/p&gt;

&lt;p&gt;The only problem here is the need for ability to wrap arbitrary Reader in a TermAttribute :/&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;No thanks, i dont want to read my entire documents into RAM and have massive gc&apos;ing going on.&lt;br/&gt;
We don&apos;t need to have a mega-tokenizer that solves everyones problems... this is just supposed to be a good &quot;general-purpose&quot; tokenizer.&lt;/p&gt;</comment>
                    <comment id="12929587" author="earwin" created="Mon, 8 Nov 2010 15:07:43 +0000"  >&lt;blockquote&gt;&lt;p&gt;No thanks, i dont want to read my entire documents into RAM and have massive gc&apos;ing going on.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;This is obvious. And that&apos;s why I was talking about wrapping Reader in an Attribute, not copying its contents.&lt;br/&gt;
How to do so, is much less obvious. And that&apos;s why I called it a problem.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;We don&apos;t need to have a mega-tokenizer that solves everyones problems... this is just supposed to be a good &quot;general-purpose&quot; tokenizer.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Exactly. That&apos;s why I&apos;m thinking of a way to get some composability, instead of having to fully rewrite tokenizer once you want extras.&lt;/p&gt;</comment>
                    <comment id="12929594" author="rcmuir" created="Mon, 8 Nov 2010 15:24:03 +0000"  >&lt;blockquote&gt;
&lt;p&gt;This is obvious. And that&apos;s why I was talking about wrapping Reader in an Attribute, not copying its contents.&lt;br/&gt;
How to do so, is much less obvious. And that&apos;s why I called it a problem.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;its not a problem, its just not possible. Because you don&apos;t know the required context of some downstream &lt;br/&gt;
&quot;composed&quot; &quot;partial-tokenizer&quot; or whatever, so it must be all read in... &lt;/p&gt;

&lt;p&gt;I don&apos;t think we need to provide FooAnalyzer or even FooTokenizer that solves everyone&apos;s special case problems,&lt;br/&gt;
its domain-dependent and not possible anyway... these are just general ones that solve a majority of use cases, examples really.&lt;/p&gt;

&lt;p&gt;This is why i think a simple UAX#29 standard should be the default... we can certainly have alternatives that do certain common things&lt;br/&gt;
that people want though, no problem with that.&lt;/p&gt;</comment>
                    <comment id="12932137" author="steve_rowe" created="Mon, 15 Nov 2010 18:27:30 +0000"  >&lt;p&gt;See &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2763&quot; title=&quot;Swap URL+Email recognizing StandardTokenizer and UAX29Tokenizer&quot;&gt;&lt;del&gt;LUCENE-2763&lt;/del&gt;&lt;/a&gt; for swapping UAX29Tokenizer and StandardTokenizer.&lt;/p&gt;</comment>
                </comments>
                <issuelinks>
                        <issuelinktype id="12310010">
                <name>Incorporates</name>
                                <outwardlinks description="incorporates">
                            <issuelink>
            <issuekey id="12415233">LUCENE-1545</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12428373">LUCENE-1702</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12416543">LUCENE-1556</issuekey>
        </issuelink>
                    </outwardlinks>
                                            </issuelinktype>
                        <issuelinktype id="10030">
                <name>Reference</name>
                                <outwardlinks description="relates to">
                            <issuelink>
            <issuekey id="12447097">LUCENE-2244</issuekey>
        </issuelink>
                    </outwardlinks>
                                                <inwardlinks description="is related to">
                            <issuelink>
            <issuekey id="12479984">LUCENE-2763</issuekey>
        </issuelink>
                    </inwardlinks>
                            </issuelinktype>
                    </issuelinks>
                <attachments>
                    <attachment id="12449198" name="LUCENE-2167.benchmark.patch" size="34390" author="steve_rowe" created="Sun, 11 Jul 2010 21:41:25 +0100" />
                    <attachment id="12446967" name="LUCENE-2167.benchmark.patch" size="33757" author="steve_rowe" created="Sun, 13 Jun 2010 05:08:08 +0100" />
                    <attachment id="12444592" name="LUCENE-2167.benchmark.patch" size="31786" author="steve_rowe" created="Sun, 16 May 2010 02:13:30 +0100" />
                    <attachment id="12446003" name="LUCENE-2167-jflex-tld-macro-gen.patch" size="14630" author="thetaphi" created="Tue, 1 Jun 2010 09:10:23 +0100" />
                    <attachment id="12445664" name="LUCENE-2167-jflex-tld-macro-gen.patch" size="14657" author="thetaphi" created="Thu, 27 May 2010 16:47:18 +0100" />
                    <attachment id="12445663" name="LUCENE-2167-jflex-tld-macro-gen.patch" size="14603" author="thetaphi" created="Thu, 27 May 2010 16:19:00 +0100" />
                    <attachment id="12445626" name="LUCENE-2167-lucene-buildhelper-maven-plugin.patch" size="40428" author="steve_rowe" created="Thu, 27 May 2010 07:23:06 +0100" />
                    <attachment id="12455801" name="LUCENE-2167.patch" size="906283" author="steve_rowe" created="Tue, 28 Sep 2010 07:06:48 +0100" />
                    <attachment id="12454634" name="LUCENE-2167.patch" size="850463" author="steve_rowe" created="Wed, 15 Sep 2010 09:07:54 +0100" />
                    <attachment id="12450449" name="LUCENE-2167.patch" size="894944" author="steve_rowe" created="Mon, 26 Jul 2010 07:27:42 +0100" />
                    <attachment id="12450447" name="LUCENE-2167.patch" size="907881" author="steve_rowe" created="Mon, 26 Jul 2010 07:18:21 +0100" />
                    <attachment id="12449807" name="LUCENE-2167.patch" size="602218" author="steve_rowe" created="Mon, 19 Jul 2010 07:44:29 +0100" />
                    <attachment id="12448461" name="LUCENE-2167.patch" size="542173" author="rcmuir" created="Thu, 1 Jul 2010 01:02:51 +0100" />
                    <attachment id="12448401" name="LUCENE-2167.patch" size="831121" author="rcmuir" created="Wed, 30 Jun 2010 15:21:30 +0100" />
                    <attachment id="12446744" name="LUCENE-2167.patch" size="763688" author="steve_rowe" created="Thu, 10 Jun 2010 05:27:06 +0100" />
                    <attachment id="12446464" name="LUCENE-2167.patch" size="879609" author="steve_rowe" created="Mon, 7 Jun 2010 09:00:47 +0100" />
                    <attachment id="12445624" name="LUCENE-2167.patch" size="54039" author="steve_rowe" created="Thu, 27 May 2010 07:01:01 +0100" />
                    <attachment id="12444631" name="LUCENE-2167.patch" size="51534" author="steve_rowe" created="Mon, 17 May 2010 06:49:09 +0100" />
                    <attachment id="12444626" name="LUCENE-2167.patch" size="51544" author="steve_rowe" created="Mon, 17 May 2010 01:22:28 +0100" />
                    <attachment id="12444590" name="LUCENE-2167.patch" size="49770" author="steve_rowe" created="Sat, 15 May 2010 23:52:07 +0100" />
                    <attachment id="12444579" name="LUCENE-2167.patch" size="48363" author="steve_rowe" created="Sat, 15 May 2010 17:12:38 +0100" />
                    <attachment id="12444049" name="LUCENE-2167.patch" size="46627" author="steve_rowe" created="Sun, 9 May 2010 06:07:20 +0100" />
                    <attachment id="12444037" name="LUCENE-2167.patch" size="57746" author="steve_rowe" created="Sat, 8 May 2010 17:51:36 +0100" />
                    <attachment id="12443823" name="LUCENE-2167.patch" size="57210" author="steve_rowe" created="Thu, 6 May 2010 06:01:02 +0100" />
                    <attachment id="12436802" name="LUCENE-2167.patch" size="1920" author="shyamal" created="Wed, 24 Feb 2010 02:13:56 +0000" />
                    <attachment id="12428207" name="LUCENE-2167.patch" size="2888" author="shyamal" created="Wed, 16 Dec 2009 18:55:05 +0000" />
                    <attachment id="12449805" name="StandardTokenizerImpl.jflex" size="14625" author="steve_rowe" created="Mon, 19 Jul 2010 06:18:32 +0100" />
                    <attachment id="12448321" name="standard.zip" size="165834" author="rcmuir" created="Tue, 29 Jun 2010 17:03:38 +0100" />
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>28.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Fri, 18 Dec 2009 01:19:51 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>11618</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10120"><![CDATA[Patch Available]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>25558</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                    <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                <customfieldname>Time in Status</customfieldname>
                <customfieldvalues>
                    
                </customfieldvalues>
            </customfield>
                            </customfields>
    </item>
</channel>
</rss>