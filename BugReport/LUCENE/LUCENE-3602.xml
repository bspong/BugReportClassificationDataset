<!-- 
RSS generated by JIRA (5.2.8#851-sha1:3262fdc28b4bc8b23784e13eadc26a22399f5d88) at Tue Jul 16 13:17:37 UTC 2013

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/LUCENE-3602/LUCENE-3602.xml?field=key&field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>5.2.8</version>
        <build-number>851</build-number>
        <build-date>26-02-2013</build-date>
    </build-info>

<item>
            <title>[LUCENE-3602] Add join query to Lucene</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-3602</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Solr has (psuedo) join query for a while now. I think this should also be available in Lucene.  &lt;/p&gt;</description>
                <environment></environment>
            <key id="12532847">LUCENE-3602</key>
            <summary>Add join query to Lucene</summary>
                <type id="2" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/newfeature.png">New Feature</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png">Closed</status>
                    <resolution id="1">Fixed</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="martijn.v.groningen">Martijn van Groningen</reporter>
                        <labels>
                    </labels>
                <created>Sun, 27 Nov 2011 20:31:18 +0000</created>
                <updated>Fri, 10 May 2013 11:45:00 +0100</updated>
                    <resolved>Tue, 7 Feb 2012 22:43:27 +0000</resolved>
                                            <fixVersion>3.6</fixVersion>
                <fixVersion>4.0-ALPHA</fixVersion>
                                <component>modules/join</component>
                        <due></due>
                    <votes>0</votes>
                        <watches>1</watches>
                                                    <comments>
                    <comment id="13157997" author="martijn.v.groningen" created="Sun, 27 Nov 2011 20:34:18 +0000"  >&lt;p&gt;Attached initial version of the JoinQuery which based on the one in Solr.&lt;/p&gt;</comment>
                    <comment id="13159006" author="mikemccand" created="Tue, 29 Nov 2011 01:12:35 +0000"  >&lt;p&gt;Awesome to finally bring JoinQuery to pure Lucene apps!&lt;/p&gt;

&lt;p&gt;Can we cut back to normal ctor (not builder API) to create the&lt;br/&gt;
JoinQuery?  One can always create a builder API layer on top if&lt;br/&gt;
necessary.&lt;/p&gt;

&lt;p&gt;How does the preComputedFromDocs work?  It&apos;s not per-segment?  Like&lt;br/&gt;
it&apos;s a bitset across entire toplevel doc space?&lt;/p&gt;

&lt;p&gt;Hmm we are also using MultiFields.getLiveDocs, which is quite slow to&lt;br/&gt;
use (must do binary search on each doc lookup).&lt;/p&gt;

&lt;p&gt;I wonder if we can make this work per-segment... but that can be a 2nd&lt;br/&gt;
phase.&lt;/p&gt;

&lt;p&gt;I think you can use seekExact instead of seekCeil?  Better&lt;br/&gt;
performance...&lt;/p&gt;

&lt;p&gt;What is the AdjustedDISI for (and when would Weight.scorer get a&lt;br/&gt;
top-level context...)?&lt;/p&gt;</comment>
                    <comment id="13159124" author="martijn.v.groningen" created="Tue, 29 Nov 2011 06:54:55 +0000"  >&lt;p&gt;I&apos;ll remove the builder api. This was just my sugar api. I&apos;ll change that in a constructor where the toSearcher and preComputedFromDocs are optional.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;How does the preComputedFromDocs work? It&apos;s not per-segment? Like it&apos;s a bitset across entire toplevel doc space?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Yes, it is a bitset across all segments, so toplevel. The whole query implementation is top level. People can use this the execute the real query outside the JoinQuery (if this isn&apos;t specified the query is executed when the Weight is created for the JoinQuery). I think this would be nice if people want the for example cache the encapsulated query.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I wonder if we can make this work per-segment... but that can be a 2nd phase.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I thought about that. But this would mean we need to execute the join query in two phases. The first phase would collect all the &quot;from&quot; values from the documents matching the encapsulated query. The second phase would match the documents that have matching on the &quot;to&quot; side with the &quot;from&quot; values collected in the first phase. The will work per segment and I think would also make the JoinQuery work in distributed environment. We can do this in a second development phase.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I think you can use seekExact instead of seekCeil? Better performance...&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I&apos;ll change that.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;What is the AdjustedDISI for (and when would Weight.scorer get a top-level context...)? &lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Actually that is only used to map top level docids to segment docids (Basically doing: toplevel_docid - base). This is necessary because Weight.scorer works per segment and the query implementation not.&lt;/p&gt;</comment>
                    <comment id="13159335" author="jasonrutherglen" created="Tue, 29 Nov 2011 16:08:50 +0000"  >&lt;p&gt;Great to see this moving out of Solr and getting new eyes on it (with added improvements)!&lt;/p&gt;</comment>
                    <comment id="13165359" author="martijn.v.groningen" created="Thu, 8 Dec 2011 17:53:19 +0000"  >&lt;p&gt;Updated patch.&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Removed builder api.&lt;/li&gt;
	&lt;li&gt;Updated to latest Lucene api changes.&lt;/li&gt;
	&lt;li&gt;Use seekExact instead of seekCeil method.&lt;/li&gt;
	&lt;li&gt;Added random test.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="13167493" author="mikemccand" created="Mon, 12 Dec 2011 13:53:13 +0000"  >&lt;p&gt;Patch looks good!&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;I like the test...&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Maybe rename actualQuery to fromQuery?  (So it&apos;s clear that&lt;br/&gt;
    JoinQuery runs fromQuery using fromSearcher, joining on&lt;br/&gt;
    fromSearcher.fromField to toSearcher.toField).&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Why preComputedFromDocs...?  Like if you were to cache something,&lt;br/&gt;
    wouldn&apos;t you want cache the toSearcher&apos;s bitset instead?&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Maybe rename JoinQueryWeight.joinResult to topLevelJoinResult, to&lt;br/&gt;
    contrast it w/ the per-segment scoring?  And add a comment&lt;br/&gt;
    explaining that we compute it once (on first segment) and all&lt;br/&gt;
    later segments then reuse it?&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;I wonder if we could make this a Filter instead, somehow?  Ie, at&lt;br/&gt;
    its core it converts a top-level bitset in the fromSearcher doc&lt;br/&gt;
    space into the joined bitset in the toSearcher doc space.  It&lt;br/&gt;
    could even maybe just be a static method taking in fromBitset and&lt;br/&gt;
    returning toBitset, which could operate per-segment on the&lt;br/&gt;
    toSearcher side?  (Separately: I wonder if JoinQuery should do&lt;br/&gt;
    something with the scores of the fromQuery....?  Not right now but&lt;br/&gt;
    maybe later...).&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Why does the JoinQuery javadoc say &quot;The downside of this&lt;br/&gt;
    is that in a sharded environment not all documents might get&lt;br/&gt;
    joined / linked.&quot; as a downside to the top-level approach?  Maybe&lt;br/&gt;
    reword that to state that all joined to/from docs must reside in&lt;br/&gt;
    the same shard?  In theory we could (later) make a shard friendly&lt;br/&gt;
    approach?  Eg, first pass builds up all unique Terms in the&lt;br/&gt;
    fromSearcher.fromField for docs matching the query (across all&lt;br/&gt;
    shards) and 2nd pass is basically a TermFilter on those...&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Not sure it matters, but... including the preComputedFromDocs in&lt;br/&gt;
    hashCode/equals is quite costly (it goes bit by bit...).  Maybe it&lt;br/&gt;
    shouldn&apos;t be included, since it contains details about the&lt;br/&gt;
    particular searcher that query had been run against?  (In theory&lt;br/&gt;
    Query instances are searcher independent.)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;In general I think this approach is somewhat inefficient, because it&lt;br/&gt;
always iterates over every possible term in fromSearcher.fromField,&lt;br/&gt;
checking the docs for each to see if there is a match in the query.&lt;br/&gt;
Ie, it&apos;s like FieldCache, in that it un-inverts, but it&apos;s uninverting&lt;br/&gt;
on every query.&lt;/p&gt;

&lt;p&gt;I wonder if we could DocTermOrds instead?  (Or,&lt;br/&gt;
FieldCache.DocTermsIndex or DocValues.BYTES_*, if we know fromSearcher.fromField is&lt;br/&gt;
single-valued).  This way we uninvert once (on init), and then doing&lt;br/&gt;
the join should be much faster since for each fromDocID we can lookup&lt;br/&gt;
the term(s) to join on.&lt;/p&gt;

&lt;p&gt;Likewise on the toSearcher side, if we had doc &amp;lt;-&amp;gt; ord/term loaded we&lt;br/&gt;
could do the forward (term -&amp;gt; ord) lookup quickly (in memory binary&lt;br/&gt;
search).&lt;/p&gt;

&lt;p&gt;But then this will obviously use RAM... so we should have the choice&lt;br/&gt;
(and start w/ the current patch!).&lt;/p&gt;</comment>
                    <comment id="13167571" author="martijn.v.groningen" created="Mon, 12 Dec 2011 16:12:13 +0000"  >&lt;blockquote&gt;&lt;p&gt;Maybe rename actualQuery to fromQuery?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Yes, fromQuery makes more sense than actualQuery.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Why preComputedFromDocs...? Like if you were to cache something,&lt;br/&gt;
wouldn&apos;t you want cache the toSearcher&apos;s bitset instead?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;This is in the case if your from query was cached and your toSearch&apos;s&lt;br/&gt;
bitset isn&apos;t, which is a likely scenario.&lt;br/&gt;
But caching the toSearcher&apos;s bitset is better off course when&lt;br/&gt;
possible. But this should be happen outside the JoinQuery, right?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Maybe rename JoinQueryWeight.joinResult to topLevelJoinResult,&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I agree a much more descriptive name.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;I wonder if we could make this a Filter instead, somehow? Ie, at&lt;br/&gt;
its core it converts a top-level bitset in the fromSearcher doc&lt;br/&gt;
space into the joined bitset in the toSearcher doc space. It&lt;br/&gt;
could even maybe just be a static method taking in fromBitset and&lt;br/&gt;
returning toBitset, which could operate per-segment on the&lt;br/&gt;
toSearcher side? (Separately: I wonder if JoinQuery should do&lt;br/&gt;
something with the scores of the fromQuery....? Not right now but&lt;br/&gt;
maybe later...).&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;It just matches docs from one side to the to side. That is all... So static method / filter should be able to do the job.&lt;br/&gt;
I&apos;m not sure, but if it is a query it might be able to one day encapsulate the joining in the Lucene query language?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Maybe reword that to state that all joined to/from docs must reside in the same shard?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;+1&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;I wonder if we could DocTermOrds instead? (Or,&lt;br/&gt;
FieldCache.DocTermsIndex or DocValues.BYTES_*, if we know&lt;br/&gt;
fromSearcher.fromField is&lt;br/&gt;
single-valued). This way we uninvert once (on init), and then doing&lt;br/&gt;
the join should be much faster since for each fromDocID we can lookup&lt;br/&gt;
the term(s) to join on.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I really like that idea! This already crossed my mind a few days ago&lt;br/&gt;
as an improvement to speedup the joining. Would be nice if the user can &lt;br/&gt;
choose between a more ram but faster variant and a less ram but slower variant.&lt;br/&gt;
I think we can just make two concrete JoinQuery impl that both have a different&lt;br/&gt;
joinResult(...) impl.&lt;/p&gt;</comment>
                    <comment id="13167686" author="mikemccand" created="Mon, 12 Dec 2011 19:10:51 +0000"  >&lt;blockquote&gt;
&lt;p&gt;This is in the case if your from query was cached and your toSearch&apos;s&lt;br/&gt;
bitset isn&apos;t, which is a likely scenario.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Hmm can you describe this?  You mean the app sometimes actually uses the fromSearcher.fromQuery&apos;s results, directly, without joining?&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;It just matches docs from one side to the to side. That is all... So static method / filter should be able to do the job.&lt;br/&gt;
I&apos;m not sure, but if it is a query it might be able to one day encapsulate the joining in the Lucene query language?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yeah... the core API is really the join method, to translate top-level docIDs in fromSearcher over to toSearcher&apos;s top-level docIDs.&lt;/p&gt;

&lt;p&gt;The AdjustedDISI (maybe rename SliceDISI?  SubReaderDISI?  ie, something to indicate it &quot;slices&quot; a sub-reader&apos;s portion of the top-level docID space) can then be used to translate back into a per-segment Filter.&lt;/p&gt;

&lt;p&gt;I think it would be cleaner as a Filter...?  This is actually similar to DuplicateFilter, which also must operate on top-level docIDs (since dups can happen in different segments).&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Would be nice if the user can choose between a more ram but faster variant and a less ram but slower variant.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yeah I agree... but what worries me is just how slow this non-RAM version will be.  Ie, it must do the full join and uninvert every time; so even if your fromQuery only matches a tiny number of docs... you pay massive cost of the full join.  Even better than using FC/DV/DTO to map docID -&amp;gt; term(s) per query, we could hold in RAM the join result itself (docID -&amp;gt; docID(s)) in some form, then the query just directly maps the docIDs w/o having to lookup terms again.&lt;/p&gt;

&lt;p&gt;Stepping back a bit... do we know how this impl compares to how ElasticSearch does joins?  And to how Solr does...?&lt;/p&gt;</comment>
                    <comment id="13167865" author="martijn.v.groningen" created="Mon, 12 Dec 2011 22:18:03 +0000"  >&lt;blockquote&gt;&lt;p&gt;You mean the app sometimes actually uses the fromSearcher.fromQuery&apos;s results, directly, without joining?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Yes. In the case of Solr it is checking the filter cache.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;but what worries me is just how slow this non-RAM version will be.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I have been running the JoinQuery on my test data set (10.1 M docs) and it isn&apos;t as bad as I expect it would be. This data set contains 100000 products each product having 100 offers. The JoinQuery with a &lt;b&gt;:&lt;/b&gt; query as fromQuery takes about 900 ms and a fromQuery selecting all products with a specific keyword takes about 350 ms. I think this specific query implementation is suitable for environments where RAM might be scarce. The RAM version should be the default.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Stepping back a bit... do we know how this impl compares to how ElasticSearch does joins? And to how Solr does...?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;ES only has index time joining, right? Solr basically uses the same mechanism as the JoinQuery in this patch, but a bit smarter. It tries to cache the from term to to term lookup (see JoinQParserPlugin.java line 367). I couldn&apos;t port this part to joining module since this caching relies heavily on the SolrIndexSearcher.&lt;/p&gt;</comment>
                    <comment id="13167959" author="jasonrutherglen" created="Tue, 13 Dec 2011 00:00:53 +0000"  >&lt;p&gt;Maybe we can (in another issue) move bit set filter caching into SearchManager, for use by Lucene Join (here), and others.  At the same time making bitset filtering per-segment, a fundamental improvement from the existing (old) Solr code.&lt;/p&gt;</comment>
                    <comment id="13182066" author="martijn.v.groningen" created="Sat, 7 Jan 2012 19:11:34 +0000"  >&lt;p&gt;Attached a new patch with a completely different joining implementation. Basically the joining happens inside a static method, works per segment and uses FieldCache.&lt;/p&gt;

&lt;p&gt;This is just a small try-out and is lacking any tests.&lt;/p&gt;</comment>
                    <comment id="13182269" author="mikemccand" created="Sun, 8 Jan 2012 19:05:32 +0000"  >&lt;p&gt;Wow new patch is tiny &amp;#8211; just 2 static methods!&lt;/p&gt;

&lt;p&gt;Right now you do 3 passes &amp;#8211; 1st pass records fromSearcher&apos;s docIDs&lt;br/&gt;
matching fromQuery; 2nd pass translates those matching docIDs into&lt;br/&gt;
the joinable terms in fromSearcher.fromField; 3rd pass then records&lt;br/&gt;
toSearcher docIDs matching those terms in toField.&lt;/p&gt;

&lt;p&gt;But I think the first 2 passes could be combined?  Ie, as you collect&lt;br/&gt;
each hit from fromQuery, instead of recording the docID, go and look up&lt;br/&gt;
the term in fromField for that doc and save it away?  Then you don&apos;t&lt;br/&gt;
need to save the fromSearcher docIDs?  (3rd pass would then be the&lt;br/&gt;
same).&lt;/p&gt;

&lt;p&gt;Also, instead of making a toplevel bit set as the return&lt;br/&gt;
result... could it be an ordinary filter?  Then the 3rd pass would be&lt;br/&gt;
implemented in Filter.getDocIDSet, and the Filter instance would hold&lt;br/&gt;
onto these terms computed by the combined 1st/2nd pass?&lt;/p&gt;

&lt;p&gt;I think this is a great step forward over previous patch... so tiny&lt;br/&gt;
too &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;The 1st/2nd pass would have &quot;expected&quot; cost, ie on the order of how&lt;br/&gt;
many hits matched in fromQuery.  But the 3rd pass has a high cost even&lt;br/&gt;
for tiny queries since it visits every doc, checking whether its terms&lt;br/&gt;
are in the set.  We might be able to improve on that somehow... eg if&lt;br/&gt;
the number of terms is small, really you want to invert that process&lt;br/&gt;
(ie visit the postings and gather all matching docs), either with an&lt;br/&gt;
OR query or with TermsFilter (in modules/queries)?  Hmm, in fact,&lt;br/&gt;
isn&apos;t this just a MultiTermQuery?  We can then use auto rewrite mode&lt;br/&gt;
to rewrite as filter or small BooleanQuery?&lt;/p&gt;</comment>
                    <comment id="13182329" author="martijn.v.groningen" created="Sun, 8 Jan 2012 23:57:27 +0000"  >&lt;blockquote&gt;&lt;p&gt;But I think the first 2 passes could be combined?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;That idea also crossed my mind. We should definitely do this.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Also, instead of making a toplevel bit set as the return result... could it be an ordinary filter? &lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I like that approach. It can just return a TermsFilter instance, right?&lt;/p&gt;

&lt;p&gt;Maybe we should have to variants one returns a MTQ and one a TermsFilter? So users can choose whether to want a filter or a query.&lt;/p&gt;</comment>
                    <comment id="13182332" author="rcmuir" created="Mon, 9 Jan 2012 00:14:55 +0000"  >&lt;blockquote&gt;
&lt;p&gt;Maybe we should have to variants one returns a MTQ and one a TermsFilter? So users can choose whether to want a filter or a query.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;But they can always choose the rewriteMode to not be AUTO and explicitly choose filter if they want this.&lt;/p&gt;</comment>
                    <comment id="13183673" author="martijn.v.groningen" created="Tue, 10 Jan 2012 23:01:58 +0000"  >&lt;p&gt;Attached a new patch. Changes:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Everything happens now in two passes.&lt;/li&gt;
	&lt;li&gt;The second pass is in the custom MTQ impl.&lt;/li&gt;
	&lt;li&gt;JoinUtil contains one method that returns a MTQ impl.&lt;/li&gt;
	&lt;li&gt;Reused the tests from previous patch for the JoinUtil.&lt;/li&gt;
&lt;/ul&gt;


&lt;blockquote&gt;&lt;p&gt;But they can always choose the rewriteMode to not be AUTO and explicitly choose filter if they want this.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Yes makes sense. First time I used the MTQ directly. So feedback on my on MTQ impl is welcome &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;I think that this patch can easily be backported to 3x.&lt;/p&gt;</comment>
                    <comment id="13184191" author="mikemccand" created="Wed, 11 Jan 2012 16:56:46 +0000"  >&lt;p&gt;Looking great!  I think we are getting close &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;Maybe use BytesRefHash (oal.util) to gather the terms and sort&lt;br/&gt;
them in the end?  Then hold onto the BytesRef[] (sorted).  It&apos;s&lt;br/&gt;
designed for exactly this usage...&lt;/p&gt;

&lt;p&gt;Also, I think you should always seek in your FilteredTermsEnum, and&lt;br/&gt;
simply iterate yourself through the BytesRef[]?  Probably perf wasn&apos;t&lt;br/&gt;
great before because you were calling TreeSet.higher on each term?&lt;/p&gt;

&lt;p&gt;I think you should call setRewriteMethod in your TermSetQuery ctor, to&lt;br/&gt;
CONSTANT_SCORE_AUTO_REWRITE_DEFAULT, since we have no scores here.&lt;br/&gt;
This rewrite method uses BQ if number of terms/docs is &quot;smallish&quot; else&lt;br/&gt;
creates a filter, so it should give good perf.&lt;/p&gt;</comment>
                    <comment id="13185527" author="martijn.v.groningen" created="Fri, 13 Jan 2012 09:59:37 +0000"  >&lt;p&gt;Cool. I will use the BytesRefHash in the patch and see how this pans out.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Also, I think you should always seek in your FilteredTermsEnum, and&lt;br/&gt;
simply iterate yourself through the BytesRef[]? Probably perf wasn&apos;t&lt;br/&gt;
great before because you were calling TreeSet.higher on each term?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I&apos;ve been testing this out with treeSet.higher(). In the case when there&apos;re gaps between the terms the performance is much better with seeking then not seeking (I picked random terms in a small test). In the terms are next / near to each other it seems it is better use a non seeking implementation. I think we should have a seeking and non seeking impl available. Maybe there should should be an option named useSeekingTermsEnum which decides what impl to us?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I think you should call setRewriteMethod in your TermSetQuery ctor, to CONSTANT_SCORE_AUTO_REWRITE_DEFAULT&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;CONSTANT_SCORE_AUTO_REWRITE_DEFAULT is already the default in MTQ, right?&lt;/p&gt;</comment>
                    <comment id="13185714" author="mikemccand" created="Fri, 13 Jan 2012 18:04:04 +0000"  >&lt;p&gt;Oh yeah MTQ already defaults to that rewrite method &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;Really, since you know the exact number of terms up front... you could specialize it to constant BQ or filter yourself... but that can be later.&lt;/p&gt;

&lt;p&gt;I don&apos;t think you should be calling TreeSet.higher on every term check &amp;#8211; that&apos;s a relatively costly binary search through the TreeSet each time.  Really you should be able to just walk through (with your own upto++) the pre-sorted BytesRef[] you pulled from the BytesRefHash.  Then we should retest seek vs next...&lt;/p&gt;</comment>
                    <comment id="13186528" author="martijn.v.groningen" created="Sun, 15 Jan 2012 16:19:37 +0000"  >&lt;p&gt;Attached new patch.&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Extract collector from JoinUtil and named it TermsCollector.&lt;/li&gt;
	&lt;li&gt;TermsCollector uses BytesRefHash.&lt;/li&gt;
	&lt;li&gt;Renamed TermSetQuery to TermsQuery.&lt;/li&gt;
	&lt;li&gt;TermsQuery uses sorted ByteRefs[]&lt;/li&gt;
	&lt;li&gt;TermsCollector has two concrete impl. One for single value per field and one for multiple values per field.&lt;/li&gt;
	&lt;li&gt;Moved the classes for query time joining under the package o.a.l.search.join.query&lt;/li&gt;
	&lt;li&gt;Added documentation.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;The JoinUtil is now just a convience class.&lt;/p&gt;

&lt;p&gt;Should we add the existing block joining classes under o.a.l.search.join.index or o.a.l.search.join.block?&lt;/p&gt;

&lt;p&gt;I think this patch is ready to get committed.&lt;/p&gt;</comment>
                    <comment id="13186589" author="martijn.v.groningen" created="Sun, 15 Jan 2012 20:42:45 +0000"  >&lt;p&gt;Oops... I found out that the assertion in FilteredTermsEnum line 207 was failing. Before I added the previous patch I ran the tests from my IDE and assertions weren&apos;t enabled...&lt;/p&gt;

&lt;p&gt;In some cases I was seeking backwards which isn&apos;t good. The attached patch fixes this issue. All the tests in the join module pass now.&lt;/p&gt;</comment>
                    <comment id="13186629" author="mikemccand" created="Mon, 16 Jan 2012 00:25:26 +0000"  >&lt;p&gt;Looking great!&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;Can&apos;t TermsCollector be package private?  Like it&apos;s only used&lt;br/&gt;
    privately in JoinUtil.createJoinQuery?&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Can JoinUtil.createJoinQuery return Query not MultiTermQuery...?&lt;br/&gt;
    Just gives us freedom in the future to impl a different Query if&lt;br/&gt;
    we want/need to...&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Typo: collecter -&amp;gt; collector&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Can TermsQuery be package private?  Also, we can save a pass over&lt;br/&gt;
    the sorted terms by having TermsQuery hold the int[] ord array and&lt;br/&gt;
    then just do the lookup (against BytesRefHash) as it&lt;br/&gt;
    goes... really TermsQuery could just take the BytesRefHash.  Then&lt;br/&gt;
    we wouldn&apos;t have to materialize a new BytesRef for each matched&lt;br/&gt;
    term... just reuse a single scratch BytesRef inside TermsQuery.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;In the TermsQuery.accept... should that &lt;tt&gt;return AcceptStatus.YES&lt;/tt&gt;&lt;br/&gt;
    in the &lt;tt&gt;if (cmp == 0)&lt;/tt&gt; be a YES_AND_SEEK&lt;br/&gt;
    (after setting the next term as the seekTerm)?&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Hmm we sort by unicode order in TermsCollector.getCollectedTerms,&lt;br/&gt;
    but then by the term dict&apos;s comparator in TermsQuery; maybe just&lt;br/&gt;
    use the UTF8AsUnicode comparator in TermsQuery too?  And note in&lt;br/&gt;
    jdocs that this is required?&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="13186635" author="mikemccand" created="Mon, 16 Jan 2012 00:44:15 +0000"  >&lt;p&gt;Also: I wonder if we should use DocValues in addition (instead of?) FieldCache.getTerms...&lt;/p&gt;</comment>
                    <comment id="13186864" author="martijn.v.groningen" created="Mon, 16 Jan 2012 10:53:12 +0000"  >&lt;p&gt;I think that joining by docvalues would be great addition! I do think that joining by indexed terms should be possible to.&lt;br/&gt;
To support joining by docvalues we can make a different TermsCollector implementation, but I&apos;m unsure how the TermsQuery can be used. Can A MTQ also iterate over docvalues? &lt;/p&gt;

&lt;p&gt;I think we should commit what we currently have first and then address joining by docvalues in a new issue.&lt;/p&gt;</comment>
                    <comment id="13186893" author="mikemccand" created="Mon, 16 Jan 2012 12:20:17 +0000"  >&lt;blockquote&gt;&lt;p&gt;To support joining by docvalues we can make a different TermsCollector implementation, but I&apos;m unsure how the TermsQuery can be used. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Actually I think TermsQuery (once we change it to just take the BytesRefHash and iterate over the int[] ord instead of BytesRef[]) can be re-used.&lt;/p&gt;

&lt;p&gt;Ie, when collecting terms from DocValues, you&apos;d just stuff them into the BytesRefHash like you do now... so supporting terms from DocValues should simply be another private impl inside TermsCollector (just like you can now pull from DocTermOrds or FieldCache).&lt;/p&gt;

&lt;p&gt;But I agree: let&apos;s do this (enable JoinQuery to use DocValues) in a follow-on issue!&lt;/p&gt;</comment>
                    <comment id="13186926" author="rcmuir" created="Mon, 16 Jan 2012 13:54:47 +0000"  >&lt;p&gt;Hi Martijn: the patch is looking really nice! &lt;/p&gt;

&lt;p&gt;Just a trivial thing, can we swap ctor arguments of&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; TermsQuery(BytesRef[] terms, &lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt; field) {
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;to&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; TermsQuery(&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt; field, BytesRef[] terms) {
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Most of the apis on indexreader etc take field,term so I think it looks more consistent.&lt;/p&gt;</comment>
                    <comment id="13187220" author="martijn.v.groningen" created="Mon, 16 Jan 2012 21:46:14 +0000"  >&lt;p&gt;Attached a new patch!&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;The static method now has Query as return type.&lt;/li&gt;
	&lt;li&gt;TermsCollector and TermsQuery are package protected.&lt;/li&gt;
	&lt;li&gt;TermsQuery now holds an int[] ords instead of an array of ByteRefs. This is a really nice improvement &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/li&gt;
	&lt;li&gt;Swapped the const args of TermQuery. Consistency is important!&lt;/li&gt;
	&lt;li&gt;After &lt;a href=&quot;http://colabti.org/irclogger/irclogger_log/lucene-dev?date=2012-01-16#l231&quot; class=&quot;external-link&quot;&gt;some discussion&lt;/a&gt; moved the classes back to o.a.l.search.join package.&lt;/li&gt;
	&lt;li&gt;The query time joining uses the dict comparator on all places now (instead of ByteRef#UTF8AsUnicode).&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;In the that cmp==0 in TermsQuery at line 121; YES_AND_SEEK can&apos;t be returned. The FilteredTermsEnum prohibits seeking to a term smaller or equal then the current term. &lt;/p&gt;</comment>
                    <comment id="13187223" author="martijn.v.groningen" created="Mon, 16 Jan 2012 21:49:21 +0000"  >&lt;p&gt;Small update. Removed unused method.&lt;/p&gt;</comment>
                    <comment id="13187266" author="martijn.v.groningen" created="Mon, 16 Jan 2012 22:33:22 +0000"  >&lt;p&gt;Fixed the YES_AND_SEEK issue. In the previously described case it uses YES_AND_SEEK now without seeking to the current term (that triggered the assertion error).&lt;/p&gt;</comment>
                    <comment id="13187276" author="mikemccand" created="Mon, 16 Jan 2012 22:51:08 +0000"  >&lt;p&gt;+1 it looks great!  Good work &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                    <comment id="13187284" author="martijn.v.groningen" created="Mon, 16 Jan 2012 22:59:05 +0000"  >&lt;p&gt;Attached a new patch.&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Fixed some small comment typos.&lt;/li&gt;
	&lt;li&gt;Removed the reuse field in the TermsQuery. Since it doesn&apos;t make sense to reuse b/c it assigned null! Also I cannot know if the caller is done with the TermsEnum.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;It is ready. I&apos;ll commit this asap.&lt;/p&gt;</comment>
                    <comment id="13187297" author="martijn.v.groningen" created="Mon, 16 Jan 2012 23:18:12 +0000"  >&lt;p&gt;Committed to trunk. I&apos;ll leave this issue open for the backport to 3.6&lt;/p&gt;</comment>
                    <comment id="13187370" author="jasonrutherglen" created="Tue, 17 Jan 2012 01:40:22 +0000"  >&lt;p&gt;Sweet!  How join would work in distributed mode, that would be very useful for BigData projects.&lt;/p&gt;</comment>
                    <comment id="13187457" author="rcmuir" created="Tue, 17 Jan 2012 05:20:11 +0000"  >&lt;p&gt;Just a few ideas about 3.x backport:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;MultiTermQuery doesn&apos;t have a real &apos;seek&apos; there... you have to open up and swap in a new &apos;actualEnum&apos; (does this clone the indexinput etc?) starting at the new seek position... I think its tricky and ugly. Maybe we can/should seek to 3.x TermEnum/FilteredTermEnum so this will work nicer...? I havent looked at how hairy this would be though.&lt;/li&gt;
	&lt;li&gt;The rewrites are not always per-segment like they are in trunk, and it could be bad to &quot;seek&quot; a lot when MTQ is in &apos;auto mode&apos; because if I recall its expensive (creating lots of multitermsenums). Maybe since Join knows up-front how many terms are in the hash it should determine which method to use itself (setRewriteMethod)... maybe on 3.x just set filter rewrite since you know will be &quot;seeking&quot;? Since you return the query the user could still always override the rewrite method if they really care, it would just be a default.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="13189316" author="martijn.v.groningen" created="Thu, 19 Jan 2012 19:26:51 +0000"  >&lt;blockquote&gt;&lt;p&gt;Sweet! How join would work in distributed mode, that would be very useful for BigData projects.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;The join is actually executed in a two pass search. During the first pass search all the terms are gathered for the matching documents based on the fromQuery. In the second pass search all documents are collected that match with the gather terms for a specific field. To only way I currently see how this can work in a distributed environment is that all machines in the cluster execute the first pass search and then copy the collected terms between machines. After this is done each machine can execute the second pass. If your data allows it, you can partition data in your cluster this allows you to skip the copying of terms. &lt;/p&gt;

&lt;p&gt;Currently the api is just one static method and assumes that the joining happens locally. I think we need to have two more methods. One method that returns the first pass terms and one method that constructs a query based on terms from the first pass.&lt;/p&gt;

&lt;p&gt;Robert: Yes I see that 3.x MTQ isn&apos;t as great as MTQ in trunk. Maybe we need a different approach (not use MTQ)? The api is clean for users, and allows us to do joining different in 3x. I&apos;ll start backporting and see how well it goes.&lt;/p&gt;</comment>
                    <comment id="13189375" author="jasonrutherglen" created="Thu, 19 Jan 2012 20:52:33 +0000"  >&lt;p&gt;I was reviewing this issue to use where Solr&apos;s join implementation may not be the right choice.&lt;/p&gt;

&lt;p&gt;In this Lucene Join implementation, a new BytesRefHash is built per query (and cannot be reused).  This could generate a fair amount of garbage quickly.  &lt;/p&gt;

&lt;p&gt;Also the sort compare using BRH is per byte (not as cheap as an ord compare).  We can probably use DocTermsIndex to replace the use of BytesRefHash by comparing DTI&apos;s ords.  Then we are saving off the bytes into BRH per query, and the comparison would be faster.&lt;/p&gt;

&lt;p&gt;Additionally, for a join with many terms, the number of postings could become a factor in performance.  Because we are not caching bitsets like Solr does, it seems like an excellent occasion for a faster less-compressed codec.&lt;/p&gt;

&lt;p&gt;Further, to save on term seeking, if the term state was cached (eg, the file pointers into the posting list), the iteration on the terms dict would be removed.&lt;/p&gt;

&lt;p&gt;Granted all this requires more RAM, however in many cases (eg, mine), that would be acceptable.&lt;/p&gt;</comment>
                    <comment id="13189469" author="martijn.v.groningen" created="Thu, 19 Jan 2012 23:01:56 +0000"  >&lt;p&gt;I&apos;m not sure how you plan to sort by DTI ords. The terms collected in the first phase are from many segments. The ords from DTI are only valid inside a segment. You can create a toplevel DTI but that is expensive... &lt;/p&gt;

&lt;p&gt;Currently caching is minimal and can be improved at the cost of more RAM. The TermsCollector caches the from terms via DocTerms in the FC (per segment). Caching can be improved in the second phase as you described, by saving a bitset per fromTerm?. But I think we first need to tackle how bitsets are cached in general. Solr caches (which the Solr JoinQuery uses) are top level (one commit and you lose it all). I&apos;m unsure how to cache the posting list file pointers with the current Lucene api... I think this (joining) caching should be addressed in a new issue.&lt;/p&gt;

&lt;p&gt;Performance of the JoinUtil looks actual quite good from what I have measured. I have a test data set containing 100000 products and 100 offers per product (10100000 docs in total). The JoinUtil is between 2 till 3 times faster than Solr&apos;s JoinQuery with this data set on my dev machine.&lt;/p&gt;</comment>
                    <comment id="13189475" author="martijn.v.groningen" created="Thu, 19 Jan 2012 23:08:52 +0000"  >&lt;p&gt;Attached initial patch for 3x branch.&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;I replaced the BytesRefHash with a Set for now. FC in 3x is String based.&lt;/li&gt;
	&lt;li&gt;TermsQuery doesn&apos;t seek, but does a binary search to find the index in the String[] (created from the Set).... Not ideal. So we need some seeking.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I ran some quick tests and performance is 3 till 4 times worse then the JoinUtil committed to trunk.&lt;/p&gt;</comment>
                    <comment id="13189501" author="jasonrutherglen" created="Thu, 19 Jan 2012 23:46:49 +0000"  >&lt;blockquote&gt;&lt;p&gt;The terms collected in the first phase are from many segments&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Why is that necessary?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Caching can be improved in the second phase as you described, by saving a bitset per fromTerm?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Possibly, only for terms with a high number of documents.  Or we can use a faster to decode (less compressed) posting codec.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;The JoinUtil is between 2 till 3 times faster than Solr&apos;s JoinQuery with this data set on my dev machine&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Interesting, thanks for sharing.&lt;/p&gt;
</comment>
                    <comment id="13189509" author="jasonrutherglen" created="Thu, 19 Jan 2012 23:56:19 +0000"  >&lt;p&gt;Just following up on the per-segment terms collection.  Join is going to be used as a filter in most cases &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/help_16.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;.  Filters can be applied per-segment (unlike scoring queries).  So it seems possible to avoid the BRH creation by using the DTI?&lt;/p&gt;</comment>
                    <comment id="13200913" author="martijn.v.groningen" created="Sun, 5 Feb 2012 21:29:36 +0000"  >&lt;p&gt;Jason: Better late then never... BRH is used to collect the matching from terms. The DTI just contains all terms / ords for a field. Comparing DTI ords isn&apos;t going to work when a term is in more than one segment or appears in a different field (fromField / toField). So I think the BRH can&apos;t be replaced by the DTI. The BRH could be cached per query.&lt;/p&gt;</comment>
                    <comment id="13200915" author="martijn.v.groningen" created="Sun, 5 Feb 2012 21:37:52 +0000"  >&lt;p&gt;Attached updated version of query time joining for 3x branch. Instead of doing a binary search for each term comparison it seeks / iterates forward. It can&apos;t do seeking like we do in trunk, so it isn&apos;t as fast as in trunk. However I do think this can be committed to at least have query time join support in 3x. Back porting per segment filtering and the MTQ that is in trunk is quite some work...&lt;/p&gt;</comment>
                    <comment id="13202806" author="martijn.v.groningen" created="Tue, 7 Feb 2012 21:23:42 +0000"  >&lt;p&gt;Committed latest 3x patch to branch3x.&lt;/p&gt;</comment>
                    <comment id="13202810" author="martijn.v.groningen" created="Tue, 7 Feb 2012 21:27:50 +0000"  >&lt;p&gt;The joining in 3x is ~3 times slower than the joining committed to trunk. This is mainly caused by the fact that the MTQ in trunk can do seeking while the the MTQ in 3x can only increment to the next term.&lt;/p&gt;
</comment>
                </comments>
                    <attachments>
                    <attachment id="12513370" name="LUCENE-3602-3x.patch" size="28424" author="martijn.v.groningen" created="Sun, 5 Feb 2012 21:37:52 +0000" />
                    <attachment id="12511189" name="LUCENE-3602-3x.patch" size="24455" author="martijn.v.groningen" created="Thu, 19 Jan 2012 23:08:52 +0000" />
                    <attachment id="12510769" name="LUCENE-3602.patch" size="30633" author="martijn.v.groningen" created="Mon, 16 Jan 2012 22:59:05 +0000" />
                    <attachment id="12510767" name="LUCENE-3602.patch" size="30697" author="martijn.v.groningen" created="Mon, 16 Jan 2012 22:33:22 +0000" />
                    <attachment id="12510757" name="LUCENE-3602.patch" size="30423" author="martijn.v.groningen" created="Mon, 16 Jan 2012 21:49:21 +0000" />
                    <attachment id="12510755" name="LUCENE-3602.patch" size="30974" author="martijn.v.groningen" created="Mon, 16 Jan 2012 21:46:14 +0000" />
                    <attachment id="12510641" name="LUCENE-3602.patch" size="30663" author="martijn.v.groningen" created="Sun, 15 Jan 2012 20:42:45 +0000" />
                    <attachment id="12510630" name="LUCENE-3602.patch" size="30266" author="martijn.v.groningen" created="Sun, 15 Jan 2012 16:19:37 +0000" />
                    <attachment id="12510119" name="LUCENE-3602.patch" size="20268" author="martijn.v.groningen" created="Tue, 10 Jan 2012 23:01:58 +0000" />
                    <attachment id="12509798" name="LUCENE-3602.patch" size="4497" author="martijn.v.groningen" created="Sat, 7 Jan 2012 19:11:34 +0000" />
                    <attachment id="12506626" name="LUCENE-3602.patch" size="25562" author="martijn.v.groningen" created="Thu, 8 Dec 2011 17:53:19 +0000" />
                    <attachment id="12505263" name="LUCENE-3602.patch" size="17917" author="martijn.v.groningen" created="Sun, 27 Nov 2011 20:34:18 +0000" />
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>12.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Tue, 29 Nov 2011 01:12:35 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>218581</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>12229</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                    <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                <customfieldname>Time in Status</customfieldname>
                <customfieldvalues>
                    
                </customfieldvalues>
            </customfield>
                            </customfields>
    </item>
</channel>
</rss>