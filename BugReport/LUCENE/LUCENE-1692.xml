<!-- 
RSS generated by JIRA (5.2.8#851-sha1:3262fdc28b4bc8b23784e13eadc26a22399f5d88) at Tue Jul 16 13:13:37 UTC 2013

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/LUCENE-1692/LUCENE-1692.xml?field=key&field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>5.2.8</version>
        <build-number>851</build-number>
        <build-date>26-02-2013</build-date>
    </build-info>

<item>
            <title>[LUCENE-1692] Contrib analyzers need tests</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-1692</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;The analyzers in contrib need tests, preferably ones that test the behavior of all the Token &apos;attributes&apos; involved (offsets, type, etc) and not just what they do with token text.&lt;/p&gt;

&lt;p&gt;This way, they can be converted to the new api without breakage.&lt;/p&gt;</description>
                <environment></environment>
            <key id="12427921">LUCENE-1692</key>
            <summary>Contrib analyzers need tests</summary>
                <type id="6" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/requirement.png">Test</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png">Closed</status>
                    <resolution id="1">Fixed</resolution>
                                <assignee username="rcmuir">Robert Muir</assignee>
                                <reporter username="rcmuir">Robert Muir</reporter>
                        <labels>
                    </labels>
                <created>Mon, 15 Jun 2009 18:45:44 +0100</created>
                <updated>Mon, 16 May 2011 19:15:37 +0100</updated>
                    <resolved>Tue, 18 Aug 2009 14:01:23 +0100</resolved>
                                            <fixVersion>2.9</fixVersion>
                                <component>modules/analysis</component>
                        <due></due>
                    <votes>0</votes>
                        <watches>0</watches>
                                                    <comments>
                    <comment id="12719799" author="rcmuir" created="Mon, 15 Jun 2009 22:48:36 +0100"  >&lt;p&gt;first I looked at BrazilianAnalyzer... out of curiousity can someone explain to me how the behavior of BrazilianStemmer differs from the Portuguese snowball analyzer... because it looks to be the same algorithm to me!&lt;/p&gt;</comment>
                    <comment id="12719836" author="rcmuir" created="Mon, 15 Jun 2009 23:51:12 +0100"  >&lt;p&gt;answered my own question, here&apos;s tests for brazilian as a start.&lt;/p&gt;</comment>
                    <comment id="12719933" author="rcmuir" created="Tue, 16 Jun 2009 06:30:05 +0100"  >&lt;p&gt;add tests for dutchanalyzer.&lt;/p&gt;

&lt;p&gt;this analyzer claims to implement snowball, although tests reveal some differences. it also has about 1MB of text files that don&apos;t appear to be in use at all...&lt;/p&gt;</comment>
                    <comment id="12720144" author="mikemccand" created="Tue, 16 Jun 2009 15:17:40 +0100"  >&lt;p&gt;These are much needed... thanks Robert.  Let me know when you&apos;re done iterating (and/or when we need to wrap up 2.9) and we&apos;ll get these in.&lt;/p&gt;</comment>
                    <comment id="12720154" author="rcmuir" created="Tue, 16 Jun 2009 15:33:03 +0100"  >&lt;p&gt;Michael: &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-973&quot; title=&quot;Token of  &amp;quot;&amp;quot; returns in CJKTokenizer + new TestCJKTokenizer&quot;&gt;&lt;del&gt;LUCENE-973&lt;/del&gt;&lt;/a&gt; would save me from having to create tests for the CJKAnalyzer.&lt;/p&gt;

&lt;p&gt;It would also fix a bug.&lt;/p&gt;</comment>
                    <comment id="12720372" author="rcmuir" created="Tue, 16 Jun 2009 23:02:47 +0100"  >&lt;p&gt;thanks, i&apos;ll upload some more tests hopefully soon. I think most have rudimentary tests.&lt;/p&gt;

&lt;p&gt;but some are not sufficient to ensure any api conversion is really working.&lt;/p&gt;

&lt;p&gt;for example ThaiAnalyzer does not have any offset tests, but if that broke then highlighting would break.&lt;/p&gt;</comment>
                    <comment id="12720570" author="mikemccand" created="Wed, 17 Jun 2009 10:01:03 +0100"  >&lt;p&gt;Robert, you should probably also hold up on API conversion, since the API itself is now changing (&lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-1693&quot; title=&quot;AttributeSource/TokenStream API improvements&quot;&gt;&lt;del&gt;LUCENE-1693&lt;/del&gt;&lt;/a&gt;).&lt;/p&gt;</comment>
                    <comment id="12720696" author="rcmuir" created="Wed, 17 Jun 2009 15:54:59 +0100"  >&lt;p&gt;michael, ok. I know additional tests here (against the old api) might be more code to convert, but I think it will actually make the process easier, whenever that is or whatever is involved.&lt;/p&gt;

&lt;p&gt;i have some time this evening to try to improve the coverage here (against the old api).&lt;/p&gt;</comment>
                    <comment id="12721095" author="rcmuir" created="Thu, 18 Jun 2009 07:51:59 +0100"  >&lt;p&gt;adds tests for thaianalyzer token offsets and types, both of which have bugs!&lt;br/&gt;
tests for correct behavior are included but commented out.&lt;/p&gt;</comment>
                    <comment id="12721417" author="rcmuir" created="Thu, 18 Jun 2009 19:58:47 +0100"  >&lt;p&gt;added tests for czech.&lt;br/&gt;
added additional tests for smartchineseanalyzer, there is a bug very similar to the recent CJK one here... generating empty tokens.&lt;/p&gt;</comment>
                    <comment id="12721418" author="rcmuir" created="Thu, 18 Jun 2009 20:00:48 +0100"  >&lt;p&gt;michael: I&apos;m think I&apos;m done here.&lt;/p&gt;

&lt;p&gt;if you consider any of the bugs important just let me know, can try to help get them fixed.&lt;/p&gt;</comment>
                    <comment id="12721451" author="mikemccand" created="Thu, 18 Jun 2009 21:16:16 +0100"  >&lt;blockquote&gt;&lt;p&gt;michael: I&apos;m think I&apos;m done here.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;OK I&apos;ll review.  Thanks!!&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;if you consider any of the bugs important just let me know, can try to help get them fixed.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Likely I won&apos;t be able to judge the severity of these bugs... so please chime in if you think they should be fixed...&lt;/p&gt;</comment>
                    <comment id="12721457" author="rcmuir" created="Thu, 18 Jun 2009 21:24:14 +0100"  >&lt;p&gt;Michael, I think it would be nice to fix the Thai offset bug, so highlighter will work. this is a safe one-line fix and its an obvious error.&lt;/p&gt;

&lt;p&gt;The SmartChineseAnalyzer empty token bug is pretty serious, i think indexing empty tokens for every piece of punctuation could really hurt similarity computation (am i wrong, never tried?)&lt;/p&gt;

&lt;p&gt;The Thai .type() bug is something that could be fixed later, i don&apos;t think the token type being ALPHANUM versus NUM is really hurting anyone.&lt;/p&gt;

&lt;p&gt;The issue where DutchAnalyzer doesnt do what it claims, i think thats not really hurting anyone, and they can use the snowball version if they want accurate snowball behavior.&lt;br/&gt;
I do think the huge files in DutchAnalyzer that aren&apos;t being used can be removed if you want to save 1MB, but I&apos;m not sure how important that is.&lt;/p&gt;

&lt;p&gt;Let me know your thoughts. &lt;/p&gt;</comment>
                    <comment id="12721460" author="mikemccand" created="Thu, 18 Jun 2009 21:26:54 +0100"  >&lt;p&gt;I&apos;m seeing this test failure:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
    [junit] Testcase: testBuggyPunctuation(org.apache.lucene.analysis.cn.TestSmartChineseAnalyzer):	Caused an ERROR
    [junit] &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;
    [junit] java.lang.AssertionError
    [junit] 	at org.apache.lucene.analysis.StopFilter.next(StopFilter.java:240)
    [junit] 	at org.apache.lucene.analysis.cn.TestSmartChineseAnalyzer.testBuggyPunctuation(TestSmartChineseAnalyzer.java:51)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;It&apos;s because null is being passed to ts.next in the final assertTrue line:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
    nt = ts.next(nt);
    &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; (nt != &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;) {
      assertEquals(result[i], nt.term());
      i++;
      nt = ts.next(nt);
    }
    assertTrue(ts.next(nt) == &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                    <comment id="12721461" author="markrmiller@gmail.com" created="Thu, 18 Jun 2009 21:29:02 +0100"  >&lt;p&gt;heh -&lt;/p&gt;

&lt;p&gt;+1 on fixing them all. Including reclaiming that 1 mb of space if we can ...&lt;/p&gt;</comment>
                    <comment id="12721462" author="mikemccand" created="Thu, 18 Jun 2009 21:31:43 +0100"  >&lt;p&gt;Me too &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;  Robert can you cons up a patch?  Which files can be safely removed from the DutchAnalyzer?  (stems/words.txt?)&lt;/p&gt;</comment>
                    <comment id="12721463" author="rcmuir" created="Thu, 18 Jun 2009 21:32:17 +0100"  >&lt;p&gt;michael, i guess junit from my eclipse != junit from ant, because it passes in eclipse...annoying&lt;/p&gt;

&lt;p&gt;I will fix the test so it runs correctly from ant.&lt;/p&gt;</comment>
                    <comment id="12721466" author="rcmuir" created="Thu, 18 Jun 2009 21:33:54 +0100"  >&lt;p&gt;michael: yes the stems/words.txt&lt;/p&gt;

&lt;p&gt;for stems.txt/words.txt: I am scratching my head trying to figure out what they were originally intended to do. If its to support dictionary stemming with wordlistloader, then it really needs to be one tab-separated file, not two files.&lt;/p&gt;</comment>
                    <comment id="12721469" author="mikemccand" created="Thu, 18 Jun 2009 21:38:08 +0100"  >&lt;p&gt;Probably eclipse isn&apos;t running with asserts?&lt;/p&gt;</comment>
                    <comment id="12721475" author="rcmuir" created="Thu, 18 Jun 2009 21:46:32 +0100"  >&lt;p&gt;probably, fixed it and testing with ant now. ill upload it at least so you can verify the behavior i&apos;ve discovered.&lt;/p&gt;

&lt;p&gt;do you want me to include patch with the two bugfixes (chinese empty token and thai offsets), or give you something separate for those?&lt;/p&gt;

&lt;p&gt;for the other 2 bugs:&lt;br/&gt;
fixing the Thai tokentype bug, well its really a bug in the standardtokenizer grammar. i wasn&apos;t sure you wanted to change that at this moment, but if you want it fixed let me know!&lt;br/&gt;
in my opinion: fix for DutchAnalyzer is to deprecate/remove the contrib completely, since it claims to do snowball stemming, why shouldnt someone just use the Dutch snowball stemmer from the contrib/snowball package!&lt;/p&gt;

</comment>
                    <comment id="12721492" author="rcmuir" created="Thu, 18 Jun 2009 22:11:47 +0100"  >&lt;p&gt;Having trouble figuring this one out&lt;/p&gt;</comment>
                    <comment id="12721496" author="rcmuir" created="Thu, 18 Jun 2009 22:16:31 +0100"  >&lt;p&gt;michael: here is an updated patch.&lt;/p&gt;

&lt;p&gt;i removed that chinese test, there&apos;s something strange going on here &lt;span class=&quot;error&quot;&gt;&amp;#91;see my screenshot&amp;#93;&lt;/span&gt; but i can&apos;t seem to create a test case to show it!&lt;/p&gt;</comment>
                    <comment id="12721504" author="rcmuir" created="Thu, 18 Jun 2009 22:30:48 +0100"  >&lt;p&gt;ok got it,&lt;/p&gt;

&lt;p&gt;the IDEOGRAPHIC FULL STOP is being converted into a comma token by the tokenizer.&lt;br/&gt;
if you use the default constructor: SmartChineseAnalyzer(), it won&apos;t load the default stopwords list, such as from my Luke screenshot.&lt;br/&gt;
if you instead instantiate it like this: SmartChineseAnalyzer(true), then it loads the default stopwords list.&lt;br/&gt;
the default stopwords list includes things like comma, so it ends out getting removed.&lt;/p&gt;

&lt;p&gt;maybe its not a bug, but this is really non-obvious behavior...!&lt;/p&gt;</comment>
                    <comment id="12721507" author="rcmuir" created="Thu, 18 Jun 2009 22:33:23 +0100"  >&lt;p&gt;patch with new testcase demonstrating the chinese behavior.&lt;/p&gt;</comment>
                    <comment id="12721512" author="rcmuir" created="Thu, 18 Jun 2009 22:43:45 +0100"  >&lt;p&gt;later tonight i can workup a patch to address the thai offset issue and at least javadoc&apos;ing the chinese behavior.&lt;/p&gt;

&lt;p&gt;if you think the addt&apos;l 2 issues &lt;span class=&quot;error&quot;&gt;&amp;#91;thai tokentype, dutchanalyzer behavior/huge files&amp;#93;&lt;/span&gt; should be fixed or documented in some way, please let me know.&lt;/p&gt;</comment>
                    <comment id="12721639" author="rcmuir" created="Fri, 19 Jun 2009 04:43:02 +0100"  >&lt;p&gt;patch with the two one-line fixes:&lt;br/&gt;
1. fix offsets for thai analyzer so highlighting, etc will work.&lt;br/&gt;
2. use stopwords list by default for smartchineseanalyzer so punctuation isn&apos;t indexed in a strange way.&lt;/p&gt;

&lt;p&gt;i updated the testcases to reflect these.&lt;/p&gt;

</comment>
                    <comment id="12721711" author="mikemccand" created="Fri, 19 Jun 2009 10:41:21 +0100"  >&lt;p&gt;Latest patch looks good Robert, thanks!&lt;/p&gt;

&lt;p&gt;Deprecating DutchAnalyzer (in favor of Snowball) makes sense to me &amp;#8211; any objections out there?&lt;/p&gt;

&lt;p&gt;(And I&apos;ll &quot;svn rm&quot; the two large &amp;amp; unused files).&lt;/p&gt;

&lt;p&gt;Robert, could you open a new issue for the Thai token type bug (that requires a change to StandardTokenizer&apos;s grammar)?  We seem to be accumulating a number of these &quot;fix StandardTokeninizer&apos;s grammar&quot; but we don&apos;t have a good way to do this back-compatibly... matchVersion is a good way for the user to express compatibility requirement, but we don&apos;t know how to &lt;span class=&quot;error&quot;&gt;&amp;#91;cleanly&amp;#93;&lt;/span&gt; switch on that to different grammar variants.&lt;/p&gt;

&lt;p&gt;Is that the only issue not addressed by the latest patch?&lt;/p&gt;</comment>
                    <comment id="12721790" author="rcmuir" created="Fri, 19 Jun 2009 15:13:07 +0100"  >&lt;p&gt;michael, yes the only issue... i&apos;ll open another issue for the thai token type.&lt;/p&gt;</comment>
                    <comment id="12721824" author="mikemccand" created="Fri, 19 Jun 2009 16:38:50 +0100"  >&lt;p&gt;OK I will commit this soon.  Thanks Robert!&lt;/p&gt;</comment>
                    <comment id="12721831" author="mikemccand" created="Fri, 19 Jun 2009 16:52:45 +0100"  >&lt;p&gt;Thanks Robert!&lt;/p&gt;</comment>
                    <comment id="12721888" author="rcmuir" created="Fri, 19 Jun 2009 18:22:20 +0100"  >&lt;p&gt;michael, I updated my svn and I think you might have missed some of the tests.&lt;/p&gt;

&lt;p&gt;there are tests in the patch for BrazilianAnalyzer, CzechAnalyzer, and DutchAnalyzer... (these are new directories, maybe that is why?)&lt;/p&gt;</comment>
                    <comment id="12721906" author="mikemccand" created="Fri, 19 Jun 2009 18:56:20 +0100"  >&lt;p&gt;Duh, I forgot to svn add them!  Sorry.  I&apos;m glad you caught that.  I&apos;m really wanting &quot;svn patch&quot;....&lt;/p&gt;</comment>
                    <comment id="12721910" author="mikemccand" created="Fri, 19 Jun 2009 19:03:03 +0100"  >&lt;p&gt;OK I committed them.  Thanks for catching this Robert!&lt;/p&gt;</comment>
                    <comment id="12743973" author="rcmuir" created="Mon, 17 Aug 2009 06:26:13 +0100"  >&lt;p&gt;patch with a couple addtl tests for contrib/analysis, with some javadocs cleanup and wording.&lt;br/&gt;
there is also fix to the synonyms test to actually test its reset() ...&lt;br/&gt;
no code changes though.&lt;/p&gt;</comment>
                    <comment id="12743974" author="rcmuir" created="Mon, 17 Aug 2009 06:26:49 +0100"  >&lt;p&gt;if possible, i think these might be good to add for the release.&lt;/p&gt;</comment>
                    <comment id="12744025" author="rcmuir" created="Mon, 17 Aug 2009 12:45:04 +0100"  >&lt;p&gt;correct missing cjk test. &lt;/p&gt;

&lt;p&gt;if no one objects i would like to commit these javadocs and tests tomorrow.&lt;/p&gt;</comment>
                    <comment id="12744489" author="rcmuir" created="Tue, 18 Aug 2009 14:01:23 +0100"  >&lt;p&gt;Committed revision 805400.&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12411139" name="example.jpg" size="80997" author="rcmuir" created="Thu, 18 Jun 2009 22:11:47 +0100" />
                    <attachment id="12416764" name="LUCENE-1692_patch2.txt" size="99227" author="rcmuir" created="Mon, 17 Aug 2009 12:45:04 +0100" />
                    <attachment id="12416749" name="LUCENE-1692_patch2.txt" size="99293" author="rcmuir" created="Mon, 17 Aug 2009 06:26:13 +0100" />
                    <attachment id="12411182" name="LUCENE-1692.txt" size="21850" author="rcmuir" created="Fri, 19 Jun 2009 04:43:02 +0100" />
                    <attachment id="12411143" name="LUCENE-1692.txt" size="20850" author="rcmuir" created="Thu, 18 Jun 2009 22:33:23 +0100" />
                    <attachment id="12411142" name="LUCENE-1692.txt" size="20411" author="rcmuir" created="Thu, 18 Jun 2009 22:16:31 +0100" />
                    <attachment id="12411124" name="LUCENE-1692.txt" size="21220" author="rcmuir" created="Thu, 18 Jun 2009 19:58:47 +0100" />
                    <attachment id="12411036" name="LUCENE-1692.txt" size="15266" author="rcmuir" created="Thu, 18 Jun 2009 07:51:59 +0100" />
                    <attachment id="12410756" name="LUCENE-1692.txt" size="11084" author="rcmuir" created="Tue, 16 Jun 2009 06:30:05 +0100" />
                    <attachment id="12410724" name="LUCENE-1692.txt" size="5711" author="rcmuir" created="Mon, 15 Jun 2009 23:51:12 +0100" />
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>10.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Tue, 16 Jun 2009 14:17:40 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>12066</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    <customfieldvalue key="10120"><![CDATA[Patch Available]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>26034</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                    <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                <customfieldname>Time in Status</customfieldname>
                <customfieldvalues>
                    
                </customfieldvalues>
            </customfield>
                            </customfields>
    </item>
</channel>
</rss>