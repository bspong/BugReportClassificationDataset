<!-- 
RSS generated by JIRA (5.2.8#851-sha1:3262fdc28b4bc8b23784e13eadc26a22399f5d88) at Tue Jul 16 13:24:12 UTC 2013

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/LUCENE-753/LUCENE-753.xml?field=key&field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>5.2.8</version>
        <build-number>851</build-number>
        <build-date>26-02-2013</build-date>
    </build-info>

<item>
            <title>[LUCENE-753] Use NIO positional read to avoid synchronization in FSIndexInput</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-753</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;As suggested by Doug, we could use NIO pread to avoid synchronization on the underlying file.&lt;br/&gt;
This could mitigate any MT performance drop caused by reducing the number of files in the index format.&lt;/p&gt;</description>
                <environment></environment>
            <key id="12359055">LUCENE-753</key>
            <summary>Use NIO positional read to avoid synchronization in FSIndexInput</summary>
                <type id="2" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/newfeature.png">New Feature</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png">Resolved</status>
                    <resolution id="1">Fixed</resolution>
                                <assignee username="mikemccand">Michael McCandless</assignee>
                                <reporter username="yseeley@gmail.com">Yonik Seeley</reporter>
                        <labels>
                    </labels>
                <created>Tue, 19 Dec 2006 18:18:35 +0000</created>
                <updated>Tue, 25 Jan 2011 15:43:12 +0000</updated>
                    <resolved>Tue, 25 Jan 2011 15:43:12 +0000</resolved>
                                                            <component>core/store</component>
                        <due></due>
                    <votes>5</votes>
                        <watches>9</watches>
                                                    <comments>
                    <comment id="12459721" author="yseeley@gmail.com" created="Tue, 19 Dec 2006 18:37:43 +0000"  >&lt;p&gt;Patch for FSIndexInput to use a positional read call that doesn&apos;t use explicit synchronization.  Note that the implementation of that read call may still involve some synchronization depending on the JVM and OS (notably Windows which lacks a native pread AFAIK).&lt;/p&gt;</comment>
                    <comment id="12459724" author="yseeley@gmail.com" created="Tue, 19 Dec 2006 18:45:15 +0000"  >&lt;p&gt;This change should be faster on heavily loaded multi-threaded servers using the non-compound index format.&lt;br/&gt;
Performance tests are needed to see if there is any negative impact on single-threaded performance.&lt;/p&gt;

&lt;p&gt;Compound index format (CSIndexInput) still does synchronization because the base IndexInput is not cloned (and hence shared by all CSIndexInput clones).  It&apos;s unclear if getting rid of the synchronization is worth the cloning overhead in this case.&lt;/p&gt;</comment>
                    <comment id="12459731" author="cutting" created="Tue, 19 Dec 2006 19:17:28 +0000"  >&lt;p&gt;This patch continues to use BufferedIndexInput and allocates a new ByteBuffer for each call to read().  I wonder if it might be more efficient to instead directly extend IndexInput and always represent the buffer as a ByteBuffer?&lt;/p&gt;</comment>
                    <comment id="12459805" author="yseeley@gmail.com" created="Wed, 20 Dec 2006 01:39:13 +0000"  >&lt;p&gt;CSIndexInput synchronization could also be elimitated if there was a pread added to IndexInput&lt;/p&gt;

&lt;p&gt;  public abstract void readBytes(byte[] b, int offset, int len, long fileposition)&lt;/p&gt;

&lt;p&gt;Unfortunately, that would break any custom Directory based implementations out there, and we can&apos;t provide a suitable default with seek &amp;amp; read because we don&apos;t know what object to synchronize on.&lt;br/&gt;
Worth it or not???&lt;/p&gt;</comment>
                    <comment id="12459830" author="yseeley@gmail.com" created="Wed, 20 Dec 2006 06:01:58 +0000"  >&lt;p&gt;Here is a patch that directly extends IndexInput to make things a little easier.&lt;br/&gt;
I started with the code for BufferedIndexInput to avoid any bugs in read().&lt;br/&gt;
They share enough code that a common subclass could be factored out if desired (or changes made in BufferedIndexInput to enable easier sharing).&lt;/p&gt;

&lt;p&gt;ByteBuffer does have offset, length, etc, but I did not use them because BufferedIndexInput currently allocates the byte[] on demand, and thus would add additional checks to readByte().  Also, the NIO Buffer.get() isn&apos;t as efficient as our own array access.&lt;/p&gt;</comment>
                    <comment id="12459868" author="ghidi" created="Wed, 20 Dec 2006 09:25:18 +0000"  >&lt;p&gt;You can find a NIO variation of IndexInput attached to this issue: &lt;a href=&quot;http://issues.apache.org/jira/browse/LUCENE-519&quot; class=&quot;external-link&quot;&gt;http://issues.apache.org/jira/browse/LUCENE-519&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I had good results on multiprocessor machines under heavy load.&lt;/p&gt;

&lt;p&gt;Regards,&lt;br/&gt;
Bogdan&lt;/p&gt;</comment>
                    <comment id="12459967" author="yseeley@gmail.com" created="Wed, 20 Dec 2006 15:25:32 +0000"  >&lt;p&gt;Thanks for the pointer Bogdan, it&apos;s interesting you use transferTo instead of read... is there any advantage to this?  You still need to create a new object every read(), but at least it looks like a smaller object.&lt;/p&gt;

&lt;p&gt;It&apos;s also been pointed out to me that &lt;a href=&quot;http://issues.apache.org/jira/browse/LUCENE-414&quot; class=&quot;external-link&quot;&gt;http://issues.apache.org/jira/browse/LUCENE-414&lt;/a&gt; has some more NIO code.&lt;/p&gt;</comment>
                    <comment id="12459971" author="ghidi" created="Wed, 20 Dec 2006 15:42:18 +0000"  >&lt;p&gt;The Javadoc says that transferTo can be more efficient because the OS can transfer bytes directly from the filesystem cache to the target channel without actually copying them. &lt;/p&gt;</comment>
                    <comment id="12460289" author="yseeley@gmail.com" created="Thu, 21 Dec 2006 15:33:44 +0000"  >&lt;p&gt;&amp;gt; The Javadoc says that transferTo can be more efficient because the OS can transfer bytes&lt;br/&gt;
&amp;gt; directly from the filesystem cache to the target channel without actually copying them.&lt;/p&gt;

&lt;p&gt;Unfortunately, only for DirectByteBuffers and other FileChannels, not for HeapByteBuffers.&lt;br/&gt;
Sounds like we just need to do some benchmarking, but I have a bad feeling that all the checking overhead Sun added to NIO will cause it to be slower in the single threaded case.&lt;/p&gt;</comment>
                    <comment id="12460362" author="yseeley@gmail.com" created="Thu, 21 Dec 2006 22:45:36 +0000"  >&lt;p&gt;Attaching test that reads a file in different ways, either random access or serially, from a number of threads.&lt;/p&gt;</comment>
                    <comment id="12460363" author="yseeley@gmail.com" created="Thu, 21 Dec 2006 22:50:36 +0000"  >&lt;p&gt;Single-threaded random access performance of a fully cached 64MB file on my home PC (WinXP) , Java6:&lt;/p&gt;

&lt;p&gt;config: impl=ClassicFile serial=false nThreads=1 iterations=200 bufsize=1024 filelen=6518936&lt;br/&gt;
answer=81332126, ms=7781, MB/sec=167.5603649916463&lt;/p&gt;

&lt;p&gt;config: impl=ChannelFile serial=false nThreads=1 iterations=200 bufsize=1024 filelen=6518936&lt;br/&gt;
answer=81332126, ms=9203, MB/sec=141.66980332500273&lt;/p&gt;

&lt;p&gt;config: impl=ChannelPread serial=false nThreads=1 iterations=200 bufsize=1024 filelen=6518936&lt;br/&gt;
answer=81332126, ms=11672, MB/sec=111.70212474297463&lt;/p&gt;

&lt;p&gt;config: impl=ChannelTransfer serial=false nThreads=1 iterations=200 bufsize=1024 filelen=6518936&lt;br/&gt;
answer=81332126, ms=17328, MB/sec=75.2416435826408&lt;/p&gt;</comment>
                    <comment id="12549869" author="bripink" created="Mon, 10 Dec 2007 00:23:29 +0000"  >&lt;p&gt;Most of my workloads would benefit by removing the synchronization in FSIndexInput, so I took a closer look at this issue.  I found exactly the opposite results that Yonik did on two platforms that I use frequently in production (Solaris and Linux), and by a significant margin.  I even get the same behavior on the Mac, though I&apos;m not running Java6 there.&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;uname -a&lt;br/&gt;
Linux xxx 2.6.9-22.0.1.ELsmp #1 SMP Tue Oct 18 18:39:27 EDT 2005 i686 i686 i386 GNU/Linux&lt;/li&gt;
	&lt;li&gt;java -version&lt;br/&gt;
java version &quot;1.6.0_02&quot;&lt;br/&gt;
Java(TM) SE Runtime Environment (build 1.6.0_02-b05)&lt;br/&gt;
Java HotSpot(TM) Client VM (build 1.6.0_02-b05, mixed mode, sharing)&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;config: impl=ChannelPread serial=false nThreads=200 iterations=10 bufsize=1024 filelen=10485760&lt;br/&gt;
answer=0, ms=88543, MB/sec=236.85124741650947&lt;br/&gt;
config: impl=ClassicFile serial=false nThreads=200 iterations=10 bufsize=1024 filelen=10485760&lt;br/&gt;
answer=0, ms=150560, MB/sec=139.29011689691816&lt;/p&gt;



&lt;ol&gt;
	&lt;li&gt;uname -a&lt;br/&gt;
SunOS xxx 5.10 Generic_118844-26 i86pc i386 i86pc&lt;/li&gt;
	&lt;li&gt;java -version&lt;br/&gt;
java version &quot;1.6.0&quot;&lt;br/&gt;
Java(TM) SE Runtime Environment (build 1.6.0-b105)&lt;br/&gt;
Java HotSpot(TM) Server VM (build 1.6.0-b105, mixed mode)&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;config: impl=ChannelPread serial=false nThreads=200 iterations=10 bufsize=1024 filelen=10485760&lt;br/&gt;
answer=0, ms=39621, MB/sec=529.3031473208652&lt;/p&gt;

&lt;p&gt;config: impl=ClassicFile serial=false nThreads=200 iterations=10 bufsize=1024 filelen=10485760&lt;br/&gt;
answer=0, ms=119057, MB/sec=176.14688762525515&lt;/p&gt;
</comment>
                    <comment id="12550078" author="yseeley@gmail.com" created="Mon, 10 Dec 2007 13:56:46 +0000"  >&lt;p&gt;Brad, one possible difference is the number of threads we tested with.&lt;br/&gt;
I tested single-threaded (nThreads=1) to see what kind of slowdown a single query might see.&lt;/p&gt;

&lt;p&gt;A normal production  system shouldn&apos;t see 200 concurrent running search threads unless it&apos;s just about to fall over, or unless it&apos;s one of those massive multi-core systems.  After you pass a certain amount of parallelism, NIO can help.&lt;/p&gt;</comment>
                    <comment id="12550128" author="bripink" created="Mon, 10 Dec 2007 17:30:55 +0000"  >&lt;p&gt;Whoops; I should have paid more attention to the args.  The results in the single-threaded case still favor pread, but by a slimmer margin:&lt;/p&gt;

&lt;p&gt;Linux:&lt;/p&gt;

&lt;p&gt;config: impl=ClassicFile serial=false nThreads=1 iterations=200 bufsize=1024 filelen=10485760&lt;br/&gt;
answer=0, ms=9983, MB/sec=210.0723229490133&lt;/p&gt;

&lt;p&gt;config: impl=ChannelPread serial=false nThreads=1 iterations=200 bufsize=1024 filelen=10485760&lt;br/&gt;
answer=0, ms=9247, MB/sec=226.7926895209257&lt;/p&gt;


&lt;p&gt;Solaris 10:&lt;/p&gt;

&lt;p&gt;config: impl=ClassicFile serial=false nThreads=1 iterations=200 bufsize=1024 filelen=10485760&lt;br/&gt;
answer=0, ms=7381, MB/sec=284.12843788104595&lt;/p&gt;

&lt;p&gt;config: impl=ChannelPread serial=false nThreads=1 iterations=200 bufsize=1024 filelen=10485760&lt;br/&gt;
answer=0, ms=6245, MB/sec=335.81297037630105&lt;/p&gt;


&lt;p&gt;Mac OS X:&lt;/p&gt;

&lt;p&gt;config: impl=ChannelPread serial=false nThreads=1 iterations=200 bufsize=1024 filelen=10485760&lt;br/&gt;
answer=-914995, ms=19945, MB/sec=105.14675357232389&lt;/p&gt;

&lt;p&gt;config: impl=ClassicFile serial=false nThreads=1 iterations=200 bufsize=1024 filelen=10485760&lt;br/&gt;
answer=-914995, ms=26378, MB/sec=79.50382894836606&lt;/p&gt;
</comment>
                    <comment id="12550130" author="cutting" created="Mon, 10 Dec 2007 17:31:56 +0000"  >&lt;p&gt;&amp;gt; Brad, &lt;span class=&quot;error&quot;&gt;&amp;#91;...&amp;#93;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;That&apos;s Brian.  And right, the difference in your tests is the number of threads.&lt;/p&gt;

&lt;p&gt;Perhaps this is a case where one size will not fit all.  MmapDirectory is fastest on 64-bit platforms with lots of threads, while good-old-FSDirectory has always been fastest for single-threaded access.  Perhaps a PreadDirectory would be the Directory of choice for multi-threaded access of large indexes on 32-bit hardware?  It would be useful to benchmark this patch against MmapDirectory, since they both remove synchronization.&lt;/p&gt;</comment>
                    <comment id="12550135" author="cutting" created="Mon, 10 Dec 2007 18:00:49 +0000"  >&lt;p&gt;My prior remarks were posted before I saw Brian&apos;s latest benchmarks.&lt;/p&gt;

&lt;p&gt;While it would still be good to throw mmap into the mix, pread now looks like a strong contender for the one that might beat all.  It works well on 32-bit hardware, it&apos;s unsynchronized, and it&apos;s fast.  What&apos;s not to like?&lt;/p&gt;</comment>
                    <comment id="12550165" author="yseeley@gmail.com" created="Mon, 10 Dec 2007 19:55:14 +0000"  >&lt;p&gt;Weird... I&apos;m still getting slower results from pread on WinXP.&lt;br/&gt;
Can someone else verify on a windows box?&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;Yonik@spidey ~
$ c:/opt/jdk16/bin/java -server FileReadTest testfile ClassicFile &lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt; 1 200
config: impl=ClassicFile serial=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt; nThreads=1 iterations=200 bufsize=1024 filelen=9616000
answer=160759732, ms=14984, MB/sec=128.35024025627337

$ c:/opt/jdk16/bin/java -server FileReadTest testfile ClassicFile &lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt; 1 200
config: impl=ClassicFile serial=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt; nThreads=1 iterations=200 bufsize=1024 filelen=9616000
answer=160759732, ms=14640, MB/sec=131.36612021857923


$ c:/opt/jdk16/bin/java -server FileReadTest testfile ChannelPread &lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt; 1 200
config: impl=ChannelPread serial=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt; nThreads=1 iterations=200 bufsize=1024 filelen=9616000
answer=160759732, ms=21766, MB/sec=88.35798952494717

$ c:/opt/jdk16/bin/java -server FileReadTest testfile ChannelPread &lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt; 1 200
config: impl=ChannelPread serial=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt; nThreads=1 iterations=200 bufsize=1024 filelen=9616000
answer=160759732, ms=21718, MB/sec=88.55327378211622


$ c:/opt/jdk16/bin/java -version
java version &lt;span class=&quot;code-quote&quot;&gt;&quot;1.6.0_02&quot;&lt;/span&gt;
Java(TM) SE &lt;span class=&quot;code-object&quot;&gt;Runtime&lt;/span&gt; Environment (build 1.6.0_02-b06)
Java HotSpot(TM) Client VM (build 1.6.0_02-b06, mixed mode)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                    <comment id="12550168" author="rengels@ix.netcom.com" created="Mon, 10 Dec 2007 20:07:14 +0000"  >&lt;p&gt;I sent this via email, but probably need to add to the thread...&lt;/p&gt;

&lt;p&gt;I posted a bug on this to Sun a long while back.  This is a KNOWN BUG on Windows.&lt;/p&gt;

&lt;p&gt;NIO preads actually sync behind the scenes on some platforms.  Using multiple file descriptors is much faster.&lt;/p&gt;

&lt;p&gt;See bug &lt;a href=&quot;http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6265734&quot; class=&quot;external-link&quot;&gt;http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6265734&lt;/a&gt;&lt;/p&gt;

</comment>
                    <comment id="12550179" author="cutting" created="Mon, 10 Dec 2007 20:38:52 +0000"  >&lt;p&gt;So it looks like pread is ~50% slower on Windows, and ~5-25% faster on other platforms.  Is that enough of a difference that it might be worth having FSDirectory use different implementations of FSIndexInput based on the value of Constants.WINDOWS (and perhaps JAVA_VERSION)?&lt;/p&gt;</comment>
                    <comment id="12550182" author="mikemccand" created="Mon, 10 Dec 2007 20:56:10 +0000"  >&lt;blockquote&gt;
&lt;p&gt;Is that enough of a difference that it might be worth having FSDirectory use different implementations of FSIndexInput based on the value of Constants.WINDOWS (and perhaps JAVA_VERSION)?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;+1&lt;/p&gt;

&lt;p&gt;I think having good out-of-the-box defaults is extremely important (most users won&apos;t tune), and given the substantial cross platform differences here I think we should conditionalize the defaults according to the platform.&lt;/p&gt;</comment>
                    <comment id="12550187" author="rengels@ix.netcom.com" created="Mon, 10 Dec 2007 21:07:37 +0000"  >&lt;p&gt;As an aside, if the Lucene people voted on the Java bug (and or sent emails via the proper channels), hopefully the underlying bug can be fixed in the JVM.&lt;/p&gt;

&lt;p&gt;In my opinion it is a serious one - ruins any performance gains of using NIO on files.&lt;/p&gt;</comment>
                    <comment id="12550218" author="yseeley@gmail.com" created="Mon, 10 Dec 2007 22:34:07 +0000"  >&lt;p&gt;Updated test that fixes some thread synchronization issues to ensure that the &quot;answer&quot; is the same for all methods.&lt;/p&gt;

&lt;p&gt;Brian, in some of your tests the answer is &quot;0&quot;... is this because your test file consists of zeros (created via /dev/zero or equiv)?  UNIX systems treat blocks of zeros differently than normal files (they are stored as holes).  It shouldn&apos;t make too much of a difference in this case, but just to be sure, could you try with a real file?&lt;/p&gt;</comment>
                    <comment id="12550351" author="bripink" created="Tue, 11 Dec 2007 08:14:51 +0000"  >&lt;p&gt;Yeah, the file was full of zeroes.  But I created the files w/o holes and was using filesystems that don&apos;t compress file contents.  Just to be sure, though, I repeated the tests with a file with random contents; the results above still hold.&lt;/p&gt;</comment>
                    <comment id="12550376" author="bripink" created="Tue, 11 Dec 2007 09:47:45 +0000"  >&lt;p&gt;BTW, I think the performance win with Yonik&apos;s patch for some workloads could be far greater than what the simple benchmark illustrates.  Sure, pread might be marginally faster.   But the real win is avoiding synchronized access to the file.&lt;/p&gt;

&lt;p&gt;I did some IO tracing a while back on one particular workload that is characterized by:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;a small number of large compound indexes&lt;/li&gt;
	&lt;li&gt;short average execution time, particularly compared to disk response time&lt;/li&gt;
	&lt;li&gt;a 99+% FS cache hit rate&lt;/li&gt;
	&lt;li&gt;cache misses that tend to cluster on rare queries&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;In this workload where each query hits each compound index, the locking in FSIndexInput means that a single rare query clobbers the response time for all queries.  The requests to read cached data are serialized (fairly, even) with those that hit the disk.  While we can&apos;t get rid of the rare queries, we can allow the common ones to proceed against cached data right away.&lt;/p&gt;
</comment>
                    <comment id="12550675" author="mikemccand" created="Tue, 11 Dec 2007 18:54:28 +0000"  >&lt;p&gt;I ran Yonik&apos;s most recent FileReadTest.java on the platforms below,&lt;br/&gt;
testing single-threaded random access for fully cached 64 MB file.&lt;/p&gt;

&lt;p&gt;I tested two Windows XP Pro machines and got opposite results from&lt;br/&gt;
Yonik.  Yonik is your machine XP Home?&lt;/p&gt;

&lt;p&gt;I&apos;m showing ChannelTransfer to be much faster on all platforms except&lt;br/&gt;
Windows Server 2003 R2 Enterprise x64 where it&apos;s about the same as&lt;br/&gt;
ChannelPread and ChannelFile.&lt;/p&gt;

&lt;p&gt;The ChannelTransfer test is giving the wrong checksum, but I think&lt;br/&gt;
just a bug in how checksum is computed (it&apos;s using &quot;len&quot; which with&lt;br/&gt;
ChannelTransfer is just the chunk size written on each call to&lt;br/&gt;
write).  So I think the MB/sec is still correct.&lt;/p&gt;

&lt;p&gt;Mac OS X 10.4 (Sun java 1.5)&lt;br/&gt;
  config: impl=ClassicFile serial=false nThreads=1 iterations=200 bufsize=6518936 filelen=67108864&lt;br/&gt;
  answer=-44611, ms=32565, MB/sec=412.15331797942576&lt;/p&gt;

&lt;p&gt;  config: impl=ChannelFile serial=false nThreads=1 iterations=200 bufsize=6518936 filelen=67108864&lt;br/&gt;
  answer=-44611, ms=19512, MB/sec=687.8727347273473&lt;/p&gt;

&lt;p&gt;  config: impl=ChannelPread serial=false nThreads=1 iterations=200 bufsize=6518936 filelen=67108864&lt;br/&gt;
  answer=-44611, ms=19492, MB/sec=688.5785347835009&lt;/p&gt;

&lt;p&gt;  config: impl=ChannelTransfer serial=false nThreads=1 iterations=200 bufsize=6518936 filelen=67108864&lt;br/&gt;
  answer=147783, ms=16009, MB/sec=838.3892060715847&lt;/p&gt;

&lt;p&gt;Linux 2.6.22.1 (Sun java 1.5)&lt;br/&gt;
  config: impl=ClassicFile serial=false nThreads=1 iterations=200 bufsize=6518936 filelen=67108864&lt;br/&gt;
  answer=-44611, ms=37879, MB/sec=354.33281765622115&lt;/p&gt;

&lt;p&gt;  config: impl=ChannelFile serial=false nThreads=1 iterations=200 bufsize=6518936 filelen=67108864&lt;br/&gt;
  answer=-44611, ms=21845, MB/sec=614.4093751430535&lt;/p&gt;

&lt;p&gt;  config: impl=ChannelPread serial=false nThreads=1 iterations=200 bufsize=6518936 filelen=67108864&lt;br/&gt;
  answer=-44611, ms=21902, MB/sec=612.8103734818737&lt;/p&gt;

&lt;p&gt;  config: impl=ChannelTransfer serial=false nThreads=1 iterations=200 bufsize=6518936 filelen=67108864&lt;br/&gt;
  answer=147783, ms=15978, MB/sec=840.015821754913&lt;/p&gt;

&lt;p&gt;Windows Server 2003 R2 Enterprise x64 (Sun java 1.6)&lt;/p&gt;

&lt;p&gt;  config: impl=ClassicFile serial=false nThreads=1 iterations=200 bufsize=6518936 filelen=67108864&lt;br/&gt;
  answer=-44611, ms=32703, MB/sec=410.4141149130049&lt;/p&gt;

&lt;p&gt;  config: impl=ChannelFile serial=false nThreads=1 iterations=200 bufsize=6518936 filelen=67108864&lt;br/&gt;
  answer=-44611, ms=23344, MB/sec=574.9559972583961&lt;/p&gt;

&lt;p&gt;  config: impl=ChannelPread serial=false nThreads=1 iterations=200 bufsize=6518936 filelen=67108864&lt;br/&gt;
  answer=-44611, ms=23329, MB/sec=575.3256804835183&lt;/p&gt;

&lt;p&gt;  config: impl=ChannelTransfer serial=false nThreads=1 iterations=200 bufsize=6518936 filelen=67108864&lt;br/&gt;
  answer=147783, ms=23422, MB/sec=573.0412774314747&lt;/p&gt;

&lt;p&gt;Windows XP Pro SP2, laptop (Sun Java 1.5)&lt;/p&gt;

&lt;p&gt;  config: impl=ClassicFile serial=false nThreads=1 iterations=200 bufsize=6518936 filelen=67108864&lt;br/&gt;
  answer=-44611, ms=71253, MB/sec=188.36782731955148&lt;/p&gt;

&lt;p&gt;  config: impl=ChannelFile serial=false nThreads=1 iterations=200 bufsize=6518936 filelen=67108864&lt;br/&gt;
  answer=-44611, ms=57463, MB/sec=233.57243443607192&lt;/p&gt;

&lt;p&gt;  config: impl=ChannelPread serial=false nThreads=1 iterations=200 bufsize=6518936 filelen=67108864&lt;br/&gt;
  answer=-44611, ms=58043, MB/sec=231.23844046655068&lt;/p&gt;

&lt;p&gt;  config: impl=ChannelTransfer serial=false nThreads=1 iterations=200 bufsize=6518936 filelen=67108864&lt;br/&gt;
  answer=147783, ms=20039, MB/sec=669.7825640001995&lt;/p&gt;

&lt;p&gt;Windows XP Pro SP2, older desktop (Sun Java 1.6)&lt;/p&gt;

&lt;p&gt;  config: impl=ClassicFile serial=false nThreads=1 iterations=200 bufsize=6518936 filelen=67108864&lt;br/&gt;
  answer=-44611, ms=53047, MB/sec=253.01662299470283&lt;/p&gt;

&lt;p&gt;  config: impl=ChannelFile serial=false nThreads=1 iterations=200 bufsize=6518936 filelen=67108864&lt;br/&gt;
  answer=-44611, ms=34047, MB/sec=394.2130819161747&lt;/p&gt;

&lt;p&gt;  config: impl=ChannelPread serial=false nThreads=1 iterations=200 bufsize=6518936 filelen=67108864&lt;br/&gt;
  answer=-44611, ms=34078, MB/sec=393.8544750278772&lt;/p&gt;

&lt;p&gt;  config: impl=ChannelTransfer serial=false nThreads=1 iterations=200 bufsize=6518936 filelen=67108864&lt;br/&gt;
  answer=147783, ms=18781, MB/sec=714.6463340610192&lt;/p&gt;</comment>
                    <comment id="12550677" author="mikemccand" created="Tue, 11 Dec 2007 18:58:32 +0000"  >&lt;p&gt;I also just ran a test with 4 threads, random access, on Linux 2.6.22.1:&lt;/p&gt;

&lt;p&gt;  config: impl=ClassicFile serial=false nThreads=4 iterations=200 bufsize=6518936 filelen=67108864&lt;br/&gt;
  answer=-195110, ms=120856, MB/sec=444.22363142913883&lt;/p&gt;

&lt;p&gt;  config: impl=ChannelFile serial=false nThreads=4 iterations=200 bufsize=6518936 filelen=67108864&lt;br/&gt;
  answer=-195110, ms=88272, MB/sec=608.2006887801341&lt;/p&gt;

&lt;p&gt;  config: impl=ChannelPread serial=false nThreads=4 iterations=200 bufsize=6518936 filelen=67108864&lt;br/&gt;
  answer=-195110, ms=77672, MB/sec=691.2026367288084&lt;/p&gt;

&lt;p&gt;  config: impl=ChannelTransfer serial=false nThreads=4 iterations=200 bufsize=6518936 filelen=67108864&lt;br/&gt;
  answer=594875, ms=38390, MB/sec=1398.465517061735&lt;/p&gt;

&lt;p&gt;ChannelTransfer got even faster (scales up with added threads better).&lt;/p&gt;</comment>
                    <comment id="12550682" author="yseeley@gmail.com" created="Tue, 11 Dec 2007 19:20:04 +0000"  >&lt;p&gt;Mike, it looks like you are running with a bufsize of 6.5MB!&lt;br/&gt;
Apologies for my hard-to-use benchmark program &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/sad.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                    <comment id="12550685" author="yseeley@gmail.com" created="Tue, 11 Dec 2007 19:29:55 +0000"  >&lt;p&gt;I&apos;ll try fixing the transferTo test before anyone re-runs any tests.&lt;/p&gt;</comment>
                    <comment id="12550687" author="mikemccand" created="Tue, 11 Dec 2007 19:31:06 +0000"  >&lt;p&gt;Doh!!  Woops &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;  I will rerun...&lt;/p&gt;</comment>
                    <comment id="12550698" author="yseeley@gmail.com" created="Tue, 11 Dec 2007 20:10:36 +0000"  >&lt;p&gt;OK, uploading latest version of the test that should fix ChannelTransfer (it&apos;s also slightly optimized to not create a new object per call).&lt;/p&gt;

&lt;p&gt;Well, at least we&apos;ve learned that printing out all the input params for benchmarking programs is  good practice &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                    <comment id="12550701" author="mikemccand" created="Tue, 11 Dec 2007 20:16:37 +0000"  >&lt;p&gt;Thanks!  I&apos;ll re-run.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Well, at least we&apos;ve learned that printing out all the input params for benchmarking programs is good practice &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes indeed &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                    <comment id="12550710" author="mikemccand" created="Tue, 11 Dec 2007 20:39:44 +0000"  >&lt;p&gt;OK my results on Win XP now agree with Yonik&apos;s.&lt;/p&gt;

&lt;p&gt;On UNIX &amp;amp; OS X, ChannelPread is a bit (2-14%) better, but on windows&lt;br/&gt;
it&apos;s quite a bit (31-34%) slower.&lt;/p&gt;

&lt;p&gt;Win Server 2003 R2 Enterprise x64 (Sun Java 1.6):&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;config: impl=ClassicFile serial=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt; nThreads=1 iterations=200 bufsize=1024 filelen=67108864
answer=110480725, ms=68094, MB/sec=197.10654095808735

config: impl=ChannelFile serial=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt; nThreads=1 iterations=200 bufsize=1024 filelen=67108864
answer=110480725, ms=72594, MB/sec=184.88818359644048

config: impl=ChannelPread serial=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt; nThreads=1 iterations=200 bufsize=1024 filelen=67108864
answer=110480725, ms=98328, MB/sec=136.5000081360345

config: impl=ChannelTransfer serial=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt; nThreads=1 iterations=200 bufsize=1024 filelen=67108864
answer=110480725, ms=201563, MB/sec=66.58847506734867
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Win XP Pro SP2, laptop (Sun Java 1.5):&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;config: impl=ClassicFile serial=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt; nThreads=1 iterations=200 bufsize=1024 filelen=67108864
answer=110480725, ms=47449, MB/sec=282.8673481000653

config: impl=ChannelFile serial=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt; nThreads=1 iterations=200 bufsize=1024 filelen=67108864
answer=110480725, ms=54899, MB/sec=244.4811890926975

config: impl=ChannelPread serial=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt; nThreads=1 iterations=200 bufsize=1024 filelen=67108864
answer=110480725, ms=71683, MB/sec=187.237877878995

config: impl=ChannelTransfer serial=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt; nThreads=1 iterations=200 bufsize=1024 filelen=67108864
answer=110480725, ms=149475, MB/sec=89.79275999330991
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Linux 2.6.22.1 (Sun Java 1.5):&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;config: impl=ClassicFile serial=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt; nThreads=1 iterations=200 bufsize=1024 filelen=67108864
answer=110480725, ms=41162, MB/sec=326.0719304212623

config: impl=ChannelFile serial=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt; nThreads=1 iterations=200 bufsize=1024 filelen=67108864
answer=110480725, ms=53114, MB/sec=252.69745829724744

config: impl=ChannelPread serial=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt; nThreads=1 iterations=200 bufsize=1024 filelen=67108864
answer=110480725, ms=40226, MB/sec=333.65914582608264

config: impl=ChannelTransfer serial=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt; nThreads=1 iterations=200 bufsize=1024 filelen=67108864
answer=110480725, ms=59163, MB/sec=226.86092321214272
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Mac OS X 10.4 (Sun Java 1.5):&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;config: impl=ClassicFile serial=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt; nThreads=1 iterations=200 bufsize=1024 filelen=67108864
answer=110480725, ms=85894, MB/sec=156.25972477705076

config: impl=ChannelFile serial=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt; nThreads=1 iterations=200 bufsize=1024 filelen=67108864
answer=110480725, ms=109939, MB/sec=122.08381738964336

config: impl=ChannelPread serial=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt; nThreads=1 iterations=200 bufsize=1024 filelen=67108864
answer=110480725, ms=75517, MB/sec=177.73180608339845

config: impl=ChannelTransfer serial=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt; nThreads=1 iterations=200 bufsize=1024 filelen=67108864
answer=110480725, ms=130156, MB/sec=103.12066136021389
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                    <comment id="12558425" author="testn" created="Sun, 13 Jan 2008 17:05:11 +0000"  >&lt;p&gt;I think bufsize has way much bigger impact than the implementation. I found that 64KB buffer size is at least 5-6 times faster than 1KB. Should we tune this parameter instead for maximum performance.&lt;/p&gt;</comment>
                    <comment id="12609129" author="jasonrutherglen" created="Sun, 29 Jun 2008 19:22:22 +0100"  >&lt;p&gt;lucene-753.patch&lt;/p&gt;

&lt;p&gt;Made NIOFSDirectory that inherits from FSDirectory and includes the patch.  &lt;/p&gt;</comment>
                    <comment id="12609217" author="mikemccand" created="Mon, 30 Jun 2008 13:25:38 +0100"  >
&lt;p&gt;Carrying forward from this thread:&lt;/p&gt;

&lt;p&gt;  &lt;a href=&quot;http://mail-archives.apache.org/mod_mbox/lucene-java-dev/200806.mbox/%3C85d3c3b60806240501y96d3637r72b2181fa829fa00@mail.gmail.com%3E&quot; class=&quot;external-link&quot;&gt;http://mail-archives.apache.org/mod_mbox/lucene-java-dev/200806.mbox/%3C85d3c3b60806240501y96d3637r72b2181fa829fa00@mail.gmail.com%3E&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Jason Rutherglen &amp;lt;jason.rutherglen@gmail.com&amp;gt; wrote:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;After thinking more about the pool of RandomAccessFiles I think&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-753&quot; title=&quot;Use NIO positional read to avoid synchronization in FSIndexInput&quot;&gt;&lt;del&gt;LUCENE-753&lt;/del&gt;&lt;/a&gt; is the best solution.  I am not sure how much work nor if&lt;br/&gt;
pool of RandomAccessFiles creates more synchronization problems and if&lt;br/&gt;
it is only to benefit windows, does not seem worthwhile.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;It wasn&apos;t clear to me that pread would in fact perform better than&lt;br/&gt;
letting each thread uses its own private RandomAccessFile.&lt;/p&gt;

&lt;p&gt;So I modified (attached) FileReadTest.java to add a new SeparateFile&lt;br/&gt;
implementation, which opens a private RandomAccessFile per-thread and&lt;br/&gt;
then just does &quot;classic&quot; seeks &amp;amp; reads on that file.  Then I ran the&lt;br/&gt;
test on 3 platforms (results below), using 4 threads.&lt;/p&gt;

&lt;p&gt;The results are very interesting &amp;#8211; using SeparateFile is always&lt;br/&gt;
faster, especially so on WinXP Pro (115% faster than the next fastest,&lt;br/&gt;
ClassicFile) but also surprisingly so on Linux (44% faster than the&lt;br/&gt;
next fastest, ChannelPread).  On Mac OS X it was 5% faster than&lt;br/&gt;
ChannelPread.  So on all platforms it&apos;s faster, when using multiple&lt;br/&gt;
threads, to use separate files.&lt;/p&gt;

&lt;p&gt;I don&apos;t have a Windows server class machine readily accessible so if&lt;br/&gt;
someone could run on such a machine, and run on other machines&lt;br/&gt;
(Solaris) to see if these results are reproducible, that&apos;d be great.&lt;/p&gt;

&lt;p&gt;This is a strong argument for some sort of pooling of&lt;br/&gt;
RandomAccessFiles under FSDirectory, though the counter balance is&lt;br/&gt;
clearly added complexity.  I think if we combined the two approaches&lt;br/&gt;
(use separate RandomAccessFile objects per thread as managed by a&lt;br/&gt;
pool, and then use the best mode (classic on Windows &amp;amp; channel pread&lt;br/&gt;
on all others)) we&apos;d likely get the best performance yet.&lt;/p&gt;

&lt;p&gt;Mac OS X 10.5.3, single WD Velociraptor hard drive, Sun JRE 1.6.0_05&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
config: impl=ClassicFile serial=&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt; nThreads=4 iterations=100 bufsize=1024 filelen=67108864
answer=-23909200, ms=151884, MB/sec=176.73715203708093

config: impl=SeparateFile serial=&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt; nThreads=4 iterations=100 bufsize=1024 filelen=67108864
answer=-23909200, ms=97820, MB/sec=274.4177632386015

config: impl=ChannelPread serial=&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt; nThreads=4 iterations=100 bufsize=1024 filelen=67108864
answer=-23909200, ms=103059, MB/sec=260.4677476008888

config: impl=ChannelFile serial=&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt; nThreads=4 iterations=100 bufsize=1024 filelen=67108864
answer=-23909200, ms=176250, MB/sec=152.30380482269504

config: impl=ChannelTransfer serial=&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt; nThreads=4 iterations=100 bufsize=1024 filelen=67108864
answer=-23909200, ms=365904, MB/sec=73.36226332589969

&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;


&lt;p&gt;Linux 2.6.22.1, 6-drive RAID 5 array, Sun JRE 1.6.0_06&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
config: impl=ClassicFile serial=&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt; nThreads=4 iterations=100 bufsize=1024 filelen=67108864
answer=-23909200, ms=75592, MB/sec=355.1109323737962

config: impl=SeparateFile serial=&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt; nThreads=4 iterations=100 bufsize=1024 filelen=67108864
answer=-23909200, ms=35505, MB/sec=756.0497282072947

config: impl=ChannelPread serial=&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt; nThreads=4 iterations=100 bufsize=1024 filelen=67108864
answer=-23909200, ms=51075, MB/sec=525.5711326480665

config: impl=ChannelFile serial=&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt; nThreads=4 iterations=100 bufsize=1024 filelen=67108864
answer=-23909200, ms=95640, MB/sec=280.6727896277708

config: impl=ChannelTransfer serial=&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt; nThreads=4 iterations=100 bufsize=1024 filelen=67108864
answer=-23909200, ms=93711, MB/sec=286.45031639828835

&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;



&lt;p&gt;WIN XP PRO, laptop, Sun JRE 1.4.2_15:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
config: impl=ClassicFile serial=&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt; nThreads=4 iterations=100 bufsize=1024 filelen=67108864
answer=-23909200, ms=135349, MB/sec=198.32836297275932

config: impl=SeparateFile serial=&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt; nThreads=4 iterations=100 bufsize=1024 filelen=67108864
answer=-23909200, ms=62970, MB/sec=426.2910211211688

config: impl=ChannelPread serial=&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt; nThreads=4 iterations=100 bufsize=1024 filelen=67108864
answer=-23909200, ms=174606, MB/sec=153.73781886074937

config: impl=ChannelFile serial=&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt; nThreads=4 iterations=100 bufsize=1024 filelen=67108864
answer=-23909200, ms=152171, MB/sec=176.4038193873997

config: impl=ChannelTransfer serial=&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt; nThreads=4 iterations=100 bufsize=1024 filelen=67108864
answer=-23909200, ms=275603, MB/sec=97.39932293915524

&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                    <comment id="12609226" author="jasonrutherglen" created="Mon, 30 Jun 2008 13:56:09 +0100"  >&lt;p&gt;Interesting results.  The question would be, what would the algorithm for allocating RandomAccessFiles to which file look like?  When would a new file open, when would a file be closed?  If it is based on usage would it be based on the rate of calls to readInternal?  This seems like an OS filesystem topic that maybe there is some standard algorithm for.   How would the pool avoid the same synchronization issue given the default small buffer size of 1024?  If there are 30 threads executing searches, there will not be 30 RandomAccessFiles per file so there is still contention over the limited number of RandomAccessFiles allocated.   &lt;/p&gt;</comment>
                    <comment id="12609305" author="yseeley@gmail.com" created="Mon, 30 Jun 2008 18:47:47 +0100"  >&lt;p&gt;Added a PooledPread impl to FileReadTest, but at least on Windows it always seems slower than non-pooled.  I suppose it might be because of the extra synchronization.&lt;/p&gt;</comment>
                    <comment id="12609327" author="mikemccand" created="Mon, 30 Jun 2008 19:36:49 +0100"  >&lt;p&gt;I think you have a small bug &amp;#8211; minCount is initialized to 0 but should be something effectively infinite instead?&lt;/p&gt;</comment>
                    <comment id="12609339" author="yseeley@gmail.com" created="Mon, 30 Jun 2008 20:26:33 +0100"  >&lt;p&gt;Thanks Mike, after the bug is fixed, PooledPread is now faster on Windows when more than 1 thread is used.&lt;/p&gt;</comment>
                    <comment id="12609383" author="mikemccand" created="Mon, 30 Jun 2008 23:13:47 +0100"  >&lt;p&gt;OK I re-ran only PooledPread, SeparateFile and ChannelPread since they&lt;br/&gt;
are the &quot;leading contenders&quot; on all platforms.&lt;/p&gt;

&lt;p&gt;Also, I changed to serial=false.&lt;/p&gt;

&lt;p&gt;Now the results are very close on all but windows, but on windows I&apos;m&lt;br/&gt;
seeing the opposite of what Yonik saw: PooledPread is slowest, and&lt;br/&gt;
SeparateFile is fastest.  But this is a laptop (Win XP Pro), and it is&lt;br/&gt;
JRE 1.4.  Also I ran with pool size == number of threads == 4.&lt;/p&gt;


&lt;p&gt;Mac OS X 10.5.3, single WD Velociraptor hard drive, Sun JRE 1.6.0_05&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;config: impl=PooledPread serial=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt; nThreads=4 iterations=100 bufsize=1024 poolsize=4 filelen=67108864
answer=-23830370, ms=120807, MB/sec=222.20190551871994

config: impl=SeparateFile serial=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt; nThreads=4 iterations=100 bufsize=1024 poolsize=4 filelen=67108864
answer=-23830326, ms=119641, MB/sec=224.36744594244448

config: impl=ChannelPread serial=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt; nThreads=4 iterations=100 bufsize=1024 poolsize=4 filelen=67108864
answer=-23830370, ms=119217, MB/sec=225.1654176837196
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;


&lt;p&gt;Linux 2.6.22.1, 6-drive RAID 5 array, Sun JRE 1.6.0_06&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;config: impl=PooledPread serial=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt; nThreads=4 iterations=100 bufsize=1024 poolsize=4 filelen=67108864
answer=-23830370, ms=52613, MB/sec=510.2074696367818

config: impl=SeparateFile serial=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt; nThreads=4 iterations=100 bufsize=1024 poolsize=4 filelen=67108864
answer=-23830370, ms=52715, MB/sec=509.22025230010433

config: impl=ChannelPread serial=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt; nThreads=4 iterations=100 bufsize=1024 poolsize=4 filelen=67108864
answer=-23830370, ms=53792, MB/sec=499.0248661511005
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;


&lt;p&gt;WIN XP PRO, laptop, Sun JRE 1.4.2_15:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;config: impl=PooledPread serial=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt; nThreads=4 iterations=100 bufsize=1024 poolsize=4 filelen=67108864
answer=-23830370, ms=209956, MB/sec=127.85319590771401

config: impl=SeparateFile serial=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt; nThreads=4 iterations=100 bufsize=1024 poolsize=4 filelen=67108864
answer=-23830370, ms=89101, MB/sec=301.27098012367986

config: impl=ChannelPread serial=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt; nThreads=4 iterations=100 bufsize=1024 poolsize=4 filelen=67108864
answer=-23830370, ms=184087, MB/sec=145.81988733587923
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                    <comment id="12609401" author="bripink" created="Tue, 1 Jul 2008 00:29:47 +0100"  >&lt;p&gt;I was curious about the discrepancy between the ChannelPread implementation and the SeparateFile implementation that Yonik saw.  At least on Mac OS X, the kernel implementation of read is virtually the same as pread, so there shouldn&apos;t be any appreciable performance difference unless the VM is doing something funny.  Sure enough, the implementations of read() under RandomAccessFile and read() under FileChannel are totally different.  The former relies on a buffer allocated either on the stack or by malloc, while the latter allocates a native buffer and copies the results to the original array.&lt;/p&gt;

&lt;p&gt;Switching to a native buffer in the benchmark yields identical results for ChannelPread and SeparateFile on 1.5 and 1.6 on OS X.  I&apos;m attaching an implementation of ChannelPreadDirect that uses a native buffer.&lt;/p&gt;

&lt;p&gt;This may be a moot point because any implementation inside Lucene needs to consume a byte[] and not a ByteBuffer, but at least it&apos;s informative.&lt;/p&gt;
</comment>
                    <comment id="12609435" author="yseeley@gmail.com" created="Tue, 1 Jul 2008 04:53:37 +0100"  >&lt;p&gt;Here are some of my results with 4 threads and a pool size of 4 fds per file.  Win XP on a Pentium4 w/ Java5_0_11 -server&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;config: impl=PooledPread serial=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt; nThreads=4 iterations=100 bufsize=1024 poolsize=4 filelen=9616000
answer=322211190, ms=51891, MB/sec=74.12460735002217

config: impl=ChannelPread serial=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt; nThreads=4 iterations=100 bufsize=1024 poolsize=4 filelen=9616000
answer=322211190, ms=71175, MB/sec=54.04144713733755

config: impl=ClassicFile serial=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt; nThreads=4 iterations=100 bufsize=1024 poolsize=4 filelen=9616000
answer=322211190, ms=62699, MB/sec=61.34707092617107

config: impl=SeparateFile serial=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt; nThreads=4 iterations=100 bufsize=1024 poolsize=4 filelen=9616000
answer=322211410, ms=21324, MB/sec=180.37891577565185
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                    <comment id="12609514" author="mikemccand" created="Tue, 1 Jul 2008 11:05:33 +0100"  >
&lt;p&gt;OK it&apos;s looking like SeparateFile is the best overall choice...  it&lt;br/&gt;
matches the best performance on Unix platforms and is very much the&lt;br/&gt;
lead on Windows.&lt;/p&gt;

&lt;p&gt;It&apos;s somewhat surprising to me that after all this time, with these&lt;br/&gt;
new IO APIs, the most naive approach (using a separate&lt;br/&gt;
RandomAccessFile per thread) still yields the best performance.  In&lt;br/&gt;
fact, opening multiple IndexReaders to gain concurrency is doing just&lt;br/&gt;
this.&lt;/p&gt;

&lt;p&gt;Of course this is a synthetic benchmark.  Actual IO with Lucene is&lt;br/&gt;
somewhat different.  EG it&apos;s a mix of serial (when iterating through a&lt;br/&gt;
term&apos;s docs with no skipping) and somewhat random access (when&lt;br/&gt;
retrieving term vectors or stored fields), and presumably a mix of&lt;br/&gt;
hits &amp;amp; misses to the OS&apos;s IO cache.  So until we try this out with a&lt;br/&gt;
real index and real queries we won&apos;t know for sure.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;The question would be, what would the algorithm for allocating&lt;br/&gt;
RandomAccessFiles to which file look like?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Ideally it would be based roughly on contention.  EG a massive CFS&lt;br/&gt;
file in your index should have a separate file per-thread, if there&lt;br/&gt;
are not too many threads, whereas tiny CFS files in the index likely&lt;br/&gt;
could share / synchronize on a single file&lt;/p&gt;

&lt;p&gt;I think it would have thread affinity (if the same thread wants the&lt;br/&gt;
same file we give back the same RandomAccessFile that thread last&lt;br/&gt;
used, if it&apos;s available).&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;When would a new file open, when would a file be closed?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think this should be reference counting.  The first time Lucene&lt;br/&gt;
calls FSDirectory.openInput on a given name, we must for-real open the&lt;br/&gt;
file (Lucene relies on OS protecting open files).  Further opens on&lt;br/&gt;
that file incRef it.  Closes decRef it and when the reference count&lt;br/&gt;
gets to 0 we close it for real.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;If it is based on usage would it be based on the rate of calls to&lt;br/&gt;
readInternal?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Fortunately, Lucene tends to call IndexInput.clone() when it wants to&lt;br/&gt;
actively make use of a file.&lt;/p&gt;

&lt;p&gt;So I think the pool could work something like this: FSIndexInput.clone&lt;br/&gt;
would &quot;check out&quot; a file from the pool.  The pool decides at that&lt;br/&gt;
point to either return a SharedFile (which has locking per-read, like&lt;br/&gt;
we do now), or a PrivateFile (which has no locking because you are the&lt;br/&gt;
only thread currently using that file), based on some measure of&lt;br/&gt;
contention plus some configuration of the limit of allowed open files.&lt;/p&gt;

&lt;p&gt;One problem with this approach is I&apos;m not sure clones are always&lt;br/&gt;
closed, since they are currently very lightweight and can rely on GC&lt;br/&gt;
to reclaim them.&lt;/p&gt;

&lt;p&gt;An alternative approach would be to sync() on every block (1024 bytes&lt;br/&gt;
default now) read, find a file to use, and use it, but I fear that&lt;br/&gt;
will have poor performance.&lt;/p&gt;

&lt;p&gt;In fact, if we build this pool, we can again try all these alternative&lt;br/&gt;
IO APIs, maybe even leaving that choice to the Lucene user as&lt;br/&gt;
&quot;advanced tuning&quot;.&lt;/p&gt;</comment>
                    <comment id="12609537" author="rengels@ix.netcom.com" created="Tue, 1 Jul 2008 12:58:57 +0100"  >&lt;p&gt;As I stated quit a while ago, this has been a long accepted bug in the JDK.&lt;/p&gt;

&lt;p&gt;See &lt;a href=&quot;http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6265734&quot; class=&quot;external-link&quot;&gt;http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6265734&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;It was filed and accepted over 3 years ago.&lt;/p&gt;

&lt;p&gt;The problem is that the pread performs an unnecessary lock on the file descriptor, instead of using Windows &quot;overlapped&quot; reads.&lt;/p&gt;</comment>
                    <comment id="12609538" author="rengels@ix.netcom.com" created="Tue, 1 Jul 2008 13:06:54 +0100"  >&lt;p&gt;The point being - please vote for this issue so it can be fixed properly. It is really a trivial fix, but it needs to be done by SUN.&lt;/p&gt;</comment>
                    <comment id="12609606" author="yseeley@gmail.com" created="Tue, 1 Jul 2008 15:28:31 +0100"  >&lt;p&gt;.bq OK it&apos;s looking like SeparateFile is the best overall choice... it matches the best performance on Unix platforms and is very much the&lt;br/&gt;
lead on Windows.&lt;/p&gt;

&lt;p&gt;The other implementations are fully-featured though (they could be used in lucene w/ extra synchronization, etc).  SeparateFile (opening a new file descriptor per reader) is not a real implementation that could be used... it&apos;s more of a theoretical maximum IMO.  Also remember that you can&apos;t open a new fd on demand since the file might already be deleted.  We would need a real PooledClassicFile implementation (like PooledPread).&lt;/p&gt;

&lt;p&gt;On non-windows it looks like ChannelPread is probably the right choice.. near max performance and min fd usage&lt;/p&gt;
</comment>
                    <comment id="12609629" author="jasonrutherglen" created="Tue, 1 Jul 2008 17:15:36 +0100"  >&lt;p&gt;Core2Duo Windows XP JDK1.5.15.  PooledPread for 4 threads and pool size 2 the performance does not compare well to SeparateFile.  PooledPread for 30 threads does not improve appreciably over ClassicFile.  If there were 30 threads, how many RandomAccessFiles would there need to be to make a noticeable impact?  The problem I see with the pooled implementation is setting the global file descriptor limit properly, will the user set this?  There would almost need to be a native check to see if what the user is trying to do is possible.  &lt;/p&gt;

&lt;p&gt;The results indicate there is significant contention in the pool code.  The previous tests used a pool size the same as the number of threads which is probably not how most production systems look, at least the SOLR installations I&apos;ve worked on.  In SOLR the web request thread is the thread that executes the search, so the number of threads is determined by the J2EE server which can be quite high.  Unless the assumption is the system is set for an unusually high number of file descriptors.  &lt;/p&gt;

&lt;p&gt;There should probably be a MMapDirectory test as well.  &lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;config: impl=PooledPread serial=false nThreads=4 iterations=100 bufsize=1024 poolsize=2 filelen=18110448
answer=53797223, ms=32715, MB/sec=221.4329573590096

config: impl=SeparateFile serial=false nThreads=4 iterations=100 bufsize=1024 poolsize=2 filelen=18110448
answer=53797223, ms=18687, MB/sec=387.6587574249478

config: impl=SeparateFile serial=false nThreads=30 iterations=100 bufsize=1024 poolsize=2 filelen=18110448
answer=403087371, ms=137871, MB/sec=394.0737646060448

config: impl=PooledPread serial=false nThreads=30 iterations=100 bufsize=1024 poolsize=2 filelen=18110448
answer=403087487, ms=526504, MB/sec=103.19265190767781

config: impl=ChannelPread serial=false nThreads=30 iterations=100 bufsize=1024 poolsize=2 filelen=18110448
answer=403087487, ms=624291, MB/sec=87.02887595688549

config: impl=ClassicFile serial=false nThreads=30 iterations=100 bufsize=1024 poolsize=2 filelen=18110448
answer=403087487, ms=587430, MB/sec=92.48990347786119

config: impl=PooledPread serial=false nThreads=30 iterations=100 bufsize=1024 poolsize=4 filelen=18110448
answer=403087487, ms=552971, MB/sec=98.25351419875544
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                    <comment id="12609827" author="mikemccand" created="Wed, 2 Jul 2008 09:34:59 +0100"  >&lt;blockquote&gt;
&lt;p&gt;SeparateFile (opening a new file descriptor per reader) is not a real implementation that could be used... it&apos;s more of a theoretical maximum IMO. Also remember that you can&apos;t open a new fd on demand since the file might already be deleted. We would need a real PooledClassicFile implementation (like PooledPread).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;True, we&apos;d have to make a real pool, but I&apos;d think we want the sync() to be on cloning and not on every read.  I think Lucene&apos;s usage of the open files (clones are made &amp;amp; used up quickly and closed) would work well with that approach.  I think at this point we should build out an underlying pool and then test all of these different approaches under the pool.&lt;/p&gt;

&lt;p&gt;And yes we cannot just open a new fd on demand if the file has been deleted.  But I&apos;m thinking that may not matter in practice.  Ie if the pool wants to open a new fd, it can attempt to do so, and if the file was deleted it must then return a shared access wrapper to the fd it already has open.  Large segments are where the contention will be and large segments are not often deleted.  Plus people tend to open new readers if such a large change has taken place to the index.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;On non-windows it looks like ChannelPread is probably the right choice.. near max performance and min fd usage&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;But on Linux I saw 44% speedup for serial=true case with 4 threads using SeparateFile vs ChannelPread, which I was very surprised by.  But then again it&apos;s synthetic so it may not matter in real Lucene searches.&lt;/p&gt;</comment>
                    <comment id="12609909" author="jasonrutherglen" created="Wed, 2 Jul 2008 13:55:44 +0100"  >&lt;p&gt;lucene-753.patch&lt;/p&gt;

&lt;p&gt;Added javadoc and removed unnecessary NIOFSIndexOutput class.&lt;/p&gt;</comment>
                    <comment id="12609916" author="yseeley@gmail.com" created="Wed, 2 Jul 2008 14:20:20 +0100"  >&lt;blockquote&gt;&lt;p&gt;(clones are made &amp;amp; used up quickly and closed) &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;IIRC, clones are often not closed at all.&lt;br/&gt;
And for term expanding queries, you can get a &lt;b&gt;lot&lt;/b&gt; of them all at once.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;And yes we cannot just open a new fd on demand if the file has been deleted. But I&apos;m thinking that may not matter in practice. Ie if the pool wants to open a new fd, it can attempt to do so, and if the file was deleted it must then return a shared access wrapper to the fd it already has open.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;At first blush, sounds a bit too complex for the benefits.&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;one would have to reserve the last fd for synchronized access... can&apos;t really hand it out for unsynchronized exclusive access and then go and share it later.&lt;/li&gt;
	&lt;li&gt;the shared access should use pread... not seek+read&lt;/li&gt;
&lt;/ul&gt;


&lt;blockquote&gt;&lt;p&gt;But on Linux I saw 44% speedup for serial=true case with 4 threads using SeparateFile vs ChannelPread, which I was very surprised by.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;In the serial case, there are half the system calls (no seek).  When both implementations have a single single system call, all the extra code and complexity that Sun threw into FileChannel comes into play.  Compare that with RandomAccessFile.read() which drops down to a native call and presumably just the read with little overhead.  I wish Sun would just add a RandomAccessFile.read with a file position.&lt;/p&gt;

&lt;p&gt;If access will be truly serial sometimes, larger buffers would help with that larger read() setup cost. &lt;/p&gt;</comment>
                    <comment id="12609990" author="mikemccand" created="Wed, 2 Jul 2008 19:01:06 +0100"  >&lt;blockquote&gt;&lt;p&gt;And for term expanding queries, you can get a lot of them all at once.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Right but that&apos;d all be under one thread right?  The pool would always give the same RandomAccessFile (private or shared) for the same filename X thread.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;one would have to reserve the last fd for synchronized access... can&apos;t really hand it out for unsynchronized exclusive access and then go and share it later.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Well, I think you&apos;d hand it out first, as a shared file (so you reserve the right to hand it out again, later).  If other threads come along you would open a new one (if you are under the budget) and loan it to them privately (so no syncing during read).  I think sync&apos;ing with no contention (the first shared file we hand out) should be OK performance in modern JVMs.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;the shared access should use pread... not seek+read&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;But not on Windows...&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;At first blush, sounds a bit too complex for the benefits.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yeah I&apos;m on the fence too ... but this lack of concurrency hurts our search performance.  It&apos;s ashame users have to resort to multiple IndexReaders.  Though it still remains to be seen how much the pool or pread approaches really improve end to end search performance (vs other bottlenecks like IndexReader.isDeleted).&lt;/p&gt;

&lt;p&gt;Windows is an important platform and doing the pool approach, vs leaving Windows with classic if we do pread approach, lets us have good concurrency on Windows too.&lt;/p&gt;</comment>
                    <comment id="12614074" author="briangardner" created="Wed, 16 Jul 2008 20:32:44 +0100"  >&lt;p&gt;This probably doesn&apos;t help much, but I implemented a pool and submitted a patch very similar to the SeparateFile approach.  Before being directed to this thread: &lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-1337&quot; class=&quot;external-link&quot;&gt;https://issues.apache.org/jira/browse/LUCENE-1337&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;In our implementation the synchronization/lack of concurrency has been a big issue for us.  On several occasions we&apos;ve had to remove new features that perform searches from frequently hit pages, because threads build up waiting for synchronized access to the underlying files.  It is possible that I would still have issue even with my patch, considering from my tests that I&apos;m only increasing throughput by 300%,  but it would be easier for me to tune and scale my application since resource utilization and contention would be visible from the OS level. &lt;/p&gt;


&lt;p&gt;&amp;gt; At first blush, sounds a bit too complex for the benefits.&lt;/p&gt;

&lt;p&gt;My vote is that the benefits outway the complexity, especially considering it&apos;s an out-of-the box solutions that works well for all platforms and single threaded as well as multi-threaded envirnments.  If it&apos;s helpful, I can spend the time to implement some of the missing feature(s) of the pool that will be needed for it to be an acceptable solution (i.e, shared access once a file has been deleted, and perhaps a time-based closing mechanism).&lt;/p&gt;</comment>
                    <comment id="12614290" author="mikemccand" created="Thu, 17 Jul 2008 11:21:42 +0100"  >
&lt;blockquote&gt;
&lt;p&gt;In our implementation the synchronization/lack of concurrency has been a big issue for us. On several occasions we&apos;ve had to remove new features that perform searches from frequently hit pages, because threads build up waiting for synchronized access to the underlying files.  It is possible that I would still have issue even with my patch, considering from my tests that I&apos;m only increasing throughput by 300%, but it would be easier for me to tune and scale my application since resource utilization and contention would be visible from the OS level. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Can you describe your test &amp;#8211; OS, JRE version, size/type of your index, number of cores, amount of RAM, type of IO system, etc?  It&apos;s awesome that you see 300% gain in search throughput.  Is your index largely cached in the OS&apos;s IO cache, or not?&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;My vote is that the benefits outway the complexity, especially considering it&apos;s an out-of-the box solutions that works well for all platforms and single threaded as well as multi-threaded envirnments. If it&apos;s helpful, I can spend the time to implement some of the missing feature(s) of the pool that will be needed for it to be an acceptable solution (i.e, shared access once a file has been deleted, and perhaps a time-based closing mechanism).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;If we can see sizable concurreny gains, reliably &amp;amp; across platforms, I agree we should pursue this approach.  One particular frustration is: if you optimize your index, thinking this gains you better search performance, you&apos;re actually making things far worse as far as concurrency is concerned because now you are down to a single immense file.  I think we do need to fix this situation.&lt;/p&gt;

&lt;p&gt;On your patch, I think in addition to shared-access on a now-deleted file, we should add a global control on the &quot;budget&quot; of number of open files (right now I think your patch has a fixed cap per-filename).  Probably the budget should be expressed as a multiplier off the minimum number of open files, rather than a fixed cap, so that an index with many segments is allowed to use more.  Ideally over time the pool works out such that for small files in the index (small segments) since there is very little contention they only hold 1 descriptor open, but for large files many descriptors are opened.&lt;/p&gt;

&lt;p&gt;I created a separate test (will post a patch &amp;amp; details to this issue) to explore using SeparateFile inside FSDirectory, but unfortunately I see mixed results on both the cached &amp;amp; uncached cases.  I&apos;ll post details separately.&lt;/p&gt;

&lt;p&gt;One issue with your patch is it&apos;s using Java 5 only classes (Lucene is still on 1.4); once you downgrade to 1.4 I wonder if the added synchronization will become costly.&lt;/p&gt;

&lt;p&gt;I like how your approach is to pull a RandomAccessFile from the pool only when a read is taking place &amp;#8211; this automatically takes care of creating new descriptors when there truly is contention.  But one concern I have is that this defeats the OS&apos;s IO system&apos;s read-ahead optimization since from the OS&apos;s perspective the file descriptors are getting shuffled.  I&apos;m not sure if this really matters much in Lucene, because many things (reading stored fields &amp;amp; term vectors) are likely not helped much by read-ahead, but for example a simple TermQuery on a large term should in theory benefit from read-ahead.  You could gain this back with a simple thread affinity, such that the same thread gets the same file descriptor it got last time, if it&apos;s available.  But that added complexity may offset any gains.&lt;/p&gt;</comment>
                    <comment id="12614477" author="mikemccand" created="Thu, 17 Jul 2008 19:18:30 +0100"  >
&lt;p&gt;I attached FSDirectoryPool.patch, which adds&lt;br/&gt;
oal.store.FSDirectoryPool, a Directory that will open a new file for&lt;br/&gt;
every unique thread.&lt;/p&gt;

&lt;p&gt;This is intended only as a test (to see if shows consistent&lt;br/&gt;
improvement in concurrency) &amp;#8211; eg it does not close all these files,&lt;br/&gt;
nor make any effort to budget itself if there are too many threads,&lt;br/&gt;
it&apos;s not really a pool, etc.  But it should give us an upper bound on&lt;br/&gt;
the gains we could hope for.&lt;/p&gt;

&lt;p&gt;I also added a &quot;pool=true|false&quot; config option to contrib/benchmark so&lt;br/&gt;
you can run tests with and without separate files.&lt;/p&gt;

&lt;p&gt;I ran some quick initial tests but didn&apos;t see obvious gains.  I&apos;ll go&lt;br/&gt;
back &amp;amp; re-run more carefully to confirm, and post back.&lt;/p&gt;</comment>
                    <comment id="12614974" author="mikemccand" created="Sat, 19 Jul 2008 11:29:49 +0100"  >&lt;p&gt;I created a large index (indexed Wikipedia 4X times over, with stored&lt;br/&gt;
fields &amp;amp; tv offsets/positions = 72 GB).  I then randomly sampled 50&lt;br/&gt;
terms &amp;gt; 1 million freq, plus 200 terms &amp;gt; 100,000 freq plus 100 terms &amp;gt;&lt;br/&gt;
10,000 freq plus 100 terms &amp;gt; 1000 freq.  Then I warmed the OS so these&lt;br/&gt;
queries are fully cached in the IO cache.&lt;/p&gt;

&lt;p&gt;It&apos;s a highly synthetic test.  I&apos;d really love to test on real&lt;br/&gt;
queries, instead of single term queries.&lt;/p&gt;

&lt;p&gt;Then I ran this alg:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;analyzer=org.apache.lucene.analysis.standard.StandardAnalyzer

query.maker = org.apache.lucene.benchmark.byTask.feeds.FileBasedQueryMaker
file.query.maker.file = /lucene/wikiQueries.txt

directory=FSDirectory
pool=&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;

work.dir=/lucene/bigwork

OpenReader

{ &lt;span class=&quot;code-quote&quot;&gt;&quot;Warmup&quot;&lt;/span&gt; SearchTrav(20) &amp;gt; : 5

{ &lt;span class=&quot;code-quote&quot;&gt;&quot;Rounds&quot;&lt;/span&gt;
  [{ &lt;span class=&quot;code-quote&quot;&gt;&quot;Search&quot;&lt;/span&gt; Search &amp;gt; : 500]: 16
  NewRound
}: 2

CloseReader 

RepSumByPrefRound Search
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I ran with 2, 4, 8 and 16 threads, on a Intel quad Mac Pro (2 cpus,&lt;br/&gt;
each dual core) OS X 10.5.4, with 6 GB RAM, Sun JRE 1.6.0_05 and a&lt;br/&gt;
single WD Velociraptor hard drive.  To keep the number of searches&lt;br/&gt;
constant I changed the 500 count above to match (ie with 8 threads I&lt;br/&gt;
changed 500 -&amp;gt; 1000, 4 threads I changed it to 2000, etc.).&lt;/p&gt;

&lt;p&gt;Here&apos;re the results &amp;#8211; each run is best of 2, and all searches are&lt;br/&gt;
fully cached in OS&apos;s IO cache:&lt;/p&gt;

&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Number of Threads&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Patch rec/s&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Trunk rec/s&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Pctg gain&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;78.7&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;74.9&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;5.1%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;4&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;74.1&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;68.2&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;8.7%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;8&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;37.7&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;32.7&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;15.3%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;16&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;19.2&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;16.3&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;17.8%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;


&lt;p&gt;I also ran the same alg, replacing Search task with SearchTravRet(10)&lt;br/&gt;
(retrieves the first 10 docs (hits) of each search), first warming so&lt;br/&gt;
it&apos;s all fully cached:&lt;/p&gt;

&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Number of Threads&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Patch rec/s&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Trunk rec/s&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Pctg gain&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1589.6&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1519.8&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;4.6%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;4&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1460.9&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1395.3&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;4.7%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;8&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;748.9&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;676.0&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;10.8%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;16&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;382.7&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;338.4&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;13.1%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;


&lt;p&gt;So there are smallish gains, but rememember these are upper bounds on&lt;br/&gt;
the gains because no pooling is happening.  I&apos;ll test uncached next.&lt;/p&gt;</comment>
                    <comment id="12615727" author="mikemccand" created="Tue, 22 Jul 2008 19:44:05 +0100"  >
&lt;p&gt;OK I ran the uncached test, using the Search task.  JRE &amp;amp; hardware are&lt;br/&gt;
the same as above.&lt;/p&gt;

&lt;p&gt;I generated a larger (6150) set of queries to make sure the threads&lt;br/&gt;
never wrap around and do the same queries again.  I also run only 1&lt;br/&gt;
round for the same reason.  Between tests I evict the OS&apos;s IO cache.&lt;/p&gt;

&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Number of Threads&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Patch rec/s&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Trunk rec/s&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Pctg gain&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;32.2&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;23.8&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;35.3%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;4&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;16.4&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;12.7&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;29.1%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;8&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;8.5&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;3.5&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;142.9%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;16&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;3.8&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2.7&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;40.7%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;


&lt;p&gt;The gains are better.  The 8 thread case I don&apos;t get; I re-ran it and&lt;br/&gt;
it still came out much better (135.7%).  It could be 8 threads is the&lt;br/&gt;
sweet spot for concurrency on this hardware.&lt;/p&gt;</comment>
                    <comment id="12617880" author="mmastrac" created="Tue, 29 Jul 2008 18:57:01 +0100"  >&lt;p&gt;I just tried out the latest NIOFSDirectory patch and I&apos;m seeing a bug.  If I go back to the regular FSDirectory, everything works fine.&lt;/p&gt;

&lt;p&gt;I can&apos;t reproduce it on a smaller testcase.  It only happens with the live index.&lt;/p&gt;

&lt;p&gt;Any ideas on where to debug?&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt; 
Caused by: java.lang.IndexOutOfBoundsException: Index: 24444, Size: 4
	at java.util.ArrayList.RangeCheck(ArrayList.java:547)
	at java.util.ArrayList.get(ArrayList.java:322)
	at org.apache.lucene.index.FieldInfos.fieldInfo(FieldInfos.java:260)
	at org.apache.lucene.index.FieldInfos.fieldName(FieldInfos.java:249)
	at org.apache.lucene.index.TermBuffer.read(TermBuffer.java:68)
	at org.apache.lucene.index.SegmentTermEnum.next(SegmentTermEnum.java:123)
	at org.apache.lucene.index.SegmentTermEnum.scanTo(SegmentTermEnum.java:154)
	at org.apache.lucene.index.TermInfosReader.scanEnum(TermInfosReader.java:223)
	at org.apache.lucene.index.TermInfosReader.get(TermInfosReader.java:217)
	at org.apache.lucene.index.SegmentReader.docFreq(SegmentReader.java:678)
	at org.apache.lucene.index.MultiSegmentReader.docFreq(MultiSegmentReader.java:373)
	at org.apache.lucene.search.IndexSearcher.docFreq(IndexSearcher.java:87)
	at org.apache.lucene.search.Similarity.idf(Similarity.java:457)
	at org.apache.lucene.search.TermQuery$TermWeight.&amp;lt;init&amp;gt;(TermQuery.java:44)
	at org.apache.lucene.search.TermQuery.createWeight(TermQuery.java:146)
	at org.apache.lucene.search.BooleanQuery$BooleanWeight.&amp;lt;init&amp;gt;(BooleanQuery.java:187)
	at org.apache.lucene.search.BooleanQuery.createWeight(BooleanQuery.java:362)
	at org.apache.lucene.search.Query.weight(Query.java:95)
	at org.apache.lucene.search.Searcher.createWeight(Searcher.java:171)
	at org.apache.lucene.search.Searcher.search(Searcher.java:132)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt; 

&lt;p&gt;The index is not using the compound file format:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt; 
7731499698 Jul 28 03:46 _6zk.fdt
 232014520 Jul 28 03:50 _6zk.fdx
        32 Jul 28 03:50 _6zk.fnm
3775713450 Jul 28 04:06 _6zk.frq
  58003634 Jul 28 04:07 _6zk.nrm
2944298834 Jul 28 04:18 _6zk.prx
    432418 Jul 28 04:18 _6zk.tii
  30784106 Jul 28 04:19 _6zk.tis
 217354711 Jul 28 08:18 _76i.fdt
   6509864 Jul 28 08:18 _76i.fdx
        32 Jul 28 08:18 _76i.fnm
 144348761 Jul 28 08:18 _76i.frq
   1627470 Jul 28 08:18 _76i.nrm
 295528445 Jul 28 08:19 _76i.prx
     52622 Jul 28 08:19 _76i.tii
   3858378 Jul 28 08:19 _76i.tis
 199621206 Jul 29 13:29 _7cm.fdt
   5994720 Jul 29 13:29 _7cm.fdx
        32 Jul 29 13:29 _7cm.fnm
 136445620 Jul 29 13:29 _7cm.frq
   1498684 Jul 29 13:29 _7cm.nrm
 284805312 Jul 29 13:30 _7cm.prx
     48346 Jul 29 13:30 _7cm.tii
   3522117 Jul 29 13:30 _7cm.tis
   3914068 Jul 29 13:30 _7cn.fdt
    119184 Jul 29 13:30 _7cn.fdx
        32 Jul 29 13:30 _7cn.fnm
   2993343 Jul 29 13:30 _7cn.frq
     29800 Jul 29 13:30 _7cn.nrm
   7380878 Jul 29 13:30 _7cn.prx
      5277 Jul 29 13:30 _7cn.tii
    378816 Jul 29 13:30 _7cn.tis
    383147 Jul 29 13:30 _7cq.fdt
     11240 Jul 29 13:30 _7cq.fdx
        32 Jul 29 13:30 _7cq.fnm
    290398 Jul 29 13:30 _7cq.frq
      2814 Jul 29 13:30 _7cq.nrm
    763135 Jul 29 13:30 _7cq.prx
      1581 Jul 29 13:30 _7cq.tii
    115971 Jul 29 13:30 _7cq.tis
        19 Jul 29 13:30 date
        20 Jul 21 01:53 segments.gen
       155 Jul 29 13:30 segments_d61
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt; </comment>
                    <comment id="12618628" author="mikemccand" created="Thu, 31 Jul 2008 10:39:43 +0100"  >&lt;blockquote&gt;&lt;p&gt;I just tried out the latest NIOFSDirectory patch and I&apos;m seeing a bug. If I go back to the regular FSDirectory, everything works fine. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Is the index itself corrupt, ie, NIOFSDirectory did something bad when writing the index?  Or, is it only in reading the index with NIOFSDirectory that you see this?  IE, can you swap in FSDirectory on your existing index and the problem goes away?&lt;/p&gt;</comment>
                    <comment id="12621091" author="mmastrac" created="Sat, 9 Aug 2008 00:05:16 +0100"  >&lt;blockquote&gt;&lt;p&gt;Is the index itself corrupt, ie, NIOFSDirectory did something bad when writing the index? Or, is it only in reading the index with NIOFSDirectory that you see this? IE, can you swap in FSDirectory on your existing index and the problem goes away?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I haven&apos;t seen any issues with writing the index under NIOFSDirectory.  The failures seem to happen only when reading.  When I switch to FSDirectory (or MMapDirectory), the same index that fails under NIOFSDirectory works flawlessly (indicating that the index is not corrupt).&lt;/p&gt;

&lt;p&gt;The error with NIOFSDirectory is determinate and repeatable (same error every time, same location, same query during warmup).&lt;/p&gt;

&lt;p&gt;I couldn&apos;t reproduce this on a smaller index, unfortunately.&lt;/p&gt;</comment>
                    <comment id="12624642" author="mikemccand" created="Fri, 22 Aug 2008 11:19:43 +0100"  >&lt;blockquote&gt;&lt;p&gt;The error with NIOFSDirectory is determinate and repeatable (same error every time, same location, same query during warmup).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Did you see a prior exception, before hitting the AIOOBE?  If so, I think this is just &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-1262&quot; title=&quot;IndexOutOfBoundsException from FieldsReader after problem reading the index&quot;&gt;&lt;del&gt;LUCENE-1262&lt;/del&gt;&lt;/a&gt; all over again.  That issue was fixed in BufferedIndexInput, but the NIOFSIndexInput has copied a bunch of code from BufferedIndexInput (something I think we must fix before committing it &amp;#8211; I think it should inherit from BufferedIndexInput instead) and so it still has that bug.  I&apos;ll post a patch with the bug re-fixed so you can at least test it to see if it resolves your exception.&lt;/p&gt;</comment>
                    <comment id="12624646" author="jasonrutherglen" created="Fri, 22 Aug 2008 11:38:18 +0100"  >&lt;p&gt;I can possibly work on this, just go through and reedit the BufferedIndexInput portions of the code.  Inheriting is difficult because of the ByteBuffer code.  Needs to be done line by line.&lt;/p&gt;</comment>
                    <comment id="12624661" author="mikemccand" created="Fri, 22 Aug 2008 12:57:18 +0100"  >&lt;p&gt;Attached new rev of NIOFSDirectory.&lt;/p&gt;

&lt;p&gt;Besides re-fixing &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-1262&quot; title=&quot;IndexOutOfBoundsException from FieldsReader after problem reading the index&quot;&gt;&lt;del&gt;LUCENE-1262&lt;/del&gt;&lt;/a&gt;, I also found &amp;amp; fixed a bug in the NIOFSIndexInput.clone() method.&lt;/p&gt;

&lt;p&gt;Matthew, could you give this one a shot to see if it fixes your case?  Thanks.&lt;/p&gt;</comment>
                    <comment id="12624662" author="mikemccand" created="Fri, 22 Aug 2008 13:04:53 +0100"  >&lt;blockquote&gt;&lt;p&gt;I can possibly work on this, just go through and reedit the BufferedIndexInput portions of the code. Inheriting is difficult because of the ByteBuffer code. Needs to be done line by line.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;That would be awesome, Jason.  I think we should then commit NIOFSDirectory to core as at least a way around this bottleneck on all platforms but Windows.  Maybe we can do this in time for 2.4?&lt;/p&gt;</comment>
                    <comment id="12624905" author="mmastrac" created="Fri, 22 Aug 2008 18:06:58 +0100"  >&lt;blockquote&gt;&lt;p&gt;Matthew, could you give this one a shot to see if it fixes your case? Thanks.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Michael,&lt;/p&gt;

&lt;p&gt;I ran this new patch against our big index and it works very well.  If I have time, I&apos;ll run some benchmarks to see what our real-life performance improvements are like.  &lt;/p&gt;

&lt;p&gt;Note that I&apos;m only running it for our read-only snapshot of the index, however, so this hasn&apos;t been tested for writing to a large index.&lt;/p&gt;</comment>
                    <comment id="12624922" author="mikemccand" created="Fri, 22 Aug 2008 18:44:53 +0100"  >&lt;blockquote&gt;&lt;p&gt;I ran this new patch against our big index and it works very well. If I have time, I&apos;ll run some benchmarks to see what our real-life performance improvements are like.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Super, thanks!  This was the same index that would reliably hit the exception above?&lt;/p&gt;</comment>
                    <comment id="12624931" author="mmastrac" created="Fri, 22 Aug 2008 19:01:00 +0100"  >&lt;blockquote&gt;&lt;p&gt;Super, thanks! This was the same index that would reliably hit the exception above?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Correct - it would hit the exception every time at startup.&lt;/p&gt;

&lt;p&gt;I&apos;ve been running NIOFSDirectory for the last couple of hours with zero exceptions (except for running out of file descriptors after starting it the first time &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;).  The previous incarnation was running MMapDirectory.&lt;/p&gt;

&lt;p&gt;Thanks for all the work on this patch.&lt;/p&gt;

</comment>
                    <comment id="12624988" author="mmastrac" created="Fri, 22 Aug 2008 23:01:31 +0100"  >&lt;p&gt;This exception popped up out of the blue a few hours in.  No exceptions before it.  I&apos;ll see if I can figure out whether it was caused by our index snapshotting or if it&apos;s a bug elsewhere in NIOFSDirectory.&lt;/p&gt;

&lt;p&gt;I haven&apos;t seen any exceptions like this with MMapDirectory, but it&apos;s possible there&apos;s something that we&apos;re doing that isn&apos;t correct.&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt; 
Caused by: java.nio.channels.ClosedChannelException
	at sun.nio.ch.FileChannelImpl.ensureOpen(FileChannelImpl.java:91)
	at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:616)
	at com.dotspots.analyzer.index.NIOFSDirectory$NIOFSIndexInput.read(NIOFSDirectory.java:186)
	at com.dotspots.analyzer.index.NIOFSDirectory$NIOFSIndexInput.refill(NIOFSDirectory.java:218)
	at com.dotspots.analyzer.index.NIOFSDirectory$NIOFSIndexInput.readByte(NIOFSDirectory.java:232)
	at org.apache.lucene.store.IndexInput.readVInt(IndexInput.java:76)
	at org.apache.lucene.index.TermBuffer.read(TermBuffer.java:63)
	at org.apache.lucene.index.SegmentTermEnum.next(SegmentTermEnum.java:123)
	at org.apache.lucene.index.SegmentTermEnum.scanTo(SegmentTermEnum.java:154)
	at org.apache.lucene.index.TermInfosReader.scanEnum(TermInfosReader.java:223)
	at org.apache.lucene.index.TermInfosReader.get(TermInfosReader.java:217)
	at org.apache.lucene.index.SegmentReader.docFreq(SegmentReader.java:678)
	at org.apache.lucene.index.MultiSegmentReader.docFreq(MultiSegmentReader.java:373)
	at org.apache.lucene.index.MultiReader.docFreq(MultiReader.java:310)
	at org.apache.lucene.search.IndexSearcher.docFreq(IndexSearcher.java:87)
	at org.apache.lucene.search.Searcher.docFreqs(Searcher.java:178)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt; </comment>
                    <comment id="12625050" author="mikemccand" created="Sat, 23 Aug 2008 13:54:49 +0100"  >&lt;p&gt;Interesting...&lt;/p&gt;

&lt;p&gt;Are you really sure you&apos;re not accidentally closing the searcher before calling Searcher.docFreqs?  Are you calling docFreqs directly from your app?&lt;/p&gt;

&lt;p&gt;It looks like MMapIndexInput.close() is a noop so it would not have detected calling Searcher.docFreqs after close, whereas NIOFSdirectory (and the normal FSDirectory) will.&lt;/p&gt;

&lt;p&gt;If you try the normal FSDirectory do you also an exception like this?&lt;/p&gt;

&lt;p&gt;Incidentally, what sort of performance differences are you noticing between these three different ways of accessing an index in the file system?&lt;/p&gt;</comment>
                    <comment id="12625051" author="yseeley@gmail.com" created="Sat, 23 Aug 2008 13:56:09 +0100"  >&lt;blockquote&gt;&lt;p&gt;Maybe we can do this in time for 2.4?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;+1&lt;/p&gt;

&lt;p&gt;Latest patch is looking good to me!&lt;br/&gt;
Is there a reason we don&apos;t do lazy allocation in clone() like FSIndexInput?&lt;/p&gt;

&lt;p&gt;Also, our finalizers aren&apos;t technically thread safe which could lead to a double close in the finalizer (although I doubt if this particular case would ever happen).   If we need to keep them, we could change Descriptor.isOpen to volatile and there should be pretty much no cost since it&apos;s only checked in close().&lt;/p&gt;</comment>
                    <comment id="12625054" author="mikemccand" created="Sat, 23 Aug 2008 14:08:37 +0100"  >&lt;blockquote&gt;&lt;p&gt;Is there a reason we don&apos;t do lazy allocation in clone() like FSIndexInput?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yonik, do you mean BufferedIndexInput.clone (not FSIndexInput)?&lt;/p&gt;

&lt;p&gt;I think once we fix NIOFSIndexInput to subclass from BufferedIndexInput, then cloning should be lazy again.  Jason are you working on this (subclassing from BufferedIndexInput)?  If not I can take it.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Also, our finalizers aren&apos;t technically thread safe which could lead to a double close in the finalizer&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Hmmm... I&apos;ll update both FSDirectory and NIOFSDiretory&apos;s isOpen&apos;s to be volatile.&lt;/p&gt;</comment>
                    <comment id="12625056" author="jasonrutherglen" created="Sat, 23 Aug 2008 14:21:41 +0100"  >&lt;p&gt;Mike I have have not started on the subclassing from BufferedIndexInput yet.  I can work on it monday though.  &lt;/p&gt;</comment>
                    <comment id="12625057" author="mikemccand" created="Sat, 23 Aug 2008 14:23:15 +0100"  >&lt;blockquote&gt;&lt;p&gt;Mike I have have not started on the subclassing from BufferedIndexInput yet. I can work on it monday though. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;OK, thanks!&lt;/p&gt;</comment>
                    <comment id="12625058" author="mikemccand" created="Sat, 23 Aug 2008 14:24:49 +0100"  >&lt;p&gt;Updated patch with Yonik&apos;s volatile suggestion &amp;#8211; thanks Yonik!&lt;/p&gt;

&lt;p&gt;Also, I removed NIOFSDirectory.createOutput since it was doing the same thing as super().&lt;/p&gt;</comment>
                    <comment id="12625117" author="mmastrac" created="Sat, 23 Aug 2008 23:30:41 +0100"  >&lt;p&gt;Michael,&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Are you really sure you&apos;re not accidentally closing the searcher before calling Searcher.docFreqs? Are you calling docFreqs directly from your app?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Our IndexReaders are actually managed in a shared pool (currently 8 IndexReaders, shared round-robin style as requests come in).  We have some custom reference counting logic that&apos;s supposed to keep the readers alive as long as somebody has them open.  As new index snapshots come in, the IndexReaders are re-opened and reference counts ensure that any old index readers in use are kept alive until the searchers are done with them.  I&apos;m guessing we have an error in our reference counting logic that just doesn&apos;t show up under MMapDirectory (as you mentioned, close() is a no-op).&lt;/p&gt;

&lt;p&gt;We&apos;re calling docFreqs directly from our app.  I&apos;m guessing that it just happens to be the most likely item to be called after we roll to a new index snapshot.&lt;/p&gt;

&lt;p&gt;I don&apos;t have hard performance numbers right now, but we were having a hard time saturating I/O or CPU with FSDirectory.  The locking was basically killing us.  When we switched to MMapDirectory and turned on compound files, our performance jumped at least 2x.  The preliminary results I&apos;m seeing with NIOFSDirectory seem to indicate that it&apos;s slightly faster than MMapDirectory.&lt;/p&gt;

&lt;p&gt;I&apos;ll try setting our app back to using the old FSDirectory and see if the exceptions still occur.  I&apos;ll also try to fiddle with our unit tests to make sure we&apos;re correctly ref-counting all of our index readers.&lt;/p&gt;

&lt;p&gt;BTW, I ran a quick FSDirectory/MMapDirectory/NIOFSDirectory shootout.  It uses a parallel benchmark that roughly models what our real-life benchmark is like.  I ran the benchmark once through to warm the disk cache, then got the following.  The numbers are fairly stable across various runs once the disk caches are warm:&lt;/p&gt;

&lt;p&gt;FS: 33644ms&lt;br/&gt;
MMap: 28616ms&lt;br/&gt;
NIOFS: 33189ms&lt;/p&gt;

&lt;p&gt;I&apos;m a bit surprised at the results myself, but I&apos;ve spent a bit of time tuning the indexes to maximize concurrency.  I&apos;ll double-check that the benchmark is correctly running all of the tests.&lt;/p&gt;

&lt;p&gt;The benchmark effectively runs 10-20 queries in parallel at a time, then waits for all queries to complete.  It does this end-to-end for a number of different query batches, then totals up the time to complete each batch.&lt;/p&gt;</comment>
                    <comment id="12625380" author="jasonrutherglen" created="Mon, 25 Aug 2008 15:46:11 +0100"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-753&quot; title=&quot;Use NIO positional read to avoid synchronization in FSIndexInput&quot;&gt;&lt;del&gt;LUCENE-753&lt;/del&gt;&lt;/a&gt;.patch&lt;/p&gt;

&lt;p&gt;NIOFSIndexInput now extends BufferedIndexInput.  I was unable to test however and wanted to just get this up.  &lt;/p&gt;</comment>
                    <comment id="12625679" author="mikemccand" created="Tue, 26 Aug 2008 10:31:26 +0100"  >&lt;blockquote&gt;
&lt;p&gt;FS: 33644ms&lt;br/&gt;
MMap: 28616ms&lt;br/&gt;
NIOFS: 33189ms&lt;/p&gt;

&lt;p&gt;I&apos;m a bit surprised at the results myself, but I&apos;ve spent a bit of time tuning the indexes to maximize concurrency. I&apos;ll double-check that the benchmark is correctly running all of the tests.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;This is surprising &amp;#8211; your benchmark is very concurrent, yet FSDir and NIOFSDir are close to the same net throughput, while MMapDir is quite a bit faster.  Is this on a non-Windows OS?&lt;/p&gt;</comment>
                    <comment id="12625747" author="mikemccand" created="Tue, 26 Aug 2008 15:44:53 +0100"  >
&lt;p&gt;New patch attached.  Matthew if you could try this version out on your&lt;br/&gt;
index, that&apos;d be awesome.&lt;/p&gt;

&lt;p&gt;I didn&apos;t like how we were still copying the hairy readBytes &amp;amp; refill&lt;br/&gt;
methods from BufferedIndexInput, so I made some small additional mods&lt;br/&gt;
to BufferedIndexInput to notify subclass when a byte[] buffer gets&lt;br/&gt;
allocated, which then allowed us to fully inherit these methods.&lt;/p&gt;

&lt;p&gt;But, then I realized we were duplicating alot of code from&lt;br/&gt;
FSIndexInput, so I switched to subclassing that instead and that made&lt;br/&gt;
things even simpler.&lt;/p&gt;

&lt;p&gt;Some other things also fixed:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;We were ignoring bufferSize (eg setBufferSize).&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;We weren&apos;t closing the FileChannel&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;clone() now lazily clones the buffer again&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;To test this, I made NIOFSDirectory the default IMPL in&lt;br/&gt;
FSDirectory.getDirectory and ran all tests.  One test failed at first&lt;br/&gt;
(because we were ignoring setBufferSize calls); with the new patch,&lt;br/&gt;
all tests pass.&lt;/p&gt;

&lt;p&gt;I also built first 150K docs of wikipedia and ran various searches&lt;br/&gt;
using NIOFSDirectory and all seems good.&lt;/p&gt;

&lt;p&gt;The class is quite a bit simpler now, however there&apos;s one thing I&lt;br/&gt;
don&apos;t like: when you use CFS, the NIOFSIndexInput.readInternal method&lt;br/&gt;
will wrap the CSIndexInput&apos;s byte[] (from it&apos;s parent&lt;br/&gt;
BufferedIndexInput class) for every call (every 1024 bytes read from&lt;br/&gt;
the file).  I&apos;d really like to find a clean way to reuse a single&lt;br/&gt;
ByteBuffer.  Not yet sure how to do that though...&lt;/p&gt;
</comment>
                    <comment id="12626672" author="mikemccand" created="Thu, 28 Aug 2008 18:53:03 +0100"  >&lt;p&gt;New version attached.  This one re-uses a wrapped byte buffer even when it&apos;s CSIndexInput that&apos;s calling it.&lt;/p&gt;

&lt;p&gt;I plan to commit in a day or two.&lt;/p&gt;</comment>
                    <comment id="12627225" author="mikemccand" created="Sat, 30 Aug 2008 18:33:54 +0100"  >&lt;p&gt;I just committed revision 690539, adding NIOFSDirectory.  I will leave this open, but move off of 2.4, until we can get similar performance gains on Windows...&lt;/p&gt;</comment>
                    <comment id="12627313" author="rengels@ix.netcom.com" created="Sun, 31 Aug 2008 16:21:49 +0100"  >&lt;p&gt;SUN is accepting outside bug fixes to the Open JDK, and merging them to the commercial JDK (in most cases).&lt;/p&gt;

&lt;p&gt;If the underlying bug is fixed in the Windows JDK - not too hard - then you fix this properly in Lucene.&lt;/p&gt;

&lt;p&gt;If you don&apos;t fix it in the JDK you are always going to have the &apos;running out of file handles&apos; synchronization, vs, the &quot;locked position&quot; synchronization - there is no way to fix this in user code...&lt;/p&gt;</comment>
                    <comment id="12755680" author="yseeley@gmail.com" created="Tue, 15 Sep 2009 21:32:05 +0100"  >&lt;p&gt;Attaching new FileReadTest.java that fixes a concurrency bug in SeparateFile - each reader needed it&apos;s own file position.&lt;/p&gt;</comment>
                    <comment id="12986445" author="thetaphi" created="Tue, 25 Jan 2011 15:43:12 +0000"  >&lt;p&gt;This issue was resolved a long time ago, but left open for the stupid Windows Sun JRE bug which was never resolved. With Lucene 3.x and trunk we have better defaults (use e.g. MMapDirectory on Windows-64).&lt;/p&gt;

&lt;p&gt;Users should default to FSDirectory.open() and use the returned directory for best performance.&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12419678" name="FileReadTest.java" size="8144" author="yseeley@gmail.com" created="Tue, 15 Sep 2009 21:32:05 +0100" />
                    <attachment id="12384993" name="FileReadTest.java" size="8530" author="bripink" created="Tue, 1 Jul 2008 00:31:30 +0100" />
                    <attachment id="12384980" name="FileReadTest.java" size="8048" author="yseeley@gmail.com" created="Mon, 30 Jun 2008 20:26:33 +0100" />
                    <attachment id="12384971" name="FileReadTest.java" size="8038" author="yseeley@gmail.com" created="Mon, 30 Jun 2008 18:47:47 +0100" />
                    <attachment id="12384945" name="FileReadTest.java" size="6498" author="mikemccand" created="Mon, 30 Jun 2008 13:25:38 +0100" />
                    <attachment id="12371453" name="FileReadTest.java" size="5811" author="yseeley@gmail.com" created="Tue, 11 Dec 2007 20:10:36 +0000" />
                    <attachment id="12371384" name="FileReadTest.java" size="5380" author="yseeley@gmail.com" created="Mon, 10 Dec 2007 22:34:07 +0000" />
                    <attachment id="12347704" name="FileReadTest.java" size="5448" author="yseeley@gmail.com" created="Thu, 21 Dec 2006 22:45:36 +0000" />
                    <attachment id="12386329" name="FSDirectoryPool.patch" size="5658" author="mikemccand" created="Thu, 17 Jul 2008 19:18:30 +0100" />
                    <attachment id="12347535" name="FSIndexInput.patch" size="6830" author="yseeley@gmail.com" created="Wed, 20 Dec 2006 06:01:58 +0000" />
                    <attachment id="12347511" name="FSIndexInput.patch" size="2171" author="yseeley@gmail.com" created="Tue, 19 Dec 2006 18:37:43 +0000" />
                    <attachment id="12385120" name="lucene-753.patch" size="8559" author="jasonrutherglen" created="Wed, 2 Jul 2008 13:55:44 +0100" />
                    <attachment id="12384924" name="lucene-753.patch" size="9467" author="jasonrutherglen" created="Sun, 29 Jun 2008 19:22:22 +0100" />
                    <attachment id="12389109" name="LUCENE-753.patch" size="11049" author="mikemccand" created="Thu, 28 Aug 2008 18:53:03 +0100" />
                    <attachment id="12388910" name="LUCENE-753.patch" size="10501" author="mikemccand" created="Tue, 26 Aug 2008 15:44:52 +0100" />
                    <attachment id="12388846" name="LUCENE-753.patch" size="13587" author="jasonrutherglen" created="Mon, 25 Aug 2008 15:46:11 +0100" />
                    <attachment id="12388794" name="LUCENE-753.patch" size="9935" author="mikemccand" created="Sat, 23 Aug 2008 14:24:49 +0100" />
                    <attachment id="12388734" name="LUCENE-753.patch" size="8639" author="mikemccand" created="Fri, 22 Aug 2008 12:57:18 +0100" />
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>18.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Tue, 19 Dec 2006 19:17:28 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>12999</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>26977</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                    <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                <customfieldname>Time in Status</customfieldname>
                <customfieldvalues>
                    
                </customfieldvalues>
            </customfield>
                            </customfields>
    </item>
</channel>
</rss>