<!-- 
RSS generated by JIRA (5.2.8#851-sha1:3262fdc28b4bc8b23784e13eadc26a22399f5d88) at Tue Jul 16 13:17:24 UTC 2013

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/LUCENE-1960/LUCENE-1960.xml?field=key&field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>5.2.8</version>
        <build-number>851</build-number>
        <build-date>26-02-2013</build-date>
    </build-info>

<item>
            <title>[LUCENE-1960] Remove deprecated Field.Store.COMPRESS</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-1960</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Also remove FieldForMerge and related code.&lt;/p&gt;</description>
                <environment></environment>
            <key id="12437564">LUCENE-1960</key>
            <summary>Remove deprecated Field.Store.COMPRESS</summary>
                <type id="3" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/task.png">Task</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png">Closed</status>
                    <resolution id="1">Fixed</resolution>
                                <assignee username="thetaphi">Uwe Schindler</assignee>
                                <reporter username="michaelbusch">Michael Busch</reporter>
                        <labels>
                    </labels>
                <created>Thu, 8 Oct 2009 01:13:44 +0100</created>
                <updated>Wed, 25 Nov 2009 16:47:52 +0000</updated>
                    <resolved>Mon, 26 Oct 2009 21:17:45 +0000</resolved>
                                            <fixVersion>3.0</fixVersion>
                                        <due></due>
                    <votes>0</votes>
                        <watches>0</watches>
                                                    <comments>
                    <comment id="12763315" author="michaelbusch" created="Thu, 8 Oct 2009 01:16:05 +0100"  >&lt;p&gt;All core &amp;amp; contrib tests pass.&lt;/p&gt;</comment>
                    <comment id="12763323" author="michaelbusch" created="Thu, 8 Oct 2009 01:55:11 +0100"  >&lt;p&gt;Committed revision 822978.&lt;/p&gt;</comment>
                    <comment id="12763352" author="thetaphi" created="Thu, 8 Oct 2009 05:27:44 +0100"  >&lt;p&gt;Just one question: What happens with indexes that already had compressed fields. Do they behave as before?&lt;/p&gt;

&lt;p&gt;&lt;b&gt;edit&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;From the patch I see that you also removed the bitmask for testing compression in index format. So an index with compressed fields from 2.9 should behave undefined. In my opinion, support for reading compressed fields should stay available, but not for writing. And: as soon as segments are merged, they should get silently uncompressed (what should be no problem if the special FieldForMerge is no longer used).&lt;/p&gt;

&lt;p&gt;Also the constant bitmask for compression should stay &quot;reserved&quot; for futrure use.&lt;/p&gt;</comment>
                    <comment id="12763389" author="michaelbusch" created="Thu, 8 Oct 2009 08:26:03 +0100"  >&lt;p&gt;Users can use CompressionTools#decompress() now. They just must know now which binary fields are compressed.&lt;/p&gt;

&lt;p&gt;I don&apos;t think the SegmentMerger should uncompress automatically? That&apos;d make &amp;lt;=2.9 indexes suddenly bigger.&lt;/p&gt;</comment>
                    <comment id="12763399" author="michaelbusch" created="Thu, 8 Oct 2009 09:04:17 +0100"  >&lt;blockquote&gt;
&lt;p&gt;Also the constant bitmask for compression should stay &quot;reserved&quot; for futrure use.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yeah I think you&apos;re right, we must make sure that we don&apos;t use this bit for something else, as old indexes might have it set to true already. I&apos;ll add it back with a deprecation comment saying that we&apos;ll remove it in 4.0. (4.0 won&apos;t have to be able to read &amp;lt;3.0 indexes anymore).&lt;/p&gt;</comment>
                    <comment id="12763401" author="michaelbusch" created="Thu, 8 Oct 2009 09:04:56 +0100"  >&lt;p&gt;Reopening so that I don&apos;t forget to add back the COMPRESS bit.&lt;/p&gt;</comment>
                    <comment id="12763403" author="thetaphi" created="Thu, 8 Oct 2009 09:11:54 +0100"  >&lt;p&gt;In the discussion with Mike, we said, that all pre-2.9 compressed fields should behave as before, e.g. they should automatically decompress. It should not be possibile to create new ones. This is just index compatibility, in the current version it is simply not defined what happens with pre-2.9 fields. The second problem are older compressed fields using the modified Java-UTF-8 encoding, which may not correctly decompress now (if you receive with getByte())&lt;/p&gt;

&lt;p&gt;The problem with your patch: If the field is compressed and you try to get it, you would not hit it (because it is marked as String, not binary). The new self-compressed fields are now should be &quot;binary&quot;, before they were binary &lt;b&gt;or&lt;/b&gt; string. See the discussion in &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-652&quot; title=&quot;Compressed fields should be &amp;quot;externalized&amp;quot; (from Fields into Document)&quot;&gt;&lt;del&gt;LUCENE-652&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;</comment>
                    <comment id="12763408" author="michaelbusch" created="Thu, 8 Oct 2009 09:28:43 +0100"  >&lt;blockquote&gt;
&lt;p&gt;The problem with your patch: If the field is compressed and you try to get it, you would not hit it (because it is marked as String, not binary). The new self-compressed fields are now should be &quot;binary&quot;, before they were binary or string. See the discussion in &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-652&quot; title=&quot;Compressed fields should be &amp;quot;externalized&amp;quot; (from Fields into Document)&quot;&gt;&lt;del&gt;LUCENE-652&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Hmm I see. I should have waited a bit with committing - sorry!&lt;br/&gt;
I&apos;ll take care of it tomorrow, it&apos;s getting too late now.&lt;/p&gt;</comment>
                    <comment id="12763410" author="thetaphi" created="Thu, 8 Oct 2009 09:33:44 +0100"  >&lt;p&gt;No problem. &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;I think it should not be a big task to preserve the decompression of previously compressed fields. Just revert FieldsReader changes and modify a little bit.&lt;/p&gt;</comment>
                    <comment id="12763417" author="thetaphi" created="Thu, 8 Oct 2009 09:46:40 +0100"  >&lt;blockquote&gt;&lt;p&gt;I don&apos;t think the SegmentMerger should uncompress automatically? That&apos;d make &amp;lt;=2.9 indexes suddenly bigger.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;If we re-add support for compressed fields, we have to also provide the special FieldForMerge again. To get rid of this code (which is the source of the problems behind the whole COMPRESS problem), we could simply let it as it is now without FieldForMerge. If you merge two segmets with compressed data then, without FieldToMerge it gets automatically decompressed and written in uncompressed variant to the new segment. As compressed fields are no longer supported, this is the correct behaviour. During merging, the compress bit must be removed.&lt;/p&gt;

&lt;p&gt;The problem are suddenly bigger indexes, but we should note this in docs: &quot;As compressed fields are no longer supported, during mering the compression is removed. If you want to compress your fields, do it yourself and store it as binary stored field.&quot;&lt;/p&gt;

&lt;p&gt;Just for confirmation:&lt;br/&gt;
I have some indexes with compress enabled (for some of the documents, since 2.9 we do not compress anymore, newly added docs have no compression anymore &lt;span class=&quot;error&quot;&gt;&amp;#91;it was never an good idea because of performance&amp;#93;&lt;/span&gt;). So I have no possibility to get these fields anymore, because I do not know if they are compressed and cannot do the decompression myself. For me, this data is simply lost.&lt;/p&gt;

&lt;p&gt;I think Solr will have the same problem.&lt;/p&gt;</comment>
                    <comment id="12763726" author="michaelbusch" created="Thu, 8 Oct 2009 23:06:59 +0100"  >&lt;p&gt;Adds the compress bit back to FieldsWriter and the uncompress code to FieldsReader.&lt;/p&gt;

&lt;p&gt;Uwe, could you review the patch?&lt;/p&gt;</comment>
                    <comment id="12763729" author="thetaphi" created="Thu, 8 Oct 2009 23:19:34 +0100"  >&lt;p&gt;I will look into this tomorrow morning. I am to tired now, have to go to bed.&lt;/p&gt;

&lt;p&gt;I will also check the implications of not having the FieldForMerge.&lt;/p&gt;</comment>
                    <comment id="12764512" author="thetaphi" created="Sun, 11 Oct 2009 22:17:24 +0100"  >&lt;p&gt;Sorry, I forgot this one, will check tomorrow with some old indexes using compressed fields.&lt;/p&gt;</comment>
                    <comment id="12764563" author="michaelbusch" created="Mon, 12 Oct 2009 07:04:04 +0100"  >&lt;p&gt;I created an index with some compressed binary and String fields with 2.4 and verified that it gets decompressed correctly. The test fails currently on trunk (as expected) and passes with the latest patch.&lt;/p&gt;

&lt;p&gt;However, there&apos;s one issue here: the compressed field gets silently uncompressed during merge, &lt;b&gt;only&lt;/b&gt; if in the less efficient merge mode that doesn&apos;t use FieldsReader#rawDocs() and FieldsWriter#addRawDocuments(). So now this doesn&apos;t sound like a great solution that we sometimes uncompress the fields automatically and sometimes don&apos;t. &lt;/p&gt;

&lt;p&gt;I think we have three options:&lt;br/&gt;
1. Change FieldsWriter#addRawDocuments() to uncompress on-the-fly&lt;br/&gt;
2. Revert the FieldForMerge changes too and never uncompress automatically during merge&lt;br/&gt;
3. Make it possible for the user to uncompress fields with CompressionTools, no matter which UTF format the data was stored with&lt;/p&gt;

&lt;p&gt;I don&apos;t really want to do 1., because it will have a performance impact for all fields (you have to look at the field bits even in raw merge mode). With 2. we will have to keep most of the compress/uncompress code in Lucene until 4.0, we&apos;ll just not make it possible anymore to add Store.COMPRESS fields with 3.0 (that&apos;s already how trunk is). For 3. we&apos;d have to add a deprecated isCompressed() method that the user can call.&lt;/p&gt;</comment>
                    <comment id="12765344" author="michaelbusch" created="Wed, 14 Oct 2009 01:47:01 +0100"  >&lt;p&gt;How shall we proceed here? (see my previous comment)&lt;/p&gt;</comment>
                    <comment id="12768916" author="thetaphi" created="Thu, 22 Oct 2009 22:24:35 +0100"  >&lt;p&gt;I still prefer 1, but maybe it&apos;s not so good. Else I would implement 2 (even if we need FieldForMerge). Just remove the COMPRES flag that nobody can add any compressed fields anymore.&lt;/p&gt;

&lt;p&gt;3 is bad, because it needs you to change your code on the change between 2.9 and 3.0 if you had compressed fields. In 2.9 they were automatically uncompressed, in 3.0 not. This would make it impossible to replace the lucene jar (which is currently possible if you remove all deprecated calls in 2.9).&lt;/p&gt;</comment>
                    <comment id="12769489" author="thetaphi" created="Fri, 23 Oct 2009 23:24:47 +0100"  >&lt;p&gt;If we want to stay with the current patch, we place a warning that indexes can suddenly get bigger on merges. We note this in changes.txt. &lt;/p&gt;

&lt;p&gt;If one wants to regenerate the index with the stored fields decompressed, he could simply use the IndexSplitter contrib module recently added. This command line tool uses addIndexes and therefore merges all segments into a new index. With option 1, they get decompressed.&lt;/p&gt;

&lt;p&gt;If somebody wants real compressed fields again, he has to write code and reindex using CompressableStringTools.&lt;/p&gt;</comment>
                    <comment id="12769490" author="michaelbusch" created="Fri, 23 Oct 2009 23:30:54 +0100"  >&lt;p&gt;I&apos;m actually -1 for option 1). The whole implementation of addRawDocuments() would have to change, and the necessary changes would kind of defeat its purpose.&lt;/p&gt;

&lt;p&gt;If we do 2) nobody will be able to use an index that has compressed fields in 4.0 anymore, and to convert it they have to manually reindex (which might not always be possible).&lt;/p&gt;

&lt;p&gt;Of course our policy says that 4.0 must not be able to read &amp;lt;3.0 indexes anymore, however normally users can take a 2.x index, optimize it with 3.x, and then 4.0 can read it without problems. This wouldn&apos;t be possible with 2).&lt;/p&gt;
</comment>
                    <comment id="12769494" author="thetaphi" created="Fri, 23 Oct 2009 23:35:31 +0100"  >&lt;p&gt;And how about keeping the current lucene-1960-1.patch? It works for me as I exspected. The only problem is that we do not decompress the fields for sure on optimizing?&lt;/p&gt;</comment>
                    <comment id="12769497" author="mikemccand" created="Fri, 23 Oct 2009 23:37:15 +0100"  >&lt;p&gt;Can&apos;t we detect that we&apos;re dealing w/ an older version segment and not use addRawDocuments when merging them (and uncompress when we merge)?&lt;/p&gt;</comment>
                    <comment id="12769499" author="michaelbusch" created="Fri, 23 Oct 2009 23:38:50 +0100"  >&lt;p&gt;Right, because FieldsReader#rawDocs() does not decode the field bits, so it doesn&apos;t know which fields are compressed.&lt;/p&gt;

&lt;p&gt;If we want to change that it would have a significant negative performance impact on &lt;b&gt;all&lt;/b&gt; stored fields.&lt;/p&gt;</comment>
                    <comment id="12769503" author="thetaphi" created="Fri, 23 Oct 2009 23:42:37 +0100"  >&lt;p&gt;Good idea, from where take the version?&lt;/p&gt;

&lt;p&gt;Or better, we look into the FieldInfos of the segment-to-merge and look if there is the compressed flag set for one of the fields. If yes, do not use addRawDocuments. It there the possibility to see this flag also or&apos;ed segment-wise (like a field is omitNors is per-segment)?&lt;/p&gt;</comment>
                    <comment id="12769507" author="michaelbusch" created="Fri, 23 Oct 2009 23:44:17 +0100"  >&lt;blockquote&gt;
&lt;p&gt;Can&apos;t we detect that we&apos;re dealing w/ an older version segment and not use addRawDocuments when merging them (and uncompress when we merge)?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;So then any 2.x index (including 2.9) would not be merged in the optimized way with 3.x. I&apos;m actually not even sure how much of a slowdown this is. Did you (or anyone else) ever measure that? &lt;/p&gt;</comment>
                    <comment id="12769510" author="thetaphi" created="Fri, 23 Oct 2009 23:46:35 +0100"  >&lt;p&gt;But this is only one-time. As soon as it is optimized it is fast again. Because of that I said, one could use a tool to enforce optimization or the new IndexSplitter can also do the copy old to new index.&lt;/p&gt;</comment>
                    <comment id="12769511" author="michaelbusch" created="Fri, 23 Oct 2009 23:47:08 +0100"  >&lt;blockquote&gt;
&lt;p&gt;Or better, we look into the FieldInfos of the segment-to-merge and look if there is the compressed flag set for one of the fields.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;For a second earlier I had the same idea - it would be the most convenient solution. BUT: bummer! no compressed flag in the fieldinfos...&lt;/p&gt;

&lt;p&gt;It&apos;s a bit per stored field &lt;b&gt;instance&lt;/b&gt;.&lt;/p&gt;</comment>
                    <comment id="12769517" author="michaelbusch" created="Fri, 23 Oct 2009 23:56:01 +0100"  >&lt;blockquote&gt;
&lt;p&gt;But this is only one-time. As soon as it is optimized it is fast again. Because of that I said, one could use a tool to enforce optimization or the new IndexSplitter can also do the copy old to new index.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;That&apos;s right, I&apos;m just trying to make sure we all understand the consequences. Would be nice to know how much longer it takes though.&lt;/p&gt;

&lt;p&gt;If everyone else is ok with this approach I can work on a patch. &lt;/p&gt;</comment>
                    <comment id="12769520" author="thetaphi" created="Sat, 24 Oct 2009 00:02:43 +0100"  >&lt;p&gt;So the idea is to raise the version number of the stored fields file by one in 3.0. All new or merged segments get this version number? When merging, for all versions before the actual one we do not use addRawDocuments() when copying contents. The current lucene-1960-1.patch stays unchanged.&lt;/p&gt;</comment>
                    <comment id="12769527" author="michaelbusch" created="Sat, 24 Oct 2009 00:11:30 +0100"  >&lt;p&gt;Yes, I believe this would work.&lt;/p&gt;</comment>
                    <comment id="12769528" author="thetaphi" created="Sat, 24 Oct 2009 00:11:59 +0100"  >&lt;p&gt;Then +1 from me!&lt;/p&gt;</comment>
                    <comment id="12769531" author="thetaphi" created="Sat, 24 Oct 2009 00:30:20 +0100"  >&lt;p&gt;I have some large indexes here from 2.9 with compressed XML documents in stored fields. I can compare the optimization time for Lucene 2.9 and Lucene 3.0 with your patch.&lt;/p&gt;</comment>
                    <comment id="12769565" author="michaelbusch" created="Sat, 24 Oct 2009 02:35:30 +0100"  >&lt;p&gt;It was as easy as changing this method in FieldsReader:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
  &lt;span class=&quot;code-object&quot;&gt;boolean&lt;/span&gt; canReadRawDocs() {
    &lt;span class=&quot;code-comment&quot;&gt;// Disable reading raw docs in 2.x format, because of the removal of compressed
&lt;/span&gt;    &lt;span class=&quot;code-comment&quot;&gt;// fields in 3.0. We don&apos;t want rawDocs() to decode field bits to figure out
&lt;/span&gt;    &lt;span class=&quot;code-comment&quot;&gt;// &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; a field was compressed, hence we enforce ordinary (non-raw) stored field merges
&lt;/span&gt;    &lt;span class=&quot;code-comment&quot;&gt;// &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; &amp;lt;3.0 indexes.
&lt;/span&gt;    &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; format &amp;gt;= FieldsWriter.FORMAT_LUCENE_3_0_NO_COMPRESSED_FIELDS;
  }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Uwe, I made some quick tests and it looks good. But I don&apos;t have any indexes with compressed fields (we don&apos;t use them), so I&apos;ll wait for you to test it out with your indexes that you mentioned.&lt;/p&gt;</comment>
                    <comment id="12769680" author="thetaphi" created="Sat, 24 Oct 2009 18:11:08 +0100"  >&lt;p&gt;Attached is my comparison of an unoptimzed Lucene 2.9 index optimized with 2.9 and optimized with 3.0 with the latest patch. The index was about 5.7 GB big and contained 4 compressed stored fields per document (in addition to ther fields uncompressed) containing XML documents.&lt;/p&gt;

&lt;p&gt;After optimization with 3.0, the size doubled (which is because of the very good compression of XML documents). The optimization took about double time with 3.0, because the fields were decompressed and no addRawDocuments was used.&lt;/p&gt;

&lt;p&gt;To confirm, that everything worked normal after this initial optimization, I updated 38 documents in both indexes and optimized again. The optimization time of 2.9 was identical, 3.0 took a little bit longer, which is because the uncompressed field created more copy i/o, which you see in the &quot;time&quot; output as system time. User time during the initial 2.9 -&amp;gt; 3.0 optimization was much larger.&lt;/p&gt;

&lt;p&gt;The attached document contains all numbers together with the checkindex outputs before and after each step.&lt;/p&gt;</comment>
                    <comment id="12769683" author="thetaphi" created="Sat, 24 Oct 2009 18:46:27 +0100"  >&lt;p&gt;I forgot the infos about the used system:&lt;br/&gt;
Sun X4600 Server, Solaris 10 x64, 16 Cores, 32 GB RAM, 64 bit JVM 1.5.0_21, -Xmx1512M, RAID 5&lt;/p&gt;</comment>
                    <comment id="12769946" author="thetaphi" created="Mon, 26 Oct 2009 08:56:29 +0000"  >&lt;p&gt;I do not know if this is a bug in 2.9.0, but it seems that segments with all documents deleted are not automatically removed:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;4 of 14: name=_dlo docCount=5
  compound=true
  hasProx=true
  numFiles=2
  size (MB)=0.059
  diagnostics = {java.version=1.5.0_21, lucene.version=2.9.0 817268P - 2009-09-21 10:25:09, os=SunOS,
     os.arch=amd64, java.vendor=Sun Microsystems Inc., os.version=5.10, source=flush}
  has deletions [delFileName=_dlo_1.del]
  test: open reader.........OK [5 deleted docs]
  test: fields..............OK [136 fields]
  test: field norms.........OK [136 fields]
  test: terms, freq, prox...OK [1698 terms; 4236 terms/docs pairs; 0 tokens]
  test: stored fields.......OK [0 total field count; avg ? fields per doc]
  test: term vectors........OK [0 total vector count; avg ? term/freq vector fields per doc]
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Shouldn&apos;t such segments not be removed automatically during the next &lt;b&gt;commit&lt;/b&gt;/close of IndexWriter?&lt;/p&gt;

&lt;p&gt;But this would be another issue. In my opinion, we are fine with the current approach, the longer optimization time is rectified by the larger index size because of no compression anymore and the more heavyer initial merge without addRawDocument is only 30% slower (one time!).&lt;/p&gt;

&lt;p&gt;+1 for committing&lt;/p&gt;</comment>
                    <comment id="12769966" author="mikemccand" created="Mon, 26 Oct 2009 09:57:44 +0000"  >&lt;blockquote&gt;&lt;p&gt;I do not know if this is a bug in 2.9.0, but it seems that segments with all documents deleted are not automatically removed&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Lucene doesn&apos;t actually short-circuit this case, ie, if every single doc in a given segment has been deleted, it will still merge it &lt;span class=&quot;error&quot;&gt;&amp;#91;away&amp;#93;&lt;/span&gt; like normal, rather than simply dropping it immediately from the index, which I agree would be a simple optimization.  Can you open a new issue?  I would think IW can drop such a segment immediately (ie not wait for a merge or optimize) on flushing new deletes.&lt;/p&gt;</comment>
                    <comment id="12769969" author="thetaphi" created="Mon, 26 Oct 2009 10:07:55 +0000"  >&lt;p&gt;Will do! -&amp;gt; &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2010&quot; title=&quot;Remove segments with all documents deleted in commit/flush/close of IndexWriter instead of waiting until a merge occurs.&quot;&gt;&lt;del&gt;LUCENE-2010&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;</comment>
                    <comment id="12769971" author="thetaphi" created="Mon, 26 Oct 2009 10:19:14 +0000"  >&lt;p&gt;For this case: Should we add some testcase in TestBackwardsCompatibility, that tests, that pre 3.0 indexes with compressed fields are correctly uncompressed on optimize and also can be correctly read?&lt;/p&gt;

&lt;p&gt;I do not know how to do this and if the current BW test indexes in the zip files contain compressed fields.&lt;/p&gt;</comment>
                    <comment id="12769978" author="mikemccand" created="Mon, 26 Oct 2009 10:42:18 +0000"  >&lt;blockquote&gt;&lt;p&gt;Should we add some testcase in TestBackwardsCompatibility&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;+1, that&apos;d be great&lt;/p&gt;

&lt;p&gt;What I usually do is make a mod that creates compressed fields, in the prior release (2.9.x), then uncomment the two methods that create new back compat indexes, zip them up, carry them forward to trunk, and modify the trunk test to test them.  Best to also carry forward the code that generated the back compat index, though in this since COMPRESS is removed, you&apos;ll have to comment it out w/ a comment stating &quot;this was used in 2.9 to create field XXX&quot;.&lt;/p&gt;</comment>
                    <comment id="12769981" author="thetaphi" created="Mon, 26 Oct 2009 10:51:08 +0000"  >&lt;p&gt;I am working on it. I already patched the 2.9 version and created the test index.&lt;br/&gt;
In 3.0 I use if dirName.startsWith(&quot;29.&quot;) to only do the optimize tests for this index and no other version.&lt;/p&gt;</comment>
                    <comment id="12770002" author="thetaphi" created="Mon, 26 Oct 2009 12:15:33 +0000"  >&lt;p&gt;Modified patch with testcase for backwards compatibility. It was a little bit trick to check if a field was actually compressed in the source index, but it worked with FieldSelectorResult.SIZE and a test-compression/chars*2. The test was done before/after optimize and it is verified that before the field was still compressed (even it is larger in reality, harhar) and uncompressed after optimize.&lt;/p&gt;

&lt;p&gt;Should I commit the index creation to the current 2.9 branch or not?&lt;/p&gt;

&lt;p&gt;I can commit this, if everybody is happy (because I have the zip files already in my checkout added). Or will you do it, Michael?&lt;/p&gt;</comment>
                    <comment id="12770007" author="mikemccand" created="Mon, 26 Oct 2009 12:27:09 +0000"  >&lt;blockquote&gt;&lt;p&gt;Should I commit the index creation to the current 2.9 branch or not?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;+1, and ideally also, commented out, in trunk&lt;/p&gt;</comment>
                    <comment id="12770011" author="thetaphi" created="Mon, 26 Oct 2009 12:54:10 +0000"  >&lt;p&gt;OK. I will post new test indexes and a new patch, because I want to also test binary stored fields. I will do it by creating a binary/string field for id%2==0 or 1.&lt;/p&gt;

&lt;p&gt;The patch for the index creating must be committed to 2.9 branch, not the backwards tests (because COMPRESS is undefined there, too!)&lt;/p&gt;</comment>
                    <comment id="12770014" author="thetaphi" created="Mon, 26 Oct 2009 13:12:33 +0000"  >&lt;p&gt;Here updated patch and ZIP index files.&lt;/p&gt;

&lt;p&gt;Now also binary fields are tested in the same way. Even document ids get a string compressed field, odd document ids a binary one.&lt;/p&gt;

&lt;p&gt;Also attached is the patch for the 2.9 branch (not BW branch!!!). In trunk, the index creation is commented out, it&apos;s just there for reference.&lt;/p&gt;</comment>
                    <comment id="12770035" author="thetaphi" created="Mon, 26 Oct 2009 14:18:00 +0000"  >&lt;p&gt;Small optimization.&lt;/p&gt;</comment>
                    <comment id="12770044" author="thetaphi" created="Mon, 26 Oct 2009 14:44:32 +0000"  >&lt;p&gt;An additional check, that the raw merge mode is disabled for all segments in 2.9 index and enabled after optimizing with 3.0.&lt;/p&gt;</comment>
                    <comment id="12770054" author="thetaphi" created="Mon, 26 Oct 2009 15:27:17 +0000"  >&lt;p&gt;After thinking a little bit about it:&lt;/p&gt;

&lt;p&gt;Is it ok to test the size of the compressed field by recompressing it with another target VM? E.g., maybe I created the test 2.9 index with another Java Version (1.5.0_21)  where the deflate function is a little bit different implemented and so the test in 3.0 will fail, because maybe someone with Java 6 ran the test using another libzip?&lt;/p&gt;

&lt;p&gt;In this case, I would add another stored field in the test index, that contains the length of the compressed data during creation of the index in the source VM, to be checked with FieldSelectorResult.SIZE?&lt;/p&gt;

&lt;p&gt;Opinions?&lt;/p&gt;</comment>
                    <comment id="12770069" author="thetaphi" created="Mon, 26 Oct 2009 15:53:36 +0000"  >&lt;p&gt;Attached the new indexes and patch that stores the compressed size together with the compressed value as stored field in the 2.9 index. This is now secure and invariant on different in-JDK compression implementations.&lt;/p&gt;</comment>
                    <comment id="12770107" author="thetaphi" created="Mon, 26 Oct 2009 17:54:52 +0000"  >&lt;p&gt;Add an assertion in FieldsReader that checks, that 3.0 indexes have no compressed fields. Also a small test cleanup.&lt;/p&gt;

&lt;p&gt;Its ready to commit now!&lt;/p&gt;</comment>
                    <comment id="12770122" author="michaelbusch" created="Mon, 26 Oct 2009 18:26:41 +0000"  >&lt;blockquote&gt;
&lt;p&gt;I can commit this, if everybody is happy (because I have the zip files already in my checkout added). Or will you do it, Michael?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Go ahead!&lt;/p&gt;

&lt;p&gt;Cool that you added the bw-test. I&apos;ve been wanting to do that too, but I didn&apos;t have time yet. Thank you!&lt;/p&gt;</comment>
                    <comment id="12770127" author="thetaphi" created="Mon, 26 Oct 2009 18:33:55 +0000"  >&lt;p&gt;OK, will do it a later in the evening (MEZ), have no time now. I will also add a good changes.txt entry in behaviour change or like that.&lt;/p&gt;</comment>
                    <comment id="12770204" author="thetaphi" created="Mon, 26 Oct 2009 21:17:45 +0000"  >&lt;p&gt;Committed revision 829972. Thanks also to Michael Busch!&lt;/p&gt;</comment>
                </comments>
                <issuelinks>
                        <issuelinktype id="10030">
                <name>Reference</name>
                                                <inwardlinks description="is related to">
                            <issuelink>
            <issuekey id="12439063">LUCENE-2010</issuekey>
        </issuelink>
                    </inwardlinks>
                            </issuelinktype>
                    </issuelinks>
                <attachments>
                    <attachment id="12423211" name="index.29.cfs.zip" size="4531" author="thetaphi" created="Mon, 26 Oct 2009 15:52:06 +0000" />
                    <attachment id="12423189" name="index.29.cfs.zip" size="4399" author="thetaphi" created="Mon, 26 Oct 2009 13:12:33 +0000" />
                    <attachment id="12423184" name="index.29.cfs.zip" size="4324" author="thetaphi" created="Mon, 26 Oct 2009 12:15:33 +0000" />
                    <attachment id="12423212" name="index.29.nocfs.zip" size="8733" author="thetaphi" created="Mon, 26 Oct 2009 15:52:06 +0000" />
                    <attachment id="12423190" name="index.29.nocfs.zip" size="8621" author="thetaphi" created="Mon, 26 Oct 2009 13:12:33 +0000" />
                    <attachment id="12423185" name="index.29.nocfs.zip" size="8541" author="thetaphi" created="Mon, 26 Oct 2009 12:15:33 +0000" />
                    <attachment id="12423214" name="lucene-1960-1-branch29.patch" size="2717" author="thetaphi" created="Mon, 26 Oct 2009 15:53:36 +0000" />
                    <attachment id="12423188" name="lucene-1960-1-branch29.patch" size="1217" author="thetaphi" created="Mon, 26 Oct 2009 13:09:46 +0000" />
                    <attachment id="12423226" name="lucene-1960-1.patch" size="24168" author="thetaphi" created="Mon, 26 Oct 2009 17:54:52 +0000" />
                    <attachment id="12423213" name="lucene-1960-1.patch" size="23997" author="thetaphi" created="Mon, 26 Oct 2009 15:53:36 +0000" />
                    <attachment id="12423203" name="lucene-1960-1.patch" size="23256" author="thetaphi" created="Mon, 26 Oct 2009 14:44:32 +0000" />
                    <attachment id="12423197" name="lucene-1960-1.patch" size="22435" author="thetaphi" created="Mon, 26 Oct 2009 14:18:00 +0000" />
                    <attachment id="12423183" name="lucene-1960-1.patch" size="21110" author="thetaphi" created="Mon, 26 Oct 2009 12:15:33 +0000" />
                    <attachment id="12423094" name="lucene-1960-1.patch" size="14689" author="michaelbusch" created="Sat, 24 Oct 2009 02:35:30 +0100" />
                    <attachment id="12421672" name="lucene-1960-1.patch" size="14269" author="michaelbusch" created="Thu, 8 Oct 2009 23:06:59 +0100" />
                    <attachment id="12421592" name="lucene-1960.patch" size="41420" author="michaelbusch" created="Thu, 8 Oct 2009 01:16:05 +0100" />
                    <attachment id="12423123" name="optimize-time.txt" size="31860" author="thetaphi" created="Sat, 24 Oct 2009 18:11:08 +0100" />
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>17.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Thu, 8 Oct 2009 04:27:44 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>11811</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    <customfieldvalue key="10120"><![CDATA[Patch Available]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>25765</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                    <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                <customfieldname>Time in Status</customfieldname>
                <customfieldvalues>
                    
                </customfieldvalues>
            </customfield>
                            </customfields>
    </item>
</channel>
</rss>