<!-- 
RSS generated by JIRA (5.2.8#851-sha1:3262fdc28b4bc8b23784e13eadc26a22399f5d88) at Tue Jul 16 12:58:06 UTC 2013

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/LUCENE-1488/LUCENE-1488.xml?field=key&field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>5.2.8</version>
        <build-number>851</build-number>
        <build-date>26-02-2013</build-date>
    </build-info>

<item>
            <title>[LUCENE-1488] multilingual analyzer based on icu</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-1488</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;The standard analyzer in lucene is not exactly unicode-friendly with regards to breaking text into words, especially with respect to non-alphabetic scripts.  This is because it is unaware of unicode bounds properties.&lt;/p&gt;

&lt;p&gt;I actually couldn&apos;t figure out how the Thai analyzer could possibly be working until i looked at the jflex rules and saw that codepoint range for most of the Thai block was added to the alphanum specification. defining the exact codepoint ranges like this for every language could help with the problem but you&apos;d basically be reimplementing the bounds properties already stated in the unicode standard. &lt;/p&gt;

&lt;p&gt;in general it looks like this kind of behavior is bad in lucene for even latin, for instance, the analyzer will break words around accent marks in decomposed form. While most latin letter + accent combinations have composed forms in unicode, some do not. (this is also an issue for asciifoldingfilter i suppose). &lt;/p&gt;

&lt;p&gt;I&apos;ve got a partially tested standardanalyzer that uses icu Rule-based BreakIterator instead of jflex. Using this method you can define word boundaries according to the unicode bounds properties. After getting it into some good shape i&apos;d be happy to contribute it for contrib but I wonder if theres a better solution so that out of box lucene will be more friendly to non-ASCII text. Unfortunately it seems jflex does not support use of these properties such as [\p&lt;/p&gt;
{Word_Break = Extend}
&lt;p&gt;] so this is probably the major barrier.&lt;/p&gt;

&lt;p&gt;Thanks,&lt;br/&gt;
Robert&lt;/p&gt;


</description>
                <environment></environment>
            <key id="12410485">LUCENE-1488</key>
            <summary>multilingual analyzer based on icu</summary>
                <type id="2" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/newfeature.png">New Feature</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png">Closed</status>
                    <resolution id="1">Fixed</resolution>
                                <assignee username="rcmuir">Robert Muir</assignee>
                                <reporter username="rcmuir">Robert Muir</reporter>
                        <labels>
                    </labels>
                <created>Thu, 11 Dec 2008 15:27:04 +0000</created>
                <updated>Mon, 16 May 2011 19:15:32 +0100</updated>
                    <resolved>Mon, 3 May 2010 14:22:21 +0100</resolved>
                                            <fixVersion>3.1</fixVersion>
                <fixVersion>4.0-ALPHA</fixVersion>
                                <component>modules/analysis</component>
                        <due></due>
                    <votes>3</votes>
                        <watches>2</watches>
                                                    <comments>
                    <comment id="12655808" author="gsingers" created="Thu, 11 Dec 2008 23:26:28 +0000"  >&lt;p&gt;Very interesting, Robert.  I&apos;d like to see your patch.  I don&apos;t think we need to think of it as a Std. Analyzer replacement, but I could totally see offering it as the the ICUAnalyzer or some other better name.  In other words, I&apos;d approach this as another Analyzer in the arsenal of Analyzers, otherwise, we&apos;ll have to deal with back-compatibility issues, etc.&lt;/p&gt;</comment>
                    <comment id="12655840" author="rcmuir" created="Fri, 12 Dec 2008 01:04:12 +0000"  >&lt;p&gt;thats a good idea. you know, currently trying to get it to pass all the standard analyzer unit tests causes some problems since lucene has some rather obscure definitions of &apos;number&apos; (i think ip addresses, etc are included) which differ dramatically from the basic unicode definition.&lt;/p&gt;

&lt;p&gt;Other things of note:&lt;/p&gt;

&lt;p&gt;instantiating the analyzer takes a long time (couple seconds) because ICU must &quot;compile&quot; the rules. I&apos;m not sure of the specifics but by compile I think that means building massive FSM or similar based on all the unicode data. Its possible to precompile the rules into binary format but I think this is not currently exposed in ICU.&lt;/p&gt;

&lt;p&gt;the lucene tokenization pipeline makes the implementation a little hairy. I hack around it by tokenizing on whitespace first, then acting as a token filter (just like the Thai analyzer does, which also uses RBBI). I don&apos;t think this really is that bad from a linguistic standpoint because the rare cases where &apos;token&apos; can have whitespace inside of it (persian, etc) need serious muscle somewhere else and should be handled by a language analyzer.&lt;/p&gt;

&lt;p&gt;i&apos;ll try to get this thing in reasonable shape at least to document the approach.&lt;/p&gt;</comment>
                    <comment id="12655898" author="rcmuir" created="Fri, 12 Dec 2008 05:58:16 +0000"  >&lt;p&gt;i&apos;ve attached a patch for &apos;ICUAnalyzer&apos;. I see that some things involving Token have changed but I created it before that point.&lt;/p&gt;

&lt;p&gt;I stole the unit tests from standard analyzer and put comments as to why certain ones arent appropriate and disabled those.&lt;/p&gt;

&lt;p&gt;i added some unit tests that demonstrate some of the value, correct analysis for arabic numerals, hindi text, decomposed latin diacritics, hebrew punctuation, cantonese and linear-b text outside of the BMP, etc.&lt;/p&gt;

&lt;p&gt;one issue is that setMaxTokenLength() doesnt work correctly for values &amp;gt; 255 because CharTokenizer has a hardcoded private limit of 255 that i can&apos;t override. This is a problem since i use WhitespaceTokenizer first and then break down those tokens with the RBBI.&lt;/p&gt;</comment>
                    <comment id="12656040" author="rcmuir" created="Fri, 12 Dec 2008 15:38:32 +0000"  >&lt;p&gt;as soon as I figure out how to invoke the ICU RBBI compiler i&apos;ll see if i can update the patch with compiled rules so instantiation of this thing is cheap...&lt;/p&gt;</comment>
                    <comment id="12703598" author="talktoudaykunar" created="Tue, 28 Apr 2009 12:31:12 +0100"  >&lt;p&gt;hi,&lt;/p&gt;

&lt;p&gt;i too just facing the same problem. my documet contains english as well as danish elements.&lt;/p&gt;

&lt;p&gt;I tried to use this analyzer. when i try to use this i got this error .&lt;/p&gt;

&lt;p&gt;Exception in thread &quot;main&quot; java.lang.ExceptionInInitializerError&lt;br/&gt;
	at org.apache.lucene.analysis.icu.ICUAnalyzer.tokenStream(ICUAnalyzer.java:74)&lt;br/&gt;
	at org.apache.lucene.analysis.Analyzer.reusableTokenStream(Analyzer.java:48)&lt;br/&gt;
	at org.apache.lucene.index.DocInverterPerField.processFields(DocInverterPerField.java:117)&lt;br/&gt;
	at org.apache.lucene.index.DocFieldConsumersPerField.processFields(DocFieldConsumersPerField.java:36)&lt;br/&gt;
	at org.apache.lucene.index.DocFieldProcessorPerThread.processDocument(DocFieldProcessorPerThread.java:234)&lt;br/&gt;
	at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:765)&lt;br/&gt;
	at org.apache.lucene.index.DocumentsWriter.addDocument(DocumentsWriter.java:743)&lt;br/&gt;
	at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1918)&lt;br/&gt;
	at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1895)&lt;br/&gt;
	at com.IndexFiles.indexDocs(IndexFiles.java:87)&lt;br/&gt;
	at com.IndexFiles.indexDocs(IndexFiles.java:80)&lt;br/&gt;
	at com.IndexFiles.main(IndexFiles.java:57)&lt;br/&gt;
Caused by: java.lang.IllegalArgumentException: Error 66063 at line 2 column 17&lt;br/&gt;
	at com.ibm.icu.text.RBBIRuleScanner.error(RBBIRuleScanner.java:505)&lt;br/&gt;
	at com.ibm.icu.text.RBBIRuleScanner.scanSet(RBBIRuleScanner.java:1047)&lt;br/&gt;
	at com.ibm.icu.text.RBBIRuleScanner.doParseActions(RBBIRuleScanner.java:484)&lt;br/&gt;
	at com.ibm.icu.text.RBBIRuleScanner.parse(RBBIRuleScanner.java:912)&lt;br/&gt;
	at com.ibm.icu.text.RBBIRuleBuilder.compileRules(RBBIRuleBuilder.java:298)&lt;br/&gt;
	at com.ibm.icu.text.RuleBasedBreakIterator.compileRules(RuleBasedBreakIterator.java:316)&lt;br/&gt;
	at com.ibm.icu.text.RuleBasedBreakIterator.&amp;lt;init&amp;gt;(RuleBasedBreakIterator.java:71)&lt;br/&gt;
	at org.apache.lucene.analysis.icu.ICUBreakIterator.&amp;lt;init&amp;gt;(ICUBreakIterator.java:53)&lt;br/&gt;
	at org.apache.lucene.analysis.icu.ICUBreakIterator.&amp;lt;init&amp;gt;(ICUBreakIterator.java:45)&lt;br/&gt;
	at org.apache.lucene.analysis.icu.ICUTokenizer.&amp;lt;clinit&amp;gt;(ICUTokenizer.java:58)&lt;br/&gt;
	... 12 more&lt;/p&gt;

&lt;p&gt;please help me in this.&lt;/p&gt;</comment>
                    <comment id="12703645" author="rcmuir" created="Tue, 28 Apr 2009 14:58:14 +0100"  >&lt;p&gt;what version of icu4j are you using? needs to be &amp;gt;= 4.0&lt;/p&gt;</comment>
                    <comment id="12716289" author="rcmuir" created="Thu, 4 Jun 2009 15:20:05 +0100"  >&lt;p&gt;updated patch, not ready yet but you can see where i am going.&lt;/p&gt;

&lt;p&gt;ICUTokenizer: Breaks text into words according to UAX #29: Unicode Text Segmentation. Text is divided across script boundaries so that this segmentation can be tailored for different writing systems; for example Thai text is segmented with a different method. The default and script-specific rules can be tailored. In the resources folder i have some examples for Southeast Asian scripts, etc.  Since i need script boundaries for tailoring, i stuff the ISO 15924 script code constant in the flags; this could be useful for downstream consumers.&lt;/p&gt;

&lt;p&gt;ICUCaseFoldingFilter: Fold case according to Unicode Default Caseless Matching; Full case folding. This may change the length of the token, for example german sharp s is folded to &apos;ss&apos;. This filter interacts with the downstream normalization filter in a special way, so you can provide a hint as to what the desired normalization form will be. In the NFKC or NFKD case it will apply the NFKC_Closure set so you do not have to Normalize(Fold(Normalize(Fold&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/error.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;)))&lt;/p&gt;

&lt;p&gt;ICUDigitFoldingFilter: Standardize digits from different scripts to the latin values, 0-9.&lt;/p&gt;

&lt;p&gt;ICUFormatFilter: Remove identifier-ignorable codepoints, specifically those from the Format category. &lt;/p&gt;

&lt;p&gt;ICUNormalizationFilter: Apply unicode normalization to text. This is accelerated with a quick-check.&lt;/p&gt;

&lt;p&gt;ICUAnalyzer ties all this together. All of these components should also work correctly with surrogate-pair data. &lt;/p&gt;

&lt;p&gt;Needs more doc and tests. any comments appreciated.&lt;/p&gt;</comment>
                    <comment id="12716686" author="rcmuir" created="Fri, 5 Jun 2009 18:42:21 +0100"  >&lt;p&gt;here&apos;s a simple description of what the current functionality buys you, its this:&lt;/p&gt;

&lt;p&gt;all indic languages (Hindi, Bengali, Tamil, ...), middle eastern languages (Arabic, Hebrew, etc) will work pretty well here (by that I mean tokenized, normalized, etc). Most of these lucene cannot parse correctly with any of the built-in analyzers.&lt;/p&gt;

&lt;p&gt;obviously european languages lucene handles quite well already, but unicode still has some improvements here, i.e. better case-folding.&lt;/p&gt;

&lt;p&gt;And finally, of course, the situation where you have data in a bunch of these different languages!&lt;/p&gt;

&lt;p&gt;in general, the unicode defaults work quite well for almost all languages, with the exception of CJK and southeast-asian languages. &lt;br/&gt;
its not my intent to really solve those harder cases, only to provide a mechanism for someone else to deal with it if they don&apos;t like the defaults.&lt;/p&gt;

&lt;p&gt;a great example is the arabic tokenizer, it should not exist. unicode defaults work great for that language. and it would be silly to think about HindiTokenizer, BengaliTokenizer, etc etc when unicode defaults will tokenize those correctly as well. &lt;/p&gt;

&lt;p&gt;there&apos;s still some annoying complexity here, and any comments are appreciated. Especially tricky is the complexity-performance-maintenance balance, i.e. the case-folding filter could be a lot faster, but then it would have to be updated when a new unicode version is released... Another thing is i didn&apos;t optimize the BMP case anywhere &lt;span class=&quot;error&quot;&gt;&amp;#91;i.e. working at 32-bit codepoint to ensure surrogate data works&amp;#93;&lt;/span&gt;, and I think thats worth considering... like 99.9% of data is in the BMP &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;Thanks,&lt;br/&gt;
Robert&lt;/p&gt;</comment>
                    <comment id="12719074" author="rcmuir" created="Sat, 13 Jun 2009 04:59:48 +0100"  >&lt;p&gt;just an update, still more work to be done.&lt;/p&gt;

&lt;p&gt;some of the components are javadoc&apos;ed and have pretty good tests (case folding and normalization). These might be useful to someone in the meantime.&lt;/p&gt;

&lt;p&gt;also added some tests to TestICUAnalyzer for various jira issues (&lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-1032&quot; title=&quot;CJKAnalyzer should convert half width katakana to full width katakana&quot;&gt;&lt;del&gt;LUCENE-1032&lt;/del&gt;&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-1215&quot; title=&quot;Support of Unicode Collation&quot;&gt;&lt;del&gt;LUCENE-1215&lt;/del&gt;&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-1343&quot; title=&quot;A replacement for AsciiFoldingFilter that does a more thorough job of removing diacritical marks or non-spacing modifiers.&quot;&gt;&lt;del&gt;LUCENE-1343&lt;/del&gt;&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-1545&quot; title=&quot;Standard analyzer does not correctly tokenize combining character U+0364 COMBINING LATIN SMALL LETTRE E&quot;&gt;&lt;del&gt;LUCENE-1545&lt;/del&gt;&lt;/a&gt;, etc) that are solved here. &lt;/p&gt;</comment>
                    <comment id="12719109" author="mikemccand" created="Sat, 13 Jun 2009 11:21:22 +0100"  >&lt;p&gt;ICUAnalyzer looks very useful!  Good work Robert.  (And, thanks!).&lt;/p&gt;

&lt;p&gt;Do you think this&apos;ll be ready to go in time for 2.9 (which we are&lt;br/&gt;
trying to wrap up soonish)?&lt;/p&gt;

&lt;p&gt;It seems like this absorbs the functionality of many of Lucene&apos;s&lt;br/&gt;
current analyzers.  EG you mentioned ArabicAnalyzer already.  What&lt;br/&gt;
other analyzers (eg in contrib/analyzers/*) would you say are&lt;br/&gt;
logically subsumed by this?&lt;/p&gt;

&lt;p&gt;Also, this seems quite different from StandardAnalyzer, in that it&lt;br/&gt;
focuses entirely on doing &quot;good&quot; tokenization, by relying on the&lt;br/&gt;
Unicode standard (defaults) instead of fixed char ranges in&lt;br/&gt;
StandardAnalyzer.  So it fixes many bugs in how StandardAnalyzer&lt;br/&gt;
tokenizes, especially on non-European languages.&lt;/p&gt;

&lt;p&gt;Also, StandardAnalyzer goes beyond making the initial tokens: it also&lt;br/&gt;
tries to label things as acronym, host name, number, etc.; tries to&lt;br/&gt;
filter out stop words.&lt;/p&gt;

&lt;p&gt;I assume ICUCaseFoldingFilter logically subsumes LowercaseFilter?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Especially tricky is the complexity-performance-maintenance balance, i.e. the case-folding filter could be a lot faster, but then it would have to be updated when a new unicode version is released.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think it&apos;s fine to worry about this later.  Correctness is more&lt;br/&gt;
important than performance at this point.&lt;/p&gt;</comment>
                    <comment id="12719126" author="rcmuir" created="Sat, 13 Jun 2009 15:47:52 +0100"  >&lt;p&gt;Michael, I don&apos;t think it will be ready for 2.9, here is some answers to your questions.&lt;/p&gt;

&lt;p&gt;going with your arabic example:&lt;br/&gt;
The only thing this absorbs is language-specific tokenization (like ArabicLetterTokenizer), because as mentioned I think thats generally the wrong approach.&lt;br/&gt;
But this can&apos;t replace ArabicAnalyzer completely, because ArabicAnalyzer stems arabic text in a language-specific way, which has a huge effect on retrieval quality for Arabic language text.&lt;/p&gt;

&lt;p&gt;Some of what it does the language-specific analyzers don&apos;t do though.&lt;/p&gt;

&lt;p&gt;In this specific example, it would be nice if ArabicAnalyzer really used the functionality here, then did its Arabic-specific stuff!&lt;br/&gt;
Because this functionality will do things like normalize &apos;Arabic Presentation Forms&apos; and deal with Arabic digits and things that aren&apos;t in the ArabicAnalyzer. It also will treat any non-Arabic text in your corpus very nicely!&lt;/p&gt;

&lt;p&gt;Yes, you are correct about the difference from StandardAnalyzer and I would argue there are tokenization bugs in how StandardAnalyzer works with European languages too, just see &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-1545&quot; title=&quot;Standard analyzer does not correctly tokenize combining character U+0364 COMBINING LATIN SMALL LETTRE E&quot;&gt;&lt;del&gt;LUCENE-1545&lt;/del&gt;&lt;/a&gt;!&lt;/p&gt;

&lt;p&gt;I know StandardAnalyzer does these things. This tokenizer has some built-in types already, such as number. If you want to add more types, its easy. Just make a .txt file with your grammar, create a RuleBasedBreakIterator with it, and pass it along to the tokenizer constructor. you will have to subclass the tokenizer&apos;s getType() for any new types though, because RBBI &apos;types&apos; are really just integer codes in the rule file, and you have to map them to some text such as &quot;WORD&quot;.&lt;/p&gt;

&lt;p&gt;Yes, case-folding will work better than lowercase for a few european languages.&lt;/p&gt;</comment>
                    <comment id="12719322" author="earwin" created="Sun, 14 Jun 2009 20:25:22 +0100"  >&lt;blockquote&gt;&lt;p&gt;But this can&apos;t replace ArabicAnalyzer completely, because ArabicAnalyzer stems arabic text in a language-specific way, which has a huge effect on retrieval quality for Arabic language text.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;What about separating word-tokenizing from morphological processing?&lt;/p&gt;</comment>
                    <comment id="12719342" author="rcmuir" created="Sun, 14 Jun 2009 23:08:17 +0100"  >&lt;p&gt;Earwin, I don&apos;t understand your question... &lt;br/&gt;
There is no morphological processing or any other language-specific functionality in this patch...&lt;/p&gt;</comment>
                    <comment id="12720364" author="rcmuir" created="Tue, 16 Jun 2009 22:40:14 +0100"  >&lt;p&gt;add analysis tests for a few languages to demonstrate what this does.&lt;/p&gt;</comment>
                    <comment id="12726571" author="earwin" created="Thu, 2 Jul 2009 18:34:49 +0100"  >&lt;blockquote&gt;&lt;p&gt;There is no morphological processing or any other language-specific functionality in this patch... &lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I&apos;m speaking of stemming in ArabicAnalyzer. Why can&apos;t you use its stemming tokenfilter over all ICU goodness from this patch? Everything else ArabicAnalyzer consists of might as well be deleted right after.&lt;/p&gt;</comment>
                    <comment id="12726670" author="rcmuir" created="Thu, 2 Jul 2009 21:42:00 +0100"  >&lt;p&gt;Earwin, you are absolutely correct.&lt;/p&gt;

&lt;p&gt;though i would also want to keep the ArabicNormalizationFilter as it does &quot;non-standard&quot; normalization that is usually helpful for arabic text.&lt;/p&gt;</comment>
                    <comment id="12752331" author="rcmuir" created="Tue, 8 Sep 2009 05:57:39 +0100"  >&lt;p&gt;this is latest copy of my code (in response to java-user discussion).&lt;/p&gt;

&lt;p&gt;not many changes except tokenstream changes and work for writing systems with no word separation: lao, myanmar, cjk, etc.&lt;br/&gt;
for these, the tokenizer does not break text into words, but subwords (syllables), and unigrams&amp;amp;bigrams of these are indexed.&lt;/p&gt;</comment>
                    <comment id="12759702" author="rcmuir" created="Fri, 25 Sep 2009 20:03:56 +0100"  >&lt;p&gt;here I complete Lao support (fully implementing &lt;a href=&quot;http://www.panl10n.net/english/final%20reports/pdf%20files/Laos/LAO06.pdf&quot; class=&quot;external-link&quot;&gt;http://www.panl10n.net/english/final%20reports/pdf%20files/Laos/LAO06.pdf&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;Also fix a tokenstream bug (not back-compat issue!) in the bigramfilter.&lt;/p&gt;

&lt;p&gt;I think all language/unicode features are done, basically we can get better language support in the future from ICU automatically, but I think all languages are handled in a reasonable way for now. &lt;/p&gt;

&lt;p&gt;imho all that is left is:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;fix docs, improve tests, java api, rbbi grammars, any bugs, TODOs&lt;/li&gt;
	&lt;li&gt;decide if we want to merge this with the collation contrib (I think it might be a good idea)&lt;/li&gt;
	&lt;li&gt;test various versions of ICU to know which ones it works with&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;it works and the tests pass, but some tests are slow (10+ seconds, though I made them faster).&lt;br/&gt;
The problem is these slow tests have found bugs and will help test version compatibility, so I like them.&lt;/p&gt;</comment>
                    <comment id="12759731" author="thetaphi" created="Fri, 25 Sep 2009 21:30:53 +0100"  >&lt;p&gt;Hi Robert: If you do a restoreState() no clearAttributes() is needed before, as the restoreState overwrites all attributes. Everything else looks good.&lt;/p&gt;</comment>
                    <comment id="12759735" author="rcmuir" created="Fri, 25 Sep 2009 21:40:40 +0100"  >&lt;p&gt;Uwe, thanks for taking a look! I&apos;ll fix this.&lt;/p&gt;</comment>
                    <comment id="12777745" author="rcmuir" created="Fri, 13 Nov 2009 23:44:33 +0000"  >&lt;p&gt;setting a fix version, setting a correct description of the issue&lt;/p&gt;</comment>
                    <comment id="12785036" author="dmsmith" created="Wed, 2 Dec 2009 22:45:47 +0000"  >&lt;p&gt;Robert, just finished reviewing the code. Looks great! Doesn&apos;t look like there&apos;s too much left. All I see is a bit of JavaDoc and an extraneous unused variable (ICUTokenizer: private PositionIncrementAttribute posIncAtt&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/wink.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;The documentation in ICUNormalizationFilter is very instructive. Kudos. The only part that&apos;s hard to for me to understand is the filter order dependency, but then again that&apos;s a hard topic in the first place.&lt;/p&gt;

&lt;p&gt;I&apos;m wondering whether it would make sense to have multiple representations of a token with the same position in the index. Specifically, transliterations and case-folding. That is, the one is a &quot;synonym&quot; for the other. Is that possible and does it make sense? I&apos;m imagining a use case where a end user enters for a search request a Latin script transliteration of Greek &quot;uios&quot; but might also enter &quot;&#965;&#953;&#959;&#962;&quot;.&lt;/p&gt;

&lt;p&gt;The other question on my mind is that given a text of German, Greek and Hebrew (three distinct scripts) does it make sense to apply stop words to them based on script? And should stop words be normalized on load with the ICUNormalizationFilter? Or is it a given that they work as is?&lt;/p&gt;

&lt;p&gt;Can/How does all this integrate with stemmers?&lt;/p&gt;

&lt;p&gt;Again, many thanks! (Btw, special thanks for this working with 2.9 and Java 1.4!)&lt;/p&gt;
</comment>
                    <comment id="12785054" author="rcmuir" created="Wed, 2 Dec 2009 23:19:23 +0000"  >&lt;p&gt;DM, I really appreciate your review. You have brought up some good ideas that I haven&apos;t yet thought about.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;All I see is a bit of JavaDoc and an extraneous unused variable (ICUTokenizer: private PositionIncrementAttribute posIncAtt&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yeah there are some TODOs, and cleanup on the tokenstreams, and the API in general. its not easy to customize the way its supposed to be: where you as a user can actually supply BreakIterator impls to the tokenizer and say &quot;use these rules/dictionary/whatever for tokenizing XYZ script only&quot;.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I&apos;m wondering whether it would make sense to have multiple representations of a token with the same position in the index. Specifically, transliterations and case-folding. That is, the one is a &quot;synonym&quot; for the other. Is that possible and does it make sense? I&apos;m imagining a use case where a end user enters for a search request a Latin script transliteration of Greek &quot;uios&quot; but might also enter &quot;&#965;&#953;&#959;&#962;&quot;.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yeah this is something to consider. I don&apos;t think it makes sense for the case folding filter, but maybe for the transform filter? will have to think about it.&lt;br/&gt;
There&apos;s use cases here like what you mentioned, also real-world ones like invoking Serbian-Latin or something, where you want users to search in either writing system and there actually is a clearly defined transformation.&lt;/p&gt;

&lt;p&gt;I guess on the other hand, you could always use a separate field (with different analysis/transforms) for each and search both.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;The other question on my mind is that given a text of German, Greek and Hebrew (three distinct scripts) does it make sense to apply stop words to them based on script? And should stop words be normalized on load with the ICUNormalizationFilter? Or is it a given that they work as is?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;You could put them all in one list with regular stopfilter now. They won&apos;t clash since they are different unicode Strings. Obviously I would normalize this list with the same stuff (normalization form/case folding/whatever) that your analyzer users.&lt;/p&gt;

&lt;p&gt;I don&apos;t put any stopwords in this, because thats language dependent, trying to stick with language-independent (either stuff that applies to unicode as a whole, or specific writing systems, which can be accurately detected).&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Can/How does all this integrate with stemmers?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Right this is just supposed to be what &quot;StandardTokenizer&quot;-type stuff does, and you would add stemming on top of it. The idea is you would use this even if you think you only have english text, maybe then applying your porter english stemmer. But if it happens to stumble upon some CJK or Thai or something along the way, everything will be ok.&lt;/p&gt;

&lt;p&gt;In all honesty, I probably put 90% of the work into the Khmer, Myanmar, Lao, etc cases. Having good tokenization I think makes a usable search engine, for a lot of languages stemming is only a bonus.&lt;/p&gt;

&lt;p&gt;However, one thing it also does is put the script value in the flags for each token. This can work pretty well: if its Greek script, its probably Greek language, but if its Hebrew script, well it could be Yiddish too.  If its Latin script, could be english, german, etc. Its ended only to make life easier since the information is already available... but I don&apos;t know yet how to make use of it in a nice way.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Again, many thanks! (Btw, special thanks for this working with 2.9 and Java 1.4!)&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yeah i haven&apos;t updated it to java 5/Lucene 3.x yet, started working it, but kinda forgot about that so far. I guess this is a good thing, so you can play with it if you want.&lt;/p&gt;
</comment>
                    <comment id="12786976" author="rcmuir" created="Mon, 7 Dec 2009 16:29:42 +0000"  >&lt;p&gt;Linking this issue to &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2124&quot; title=&quot;move JDK collation to core, ICU collation to ICU contrib&quot;&gt;&lt;del&gt;LUCENE-2124&lt;/del&gt;&lt;/a&gt;. Once contrib/collation has been renamed contrib/icu, I want to split out two of the tokenfilters (case folding and normalization) out as a separate smaller issue to start. &lt;/p&gt;

&lt;p&gt;These are useful on their own, yet inseparable because of the special K mappings: you must tell the case folding filter what your targeted normalization form will be.&lt;/p&gt;</comment>
                    <comment id="12802568" author="vstoto" created="Wed, 20 Jan 2010 00:35:59 +0000"  >&lt;p&gt;I am developing an IR system for Lao. I&apos;ve been searching for this kind of analyzers to be used in my development to index documents containing languages like Lao, French and English in one single passage.&lt;/p&gt;

&lt;p&gt;I tested it for Lao language for Lucene 2.9 and 3.0 using my short passage. It worked correctly for both versions as I expected, especially for segmenting Lao single syllables. I also tried it with the bi-gram filter option for two syllables, which worked fine for simple words. The result contained some two-syllable words which do not make sense in Lao language. I guess this not a big issue. As Robert pointed out (in an email to me), we still need dictionary-based word segmentation for Lao, which can be integrated in ICU and used by this analyzer.&lt;/p&gt;

&lt;p&gt;Any way, thanks for your assistance. This work will be helpful not only for Lao, but others as well because it&apos;s good to have a common analyzer for unicode characters.&lt;/p&gt;

&lt;p&gt;I&apos;ll continue testing it and report any problems if I find one. &lt;/p&gt;</comment>
                    <comment id="12802596" author="rcmuir" created="Wed, 20 Jan 2010 01:25:05 +0000"  >&lt;p&gt;Thanks for sharing those results! Yes the bigram behavior (right now enabled for Han, Lao, Khmer, and Myanmar) is an attempt to boost relevance in a consistent way since we do not have dictionary-based word segmentation for those writing systems, only the ability to segment into syllables.&lt;/p&gt;

&lt;p&gt;In the next patch I&apos;ll make it easier to configure this behavior, and turn it off when you want, without writing your own analyzer.&lt;/p&gt;

&lt;p&gt;I am glad to hear the syllable segmentation algorithm is working well! &lt;br/&gt;
The credit really belongs to the Pan Localization Project, I simply implemented the algorithm described here: &lt;a href=&quot;http://www.panl10n.net/english/final%20reports/pdf%20files/Laos/LAO06.pdf&quot; class=&quot;external-link&quot;&gt;http://www.panl10n.net/english/final%20reports/pdf%20files/Laos/LAO06.pdf&lt;/a&gt;&lt;br/&gt;
You can see the code in Lao.rbbi in the patch, warning, as it mentions, I am pretty sure Lao numeric digits are not yet working correctly, but hopefully I will fix those too in the next version.&lt;/p&gt;</comment>
                    <comment id="12803058" author="vstoto" created="Wed, 20 Jan 2010 23:37:08 +0000"  >&lt;p&gt;I tested Lao numbers. It only worked for 2 digit numbers (because of two syllable segmentation),  but the result tokens were converted to Arabic numbers (instead of Lao). This is not too bad for analyzing heading numbers and ordered lists with less than 100 items (the meaning and order are preserved).&lt;/p&gt;

&lt;p&gt;In the documents i encountered most of scientific numerals or financial figures (complex numeric digits) were written using Arabic numbers.&lt;/p&gt;

&lt;p&gt;Nevertheless, recognizing long Lao numeric digits is a &quot;must-to-have&quot; to complete this set for Laos.&lt;/p&gt;
</comment>
                    <comment id="12803061" author="rcmuir" created="Wed, 20 Jan 2010 23:42:57 +0000"  >&lt;p&gt;Hi,this is not intentional to split them into 2 digits, it is really only because of rbbi rule-chaining turned off.&lt;br/&gt;
So now &#3792;&#3793;&#3794;&#3795; stays as a single token, and later becomes 0123.&lt;/p&gt;

&lt;p&gt;I&apos;ve written tests for, and fixed numerics in my local copy for lao, myanmar, and khmer. I will post an updated patch hopefully soon with all the improvements.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;but the result tokens were converted to Arabic numbers (instead of Lao).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes this is intentional, later there is a filter that converts all numeric digits to Arabic so the search will match either.&lt;/p&gt;</comment>
                    <comment id="12845914" author="rcmuir" created="Tue, 16 Mar 2010 14:22:05 +0000"  >&lt;p&gt;uploading a dump of my workspace, so Uwe can review the new attribute.&lt;/p&gt;</comment>
                    <comment id="12845917" author="thetaphi" created="Tue, 16 Mar 2010 14:36:11 +0000"  >&lt;p&gt;Attribute looks good! I would only fix toString() to match the defaulkt impl by using syntax variableName + &quot;=&quot; + value, here  &quot;code=&quot;+getName(code). This makes AttrubuteSource.toString() look nice.&lt;/p&gt;</comment>
                    <comment id="12845951" author="rcmuir" created="Tue, 16 Mar 2010 15:55:43 +0000"  >&lt;p&gt;Thanks for the review Uwe! moving forwards...&lt;/p&gt;</comment>
                    <comment id="12851713" author="dbowen" created="Wed, 31 Mar 2010 04:44:37 +0100"  >&lt;p&gt;I have a possibly naive question on the bigram filter: why would you want to index the individual one-character-tokens, as well as the bigrams?   The CJK Tokenizer just emits the bigrams.  Wouldn&apos;t indexing and searching on the unigrams as well as the bigrams just slow down search?&lt;/p&gt;
</comment>
                    <comment id="12851714" author="rcmuir" created="Wed, 31 Mar 2010 04:55:36 +0100"  >&lt;blockquote&gt;&lt;p&gt;I have a possibly naive question on the bigram filter&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Its not naive at all really! I think we to do exactly what you suggest.&lt;/p&gt;

&lt;p&gt;change it slightly to behave just like CJKTokenizer (except of course, working with mixed language text, and supporting UCS-4)&lt;/p&gt;</comment>
                    <comment id="12863341" author="rcmuir" created="Mon, 3 May 2010 14:22:21 +0100"  >&lt;p&gt;Marking this fixed, as all the icu functionality has been broken into smaller issues and now resolved (and simpler due to ICU 4.4 changes)&lt;/p&gt;</comment>
                    <comment id="13013359" author="gsingers" created="Wed, 30 Mar 2011 16:50:04 +0100"  >&lt;p&gt;Bulk close for 3.1&lt;/p&gt;</comment>
                </comments>
                <issuelinks>
                        <issuelinktype id="10032">
                <name>Blocker</name>
                                                <inwardlinks description="is blocked by">
                            <issuelink>
            <issuekey id="12442591">LUCENE-2124</issuekey>
        </issuelink>
                    </inwardlinks>
                            </issuelinktype>
                        <issuelinktype id="12310010">
                <name>Incorporates</name>
                                <outwardlinks description="incorporates">
                            <issuelink>
            <issuekey id="12415233">LUCENE-1545</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12462819">LUCENE-2414</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12462759">LUCENE-2409</issuekey>
        </issuelink>
                    </outwardlinks>
                                            </issuelinktype>
                        <issuelinktype id="10030">
                <name>Reference</name>
                                <outwardlinks description="relates to">
                            <issuelink>
            <issuekey id="12462300">LUCENE-2399</issuekey>
        </issuelink>
                    </outwardlinks>
                                            </issuelinktype>
                    </issuelinks>
                <attachments>
                    <attachment id="12395908" name="ICUAnalyzer.patch" size="33315" author="rcmuir" created="Fri, 12 Dec 2008 05:58:16 +0000" />
                    <attachment id="12438922" name="LUCENE-1488.patch" size="179498" author="rcmuir" created="Tue, 16 Mar 2010 14:22:04 +0000" />
                    <attachment id="12420596" name="LUCENE-1488.patch" size="175741" author="rcmuir" created="Fri, 25 Sep 2009 20:03:56 +0100" />
                    <attachment id="12418878" name="LUCENE-1488.patch" size="150086" author="rcmuir" created="Tue, 8 Sep 2009 05:57:39 +0100" />
                    <attachment id="12409885" name="LUCENE-1488.patch" size="79653" author="rcmuir" created="Thu, 4 Jun 2009 15:20:05 +0100" />
                    <attachment id="12410856" name="LUCENE-1488.txt" size="137508" author="rcmuir" created="Tue, 16 Jun 2009 22:40:14 +0100" />
                    <attachment id="12410530" name="LUCENE-1488.txt" size="120544" author="rcmuir" created="Sat, 13 Jun 2009 04:59:47 +0100" />
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>7.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Thu, 11 Dec 2008 23:26:28 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>12263</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    <customfieldvalue key="10120"><![CDATA[Patch Available]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>26239</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                    <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                <customfieldname>Time in Status</customfieldname>
                <customfieldvalues>
                    
                </customfieldvalues>
            </customfield>
                            </customfields>
    </item>
</channel>
</rss>