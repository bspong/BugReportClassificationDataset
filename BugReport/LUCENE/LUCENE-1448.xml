<!-- 
RSS generated by JIRA (5.2.8#851-sha1:3262fdc28b4bc8b23784e13eadc26a22399f5d88) at Tue Jul 16 13:19:17 UTC 2013

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/LUCENE-1448/LUCENE-1448.xml?field=key&field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>5.2.8</version>
        <build-number>851</build-number>
        <build-date>26-02-2013</build-date>
    </build-info>

<item>
            <title>[LUCENE-1448] add getFinalOffset() to TokenStream</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-1448</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;If you add multiple Fieldable instances for the same field name to a document, and you then index those fields with TermVectors storing offsets, it&apos;s very likely the offsets for all but the first field instance will be wrong.&lt;/p&gt;

&lt;p&gt;This is because IndexWriter under the hood adds a cumulative base to the offsets of each field instance, where that base is 1 + the endOffset of the last token it saw when analyzing that field.&lt;/p&gt;

&lt;p&gt;But this logic is overly simplistic.  For example, if the WhitespaceAnalyzer is being used, and the text being analyzed ended in 3 whitespace characters, then that information is lost and then next field&apos;s offsets are then all 3 too small.  Similarly, if a StopFilter appears in the chain, and the last N tokens were stop words, then the base will be 1 + the endOffset of the last non-stopword token.&lt;/p&gt;

&lt;p&gt;To fix this, I&apos;d like to add a new getFinalOffset() to TokenStream.  I&apos;m thinking by default it returns -1, which means &quot;I don&apos;t know so you figure it out&quot;, meaning we fallback to the faulty logic we have today.&lt;/p&gt;

&lt;p&gt;This has come up several times on the user&apos;s list.&lt;/p&gt;</description>
                <environment></environment>
            <key id="12408226">LUCENE-1448</key>
            <summary>add getFinalOffset() to TokenStream</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png">Closed</status>
                    <resolution id="1">Fixed</resolution>
                                <assignee username="michaelbusch">Michael Busch</assignee>
                                <reporter username="mikemccand">Michael McCandless</reporter>
                        <labels>
                    </labels>
                <created>Tue, 11 Nov 2008 09:27:35 +0000</created>
                <updated>Thu, 2 May 2013 03:29:20 +0100</updated>
                    <resolved>Sat, 25 Jul 2009 05:13:06 +0100</resolved>
                                            <fixVersion>2.9</fixVersion>
                                <component>modules/analysis</component>
                        <due></due>
                    <votes>2</votes>
                        <watches>2</watches>
                                                    <comments>
                    <comment id="12646515" author="mikemccand" created="Tue, 11 Nov 2008 09:52:17 +0000"  >&lt;p&gt;First cut patch.  It&apos;s not ready, though.&lt;/p&gt;

&lt;p&gt;First: this patch only addresses the final offset, but shouldn&apos;t we also address the final position?  Eg if StopFilter removes the last few tokens, shouldn&apos;t we make it possible to report those skipped positonIncrements?  Note that Analyzer does provide a getPositionIncrementGap(String fieldName), which could be used as a workaround if we don&apos;t do this.  This does seem less important so I&apos;d be OK with doing nothing about positions for now, but I&apos;d like to hear other&apos;s takes.&lt;/p&gt;

&lt;p&gt;Second: I can&apos;t figure out how to ask StandardTokenizerImpl for the number of chars it pulled from the Reader.  Can someone (who understands JFlex well) help out here?&lt;/p&gt;

&lt;p&gt;Third: I haven&apos;t done all Tokenizers; eg, SinkTokenizer will need a new ctor so that it&apos;s told the final offset.  And I&apos;d like to fix all contrib tokenizers too.&lt;/p&gt;

&lt;p&gt;Fourth: I&apos;m nervous about an off-by-one error here.  Here&apos;s the diff for DocInverterPerField:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;@@ -161,7 +161,13 @@
                 &lt;span class=&quot;code-keyword&quot;&gt;break&lt;/span&gt;;
               }
             }
-            fieldState.offset = offsetEnd+1;
+
+            &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; finalOffset = stream.getFinalOffset();
+            &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (finalOffset == -1)
+              fieldState.offset = offsetEnd+1;
+            &lt;span class=&quot;code-keyword&quot;&gt;else&lt;/span&gt;
+              fieldState.offset += finalOffset;
+
           } &lt;span class=&quot;code-keyword&quot;&gt;finally&lt;/span&gt; {
             stream.close();
           }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;What makes me nervous is: our current logic is to set the base for future instances of the same field to the last endOffset, plus 1.  Why do we have that plus 1?  Logically, I think it should be as if you had simply concatenated the strings together, which would not add an extra +1?  So, I&apos;m not adding +1 in the getFinalOffset()&apos;s that I&apos;ve added, but I&apos;m hoping someone can explain why we are adding +1 currently.&lt;/p&gt;

&lt;p&gt;Note that endOffset is normally &quot;exclusive&quot; while startOffset is &quot;inclusive&quot;.  Ie if a String starts with &quot;abcd &quot; then WhitespaceTokenizer would report a startOffset of 0 and endOffset of 4 for that first token.&lt;/p&gt;</comment>
                    <comment id="12646549" author="markrmiller@gmail.com" created="Tue, 11 Nov 2008 14:14:17 +0000"  >&lt;p&gt;You need that +1 or you will have the subsequent token starting on the tail of the &apos;stopword&apos;.&lt;/p&gt;

&lt;p&gt;What I can&apos;t figure it out is how exactly these offsets are supposed to match up...abcd has offsets of s:0 e:4, which seems to imply it thinks abcd is 5 chars or the end is one greater than the end index (like with spans). In either case, it seems even if you put back the +1, the endoffsets are off somehow, because some will have an end of +1 the end index, while secondary multi-fields will have an end equal to the end index.&lt;/p&gt;

&lt;p&gt;Would be cool to have fixed as this also stymies highlighting with multi-fields.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;edit&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;I see. You need that +1 you took out and you need fields after the first to have +1 more for an end offset. Looks they are supposed to be end index +1.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;edit2&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;Nm &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; I am a bad counter. I think you only need the +1 back.&lt;/p&gt;</comment>
                    <comment id="12646563" author="markrmiller@gmail.com" created="Tue, 11 Nov 2008 14:55:20 +0000"  >&lt;blockquote&gt;&lt;p&gt;Second: I can&apos;t figure out how to ask StandardTokenizerImpl for the number of chars it pulled from the Reader. Can someone (who understands JFlex well) help out here?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Whats wrong with?&lt;/p&gt;

&lt;p&gt;&amp;lt;code&amp;gt;&lt;/p&gt;

&lt;p&gt;  public int getFinalOffset() &lt;/p&gt;
{
    return scanner.yychar() + scanner.yylength();
  }

&lt;p&gt;&amp;lt;/code&amp;gt;&lt;/p&gt;</comment>
                    <comment id="12646574" author="markrmiller@gmail.com" created="Tue, 11 Nov 2008 15:45:45 +0000"  >&lt;p&gt;Your tests had an off by error.&lt;/p&gt;

&lt;p&gt;I corrected them and added the standardanalyzer piece so that both tests pass. (i didnt correctly put the SA piece in the jflex file)&lt;/p&gt;</comment>
                    <comment id="12646646" author="mikemccand" created="Tue, 11 Nov 2008 19:43:39 +0000"  >&lt;p&gt;Attached new patch (changes described below):&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;You need that +1 or you will have the subsequent token starting on the tail of the &apos;stopword&apos;. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;So logically it&apos;s like we silently &amp;amp; forcefully insert a space between&lt;br/&gt;
the Fieldable instances?&lt;/p&gt;

&lt;p&gt;Maybe we should add Analyzer.getOffsetGap(String fieldName), which by&lt;br/&gt;
default would return 1, and we then add that into the offset for&lt;br/&gt;
subsequent field instances?&lt;/p&gt;

&lt;p&gt;But then here&apos;s another challenge: for NOT_ANALYZED fields we don&apos;t&lt;br/&gt;
add this extra +1.  We just add the string length.  Hmm.&lt;/p&gt;

&lt;p&gt;OK I added Analyzer.getOffsetGap(Fieldable), and defaulted it to&lt;br/&gt;
return 1 for analyzed fields and 0 for unanalyzed fields.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;What&apos;s wrong with public int getFinalOffset() { return scanner.yychar() + scanner.yylength(); }&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Does that handle spaces at the end of the text?  (Oh it seems like it&lt;br/&gt;
does...I added a test case...hmm).&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;i didnt correctly put the SA piece in the jflex file&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think this change (adding getFinalOffset to StandardTokenizer)&lt;br/&gt;
doesn&apos;t need a change to jflex?  (It&apos;s only if you edit&lt;br/&gt;
StandardTokenizerImpl.java).&lt;/p&gt;

&lt;p&gt;Hmm another complexity is handling a field instance that produced no&lt;br/&gt;
tokens.  Currently, we do not increment the cumulative offset by +1 in&lt;br/&gt;
such cases.  But, for position increment gap we always add this gap in&lt;br/&gt;
between fields if any field from the past have produced a token.  I&lt;br/&gt;
added a couple test cases for this.&lt;/p&gt;

&lt;p&gt;Also, I fixed a bug in how CharTokenizer was computing its final&lt;br/&gt;
offset.&lt;/p&gt;

&lt;p&gt;Still todo:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;add test cases to cover NOT_ANALYZED fields&lt;/li&gt;
	&lt;li&gt;fix contrib tokenizers to implement getFinalOffset&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="12647590" author="mikemccand" created="Fri, 14 Nov 2008 11:36:42 +0000"  >&lt;p&gt;OK I added test coverage for NOT_ANAlYZED fields, and fixed contrib TokenStreams to implement getFinalOffset.&lt;/p&gt;

&lt;p&gt;All tests, and back-compat tests, pass.&lt;/p&gt;

&lt;p&gt;I think it&apos;s ready to commit... I&apos;ll wait a day or two.&lt;/p&gt;

&lt;p&gt;(Michael: sorry, I think this will cause some conflicts with the new TokenStream API changes...).&lt;/p&gt;</comment>
                    <comment id="12648080" author="michaelbusch" created="Mon, 17 Nov 2008 04:43:47 +0000"  >&lt;blockquote&gt;
&lt;p&gt;First: this patch only addresses the final offset, but shouldn&apos;t we also address the final position? Eg if StopFilter removes the last few tokens, shouldn&apos;t we make it possible to report those skipped positonIncrements?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Hmm now that we have getPositionIncrementGap() and getOffsetGap(), I think it would make sense to also add getFinalPositionIncrement()?&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;To fix this, I&apos;d like to add a new getFinalOffset() to TokenStream.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Could we add this as Attributes using the new API? FinalOffsetAttribute and FinalPositionIncrementAttribute?&lt;/p&gt;</comment>
                    <comment id="12648122" author="mikemccand" created="Mon, 17 Nov 2008 10:20:29 +0000"  >&lt;blockquote&gt;&lt;p&gt;Hmm now that we have getPositionIncrementGap() and getOffsetGap(), I think it would make sense to also add getFinalPositionIncrement()?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;We could do that.  But how would you implement it?  EG StopFilter skips tokens, and (if enabled) already tracks the skippedPositions, so it could return that PLUS whatever its input reports as its getFinalPositionIncrement, I guess?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Could we add this as Attributes using the new API? FinalOffsetAttribute and FinalPositionIncrementAttribute?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Hmm we could do that... but it seems awkward to add new attributes that apply only to ending state of the tokenizer.&lt;/p&gt;

&lt;p&gt;I wonder if instead, w/ the new API, we could simply allow querying of certain attributes (offset, posincr) after incrementToken returns &quot;false&quot;?&lt;/p&gt;

&lt;p&gt;Why don&apos;t you commit the new TokenStream API first, and we can iterate on this issue &amp;amp; commit 2nd?&lt;/p&gt;</comment>
                    <comment id="12648290" author="michaelbusch" created="Mon, 17 Nov 2008 19:53:59 +0000"  >&lt;blockquote&gt;
&lt;p&gt;Hmm we could do that... but it seems awkward to add new attributes that apply only to ending state of the tokenizer.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yeah. Also you wouldn&apos;t want to pay overhead in TokenFilters that can buffer tokens to serialize or clone those attributes for every token.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;I wonder if instead, w/ the new API, we could simply allow querying of certain attributes (offset, posincr) after incrementToken returns &quot;false&quot;?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yeah, maybe we can make the AttributeSource more sophisticated, so that it can distinguish between per-field (instance) and per-token attributes. But as a separate patch, not as part of &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-1422&quot; title=&quot;New TokenStream API&quot;&gt;&lt;del&gt;LUCENE-1422&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Why don&apos;t you commit the new TokenStream API first, and we can iterate on this issue &amp;amp; commit 2nd?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;OK, will do. I think 1422 is ready now.&lt;/p&gt;</comment>
                    <comment id="12651999" author="mikemccand" created="Mon, 1 Dec 2008 13:41:17 +0000"  >&lt;p&gt;I&apos;m torn on how to add getFinalOffset()/getFinalPositionIncrement().&lt;/p&gt;

&lt;p&gt;One option is to add a set/getFinalOffset to OffsetAttribute.  The&lt;br/&gt;
downside here is it&apos;s another int added to that class, that gets&lt;br/&gt;
copied for caching streams yet is only used at the very end.&lt;/p&gt;

&lt;p&gt;Another option is to &quot;define&quot; the API such that when incrementToken()&lt;br/&gt;
returns false, then it has actually advanced to an &quot;end-of-stream&lt;br/&gt;
token&quot;.  OffsetAttribute.getEndOffset() should return the final&lt;br/&gt;
offset.  Since we have not released the new API, we could simply make&lt;br/&gt;
this change (and fix all instances in the core/contrib that use the&lt;br/&gt;
new API accordingly).  I think I like this option best.&lt;/p&gt;

&lt;p&gt;Yet another option is to open up &quot;per stream&quot; attrs rather than &quot;per&lt;br/&gt;
token attrs&quot;.  This seems like alot of added complexity.  Are there&lt;br/&gt;
other things, besides these two, that would be an example of a &quot;per&lt;br/&gt;
stream&quot; attr?&lt;/p&gt;</comment>
                    <comment id="12653065" author="markrmiller@gmail.com" created="Wed, 3 Dec 2008 23:54:53 +0000"  >&lt;blockquote&gt;&lt;p&gt;Another option is to &quot;define&quot; the API such that when incrementToken()&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;returns false, then it has actually advanced to an &quot;end-of-stream&lt;br/&gt;
token&quot;. OffsetAttribute.getEndOffset() should return the final&lt;br/&gt;
offset. Since we have not released the new API, we could simply make&lt;br/&gt;
this change (and fix all instances in the core/contrib that use the&lt;br/&gt;
new API accordingly). I think I like this option best.&lt;/p&gt;

&lt;p&gt;+1. I like this.&lt;/p&gt;</comment>
                    <comment id="12653079" author="mikemccand" created="Thu, 4 Dec 2008 00:15:54 +0000"  >&lt;p&gt;OK, me too.  I&apos;ll move forward with that approach.&lt;/p&gt;</comment>
                    <comment id="12653513" author="michaelbusch" created="Thu, 4 Dec 2008 22:12:37 +0000"  >&lt;blockquote&gt;
&lt;p&gt;Another option is to &quot;define&quot; the API such that when incrementToken()&lt;br/&gt;
returns false, then it has actually advanced to an &quot;end-of-stream&lt;br/&gt;
token&quot;. OffsetAttribute.getEndOffset() should return the final&lt;br/&gt;
offset. Since we have not released the new API, we could simply make&lt;br/&gt;
this change (and fix all instances in the core/contrib that use the&lt;br/&gt;
new API accordingly). I think I like this option best.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This adds some &quot;cleaning up&quot; responsibilities to all existing&lt;br/&gt;
TokenFilters out there. So far it is very straightforward to change an&lt;br/&gt;
existing TokenFilter to use the new API. You simply have to:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;add  attributes the filter needs in its constructor&lt;/li&gt;
	&lt;li&gt;change next() to incrementToken() and change return calls that&lt;br/&gt;
return null to false, others to true (or what input returns)&lt;/li&gt;
	&lt;li&gt;don&apos;t access a token but the appropriate attributes to set the data&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;But maybe there&apos;s a custom filter in the end of the chain that returns&lt;br/&gt;
more tokens even after its input returned the last one. For example a&lt;br/&gt;
SynonymExpansionFilter might return a synonym for the last word it&lt;br/&gt;
received from its input before it returns false. In this case it might&lt;br/&gt;
overwrite endOffset that another filter/stream already set to the&lt;br/&gt;
final endOffset. It needs to cache that value and set it when it&lt;br/&gt;
returns false.&lt;/p&gt;

&lt;p&gt;ALso all filters that currently use an offset need to know now to&lt;br/&gt;
clean up before returning false.&lt;/p&gt;

&lt;p&gt;I&apos;m not saying this is necessarily bad. I also find this approach&lt;br/&gt;
tempting, because it&apos;s simple. But it might be a common pitfall for&lt;br/&gt;
bugs?&lt;/p&gt;

&lt;p&gt;What I&apos;d like to work on soon is an efficient way to buffer attributes&lt;br/&gt;
(maybe add methods to attribute that write into a bytebuffer). Then&lt;br/&gt;
attributes can implement what variables need to be serialized and&lt;br/&gt;
which ones don&apos;t. In that case we could add a finalOffset to&lt;br/&gt;
OffsetAttribute that does not get serialiezd/deserialized.&lt;/p&gt;

&lt;p&gt;And possibly it might be worthwhile to have explicit states defined in&lt;br/&gt;
a TokenStream that we can enforce with three methods: start(),&lt;br/&gt;
increment(), end(). Then people would now if they have to do something&lt;br/&gt;
at the end of a stream they have to do it in end().&lt;/p&gt;</comment>
                    <comment id="12653893" author="mikemccand" created="Fri, 5 Dec 2008 19:03:01 +0000"  >&lt;blockquote&gt;
&lt;p&gt;What I&apos;d like to work on soon is an efficient way to buffer attributes&lt;br/&gt;
(maybe add methods to attribute that write into a bytebuffer). Then&lt;br/&gt;
attributes can implement what variables need to be serialized and&lt;br/&gt;
which ones don&apos;t. In that case we could add a finalOffset to&lt;br/&gt;
OffsetAttribute that does not get serialiezd/deserialized.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I like that (it&apos;d make streams like CachingTokenFilter much more&lt;br/&gt;
efficient).  It&apos;d also presumably lead to more efficiently serialized&lt;br/&gt;
token streams.&lt;/p&gt;

&lt;p&gt;But: you&apos;d still need a way in this model to serialize finalOffset, once,&lt;br/&gt;
at the end?&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;And possibly it might be worthwhile to have explicit states defined in&lt;br/&gt;
a TokenStream that we can enforce with three methods: start(),&lt;br/&gt;
increment(), end(). Then people would now if they have to do something&lt;br/&gt;
at the end of a stream they have to do it in end().&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This also seems good.  So end() would be the obvious place to set&lt;br/&gt;
the OffsetAttribute.finalOffset, PositionIncrementAttribute.positionIncrementGap, etc.&lt;/p&gt;

&lt;p&gt;OK I&apos;m gonna assign this one to you, Michael &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/wink.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                    <comment id="12654044" author="michaelbusch" created="Sat, 6 Dec 2008 09:51:15 +0000"  >&lt;blockquote&gt;
&lt;p&gt;But: you&apos;d still need a way in this model to serialize finalOffset, once,&lt;br/&gt;
at the end?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Maybe we can introduce an abstract EndOfStreamAttribute and &lt;br/&gt;
FinalOffsetAttribute and FinalPosIncrAttribute that extend EOSA.&lt;/p&gt;

&lt;p&gt;Then in a stream like CachingTokenFilter a AttributeAcceptor can&lt;br/&gt;
be used that doesn&apos;t accept attributes of type EOSA in increment().&lt;/p&gt;

&lt;p&gt;In end() it would use an AttributeAcceptor that accepts EOSA atts&lt;br/&gt;
and cache those.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;OK I&apos;m gonna assign this one to you, Michael &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/wink.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Bummer! Why did I say anything? &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/wink.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; j/k&lt;/p&gt;</comment>
                    <comment id="12660074" author="mikemccand" created="Wed, 31 Dec 2008 12:17:28 +0000"  >&lt;p&gt;See also &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-579&quot; title=&quot;TermPositionVector offsets incorrect if indexed field has multiple values and one ends with non-term chars&quot;&gt;&lt;del&gt;LUCENE-579&lt;/del&gt;&lt;/a&gt; which looks like a dup of this one.&lt;/p&gt;

&lt;p&gt;Michael what&apos;s the game plan on this issue?  I think your EOSA approach makes sense...&lt;/p&gt;</comment>
                    <comment id="12660128" author="michaelbusch" created="Wed, 31 Dec 2008 19:11:35 +0000"  >&lt;p&gt;I&apos;m currently on vacation visiting my family in Germany till the 11th. I&apos;m planning to work on this as soon as I&apos;m back to get all the TokenStream changes (also &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-1460&quot; title=&quot;Change all contrib TokenStreams/Filters to use the new TokenStream API&quot;&gt;&lt;del&gt;LUCENE-1460&lt;/del&gt;&lt;/a&gt;) ready before 2.9.&lt;/p&gt;</comment>
                    <comment id="12718176" author="mikemccand" created="Wed, 10 Jun 2009 21:09:40 +0100"  >&lt;p&gt;Michael are you going to get to this soonish?  Else let&apos;s push until after 3.0?&lt;/p&gt;</comment>
                    <comment id="12723591" author="markrmiller@gmail.com" created="Wed, 24 Jun 2009 15:59:02 +0100"  >&lt;p&gt;Will you have time for this Michael?&lt;/p&gt;

&lt;p&gt;It would be great to have this bug fixed for 2.9, but if we have to push to 3.0 its not the end of the word. Wasnt it done till that darn new Token API came along? &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/wink.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                    <comment id="12723764" author="michaelbusch" created="Wed, 24 Jun 2009 22:58:05 +0100"  >&lt;p&gt;Oh man, what did I do suggesting you as the RM?&lt;span class=&quot;error&quot;&gt;Unable to render embedded object: File (? Now there&amp;#39;s another guy chasing me) not found.&lt;/span&gt; &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/wink.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;Currently I have to sacrifice some of my already very limited sleep for everything I do on Lucene. After next week I&apos;ll have more time. When everything else for 2.9 is done, then I don&apos;t think this should block it. Otherwise, I&apos;ll try to do it for 2.9.&lt;/p&gt;</comment>
                    <comment id="12734022" author="michaelbusch" created="Wed, 22 Jul 2009 08:17:49 +0100"  >&lt;p&gt;OK I think I have this basically working with old and new API (including 1693 changes).&lt;/p&gt;

&lt;p&gt;The approach I took is fairly simple, it doesn&apos;t require adding a new Attribute. I added the following method to TokenSteam:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
  /**
   * This method is called by the consumer after the last token has been consumed, 
   * i.e. after {@link #incrementToken()} returned &amp;lt;code&amp;gt;&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;&amp;lt;/code&amp;gt; (using the &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; TokenStream API)
   * or after {@link #next(Token)} or {@link #next()} returned &amp;lt;code&amp;gt;&lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;&amp;lt;/code&amp;gt; (old TokenStream API).
   * &amp;lt;p/&amp;gt;
   * This method can be used to perform any end-of-stream operations, such as setting the &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt;
   * offset of a stream. The &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; offset of a stream might differ from the offset of the last token
   * e.g. in &lt;span class=&quot;code-keyword&quot;&gt;case&lt;/span&gt; one or more whitespaces followed after the last token, but a {@link WhitespaceTokenizer}
   * was used.
   * &amp;lt;p/&amp;gt;
   * 
   * @&lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; IOException
   */
  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; void end() &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; IOException {
    &lt;span class=&quot;code-comment&quot;&gt;// &lt;span class=&quot;code-keyword&quot;&gt;do&lt;/span&gt; nothing by &lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt;
&lt;/span&gt;  }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Then I took Mike&apos;s patch and implemented end() in all classes where his patch added getFinalOffset(). &lt;br/&gt;
E.g. in CharTokenizer the implementations looks like this:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; void end() {
    &lt;span class=&quot;code-comment&quot;&gt;// set &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; offset
&lt;/span&gt;    &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; finalOffset = input.correctOffset(offset);
    offsetAtt.setOffset(finalOffset, finalOffset);
  }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I changed DocInverterPerField to call end() after the stream is fully consumed and use what &lt;br/&gt;
offsetAttribute.endOffset() returns as final offset.&lt;/p&gt;

&lt;p&gt;I also added all new tests from Mike&apos;s latest patch. &lt;br/&gt;
All unit tests, including the new ones, pass. Also test-tag.&lt;/p&gt;

&lt;p&gt;I&apos;m not posting a patch yet, because this depends on 1693.&lt;/p&gt;

&lt;p&gt;Mike, Uwe, others: could you please review if this approach makes sense?&lt;/p&gt;</comment>
                    <comment id="12734023" author="michaelbusch" created="Wed, 22 Jul 2009 08:21:57 +0100"  >&lt;p&gt;Hmm one thing I haven&apos;t done yet is changing Tee/Sink and CachingTokenFilter.&lt;/p&gt;

&lt;p&gt;But it should be simple: CachingTokenFilter.end() should call input.end() when &lt;br/&gt;
it is called for the first time and store the captured state locally as finalState. &lt;br/&gt;
Then whenever CachingTokenFilter.end() is called again, it just restores the&lt;br/&gt;
finalState.&lt;/p&gt;

&lt;p&gt;For Tee/Sink it should work similarly: The tee just puts a finalState into the&lt;br/&gt;
sink(s) the first time end() is called. And when end() of a sink is called it &lt;br/&gt;
restores the finalState.&lt;/p&gt;

&lt;p&gt;This should work?&lt;/p&gt;</comment>
                    <comment id="12734025" author="michaelbusch" created="Wed, 22 Jul 2009 08:31:06 +0100"  >&lt;p&gt;Hmm another reason why I don&apos;t like two Tees feeding one Sink:&lt;/p&gt;

&lt;p&gt;What is the finalOffset and finalState then?&lt;/p&gt;</comment>
                    <comment id="12734063" author="thetaphi" created="Wed, 22 Jul 2009 11:23:14 +0100"  >&lt;p&gt;This is not the only problem with multiple Tees: The offsets are also completely mixed together, especially if the two tees feed into the sink at the same time (not after each other). In my opinion, the last call to end should be cached by the sink as end state (so if two tees add a end state to the sink, the second one overwrites the first one).&lt;/p&gt;</comment>
                    <comment id="12734280" author="mikemccand" created="Wed, 22 Jul 2009 20:51:28 +0100"  >&lt;p&gt;This approach (adding end()) sounds good!&lt;/p&gt;</comment>
                    <comment id="12734292" author="michaelbusch" created="Wed, 22 Jul 2009 21:17:44 +0100"  >&lt;p&gt;Cool, I will take this approach and submit a patch as soon as &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-1693&quot; title=&quot;AttributeSource/TokenStream API improvements&quot;&gt;&lt;del&gt;LUCENE-1693&lt;/del&gt;&lt;/a&gt; is committed.&lt;/p&gt;</comment>
                    <comment id="12735212" author="michaelbusch" created="Sat, 25 Jul 2009 01:06:18 +0100"  >&lt;p&gt;I changed Mike&apos;s last patch so that it uses the new end() approach that I explained above.&lt;/p&gt;

&lt;p&gt;It also applies cleanly on current trunk (now that &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-1693&quot; title=&quot;AttributeSource/TokenStream API improvements&quot;&gt;&lt;del&gt;LUCENE-1693&lt;/del&gt;&lt;/a&gt; is committed).&lt;/p&gt;

&lt;p&gt;I also made the changes to CachingTokenFilter and TeeSinkTokenFilter that I mentioned above and added two more testcases to TestIndexWriter.&lt;/p&gt;

&lt;p&gt;All tests pass.&lt;/p&gt;</comment>
                    <comment id="12735213" author="michaelbusch" created="Sat, 25 Jul 2009 01:07:15 +0100"  >&lt;p&gt;Note that my latest patch only contains fixes for the core TokenStreams.&lt;/p&gt;

&lt;p&gt;I&apos;ll open a separate issue to implement end() for the contrib TokenStreams, which we can commit after &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-1460&quot; title=&quot;Change all contrib TokenStreams/Filters to use the new TokenStream API&quot;&gt;&lt;del&gt;LUCENE-1460&lt;/del&gt;&lt;/a&gt; is resolved.&lt;/p&gt;</comment>
                    <comment id="12735240" author="michaelbusch" created="Sat, 25 Jul 2009 04:50:18 +0100"  >&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;updated to trunk&lt;/li&gt;
	&lt;li&gt;made end() final in all implementing TokenStreams&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I&apos;m planning to commit this soon.&lt;/p&gt;</comment>
                    <comment id="12735241" author="michaelbusch" created="Sat, 25 Jul 2009 05:13:06 +0100"  >&lt;p&gt;Committed revision 797715.&lt;/p&gt;

&lt;p&gt;Thanks Mike &amp;amp; Mark for the original patch!&lt;/p&gt;</comment>
                </comments>
                <issuelinks>
                        <issuelinktype id="10001">
                <name>dependent</name>
                                                <inwardlinks description="is depended upon by">
                            <issuelink>
            <issuekey id="12412752">LUCENE-1522</issuekey>
        </issuelink>
                    </inwardlinks>
                            </issuelinktype>
                    </issuelinks>
                <attachments>
                    <attachment id="12414494" name="lucene-1448.patch" size="23837" author="michaelbusch" created="Sat, 25 Jul 2009 04:50:18 +0100" />
                    <attachment id="12414484" name="lucene-1448.patch" size="23743" author="michaelbusch" created="Sat, 25 Jul 2009 01:06:18 +0100" />
                    <attachment id="12393933" name="LUCENE-1448.patch" size="22321" author="mikemccand" created="Fri, 14 Nov 2008 11:36:42 +0000" />
                    <attachment id="12393726" name="LUCENE-1448.patch" size="15106" author="mikemccand" created="Tue, 11 Nov 2008 19:43:39 +0000" />
                    <attachment id="12393710" name="LUCENE-1448.patch" size="7957" author="markrmiller@gmail.com" created="Tue, 11 Nov 2008 15:45:45 +0000" />
                    <attachment id="12393694" name="LUCENE-1448.patch" size="6990" author="mikemccand" created="Tue, 11 Nov 2008 09:52:17 +0000" />
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>6.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Tue, 11 Nov 2008 14:14:17 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>12303</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>26279</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                    <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                <customfieldname>Time in Status</customfieldname>
                <customfieldvalues>
                    
                </customfieldvalues>
            </customfield>
                            </customfields>
    </item>
</channel>
</rss>