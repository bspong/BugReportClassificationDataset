<!-- 
RSS generated by JIRA (5.2.8#851-sha1:3262fdc28b4bc8b23784e13eadc26a22399f5d88) at Tue Jul 16 13:33:59 UTC 2013

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/LUCENE-1058/LUCENE-1058.xml?field=key&field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>5.2.8</version>
        <build-number>851</build-number>
        <build-date>26-02-2013</build-date>
    </build-info>

<item>
            <title>[LUCENE-1058] New Analyzer for buffering tokens</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-1058</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;In some cases, it would be handy to have Analyzer/Tokenizer/TokenFilters that could siphon off certain tokens and store them in a buffer to be used later in the processing pipeline.&lt;/p&gt;

&lt;p&gt;For example, if you want to have two fields, one lowercased and one not, but all the other analysis is the same, then you could save off the tokens to be output for a different field.&lt;/p&gt;

&lt;p&gt;Patch to follow, but I am still not sure about a couple of things, mostly how it plays with the new reuse API.&lt;/p&gt;

&lt;p&gt;See &lt;a href=&quot;http://www.gossamer-threads.com/lists/lucene/java-dev/54397?search_string=BufferingAnalyzer;#54397&quot; class=&quot;external-link&quot;&gt;http://www.gossamer-threads.com/lists/lucene/java-dev/54397?search_string=BufferingAnalyzer;#54397&lt;/a&gt;&lt;/p&gt;</description>
                <environment></environment>
            <key id="12382787">LUCENE-1058</key>
            <summary>New Analyzer for buffering tokens</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png">Closed</status>
                    <resolution id="1">Fixed</resolution>
                                <assignee username="gsingers">Grant Ingersoll</assignee>
                                <reporter username="gsingers">Grant Ingersoll</reporter>
                        <labels>
                    </labels>
                <created>Mon, 19 Nov 2007 13:45:41 +0000</created>
                <updated>Fri, 25 Jan 2008 03:24:11 +0000</updated>
                    <resolved>Thu, 29 Nov 2007 15:18:39 +0000</resolved>
                                            <fixVersion>2.3</fixVersion>
                                <component>modules/analysis</component>
                        <due></due>
                    <votes>0</votes>
                        <watches>1</watches>
                                                    <comments>
                    <comment id="12543570" author="gsingers" created="Mon, 19 Nov 2007 13:52:55 +0000"  >&lt;p&gt;First draft at a patch, provides two different approaches:&lt;/p&gt;

&lt;p&gt;1.  CachedAnalyzer and CachedTokenizer take in a list of Tokens and output them as appropriate.  Similar to CachingTokenFilter, but assumes you already have the Tokens&lt;/p&gt;

&lt;p&gt;2. In contrib/analyzers/buffered, add CollaboratingAnalyzer and related classes for creating a Analyzer, etc. that work in the stream.&lt;/p&gt;

&lt;p&gt;Still not sure if and how this plays with the Token reuse (I think it doesn&apos;t)&lt;/p&gt;</comment>
                    <comment id="12544096" author="mikemccand" created="Tue, 20 Nov 2007 22:10:19 +0000"  >&lt;p&gt;I think the discussion in &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-1063&quot; title=&quot;Token re-use API breaks back compatibility in certain TokenStream chains&quot;&gt;&lt;del&gt;LUCENE-1063&lt;/del&gt;&lt;/a&gt; is relevant to this issue: if you store (&amp;amp; re-use) Tokens you may need to return a copy of the Token from the next() method to ensure that nay filters that alter the Token don&apos;t mess up your private copy.&lt;/p&gt;</comment>
                    <comment id="12544145" author="gsingers" created="Wed, 21 Nov 2007 02:21:15 +0000"  >&lt;p&gt;Some javadoc comments for the modifyToken method in BufferingTokenFilter should be sufficient, right?  Something to the effect that if this TokenFilter is not the last in the chain that it should make a full copy.  &lt;/p&gt;

&lt;p&gt;As for the CachedTokenizer and CachedAnalyzer, those should be implied, since the user is passing them in to begin with.&lt;/p&gt;

&lt;p&gt;The other thing of interest, is that calling Analyzer.tokenStream(String, Reader) is not needed.  In fact, this somewhat suggests having a new Fieldable property akin to tokenStreamValue(), etc. that says don&apos;t even ask the Fieldable for a value.  &lt;/p&gt;

&lt;p&gt;Let me take a crack at what that means and post a patch.  It will mean some changes to invertField() in DocumentsWriter and possibly changing it to not require that one of tokenStreamValue, readerValue() or stringValue() be defined.  Not sure if that is a good idea or not.  &lt;/p&gt;
</comment>
                    <comment id="12544147" author="gsingers" created="Wed, 21 Nov 2007 02:51:51 +0000"  >&lt;p&gt;Here&apos;s a patch that modifies the DocumentsWriter to not throw an IllegalArgumentException if no Reader is specified.  Thus, an Analyzer needs to be able to handle a null Reader (this still needs to be documented).  Basically, the semantics of it are that the Analyzer is producing Tokens from some other means.  I probably should spell this out in a new Field constructor as well, but this should suffice for now, and I will revisit it after the break.&lt;/p&gt;

&lt;p&gt; I also added in a TestCollaboratingAnalyzer.  All tests pass.&lt;/p&gt;</comment>
                    <comment id="12545677" author="gsingers" created="Tue, 27 Nov 2007 01:57:26 +0000"  >&lt;p&gt;A new version of this with the following changes/additions:&lt;/p&gt;

&lt;p&gt;DocumentsWriter no longer requires that a Field have a value (i.e. stringValue, etc.)  Added a new Field constructor that allows for the construction of a Field without a value.  This would allow for Analyzer implementations that produce their own tokens (whatever that means)&lt;/p&gt;

&lt;p&gt;Moved CollaboratingAnalyzer, et. al to the core under analysis.buffered as I thought these items should be in core given the changes to Field and DocsWriter.&lt;/p&gt;

&lt;p&gt;Note, I think this is a subtle, but important change in DocumentsWriter/Field behavior.&lt;/p&gt;</comment>
                    <comment id="12545846" author="gsingers" created="Tue, 27 Nov 2007 14:10:51 +0000"  >&lt;p&gt;Added some more documentation, plus a test showing it is bad to use the no value Field constructor w/o support from the Analyzer to produce tokens.&lt;/p&gt;

&lt;p&gt;If no objections, I will commit on Thursday or Friday of this week.&lt;/p&gt;</comment>
                    <comment id="12545932" author="gsingers" created="Tue, 27 Nov 2007 17:30:21 +0000"  >&lt;p&gt;fixed a failing test&lt;/p&gt;</comment>
                    <comment id="12545959" author="michaelbusch" created="Tue, 27 Nov 2007 18:48:56 +0000"  >&lt;p&gt;Grant,&lt;/p&gt;

&lt;p&gt;I&apos;m not sure why we need this patch.&lt;/p&gt;

&lt;p&gt;For the testcase that you&apos;re describing:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;For example, if you want to have two fields, one lowercased and one not, but all the other analysis is the same, then you could save off the tokens to be output for a different field.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;can&apos;t you simply do something like this:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;Document d = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; Document();
TokenStream t1 = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; CachingTokenFilter(&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; WhitespaceTokenizer(reader));
TokenStream t2 = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; LowerCaseFilter(t1);
d.add(&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; Field(&lt;span class=&quot;code-quote&quot;&gt;&quot;f1&quot;&lt;/span&gt;, t1));
d.add(&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; Field(&lt;span class=&quot;code-quote&quot;&gt;&quot;f2&quot;&lt;/span&gt;, t2));
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Maybe I&apos;m missing something?&lt;/p&gt;</comment>
                    <comment id="12545966" author="yseeley@gmail.com" created="Tue, 27 Nov 2007 18:58:23 +0000"  >&lt;p&gt;Maybe I&apos;m not looking at it the right way yet, but I&apos;m not sure this feels &quot;right&quot;...&lt;br/&gt;
Since Field has a tokenStreamValue(), wouldn&apos;t it be easiest to just use that?&lt;br/&gt;
If the tokens of two fields are related, one could just pre-analyze those fields and set the token streams appropriately.  Seems more flexible and keeps any convoluted cross-field logic in the application domain.&lt;/p&gt;</comment>
                    <comment id="12545995" author="gsingers" created="Tue, 27 Nov 2007 20:31:07 +0000"  >&lt;blockquote&gt;
&lt;p&gt;Maybe I&apos;m missing something?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;No, I don&apos;t think you are missing anything in that use case, it&apos;s just an example of its use.  And I am not totally sold on this approach, but mostly am &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; &lt;/p&gt;

&lt;p&gt;I had originally considered your option, but didn&apos;t feel it was satisfactory for the case where you are extracting things like proper nouns or maybe it is generating a category value.  The more general case is where not all the tokens are needed (in fact, very few are).  In those cases, you have to go back through the whole list of cached tokens in order to extract the ones you want.  In fact, thinking some more of on it, I am not sure my patch goes far enough in the sense that what if you want it to buffer in mid stream.  &lt;/p&gt;

&lt;p&gt;For example, if you had:&lt;br/&gt;
StandardTokenizer&lt;br/&gt;
Proper Noun TF&lt;br/&gt;
LowerCaseTF&lt;br/&gt;
StopTF&lt;/p&gt;

&lt;p&gt;and Proper Noun TF is solely responsible for setting aside proper nouns as it comes across them in the stream.&lt;/p&gt;

&lt;p&gt;As for the convoluted cross-field logic, I don&apos;t think it is all that convoluted.  There are only two fields and the implementing Analyzer takes care of all of it.  Only real requirement the application has is that the fields be ordered correctly.  &lt;/p&gt;

&lt;p&gt;I do agree somewhat about the pre-analysis approach, except for the case where there may be a large number of tokens in the source field, in which case, you are holding them around in memory (maxFieldLength mitigates to some extent.)  Also, it puts the onus on the app. writer to do it, when it could be pretty straight forward for Lucene to do it w/o it&apos;s usual analysis pipeline.&lt;/p&gt;

&lt;p&gt;At any rate, separate of the CollaboratingAnalyzer, I do think the CachedTokenFilter is useful, especially in supporting the pre-analysis approach.&lt;/p&gt;
</comment>
                    <comment id="12545999" author="yseeley@gmail.com" created="Tue, 27 Nov 2007 20:43:05 +0000"  >&lt;blockquote&gt;&lt;p&gt;As for the convoluted cross-field logic, I don&apos;t think it is all that convoluted.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;But it&apos;s baked into CollaboratingAnalyzer... it seems like this is better left to the user.  What if they wanted 3 fields instead of two?&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;I do agree somewhat about the pre-analysis approach, except for the case where there may be a large number of tokens in the source field, in which case, you are holding them around in memory&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Isn&apos;t this what your current code does?&lt;/p&gt;</comment>
                    <comment id="12546002" author="gsingers" created="Tue, 27 Nov 2007 21:03:45 +0000"  >&lt;blockquote&gt;
&lt;p&gt;What if they wanted 3 fields instead of two?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;True.  I&apos;ll have to think about a more generic approach.  In some sense, I think 2 is often sufficient, but you are right it isn&apos;t totally generic in the spirit of Lucene.  &lt;/p&gt;

&lt;p&gt;To some extent, I was thinking that this could help optimize Solr&apos;s copyField mechanism.  In Solr&apos;s case, I think you often have copy fields that have marginal differences in the filters that are applied.  It would be useful for Solr to be able to optimize these so that it doesn&apos;t have to go through the whole analysis chain again.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Isn&apos;t this what your current code does?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;No, in my main use case (# of buffered tokens is &amp;lt;&amp;lt; # of source tokens) the only tokens kept around is the (much) smaller subset of buffered tokens.  In the pre-analysis approach you have to keep the source field tokens and the buffered tokens.  Not to mention that you are increasing the work by having to iterate over the cached tokens in the list in Lucene.  Thus, you have the cost of the analysis in your application plus the storage of both token lists (one large, one small, likely) then in Lucene you have the cost of iterating over two lists.  In my approach, I think, you have the cost of analysis plus the cost of storage of one list of tokens (small) and the cost of iterating that list.&lt;/p&gt;</comment>
                    <comment id="12546004" author="yseeley@gmail.com" created="Tue, 27 Nov 2007 21:13:06 +0000"  >&lt;blockquote&gt;&lt;p&gt;To some extent, I was thinking that this could help optimize Solr&apos;s copyField mechanism.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Maybe... it would take quite a bit of work to automate it though I think.&lt;/p&gt;

&lt;p&gt;As far as pre-analysis costs. iteration is pretty much free in comparison to everything else.  Memory is the big factor.&lt;/p&gt;

&lt;p&gt;Things like entity extraction are normally not done by lucene analyzers AFAIK... but if one wanted a framework to do that, the problem is more generic.  Your really want to be able to add to multiple fields from multiple other fields.&lt;/p&gt;</comment>
                    <comment id="12546022" author="gsingers" created="Tue, 27 Nov 2007 22:11:53 +0000"  >&lt;p&gt;Any objection to me committing the CachedAnalyzer and CachedTokenizer pieces of this patch, as I don&apos;t think they are effected by the other parts of this and they solve the pre-analysis portion of this discussion.  In the meantime, I will think some more about the generic field case, as I do think it is useful.  I am also trying out some basic benchmarking on this.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Things like entity extraction are normally not done by lucene analyzers AFAIK&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Consider yourself in the &quot;know&quot; now, as I have done this on a few occasions, but, yes, I do agree a one to many approach is probably better if it can be done in a generic way.&lt;/p&gt;</comment>
                    <comment id="12546028" author="yseeley@gmail.com" created="Tue, 27 Nov 2007 22:27:13 +0000"  >&lt;p&gt;I dunno... it feels like we should have the right generic solution (many-to-many) before committing anything in this case, simply because this is all user-level code (the absence of this patch doesn&apos;t prohibit the user from doing anything... no package protected access rights are needed, etc).&lt;/p&gt;</comment>
                    <comment id="12546031" author="michaelbusch" created="Tue, 27 Nov 2007 22:31:00 +0000"  >&lt;p&gt;I think the ideas here make sense, e. g. to have a buffering&lt;br/&gt;
TokenFilter that doesn&apos;t buffer all tokens but enables the &lt;br/&gt;
user to control which tokens to buffer. &lt;/p&gt;

&lt;p&gt;What is still not clear to me is why we have to introduce a&lt;br/&gt;
new API for this and a new kind of analyzer? To allow creating&lt;br/&gt;
an no-value field seems strange. Can&apos;t we achieve all this&lt;br/&gt;
by using the Field(String, TokenStream) API without the&lt;br/&gt;
analyzer indirection?&lt;/p&gt;

&lt;p&gt;The javadocs should make clear that the IndexWriter processes&lt;br/&gt;
fields in the same order the user added them. So if a user &lt;br/&gt;
adds TokenStream ts1 and thereafter ts2, they can be sure &lt;br/&gt;
that ts1 is processed first. With that knowledge ts1 can&lt;br/&gt;
buffer certain tokens that ts2 uses then. Adding even more&lt;br/&gt;
fields that use the same tokens is straightforward.&lt;/p&gt;</comment>
                    <comment id="12546040" author="gsingers" created="Tue, 27 Nov 2007 22:56:37 +0000"  >&lt;p&gt;OK, I am trying not be fixated on the Analyzer.   I guess I haven&apos;t fully synthesized the new TokenStream use in DocsWriter&lt;/p&gt;

&lt;p&gt;I agree, I don&apos;t like the no-value Field, and am open to suggestions.&lt;/p&gt;

&lt;p&gt;So, I guess I am going to push back and ask, how would you solve the case of where you have two fields and the Analysis given by:&lt;br/&gt;
source field:&lt;br/&gt;
StandardTokenizer&lt;br/&gt;
Proper Noun TF&lt;br/&gt;
LowerCaseTF&lt;br/&gt;
StopTF&lt;/p&gt;

&lt;p&gt;buffered1 Field:&lt;br/&gt;
Proper Noun Cache TF  (cache of all terms found to be proper nouns by the Proper Noun TF)&lt;/p&gt;

&lt;p&gt;buffered2 Field:&lt;br/&gt;
All terms lower cased&lt;/p&gt;

&lt;p&gt;And the requirement is that you only do the Analysis phase once (i.e. for the source field) and the other two fields are from memory.&lt;/p&gt;

&lt;p&gt;I am just not seeing it yet, so I appreciate the explanation as it will better cement my understanding of the new Token Stream stuff and DocsWriter&lt;/p&gt;
</comment>
                    <comment id="12546050" author="michaelbusch" created="Tue, 27 Nov 2007 23:50:55 +0000"  >&lt;p&gt;We need to change the CachingTokenFilter a bit (untested code):&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; class CachingTokenFilter &lt;span class=&quot;code-keyword&quot;&gt;extends&lt;/span&gt; TokenFilter {
  &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt; List cache;
  &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt; Iterator iterator;
  
  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; CachingTokenFilter(TokenStream input) {
    &lt;span class=&quot;code-keyword&quot;&gt;super&lt;/span&gt;(input);
    &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.cache = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; LinkedList();
  }
  
  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; Token next() &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; IOException {
    &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (iterator != &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;) {
      &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (!iterator.hasNext()) {
        &lt;span class=&quot;code-comment&quot;&gt;// the cache is exhausted, &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;
&lt;/span&gt;        &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;;
      }   
      &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; (Token) iterator.next();
    } &lt;span class=&quot;code-keyword&quot;&gt;else&lt;/span&gt; {
      Token token = input.next();
      addTokenToCache(token);
      &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; token;
    }
  }
  
  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; void reset() &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; IOException {
    &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt;(cache != &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;) {
    	iterator = cache.iterator();
    }
  }
  
  &lt;span class=&quot;code-keyword&quot;&gt;protected&lt;/span&gt; void addTokenToCache(Token token) {
    &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (token != &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;) {
      cache.add(token);
    }
  }
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Then you can implement the ProperNounTF:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;class ProperNounTF &lt;span class=&quot;code-keyword&quot;&gt;extends&lt;/span&gt; CachingTokenFilter {
  &lt;span class=&quot;code-keyword&quot;&gt;protected&lt;/span&gt; void addTokenToCache(Token token) {
    &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (token != &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt; &amp;amp;&amp;amp; isProperNoun(token)) {
      cache.add(token);
    }
  }
  
  &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;boolean&lt;/span&gt; isProperNoun() {...}
}  
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And then you add everything to Document:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;Document d = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; Document();
TokenStream properNounTf = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; ProperNounTF(&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; StandardTokenizer(reader));
TokenStream stdTf = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; CachingTokenFilter(&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; StopTokenFilter(properNounTf));
TokenStrean lowerCaseTf = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; LowerCaseTokenFilter(stdTf);


d.add(&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; Field(&lt;span class=&quot;code-quote&quot;&gt;&quot;std&quot;&lt;/span&gt;, stdTf));
d.add(&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; Field(&lt;span class=&quot;code-quote&quot;&gt;&quot;nouns&quot;&lt;/span&gt;, properNounTf));
d.add(&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; Field(&lt;span class=&quot;code-quote&quot;&gt;&quot;lowerCase&quot;&lt;/span&gt;, lowerCaseTf));
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Again, this is untested, but I believe should work? &lt;/p&gt;</comment>
                    <comment id="12546052" author="yseeley@gmail.com" created="Wed, 28 Nov 2007 00:07:51 +0000"  >&lt;p&gt;Very similar to what I came up with I think... (all untested, etc)&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;class ListTokenizer &lt;span class=&quot;code-keyword&quot;&gt;extends&lt;/span&gt; Tokenizer {
  &lt;span class=&quot;code-keyword&quot;&gt;protected&lt;/span&gt; List&amp;lt;Token&amp;gt; lst = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; ArrayList&amp;lt;Token&amp;gt;();
  &lt;span class=&quot;code-keyword&quot;&gt;protected&lt;/span&gt; Iterator&amp;lt;Token&amp;gt; iter;

  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; ListTokenizer(List&amp;lt;Token&amp;gt; input) {
    &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.lst = input;
    &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (&lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.lst==&lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;) &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.lst = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; ArrayList&amp;lt;Token&amp;gt;();
  }

  /** only valid &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; tokens have not been consumed,
   * i.e. &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; tokenizer is not part of another tokenstream
   */
  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; List&amp;lt;Token&amp;gt; getTokens() {
    &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; lst;
  }

  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; Token next(Token result) &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; IOException {
    &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (iter==&lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;) iter = lst.iterator();
    &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; iter.next();
  }

  /** Override &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; method to cache only certain tokens, or &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; tokens based
   * on the old tokens.
   */
  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; void add(Token t) {
    &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (t==&lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;) &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt;;
    lst.add((Token)t.clone());
  }

  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; void reset() &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; IOException {
    iter = lst.iterator();
  }
}

class TeeTokenFilter &lt;span class=&quot;code-keyword&quot;&gt;extends&lt;/span&gt; TokenFilter {
  ListTokenizer sink;

  &lt;span class=&quot;code-keyword&quot;&gt;protected&lt;/span&gt; TeeTokenFilter(TokenStream input, ListTokenizer sink) {
    &lt;span class=&quot;code-keyword&quot;&gt;super&lt;/span&gt;(input);
    &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.sink = sink;
  }

  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; Token next(Token result) &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; IOException {
    Token t = input.next(result);
    sink.add(t);
    &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; t;
  }
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                    <comment id="12546058" author="yseeley@gmail.com" created="Wed, 28 Nov 2007 00:31:14 +0000"  >&lt;p&gt;I think having the &quot;tee&quot; solves the many-to-many case... you can have many fields contribute tokens to a new field.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;ListTokenizer sink1 = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; ListTokenizer(&lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;);
ListTokenizer sink2 = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; ListTokenizer(&lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;);

TokenStream source1 = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; TeeTokenFilter(&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; TeeTokenFilter(&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; WhitespaceTokenizer(reader1), sink1), sink2);
TokenStream source2 = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; TeeTokenFilter(&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; TeeTokenFilter(&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; WhitespaceTokenizer(reader2), sink1), sink2);    

&lt;span class=&quot;code-comment&quot;&gt;// now sink1 and sink2 will both get tokens from both reader1 and reader2 after whitespace tokenizer
&lt;/span&gt;&lt;span class=&quot;code-comment&quot;&gt;// now we can further wrap any of these in extra analysis, and more &lt;span class=&quot;code-quote&quot;&gt;&quot;tees&quot;&lt;/span&gt; can be inserted &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; desired.
&lt;/span&gt;
TokenStream final1 = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; LowerCaseFilter(source1);
TokenStream final2 = source2;
TokenStream final3 = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; EntityDetect(sink1);
TokenStream final4 = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; URLDetect(sink2);
    
d.add(&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; Field(&lt;span class=&quot;code-quote&quot;&gt;&quot;f1&quot;&lt;/span&gt;, final1));
d.add(&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; Field(&lt;span class=&quot;code-quote&quot;&gt;&quot;f2&quot;&lt;/span&gt;, final2));
d.add(&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; Field(&lt;span class=&quot;code-quote&quot;&gt;&quot;f3&quot;&lt;/span&gt;, final3));
d.add(&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; Field(&lt;span class=&quot;code-quote&quot;&gt;&quot;f4&quot;&lt;/span&gt;, final4));
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                    <comment id="12546062" author="michaelbusch" created="Wed, 28 Nov 2007 00:44:28 +0000"  >&lt;p&gt;I like the TeeTokenFilter! +1&lt;/p&gt;</comment>
                    <comment id="12546069" author="gsingers" created="Wed, 28 Nov 2007 01:06:26 +0000"  >&lt;p&gt;OK, looks good to me and is much simpler.  Only thing that gets complicated is the constructors, but that should be manageable.  Thanks for bearing w/ me &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;One of you want to whip up a patch w/ tests or do you want me to do it?&lt;/p&gt;</comment>
                    <comment id="12546072" author="michaelbusch" created="Wed, 28 Nov 2007 01:10:14 +0000"  >&lt;p&gt;I&apos;m quite busy currently with other stuff. Feel free to go ahead &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/wink.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                    <comment id="12546255" author="gsingers" created="Wed, 28 Nov 2007 14:32:03 +0000"  >&lt;p&gt;Will do.  Patch to follow shortly&lt;/p&gt;</comment>
                    <comment id="12546271" author="gsingers" created="Wed, 28 Nov 2007 15:16:29 +0000"  >&lt;p&gt;Whew.  I think we are there and I like it!  &lt;/p&gt;

&lt;p&gt;I renamed Yonik&apos;s suggestions to be SinkTokenizer and SourceTokenFilter to model the whole source/sink notion.  Hopefully people won&apos;t think the SourceTokenFilter is for processing code.  &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;I will commit tomorrow if there are no objections.&lt;/p&gt;</comment>
                    <comment id="12546274" author="yseeley@gmail.com" created="Wed, 28 Nov 2007 15:27:43 +0000"  >&lt;p&gt;The SinkTokenizer name could make sense, but I think TeeTokenFilter makes more sense than SourceTokenFilter (it is a tee, it splits a single token stream into two, just like the UNIX tee command).&lt;/p&gt;</comment>
                    <comment id="12546282" author="gsingers" created="Wed, 28 Nov 2007 15:57:21 +0000"  >&lt;p&gt;Tee it is.  And here I just thought you liked golf!  I guess I have never used the tee command in UNIX.&lt;/p&gt;</comment>
                    <comment id="12546759" author="gsingers" created="Thu, 29 Nov 2007 15:18:39 +0000"  >&lt;p&gt;Committed revision 599478.&lt;/p&gt;</comment>
                    <comment id="12546861" author="michaelbusch" created="Thu, 29 Nov 2007 20:02:12 +0000"  >&lt;p&gt;Sorry, this review is a bit late. Only a simple remark:&lt;/p&gt;

&lt;p&gt;In SinkTokenizer you could initalize the iterator in the constructor,&lt;br/&gt;
then you can avoid the check if (iter==null) in next()?&lt;/p&gt;
</comment>
                    <comment id="12546866" author="yseeley@gmail.com" created="Thu, 29 Nov 2007 20:15:04 +0000"  >&lt;p&gt;&amp;gt; In SinkTokenizer you could initalize the iterator in the constructor&lt;/p&gt;

&lt;p&gt;Iterators are generally fail-fast, hence it would throw an exception when you tried to use it later after adding some elements.&lt;/p&gt;</comment>
                    <comment id="12546871" author="michaelbusch" created="Thu, 29 Nov 2007 20:27:08 +0000"  >&lt;p&gt;I see. Then we should set iter=null in add() in case after reset() more tokens&lt;br/&gt;
are added to the list, right?&lt;/p&gt;</comment>
                    <comment id="12546872" author="gsingers" created="Thu, 29 Nov 2007 20:31:18 +0000"  >&lt;p&gt;In looking again, I also wonder whether the getTokens() shouldn&apos;t return an immutable list?  Or should we allow adding tokens outside of the Tee process?&lt;/p&gt;</comment>
                    <comment id="12546874" author="yseeley@gmail.com" created="Thu, 29 Nov 2007 20:37:46 +0000"  >&lt;p&gt;&amp;gt; I see. Then we should set iter=null in add() in case after reset() more tokens are added to the list, right?&lt;br/&gt;
Depends on what we consider valid usecases... reset could also just null out iter.&lt;/p&gt;

&lt;p&gt;&amp;gt; Or should we allow adding tokens outside of the Tee process?&lt;br/&gt;
Why not?  Seems more flexible, and this is an expert level API.&lt;/p&gt;
</comment>
                    <comment id="12546875" author="gsingers" created="Thu, 29 Nov 2007 20:39:05 +0000"  >&lt;p&gt;And, of course, if we are calling add() outside of the Tee process, we probably don&apos;t need to clone the token, either.&lt;/p&gt;</comment>
                    <comment id="12546891" author="gsingers" created="Thu, 29 Nov 2007 20:58:47 +0000"  >&lt;blockquote&gt;
&lt;p&gt;Why not? Seems more flexible, and this is an expert level API.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Then we should document that they must call reset before calling next(), right?  Same could go for the add() method.&lt;/p&gt;</comment>
                    <comment id="12549059" author="gsingers" created="Thu, 6 Dec 2007 14:42:11 +0000"  >&lt;p&gt;I also added, in SinkTokenizer, to override the close() method on Tokenizer so that it doesn&apos;t try to close the Reader, which throws an NullPointerEx.&lt;/p&gt;</comment>
                    <comment id="12554794" author="gsingers" created="Fri, 28 Dec 2007 14:17:55 +0000"  >&lt;p&gt;&lt;a href=&quot;http://www.gossamer-threads.com/lists/lucene/java-dev/56255&quot; class=&quot;external-link&quot;&gt;http://www.gossamer-threads.com/lists/lucene/java-dev/56255&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Changed to override next() instead of next(Token)&lt;/p&gt;</comment>
                </comments>
                <issuelinks>
                        <issuelinktype id="10030">
                <name>Reference</name>
                                <outwardlinks description="relates to">
                            <issuelink>
            <issuekey id="12375615">SOLR-330</issuekey>
        </issuelink>
                    </outwardlinks>
                                            </issuelinktype>
                    </issuelinks>
                <attachments>
                    <attachment id="12370420" name="LUCENE-1058.patch" size="11518" author="gsingers" created="Wed, 28 Nov 2007 15:57:20 +0000" />
                    <attachment id="12370417" name="LUCENE-1058.patch" size="11569" author="gsingers" created="Wed, 28 Nov 2007 15:16:29 +0000" />
                    <attachment id="12370324" name="LUCENE-1058.patch" size="29558" author="gsingers" created="Tue, 27 Nov 2007 17:30:21 +0000" />
                    <attachment id="12370306" name="LUCENE-1058.patch" size="29250" author="gsingers" created="Tue, 27 Nov 2007 14:10:51 +0000" />
                    <attachment id="12370256" name="LUCENE-1058.patch" size="28187" author="gsingers" created="Tue, 27 Nov 2007 01:57:26 +0000" />
                    <attachment id="12369942" name="LUCENE-1058.patch" size="17524" author="gsingers" created="Wed, 21 Nov 2007 02:51:50 +0000" />
                    <attachment id="12369788" name="LUCENE-1058.patch" size="13022" author="gsingers" created="Mon, 19 Nov 2007 13:52:55 +0000" />
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>7.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Tue, 20 Nov 2007 22:10:19 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>12687</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>26671</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                    <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                <customfieldname>Time in Status</customfieldname>
                <customfieldvalues>
                    
                </customfieldvalues>
            </customfield>
                            </customfields>
    </item>
</channel>
</rss>