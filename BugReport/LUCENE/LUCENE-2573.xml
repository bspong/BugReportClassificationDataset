<!-- 
RSS generated by JIRA (5.2.8#851-sha1:3262fdc28b4bc8b23784e13eadc26a22399f5d88) at Tue Jul 16 13:08:45 UTC 2013

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/LUCENE-2573/LUCENE-2573.xml?field=key&field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>5.2.8</version>
        <build-number>851</build-number>
        <build-date>26-02-2013</build-date>
    </build-info>

<item>
            <title>[LUCENE-2573] Tiered flushing of DWPTs by RAM with low/high water marks</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-2573</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Now that we have DocumentsWriterPerThreads we need to track total consumed RAM across all DWPTs.&lt;/p&gt;

&lt;p&gt;A flushing strategy idea that was discussed in &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2324&quot; title=&quot;Per thread DocumentsWriters that write their own private segments&quot;&gt;&lt;del&gt;LUCENE-2324&lt;/del&gt;&lt;/a&gt; was to use a tiered approach:  &lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Flush the first DWPT at a low water mark (e.g. at 90% of allowed RAM)&lt;/li&gt;
	&lt;li&gt;Flush all DWPTs at a high water mark (e.g. at 110%)&lt;/li&gt;
	&lt;li&gt;Use linear steps in between high and low watermark:  E.g. when 5 DWPTs are used, flush at 90%, 95%, 100%, 105% and 110%.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Should we allow the user to configure the low and high water mark values explicitly using total values (e.g. low water mark at 120MB, high water mark at 140MB)?  Or shall we keep for simplicity the single setRAMBufferSizeMB() config method and use something like 90% and 110% for the water marks?&lt;/p&gt;</description>
                <environment></environment>
            <key id="12470358">LUCENE-2573</key>
            <summary>Tiered flushing of DWPTs by RAM with low/high water marks</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png">Resolved</status>
                    <resolution id="1">Fixed</resolution>
                                <assignee username="simonw">Simon Willnauer</assignee>
                                <reporter username="michaelbusch">Michael Busch</reporter>
                        <labels>
                    </labels>
                <created>Wed, 28 Jul 2010 17:38:46 +0100</created>
                <updated>Thu, 14 Apr 2011 13:19:57 +0100</updated>
                    <resolved>Thu, 14 Apr 2011 13:19:57 +0100</resolved>
                                            <fixVersion>Realtime Branch</fixVersion>
                                <component>core/index</component>
                        <due></due>
                    <votes>1</votes>
                        <watches>3</watches>
                                                    <comments>
                    <comment id="12893248" author="mikemccand" created="Wed, 28 Jul 2010 18:01:09 +0100"  >&lt;p&gt;I think just keep it simple?  User sets RAM buffer size, and we compute fixed high/low watermarks from there?&lt;/p&gt;</comment>
                    <comment id="12893256" author="michaelbusch" created="Wed, 28 Jul 2010 18:10:01 +0100"  >&lt;p&gt;Yeah I like that better too.  Will implement that approach.&lt;/p&gt;</comment>
                    <comment id="12893265" author="jasonrutherglen" created="Wed, 28 Jul 2010 18:22:42 +0100"  >&lt;p&gt;Users probably won&apos;t customize to that level of detail, and RAM usage isn&apos;t entirely accurate in Java anyways.  Lets keep it as is (ie, setting the ram buffer size).&lt;/p&gt;</comment>
                    <comment id="12893683" author="michaelbusch" created="Thu, 29 Jul 2010 17:14:11 +0100"  >&lt;p&gt;Jason, are you still up for working on a patch for this one?&lt;/p&gt;

&lt;p&gt;We should probably get the realtime branch in a healthy state first and run some performance tests before we start working on all the fun stuff.&lt;br/&gt;
Almost there!&lt;/p&gt;</comment>
                    <comment id="12893702" author="jasonrutherglen" created="Thu, 29 Jul 2010 18:03:48 +0100"  >&lt;p&gt;Michael, I could, I was working on the terms dictionary, reading&lt;br/&gt;
postings etc, so that deletes will operate correctly. It&apos;d be&lt;br/&gt;
great to nail down the concurrency of the *BlockPools first &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2575&quot; title=&quot;Concurrent byte and int block implementations&quot;&gt;LUCENE-2575&lt;/a&gt;, as&lt;br/&gt;
so much uses them, then test performance etc, otherwise we&apos;ll&lt;br/&gt;
have a lot of code relying on something that could be changing?&lt;/p&gt;</comment>
                    <comment id="12893707" author="jasonrutherglen" created="Thu, 29 Jul 2010 18:13:51 +0100"  >&lt;p&gt;So yeah I&apos;ll work on a patch for this issue.&lt;/p&gt;</comment>
                    <comment id="12893757" author="jasonrutherglen" created="Thu, 29 Jul 2010 19:29:40 +0100"  >&lt;p&gt;Michael, DWPT.numBytesUsed isn&apos;t currently being updated?&lt;/p&gt;</comment>
                    <comment id="12893789" author="michaelbusch" created="Thu, 29 Jul 2010 20:28:56 +0100"  >&lt;blockquote&gt;&lt;p&gt;Michael, DWPT.numBytesUsed isn&apos;t currently being updated? &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;You can delete that one.  I factored all the memory allocation/tracking into DocumentsWriterRAMAllocator.  You might have to get some memory related stuff from trunk, e.g. the balanceRAM() code and adapt it.&lt;/p&gt;</comment>
                    <comment id="12893891" author="jasonrutherglen" created="Fri, 30 Jul 2010 02:17:43 +0100"  >&lt;p&gt;Is DocumentsWriterRAMAllocator.numBytesUsed accurate then?  It seems like some things are being recorded into it, however I&apos;m not immediately sure everything is...&lt;/p&gt;</comment>
                    <comment id="12893928" author="michaelbusch" created="Fri, 30 Jul 2010 05:32:42 +0100"  >&lt;p&gt;I&apos;m not 100% sure, I need to review the code to refresh my memory...&lt;/p&gt;</comment>
                    <comment id="12898675" author="michaelbusch" created="Sun, 15 Aug 2010 10:52:03 +0100"  >&lt;p&gt;Hi Jason, are you still working on the patch here?&lt;/p&gt;</comment>
                    <comment id="12898742" author="jasonrutherglen" created="Sun, 15 Aug 2010 21:06:34 +0100"  >&lt;p&gt;Michael, I&apos;ve been on holiday in Europe for the last 2 weeks.  When I&apos;m back tomorrow I&apos;ll resume work on it.&lt;/p&gt;</comment>
                    <comment id="12906423" author="jasonrutherglen" created="Mon, 6 Sep 2010 03:48:01 +0100"  >&lt;p&gt;I&apos;m not sure if after a DWPT is flushing we need to decrement what would effectively be a &quot;projected RAM usage post current DWPT flush completion&quot;.  Otherwise we could in many cases, start the flush of most/all of the DWPTs.  &lt;/p&gt;</comment>
                    <comment id="12906427" author="jasonrutherglen" created="Mon, 6 Sep 2010 04:45:43 +0100"  >&lt;p&gt;It looks like StoredFieldsWriter is reused after flushing a DWPT, however we&apos;re not resetting isClosed.&lt;/p&gt;</comment>
                    <comment id="12906431" author="jasonrutherglen" created="Mon, 6 Sep 2010 06:13:14 +0100"  >&lt;p&gt;We probably need a test that delays the flush process, otherwise flushing to RAM occurs too fast to proceed to the next tier.&lt;/p&gt;</comment>
                    <comment id="12906565" author="jasonrutherglen" created="Mon, 6 Sep 2010 20:00:57 +0100"  >&lt;p&gt;Here&apos;s a first cut...&lt;/p&gt;

&lt;p&gt;The TestDWPTFlushByRAM doesn&apos;t do much at this point.  It adds documents in 2 threads, and prints the RAM usage to stdout.  It more or less shows the tiered flushing working.  &lt;/p&gt;

&lt;p&gt;I don&apos;t think we&apos;re tracking all of the RAM usage yet, maybe just the terms?  I need to review.  &lt;/p&gt;</comment>
                    <comment id="12906566" author="jasonrutherglen" created="Mon, 6 Sep 2010 20:08:17 +0100"  >&lt;p&gt;The DWPT that happens to exceed the first tier, is flushed out.  This was easier to implement than finding the highest RAM consuming DWPT and flushing it, from a different thread.&lt;/p&gt;</comment>
                    <comment id="12906568" author="jasonrutherglen" created="Mon, 6 Sep 2010 20:21:16 +0100"  >&lt;p&gt;I did a search through the code and ByteBlockAllocator.perDocAllocator has no references, it can probably be removed, unless there was some other intention for it.&lt;/p&gt;</comment>
                    <comment id="12906573" author="jasonrutherglen" created="Mon, 6 Sep 2010 20:36:39 +0100"  >&lt;p&gt;I think we can/should track the RAM consumption directly in IntBlockPool and ByteBlockPool.  I&apos;m not sure if we&apos;re tracking those allocations right now, however if we are or are not, it&apos;d be clearer to add a getBytesUsed to these pool classes.&lt;/p&gt;</comment>
                    <comment id="12906580" author="jasonrutherglen" created="Mon, 6 Sep 2010 21:15:08 +0100"  >&lt;p&gt;The last comment is incorrect, DocumentsWriterRAMAllocator does keep track of all allocations of int[] and byte[], and the number of bytes used.  I&apos;m not sure why the test is reporting zero bytes used after documents have been successfully added.  &lt;/p&gt;</comment>
                    <comment id="12906626" author="jasonrutherglen" created="Tue, 7 Sep 2010 01:12:35 +0100"  >&lt;p&gt;In DocumentsWriterRAMAllocator, we&apos;re only recording the addition of more bytes when a new block is created, however because previous blocks may be recycled, it is the recycled blocks that are not being recorded as bytes used.  Should we record all allocated blocks as &quot;in use&quot; ie, count them as bytes used, or wait until they are &quot;in use&quot; again to be counted as consuming RAM?&lt;/p&gt;</comment>
                    <comment id="12906723" author="mikemccand" created="Tue, 7 Sep 2010 10:34:13 +0100"  >&lt;blockquote&gt;&lt;p&gt;We probably need a test that delays the flush process, otherwise flushing to RAM occurs too fast to proceed to the next tier.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;We can modify MockRAMDir to optionally &quot;take its sweet time&quot; when writing certain files?&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;I&apos;m not sure if after a DWPT is flushing we need to decrement what would effectively be a &quot;projected RAM usage post current DWPT flush completion&quot;. Otherwise we could in many cases, start the flush of most/all of the DWPTs.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;But shouldn&apos;t tiered flushing take care of this?  Ie you only decr RAM consumed when the flush of the DWPT finishes, not before?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;The DWPT that happens to exceed the first tier, is flushed out. This was easier to implement than finding the highest RAM consuming DWPT and flushing it, from a different thread.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Hmm but this won&apos;t be most efficient, in general?  Ie we could end up creating tiny segments depending on luck-of-the-thread-scheduling?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I did a search through the code and ByteBlockAllocator.perDocAllocator has no references, it can probably be removed, unless there was some other intention for it.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think this makes sense &amp;#8211; each DWPT now immediately flushes to its private doc store files, so there&apos;s no longer a need to track per-doc pending RAM?&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;In DocumentsWriterRAMAllocator, we&apos;re only recording the addition of more bytes when a new block is created, however because previous blocks may be recycled, it is the recycled blocks that are not being recorded as bytes used. Should we record all allocated blocks as &quot;in use&quot; ie, count them as bytes used, or wait until they are &quot;in use&quot; again to be counted as consuming RAM?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think we have to track both.  If a buffer is not in the pool (ie not free), then it&apos;s in use and we count that as RAM used, and that counter is used to trigger tiered flushing.  Separately we have to track net allocated, in order to trim the buffers (drop them, so GC can reclaim) when we are over the .setRAMBufferSizeMB.&lt;/p&gt;</comment>
                    <comment id="12906798" author="jasonrutherglen" created="Tue, 7 Sep 2010 14:46:51 +0100"  >&lt;blockquote&gt;&lt;p&gt;shouldn&apos;t tiered flushing take care of this&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Faulty thinking for a few minutes.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;but this won&apos;t be most efficient, in general? Ie we could end up creating tiny segments depending on luck-of-the-thread-scheduling?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;True.  Instead, we may want to simply not-flush the current DWPT if it is in fact not the highest RAM user.  When addDoc is called on the thread with the highest RAM usage, we can then flush it.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;there&apos;s no longer a need to track per-doc pending RAM&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I&apos;ll remove it from the code.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;If a buffer is not in the pool (ie not free), then it&apos;s in use and we count that as RAM used&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Ok, I&apos;ll make the change.  &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;we have to track net allocated, in order to trim the buffers (drop them, so GC can reclaim) when we are over the .setRAMBufferSizeMB&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I haven&apos;t seen this in the realtime branch.  Reclamation of extra allocated free blocks may need to be reimplemented.  &lt;/p&gt;

&lt;p&gt;I&apos;ll increment num bytes used when a block is returned for use.&lt;/p&gt;

&lt;p&gt;On this topic, do you have any thoughts yet about how to make the block pools concurrent?  I&apos;m still leaning towards a random access file (seek style) interface because this is easy to make concurrent, and hides the underlying block management mechanism, rather than directly exposes it like today, which can lend itself to problematic usage in the future.&lt;/p&gt;</comment>
                    <comment id="12906801" author="jasonrutherglen" created="Tue, 7 Sep 2010 14:49:53 +0100"  >&lt;blockquote&gt;&lt;p&gt;We can modify MockRAMDir to optionally &quot;take its sweet time&quot; when writing certain files?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes, I think we need to implement something of this nature.  We &lt;b&gt;could&lt;/b&gt; even randomly assign a different delay value per flush.  Of course how the test would instigate this from outside of DW, is somewhat of a different issue.&lt;/p&gt;</comment>
                    <comment id="12906914" author="jasonrutherglen" created="Tue, 7 Sep 2010 19:09:42 +0100"  >&lt;ul&gt;
	&lt;li&gt;perDocAllocator is removed from DocumentsWriterRAMAllocator&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;getByteBlock and getIntBlock always increments the numBytesUsed&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;The test that simply prints out debugging messages looks better.  I need to figure out unit tests.&lt;/p&gt;</comment>
                    <comment id="12906918" author="jasonrutherglen" created="Tue, 7 Sep 2010 19:17:22 +0100"  >&lt;p&gt;The last patch also only flushes a DWPT if it&apos;s the highest RAM consumer.&lt;/p&gt;</comment>
                    <comment id="12907029" author="jasonrutherglen" created="Wed, 8 Sep 2010 00:42:33 +0100"  >&lt;p&gt;There was a small bug in the choice of the max DWPT, in that all DWPTs, including ones that were scheduled to flush were being compared against the current DWPT (ie the one being examined for possible flushing).&lt;/p&gt;</comment>
                    <comment id="12914296" author="jasonrutherglen" created="Fri, 24 Sep 2010 01:44:18 +0100"  >&lt;p&gt;I was hoping something clever would come to me about how to unit test this, nothing has.  We can do the slowdown of writes to the file(s) via a Thread.sleep, however this will only emulate a real file system in RAM, what then?  I thought about testing the percentage however is it going to be exact?  We could test a percentage range of each of the segments flushed?  I guess I just need to run the all of the unit tests, however some of those will fail because deletes aren&apos;t working properly yet.  &lt;/p&gt;</comment>
                    <comment id="13003426" author="simonw" created="Mon, 7 Mar 2011 16:51:07 +0000"  >&lt;p&gt;Here is my first cut / current status on this issue. First of all I have a couple of failures related to deletes but they seem not to be related (directly) to this patch since I can reproduce them even without the patch. &lt;br/&gt;
all of the failures are related to deletes in some way so I suspect that there is another issue for that, no?&lt;/p&gt;

&lt;p&gt;This patch implements a tiered flush strategy combined with a concurrent flush approach. &lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;All decisions are based on a FlushPolicy which operates on a DocumentsWriterSession (does the ram tracking and housekeeping), once the flush policy encounters a transition to the next tier it marks the &quot;largest&quot; ram consuming thread&lt;br/&gt;
as flushPending if we transition from a lower level and all threads if we transition from the upper watermark (level). DocumentsWriterSession shifts the memory of a pending thread to a new memory &quot;level&quot; (pendingBytes) and marks the thread as pending. &lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Once FlushPolicy#findFlushes(..) returns the caller tries to check if itself needs to flush and if so it &quot;checks-out&quot; its DWPT and replaces it with a complete new instance. Releases the lock on the ThreadState and continues to flush the &quot;checked-out&quot; DWPT. After this is done or if the current DWPT doesn&apos;t need flushing the indexing thread checks if there are any other pending flushes and tries to (non-blocking) obtain their lock. It only tries to get the lock and only tries once since if the lock is taken another thread is already holding it and will see the flushPending once finished adding the document.&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;This approach tries to utilize as much conccurrency as possible while flushing the DWPT and releaseing its ThreadState with an entirely new DWPT. Yet, this might also have problems especially if IO is slow and we filling up indexing RAM too fast. To prevent us from bloating up the memory too much I introduced a notation of &quot;healtiness&quot; which operates on the net-bytes used in the DocumentsWriterSession (flushBytes + pendingBytes + activeBytes) &amp;#8211; (flushBytes - mem consumption of currently flushing DWPT, pendingBytes - mem consumption of marked as pending ThreadStates / DWPT, activeBytes mem consuption of the indexing DWPT). If net-bytes reach a certain threshold (2*maxRam currently) I stop incoming threads until the session becomes healty again.&lt;/p&gt;

&lt;p&gt;I run luceneutil with trunk vs. &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2573&quot; title=&quot;Tiered flushing of DWPTs by RAM with low/high water marks&quot;&gt;&lt;del&gt;LUCENE-2573&lt;/del&gt;&lt;/a&gt; indexing 300k wikipedia docs with 1GB MaxRamBuffer and 4 Threads. Searches on both indexes yield identical results (Phew!) &lt;br/&gt;
Indexing time in ms look promising&lt;/p&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;trunk&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;patch&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt; diff &lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;134129 ms&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;102932 ms&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;&lt;font color=&quot;green&quot;&gt;23.25%&lt;/font&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;


&lt;p&gt;This patch is still kind of rough and needs iterations so reviews and questions are very much welcome.&lt;/p&gt;

</comment>
                    <comment id="13003650" author="mikemccand" created="Mon, 7 Mar 2011 22:13:20 +0000"  >&lt;p&gt;OK I tested trunk vs RT branch + patch, indexing 10M Wikipedia docs&lt;br/&gt;
with 1 GB RAM buffer and 8 threads.&lt;/p&gt;

&lt;p&gt;Trunk took 298 sec for initial indexing, then 198 sec to wait for&lt;br/&gt;
merges to catch up, and 24 sec to commit.&lt;/p&gt;

&lt;p&gt;RT+patch took 289 sec for initial indexing, then 225 sec to wait for&lt;br/&gt;
merges, then 26 sec to commit.&lt;/p&gt;

&lt;p&gt;Not sure yet why I&apos;m not seeing real concurrency gains here... is&lt;br/&gt;
there anything printed to infoStream if the safety net (hijack app&lt;br/&gt;
threads) kicks in?&lt;/p&gt;</comment>
                    <comment id="13003879" author="mikemccand" created="Tue, 8 Mar 2011 09:50:30 +0000"  >&lt;p&gt;Woops &amp;#8211; I screwed up the above test!&lt;/p&gt;

&lt;p&gt;Corrected numbers: trunk takes 439 sec to index 10M docs, 202 sec to waitForMerges, 17 sec to commit.&lt;/p&gt;

&lt;p&gt;RT + patch: 289 sec to index 10M docs, 225 sec to waitForMerges, 26 sec to commit.&lt;/p&gt;

&lt;p&gt;This is w/ 8 threads (machine has 24 cores), writing to Intel X25M G2 ssd.&lt;/p&gt;

&lt;p&gt;Awesome speedup!!  Nice work everyone &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/wink.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;  Looking forward to making a new blog post soon!&lt;/p&gt;</comment>
                    <comment id="13003903" author="simonw" created="Tue, 8 Mar 2011 11:39:03 +0000"  >&lt;p&gt;here is an updated patch that writes more stuff to the infostream including if we are unhealthy and block threads.&lt;br/&gt;
I also fixed some issues with TestIndexWriterExceptions.&lt;/p&gt;

&lt;p&gt;Another idea we should follow IMO is to see if we can biggypack the indexing threads on commit / flushAll instead of waiting to be able to lock each DWPT and flush it sequentially. This should be fairly easy since we can simply mark them as flushPending and let incoming indexing thread do the flush in parallel. Depending on how we index and how big the DWPTs are this could give us another sizable gain. For instance if  you index and frequently commit, lets say every 10k docs (so many folks do stuff like that) but keep on indexing we should see concurrency helping us a lot since commit is not blocking all incoming indexing threads. I think we should spinoff another issues once this is ready&lt;/p&gt;</comment>
                    <comment id="13003909" author="mikemccand" created="Tue, 8 Mar 2011 11:57:37 +0000"  >&lt;p&gt;Not done reviewing the patch but here&apos;s some initial feedback:&lt;/p&gt;


&lt;p&gt;Very cool (and super advanced) that this adds a FlushPolicy!  But for&lt;br/&gt;
&quot;normal&quot; usage we go and make either DocCountFP or TieredFP, depending&lt;br/&gt;
on whether IWC is flushing by docCount, RAM or both right?  Ie one&lt;br/&gt;
normally need not make their own FlushPolicy.&lt;/p&gt;

&lt;p&gt;Maybe rename TieredFP -&amp;gt; ByRAMFP?  Also, I&apos;m not sure we need the N&lt;br/&gt;
tiers?  I suspect that may flush too heavily?  Can we instead simplify&lt;br/&gt;
it and have only the low and high water marks?  So we flush when&lt;br/&gt;
active RAM is over low water mark?  (And we stall if active + flushing&lt;br/&gt;
RAM exceeds high water mark).&lt;/p&gt;

&lt;p&gt;Can we rename isHealthy to isStalled (ie, invert it)?&lt;/p&gt;

&lt;p&gt;I&apos;m still unsure we should even include any healthy check APIs.  This&lt;br/&gt;
is an exceptional situation and I don&apos;t think we need API exposure for&lt;br/&gt;
it?  If apps really want to, they can turn on infoStream (we should&lt;br/&gt;
make sure &quot;stalling&quot; is logged, just like it is for merging) and&lt;br/&gt;
debug from there?&lt;/p&gt;

&lt;p&gt;Maybe rename pendingBytes to flushingBytes?  Or maybe&lt;br/&gt;
flushPendingBytes?  (Just to make it clear what we are pending on...).&lt;/p&gt;

&lt;p&gt;Maybe rename FP.printInfo(String msg) --&amp;gt; FP.message?  (Consistent w/&lt;br/&gt;
our other classes).&lt;/p&gt;

&lt;p&gt;I wonder if FP.findFlushes should be renamed to something like&lt;br/&gt;
FP.visit, and return void?  Ie, it&apos;s called for its side effects of&lt;br/&gt;
marking DWPTs for flushing, right?  Separately, whether or not this&lt;br/&gt;
thread will go and flush a DWPT is for IW to decide?  (Like it could&lt;br/&gt;
be this thread didn&apos;t mark any new flush required, but it should go&lt;br/&gt;
off and pull a DWPT previously marked by another thread).  So then IW&lt;br/&gt;
would have a private volatile boolean recording whether any active&lt;br/&gt;
DWPTs have flushPending.&lt;/p&gt;</comment>
                    <comment id="13004006" author="simonw" created="Tue, 8 Mar 2011 15:57:28 +0000"  >&lt;blockquote&gt;
&lt;p&gt;Maybe rename TieredFP -&amp;gt; ByRAMFP? Also, I&apos;m not sure we need the N&lt;br/&gt;
tiers? I suspect that may flush too heavily? Can we instead simplify&lt;br/&gt;
it and have only the low and high water marks? So we flush when&lt;br/&gt;
active RAM is over low water mark? (And we stall if active + flushing&lt;br/&gt;
RAM exceeds high water mark).&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;this really sounds like a different FlushPolicy to me. But is worth a try - should be easy to add with this patch. so you mean we always flush ALL DWPT once we reached the low watermark? I don&apos;t think this is a good idea. And I wonder if that is a bit too aggressive to say you put DW into stalled mode if we exceed the high watermark. Anyway we can try and see what works better right?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Can we rename isHealthy to isStalled (ie, invert it)?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;sure isStalled sounds fine&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I&apos;m still unsure we should even include any healthy check APIs.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;for now this is internal only so even if we decide to I would shift that to a different issue.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Maybe rename pendingBytes to flushingBytes? Or maybe&lt;br/&gt;
flushPendingBytes? (Just to make it clear what we are pending on...).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;yeah that is true - flushPendingBytes to make it consistent - my fault...&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;I wonder if FP.findFlushes should be renamed to something like&lt;br/&gt;
FP.visit, and return void? Ie, it&apos;s called for its side effects of&lt;br/&gt;
marking DWPTs for flushing, right? Separately, whether or not this&lt;br/&gt;
thread will go and flush a DWPT is for IW to decide? (Like it could&lt;br/&gt;
be this thread didn&apos;t mark any new flush required, but it should go&lt;br/&gt;
off and pull a DWPT previously marked by another thread). So then IW&lt;br/&gt;
would have a private volatile boolean recording whether any active&lt;br/&gt;
DWPTs have flushPending.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I was unsure about the name too so I just made it consistent with MergePolicy. Visit is ok I think.&lt;br/&gt;
the return value is maybe a relict from earlier version where I haven&apos;t had the DocWriterSession#hasPendingFlushes() yeah I think we can make that void and simply check if there are any. I think I do that today already &lt;/p&gt;</comment>
                    <comment id="13004099" author="michaelbusch" created="Tue, 8 Mar 2011 18:02:18 +0000"  >&lt;blockquote&gt;&lt;p&gt;Awesome speedup!!&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;YAY! Glad the branch is actually faster &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;Thanks for helping out with this patch, Simon. I&apos;ll try to look at the patch soon. My last week was super busy.&lt;/p&gt;</comment>
                    <comment id="13004184" author="mikemccand" created="Tue, 8 Mar 2011 20:27:15 +0000"  >&lt;blockquote&gt;&lt;p&gt;so you mean we always flush ALL DWPT once we reached the low watermark? &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;No, I mean: as soon as we pass low wm, pick biggest DWPT and flush it.&lt;br/&gt;
As soon as you mark that DWPT as flushPending, its RAM used is removed&lt;br/&gt;
from active pool and added to flushPending pool.&lt;/p&gt;

&lt;p&gt;Then, if the active pool again crosses low wm, pick the biggest and&lt;br/&gt;
mark as flush pending, etc.&lt;/p&gt;

&lt;p&gt;But if the flushing cannot keep up, and the sum of active +&lt;br/&gt;
flushPending pools crosses high wm, you hijack (stall) incoming&lt;br/&gt;
threads.&lt;/p&gt;

&lt;p&gt;I think this may make a good &quot;flush by RAM&quot; policy, but I agree we should&lt;br/&gt;
test.  I think the fully tiered approach may be overly complex...&lt;/p&gt;


&lt;blockquote&gt;&lt;p&gt;for now this is internal only so even if we decide to I would shift that to a different issue.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;OK sounds good.&lt;/p&gt;

&lt;p&gt;Also, if the app really cares about this (I suspect none will) they&lt;br/&gt;
could make a custom FlushPolicy that they could directly query to find&lt;br/&gt;
out when threads get stalled.&lt;/p&gt;

&lt;p&gt;Besides this, is it only getting flushing of deletes working&lt;br/&gt;
correctly that remains, before landing RT?&lt;/p&gt;</comment>
                    <comment id="13004194" author="simonw" created="Tue, 8 Mar 2011 20:35:13 +0000"  >&lt;blockquote&gt;
&lt;p&gt;I think this may make a good &quot;flush by RAM&quot; policy, but I agree we should&lt;br/&gt;
test. I think the fully tiered approach may be overly complex...&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;yeah possibly, I think simplifying this is easy now though...&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Also, if the app really cares about this (I suspect none will) they&lt;br/&gt;
could make a custom FlushPolicy that they could directly query to find&lt;br/&gt;
out when threads get stalled.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;yeah I think we don&apos;t need to expose that through IW.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Besides this, is it only getting flushing of deletes working&lt;br/&gt;
correctly that remains, before landing RT?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;we need to fix &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2881&quot; title=&quot;Track FieldInfo per segment instead of per-IW-session&quot;&gt;&lt;del&gt;LUCENE-2881&lt;/del&gt;&lt;/a&gt; first too.&lt;/p&gt;
</comment>
                    <comment id="13004291" author="michaelbusch" created="Tue, 8 Mar 2011 23:49:42 +0000"  >&lt;blockquote&gt;&lt;p&gt;we need to fix &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2881&quot; title=&quot;Track FieldInfo per segment instead of per-IW-session&quot;&gt;&lt;del&gt;LUCENE-2881&lt;/del&gt;&lt;/a&gt; first too.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yeah, I haven&apos;t merged with trunk since we rolled back 2881, so we should fix it first, catch up with trunk, and then make deletes work.  I might have a bit time tonight to work on 2881.&lt;/p&gt;</comment>
                    <comment id="13006451" author="simonw" created="Mon, 14 Mar 2011 15:02:36 +0000"  >&lt;p&gt;next iteration on this patch. I changed some naming issues and separated a ByRAMFlushPolicy as an abstract base class. This patch contains the original MultiTierFlushPolicy and a SingleTierFlushPolicy that only has a low and a high watermark. This policy tries to flush the biggest DWPT once LW is crossed and flushed all DWPT once HW is crossed. &lt;/p&gt;

&lt;p&gt;This patch also adds a &quot;flush if stalled&quot; control that hijacks indexing threads if the DW is stalled and there are still pending flushes. If so the incoming thread tries to check out a pending DWPT and flushes it before it adds the actual document.&lt;/p&gt;

&lt;p&gt;I didn&apos;t benchmark the more complex MultiTierFP vs. SingleTierFP yet. I hope I get to this soon.&lt;/p&gt;</comment>
                    <comment id="13006579" author="mikemccand" created="Mon, 14 Mar 2011 19:06:29 +0000"  >&lt;p&gt;I still see a healtiness (mis-spelled) in DW.&lt;/p&gt;

&lt;p&gt;I&apos;d rather not have the stalling/healthiness be baked into the API, at&lt;br/&gt;
all.  Can we put the hijack logic entirely private in the flush-by-ram&lt;br/&gt;
policies?  (Ie remove isStalled()/hijackThreadsForFlush()).&lt;/p&gt;

&lt;p&gt;Instead of&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;+    synchronized (docWriter.docWriterSession) {
+      netBytes = docWriter.docWriterSession.netBytes();
+    }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;, shouldn&apos;t we just make that method sync&apos;d?&lt;/p&gt;


&lt;p&gt;Be careful defaulting TermsHash.trackAllocations to true &amp;#8211; eg term&lt;br/&gt;
vectors wants this to be false.&lt;/p&gt;

&lt;p&gt;Can we move FlushSpecification out of FlushPolicy?  Ie, it&apos;s a private&lt;br/&gt;
impl detail of DW right?  (Not part of FlushPolicy&apos;s API).  Actually&lt;br/&gt;
why do we need it?  Can&apos;t we just return the DWPT?&lt;/p&gt;

&lt;p&gt;Why do we have a separate DocWriterSession?  Can&apos;t this be absorbed&lt;br/&gt;
into DocWriter?&lt;/p&gt;

&lt;p&gt;Instead of FlushPolicy.message, can&apos;t the policy call DW.message?&lt;/p&gt;

&lt;p&gt;On the by-RAM flush policies... when you hit the high water mark, we&lt;br/&gt;
should 1) flush all DWPTs and 2) stall any other threads.&lt;/p&gt;

&lt;p&gt;Why do we dereference the DWPTs with their ord?  EG, can&apos;t we just&lt;br/&gt;
store their &quot;state&quot; (active or flushPending) on the DWPT instead of in&lt;br/&gt;
a separate states[]?&lt;/p&gt;

&lt;p&gt;Do we really need FlushState.Aborted?  And if not... do we really need&lt;br/&gt;
FlushState (since it just becomes 2 states, ie, Active or Flushing,&lt;br/&gt;
which I think is then redundant w/ flushPending boolean?).&lt;/p&gt;

&lt;p&gt;I think the default low water should be 1X of your RAM buffer?  And&lt;br/&gt;
high water maybe 2X?  (For both flush-by-RAM policies).&lt;/p&gt;</comment>
                    <comment id="13006871" author="simonw" created="Tue, 15 Mar 2011 10:48:32 +0000"  >&lt;blockquote&gt;&lt;p&gt;I still see a healtiness (mis-spelled) in DW.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;ugh. I will fix&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;I&apos;d rather not have the stalling/healthiness be baked into the API, at&lt;br/&gt;
all. Can we put the hijack logic entirely private in the flush-by-ram&lt;br/&gt;
policies? (Ie remove isStalled()/hijackThreadsForFlush()).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I agree for the hijack part but the isStalled is something I might want to control. I mean we can still open it up eventually so rather make it private for now but keep a not on in. &lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Can we move FlushSpecification out of FlushPolicy? Ie, it&apos;s a private&lt;br/&gt;
impl detail of DW right? (Not part of FlushPolicy&apos;s API). Actually&lt;br/&gt;
why do we need it? Can&apos;t we just return the DWPT?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;it currently holds the ram usage for that DWPT when it was checked out so that I can reduce the flushBytes accordingly. We can maybe get rid of it entirely but I don&apos;t want to rely on the DWPT bytesUsed() though.&lt;br/&gt;
We can certainly move it out - this inner class is a relict though.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Why do we have a separate DocWriterSession? Can&apos;t this be absorbed&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;into DocWriter?&lt;/p&gt;

&lt;p&gt;I generally don&apos;t like cluttering DocWriter and let it grow like IW. DocWriterSession might not be the ideal name for this class but its really a ram tracker for this DW. Yet, we can move out some parts that do not directly relate to mem tracking. Maybe DocWriterBytes?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Be careful defaulting TermsHash.trackAllocations to true &#8211; eg term&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;vectors wants this to be false.&lt;/p&gt;

&lt;p&gt;I need to go through the IndexingChain and check carefully where to track memory anyway. I haven&apos;t got to that yet but good that you mention it that one could easily get lost.&lt;/p&gt;





&lt;blockquote&gt;&lt;p&gt;Instead of FlushPolicy.message, can&apos;t the policy call DW.message?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I don&apos;t want to couple that API to DW. What would be the benefit beside from saving a single method?&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;On the by-RAM flush policies... when you hit the high water mark, we&#8232;should 1) flush all DWPTs and 2) stall any other threads.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Well I am not sure if we should do that. I don&apos;t really see why we should forcefully stop the world here. Incoming threads will pick up a flush immediately and if we have enough resources to index further why should we wait until all DWPT are flushed. if we stall I fear that we could queue up threads that could help flushing while stalling would simply stop them doing anything, right? You can still control this with the healthiness though. We currently do flush all DWPT btw. once we hit the HW. &lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Why do we dereference the DWPTs with their ord? EG, can&apos;t we just&#8232;store their &quot;state&quot; (active or flushPending) on the DWPT instead of in&#8232;a separate states[]?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;That is definitely an option. I will give that a go.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Do we really need FlushState.Aborted? And if not... do we really need&#8232;FlushState (since it just becomes 2 states, ie, Active or Flushing,&#8232;which I think is then redundant w/ flushPending boolean?).&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;this needs some more refactoring I will attach another iteration&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;I think the default low water should be 1X of your RAM buffer? And&#8232;high water maybe 2X? (For both flush-by-RAM policies).&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;hmm, I think we need to revise the maxRAMBufferMB Javadoc anyway so we have all the freedom to do whatever we want. yet, I think we should try to keep the RAM consumption similar to what it would have used in a previous release. So if we say HW is 2x then suddenly some apps might run out of memory. I am not sure if we should do that or rather stick to the 90% to 110% for now.  We need to find good defaults for this anyway.&lt;/p&gt;</comment>
                    <comment id="13007003" author="mikemccand" created="Tue, 15 Mar 2011 16:42:15 +0000"  >&lt;blockquote&gt;&lt;p&gt;it currently holds the ram usage for that DWPT when it was checked out so that I can reduce the flushBytes accordingly. We can maybe get rid of it entirely but I don&apos;t want to rely on the DWPT bytesUsed() though.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Hmm, but, once a DWPT is pulled from production, its bytesUsed()&lt;br/&gt;
should not be changing anymore?  Like why can&apos;t we use it to hold its&lt;br/&gt;
bytesUsed?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I generally don&apos;t like cluttering DocWriter and let it grow like IW. DocWriterSession might not be the ideal name for this class but its really a ram tracker for this DW. Yet, we can move out some parts that do not directly relate to mem tracking. Maybe DocWriterBytes?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Well DocWriter is quite small now &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; (On RT branch).  And adding&lt;br/&gt;
another class means we have to be careful about proper sync&apos;ing (lock&lt;br/&gt;
order, to avoid deadlock)... and I think it should get smaller if we&lt;br/&gt;
can remove state[] array, FlushState enum, etc. but, OK I guess we can&lt;br/&gt;
leave it as separate for now.  How about DocumentsWriterRAMUsage?&lt;br/&gt;
RAMTracker?&lt;/p&gt;

&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;Instead of FlushPolicy.message, can&apos;t the policy call DW.message?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I don&apos;t want to couple that API to DW. What would be the benefit beside from saving a single method?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Hmm, good point.  Though, it already has a SetOnce&amp;lt;DocumentsWriter&amp;gt; &amp;#8211;&lt;br/&gt;
how come?  Can the policy call IW.message?  I just think FlushPolicy&lt;br/&gt;
ought to be very lean, ie show you exactly what you need to&lt;br/&gt;
implement...&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;bq. On the by-RAM flush policies... when you hit the high water mark, we&#8232;should 1) flush all DWPTs and 2) stall any other threads.&lt;/p&gt;

&lt;p&gt;Well I am not sure if we should do that. I don&apos;t really see why we should forcefully stop the world here. Incoming threads will pick up a flush immediately and if we have enough resources to index further why should we wait until all DWPT are flushed. if we stall I fear that we could queue up threads that could help flushing while stalling would simply stop them doing anything, right? You can still control this with the healthiness though. We currently do flush all DWPT btw. once we hit the HW.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;As long as we default the high mark to something &quot;generous&quot; (2X low&lt;br/&gt;
mark), I think this approach should work well.&lt;/p&gt;

&lt;p&gt;Ie, we &quot;begin&quot; flushing as soon as low mark is crossed on active RAM.&lt;br/&gt;
We pick the biggest DWPT and take it of rotation, and immediately&lt;br/&gt;
deduct its RAM usage from the active pool.  If, while we are still&lt;br/&gt;
flushing, active RAM again grows above the low mark, then we pull&lt;br/&gt;
another DWPT, etc.  But then if ever the total flushing + active&lt;br/&gt;
exceeds the high mark, we stall.&lt;/p&gt;

&lt;p&gt;BTW why do we track flushPending RAM vs flushing RAM?  Is that&lt;br/&gt;
distinction necessary?  (Can&apos;t we just track &quot;flushing&quot; RAM?).&lt;/p&gt;</comment>
                    <comment id="13008474" author="simonw" created="Fri, 18 Mar 2011 15:36:40 +0000"  >&lt;p&gt;next iteration containing a large number of refactorings.&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;I moved all responsibilities related to flushing including synchronization into the DocsWriterSession and renamed it to DocumentsWriterFlushControl.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;DWFC now only tracks active and flush bytes since the relict from my initial patch where pending memory was tracked is not needed anymore.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;DWFC took over all synchronization so there is not synchronized (flushControl) 
{...}
&lt;p&gt; in DocumentsWriter anymore. Seem way cleaner too though.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Healthiness now blocks once we reach 2x maxMemory and SingleTierFlushPolicy uses 0.9 maxRam as low watermark and 2x low watermark as its HW to flush all threads. The multi tier one is still unchanged and flushes in linear steps from 0.9 to 1.10 x maxRam. We should actually test if this does better worse than the single tier FP.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;FlushPolicy now has only a visit method and uses the IW.message to write to info stream.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;ThreadState now holds a boolean flag that indicates if a flush is pending which is synced and written by DWFC. States[] is gone in DWFC.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;FlushSpecification is gone and DWFC returns DWPT upon checkoutForFlush. Yet, I still track the mem for the flushing DWPT seperatly since the DWPT#bytesUsed() changes during flush and I don&apos;t want to rely on that this doesn&apos;t change. As a nice side-effect I can check if a checked out DWPT is passed to doAfterFlush and assert on that.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;next steps here are benchmarking and getting good defaults for the flush policies. I think we are close though.&lt;/p&gt;</comment>
                    <comment id="13008776" author="mikemccand" created="Sat, 19 Mar 2011 14:29:09 +0000"  >&lt;ul&gt;
	&lt;li&gt;I think once we sync up to trunk again, the FP should hold the&lt;br/&gt;
    IW&apos;s config instance, and pull settings &quot;live&quot; from it?  Ie this&lt;br/&gt;
    way we keep our live changes to flush-by-RAM.  Also, Healthiness&lt;br/&gt;
    (it won&apos;t get updates to RAM buffer now).&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Should we rename *ByRAMFP --&amp;gt; *ByRAMOrDocCountFP?  Since it &quot;ors&quot;&lt;br/&gt;
    docCount and RAM usage trigger right?  Oh, I see, not quite &amp;#8211; it&lt;br/&gt;
    requires RAM buffer be set.  I think we should relax that?  Ie a&lt;br/&gt;
    single flush policy (the default) flushes by either/or?&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Shouldn&apos;t these flush policies also trigger by&lt;br/&gt;
    maxBufferedDelCount?&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Maybe FP.init should throw IllegalStateExc not IllegalArgExc?&lt;br/&gt;
    (Because, no arg is allowed once the &quot;state&quot; of FP has already&lt;br/&gt;
    been init&apos;ed).&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Probably FP.writer should be a SetOnce?&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Hmm we still have a FlushPolicy.message?  Can&apos;t we just make IW&lt;br/&gt;
    protected and then FlushPolicy impl can call IW.message?  (And&lt;br/&gt;
    also remove FP.setInfoStream).&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Is IW.FlushControl not really used anymore?  We should remove it?&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;I still think LW should be 1.0 of your RAM buffer.  Ie, IW will&lt;br/&gt;
    start flushing once that much RAM is in use.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;I still see &quot;synchronized (docWriter.flushControl) {&quot; in&lt;br/&gt;
    IndexWriter&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;We should jdoc that IWC.setFlushPolicy takes effect only on init&lt;br/&gt;
    of IW?&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Add &quot;for testing only&quot; comment to IW.getDocsWriter?&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;I wonder whether we should convey &quot;what changed&quot; to the FP?  EG,&lt;br/&gt;
    we can 1) buffer a new del term, 2) add a new doc, 3) both&lt;br/&gt;
    (updateDocument).  It could be we have onUpdate, onAdd, onDelete?&lt;br/&gt;
    Or maybe we keep single method but rename to onChange?  Ie, it&apos;s&lt;br/&gt;
    called because &lt;b&gt;something&lt;/b&gt; about the incoming DWPT has changed.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;The flush policy shouldn&apos;t have to compute &quot;delta&quot; RAM like it&lt;br/&gt;
    does now?  Actually why can&apos;t it just call&lt;br/&gt;
    flushControl.activeBytes(), and we ensure the delta was already&lt;br/&gt;
    folded into that?  Ie we&apos;d call commmitPerThreadBytes before&lt;br/&gt;
    FP.visit.  (Then commitPerThreadBytes wouldn&apos;t ever add to&lt;br/&gt;
    flushBytes, which is sort of spooky &amp;#8211; like flushBytes should get&lt;br/&gt;
    incr&apos;d only when we pull a DWPT out for flushing).&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;I don&apos;t think we should ever markAllWritersPending, ie, that&apos;s&lt;br/&gt;
    not the right &quot;reaction&quot; when flushing is too slow (eg you&apos;re on a&lt;br/&gt;
    slow hard drive) since over time this will result in flushing lots&lt;br/&gt;
    of tiny segments unnecessarily.  A better reaction is to stall the&lt;br/&gt;
    incoming threads; this way the flusher threads catch up, and once&lt;br/&gt;
    you resume, then the small DPWTs have a chance to get big before&lt;br/&gt;
    they are flushed.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Misspelled: markLargesWriterPending -&amp;gt; markLargestWriterPending&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="13010178" author="simonw" created="Wed, 23 Mar 2011 15:28:18 +0000"  >&lt;p&gt;here is my current state on this issue. I did&apos;t add all JDocs needed (by far) and I will wait until we settled on the API for FlushPolicy.&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;I removed the complex TieredFlushPolicy entirely and added one DefaultFlushPolicy that flushes at IWC.getRAMBufferSizeMB() / sets biggest DWPT pending.&lt;/li&gt;
	&lt;li&gt;DW will stall threads if we reach 2 x maxNetRam which is retrieved from FlushPolicy so folks can lower that depending on their env.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;DWFlushControl checks if a single DWPT grows too large and sets it forcefully pending once its ram consumption is &amp;gt; 1.9 GB. That should be enough buffer to not reach the 2048MB limit. We should consider making this configurable.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;FlushPolicy has now three methods onInsert, onUpdate and onDelete while DefaultFlushPolicy only implements onInsert and onDelete, the Abstract base class just calls those on an update.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;I removed FlushControl from IW&lt;/li&gt;
	&lt;li&gt;added documentation on IWC for FlushPolicy and removed the jdocs for the RAM limit. I think we should add some lines about how RAM is now used and that users should balance the RAM with the number of threads they are using. Will do that later on though.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;For testing I added a ThrottledIndexOutput that makes flushing slow so I can test if we are stalled and / or blocked. This is passed to MockDirectoryWrapper. Its currently under util but it rather should go under store, no?&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;byte consumption is now committed before FlushPolicy is called since we don&apos;t have the multitier flush which required that to reliably proceed across tier boundaries (not required but it was easier to test really). So FP doesn&apos;t need to take care of the delta&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;FlushPolicy now also flushes on maxBufferedDeleteTerms while the buffered delete terms is not yet connected to the DW#getNumBufferedDeleteTerms() which causes some failures though. I added //nocommit &amp;amp; @Ignore to those tests.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;this patch also contains a @Ignore on TestPersistentSnapshotDeletionPolicy which I couldn&apos;t figure out why it is failing but it could be due to an old version of &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2881&quot; title=&quot;Track FieldInfo per segment instead of per-IW-session&quot;&gt;&lt;del&gt;LUCENE-2881&lt;/del&gt;&lt;/a&gt; on this branch. I will see if it still fails once we merged.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Healthiness now doesn&apos;t stall if we are not flushing on RAM consumption to ensure we don&apos;t lock in threads.&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;over all this seems much closer now. I will start writing jdocs. Flush on buffered delete terms might need some tests and I should also write a more reliable test for Healthiness... current it relies on that the ThrottledIndexOutput is slowing down indexing enough to block which might not be true all the time. It didn&apos;t fail yet. &lt;/p&gt;
</comment>
                    <comment id="13011333" author="mikemccand" created="Fri, 25 Mar 2011 18:27:59 +0000"  >&lt;p&gt;Patch is looking better!  I love how simple DefaultFP is now &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;1900 * 1024 * 1024 is actually 1.86 GB; maybe just change comment&lt;br/&gt;
    to 1900 MB?  Or we could really make the limit 1.9 GB (= 1945.6&lt;br/&gt;
    MB)&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;I think we should make the 1.9 GB changeable&lt;br/&gt;
    (setRAMPerThreadHardLimitMB?)&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;How come we lost &apos;assert !bufferedDeletesStream.any();&apos; in&lt;br/&gt;
    IndexWriter.java?&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Why default trackAllocations to true when ctor always sets it (in&lt;br/&gt;
    TermsHash.java)?&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Can we not simply not invoke the FP if flushPending is already set&lt;br/&gt;
    for the given DWPT?  (So that every FP need not check that).&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;In DefaultFP.onDelete &amp;#8211; we shouldn&apos;t just return if numDocsInRAM&lt;br/&gt;
    is 0?  Ie an app could open IW, delete 2M terms, close, and we&lt;br/&gt;
    need to flush several times due to RAM usage or del term count...&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Maybe rename DefaultFP --&amp;gt; FlushByRAMOrCounts?&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Won&apos;t the new do/while loop added to ThreadAffinityDWThreadPool&lt;br/&gt;
    run hot, if minThreadState is constantly null...?  (Separately&lt;br/&gt;
    that source needs a header)&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;I love the ThrottledIndexOutput!&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;For the InterruptedException in ThrottledIndexOutput.sleep, we&lt;br/&gt;
    should rethrow w/ oal.util.ThreadInterruptedException (best&lt;br/&gt;
    practice... probably doesn&apos;t really matter here)&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;We should fix DefaultFlushPolicy to first pull the relevant config&lt;br/&gt;
    from IWC (eg maxBufferedDocs), then check if that config is -1 or&lt;br/&gt;
    not, etc., because IWC&apos;s config can be changed at any time (live)&lt;br/&gt;
    so we may read eg 10000 at first and then -1 the second time.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Maybe, for stalling, instead of triggering by max RAM, we can take&lt;br/&gt;
this simple approach: if the number of flushing DWPTs ever exceeds one&lt;br/&gt;
plus number of active DWPTs, then we stall (and resume once it&apos;s below&lt;br/&gt;
again).&lt;/p&gt;

&lt;p&gt;This approach would then work for flush-by-docCount policies too, and&lt;br/&gt;
would still roughly equate to up to 2X RAM usage for flush-by.&lt;/p&gt;

&lt;p&gt;It&apos;s really odd that TestPersistentSDP fails now... this should be&lt;br/&gt;
unrelated to the (admittedly, major) changes we&apos;re making here...&lt;/p&gt;

&lt;p&gt;Hmm.... deletes are actually tricky, because somehow the FlushPolicy&lt;br/&gt;
needs access to the &quot;global&quot; deletes count (and also the to per-DWPT&lt;br/&gt;
deletes count).  If a given DWPT has 0 buffered docs, then indeed the&lt;br/&gt;
buffered deletes in its pool doesn&apos;t matter.  But, we do need to respect&lt;br/&gt;
the buffered deletes in the global pool...&lt;/p&gt;</comment>
                    <comment id="13012069" author="simonw" created="Mon, 28 Mar 2011 15:11:34 +0100"  >&lt;blockquote&gt;&lt;p&gt;How come we lost &apos;assert !bufferedDeletesStream.any();&apos; inIndexWriter.java?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;so this is tricky here. Since we are flushing concurrently this could false fail. The same assertion is in bufferedDeletesStream.prune(segmentInfos); which is synced. But another thread could sneak in between the prune and the any() check updating / deleting a document this could false fail. Or do I miss something here?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Maybe, for stalling, instead of triggering by max RAM, we can take&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;this simple approach: if the number of flushing DWPTs ever exceeds one&lt;br/&gt;
plus number of active DWPTs, then we stall (and resume once it&apos;s below&lt;br/&gt;
again).&lt;/p&gt;

&lt;p&gt;Awesome idea mike! I will do that!&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;We should fix DefaultFlushPolicy to first pull the relevant config&lt;br/&gt;
from IWC (eg maxBufferedDocs), then check if that config is -1 or&lt;br/&gt;
not, etc., because IWC&apos;s config can be changed at any time (live)&lt;br/&gt;
so we may read eg 10000 at first and then -1 the second time.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;What you mean is we should check if we flush by Ram, DocCount etc. and only if so we check the live values for Ram, DocCount etc.?&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Hmm.... deletes are actually tricky, because somehow the FlushPolicy&lt;br/&gt;
needs access to the &quot;global&quot; deletes count (and also the to per-DWPT&lt;br/&gt;
deletes count). If a given DWPT has 0 buffered docs, then indeed the&lt;br/&gt;
buffered deletes in its pool doesn&apos;t matter. But, we do need to respect&lt;br/&gt;
the buffered deletes in the global pool...&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I think it does not make sense to check both the global count and the DWPT count against the same value. If we have a DWPT that exceeds it we also exceed globally or could it happen that a DWPT has more deletes than the global pool? Further if we observe the global pool and we exceed the limit do we flush all as written on the IWC documentation?&lt;/p&gt;

&lt;p&gt;once we sort this out I upload a new patch with javadoc etc for flush policy. we seem to be close here man! &lt;/p&gt;</comment>
                    <comment id="13012509" author="simonw" created="Tue, 29 Mar 2011 15:46:29 +0100"  >&lt;p&gt;next iterations.&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;added JavaDoc to all classes and interfaces.&lt;/li&gt;
	&lt;li&gt;fixed the possible hot loop in DWPTThreadPool&lt;/li&gt;
	&lt;li&gt;changed stalling logic to block if more flushing DWPT are around than active ones.&lt;/li&gt;
	&lt;li&gt;check IWC setting on init and listen to live changes for those who have not been disabled.&lt;/li&gt;
	&lt;li&gt;made the hard per thread RAM limit configurable and added DEFAULT_RAM_PER_THREAD_HARD_LIMIT set to 1945MB&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;renamed  DefaultFP --&amp;gt; FlushByRAMOrCounts&lt;/li&gt;
	&lt;li&gt;added setFlushDeletes to DWFlushControl that is checked each time we add a delete and if set we flush the global deletes.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;this seems somewhat close though. its time to benchmark it again.&lt;/p&gt;



</comment>
                    <comment id="13012953" author="simonw" created="Wed, 30 Mar 2011 14:36:27 +0100"  >&lt;p&gt;Today I merged with trunk and updated my patch. I fixed the testcases that had an @Ignore on them and run tests. 1 out of 5 tests fails on TestStressIndexing and TestNRTThreads which is due to the update issues which should be addressed in &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2956&quot; title=&quot;Support updateDocument() with DWPTs&quot;&gt;&lt;del&gt;LUCENE-2956&lt;/del&gt;&lt;/a&gt;. All other tests pass including all RAM / NumDoc / BufferedDeleteTerms related tests. As a consequence I committed the current state of this issue to the RT branch too. I will keep this issue open for now.&lt;/p&gt;</comment>
                    <comment id="13013559" author="michaelbusch" created="Wed, 30 Mar 2011 18:53:53 +0100"  >&lt;p&gt;Thanks Simon!&lt;br/&gt;
I&apos;ll work on &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2956&quot; title=&quot;Support updateDocument() with DWPTs&quot;&gt;&lt;del&gt;LUCENE-2956&lt;/del&gt;&lt;/a&gt; next.&lt;/p&gt;</comment>
                    <comment id="13013974" author="simonw" created="Thu, 31 Mar 2011 14:42:55 +0100"  >&lt;p&gt;I run a couple of benchmarks with interesting results the graph below show documents per second for the RT branch with DWPT yielding a very good IO/CPU utilization and overall throughput is much better than trunks.&lt;br/&gt;
&lt;img src=&quot;http://people.apache.org/~simonw/DocumentsWriterPerThread_dps.png&quot; align=&quot;absmiddle&quot; border=&quot;0&quot; /&gt; &lt;br/&gt;
Yet, when we look at trunk the peak performance is much better on trunk than on DWPT. The reason for that I think is that we flush concurrently which takes at most one thread out of the loop, those are the little drops in docs/sec. This does not yet explain the reason for the constantly lower max indexing rate, I suspect that this is at least influenced due to the fact that flushing is very very CPU intensive. At the same time CMS might kick in way more often since we are writing more segments which are also smaller compared to trunk. Eventually, I need to run a profiler and see what is going on.&lt;br/&gt;
&lt;img src=&quot;http://people.apache.org/~simonw/Trunk_dps.png&quot; align=&quot;absmiddle&quot; border=&quot;0&quot; /&gt; &lt;/p&gt;

&lt;p&gt;Interesting is that beside the nice CPU utilization we also have an nearly perfect IO utilization. The graph below shows that we are consistently using IO to flush segments. the width of the bars show the time it took to flush a single DWPT, there is almost no overlap.&lt;br/&gt;
&lt;img src=&quot;http://people.apache.org/~simonw/DocumentsWriterPerThread_flush.png&quot; align=&quot;absmiddle&quot; border=&quot;0&quot; /&gt; &lt;/p&gt;

&lt;p&gt;Overall those are super results! Good job everybody!&lt;/p&gt;

&lt;p&gt;simon&lt;/p&gt;</comment>
                    <comment id="13014016" author="jasonrutherglen" created="Thu, 31 Mar 2011 16:26:46 +0100"  >&lt;blockquote&gt;&lt;p&gt;influenced due to the fact that flushing is very very CPU intensive&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Do you think this is due mostly to the vint decoding?  We&apos;re not interleaving postings on flush with this patch so the CPU consumption should be somewhat lower.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;At the same time CMS might kick in way more often since we are writing more segments which are also smaller compared to trunk&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This&apos;s probably the more likely case.  In general, we may be able to default to a higher overall RAM buffer size, and perhaps there won&apos;t be degradation in indexing performance like there is with trunk?  In the future with RT we could get fancy and selectively merge segments as we&apos;re flushing, if writing larger segments is important.  &lt;/p&gt;

&lt;p&gt;I&apos;d personally prefer to write out 1-2 GB segments, and limit the number of DWPTs to 2-3, mainly for servers that are concurrently indexing and searching (eg, the RT use case).  I think the current default number of thread states is a bit high.  &lt;/p&gt;</comment>
                    <comment id="13014046" author="michaelbusch" created="Thu, 31 Mar 2011 17:07:54 +0100"  >&lt;p&gt;Thanks, Simon, for running the benchmarks! Good results overall, even though it&apos;s puzzling why flushing would be CPU intensive.&lt;/p&gt;

&lt;p&gt;We should probably do some profiling to figure out where the time is spent. I can probably do that Sunday, but feel free to beat me &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                    <comment id="13014538" author="simonw" created="Fri, 1 Apr 2011 13:35:47 +0100"  >&lt;blockquote&gt;&lt;p&gt;Thanks, Simon, for running the benchmarks! Good results overall, even though it&apos;s puzzling why flushing would be CPU intensive.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;well during flush we are encoding lots of VInts thats making it cpu intensive.&lt;/p&gt;

&lt;p&gt;I actually run the benchmark through a profiler and found out what the problem was with my benchmarks.&lt;br/&gt;
When I indexed with DWPT my HDD was soo busy flushing segments concurrently that the read performance suffered and my indexing threads blocked on the line doc file where I read the records from. This explains the large amounts of spikes towards 0 doc/sec. The profiler also showed that we are waiting on ThreadState#lock() constantly with at least 3 threads. I changed the current behavior of the threadpool to not clear the thread bindings when I replace a DWPT for flushing an voila! we have comparable peak ingest rate. &lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://people.apache.org/~simonw/DocumentsWriterPerThread_dps_01.png&quot; align=&quot;absmiddle&quot; border=&quot;0&quot; /&gt; &lt;/p&gt;

&lt;p&gt;Note the difference DWPT indexes the documents in 6 min 15 seconds!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://people.apache.org/~simonw/Trunk_dps_01.png&quot; align=&quot;absmiddle&quot; border=&quot;0&quot; /&gt; &lt;/p&gt;

&lt;p&gt;Here we have 13 min 40 seconds! NICE!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://people.apache.org/~simonw/DocumentsWriterPerThread_flush_01.png&quot; align=&quot;absmiddle&quot; border=&quot;0&quot; /&gt; &lt;/p&gt;</comment>
                    <comment id="13014724" author="michaelbusch" created="Fri, 1 Apr 2011 17:32:19 +0100"  >&lt;p&gt;Awesome speedup! Finally all this work shows great results!!&lt;/p&gt;

&lt;p&gt;What&apos;s surprising is that the merge time is lower with DWPT. How can that be, considering we&apos;re doing more merges?&lt;/p&gt;</comment>
                    <comment id="13019802" author="simonw" created="Thu, 14 Apr 2011 13:19:57 +0100"  >&lt;p&gt;this is committed to branch reviews should go through &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-3023&quot; title=&quot;Land DWPT on trunk&quot;&gt;&lt;del&gt;LUCENE-3023&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;</comment>
                </comments>
                <issuelinks>
                        <issuelinktype id="10032">
                <name>Blocker</name>
                                <outwardlinks description="blocks">
                            <issuelink>
            <issuekey id="12470294">LUCENE-2571</issuekey>
        </issuelink>
                    </outwardlinks>
                                            </issuelinktype>
                        <issuelinktype id="10030">
                <name>Reference</name>
                                                <inwardlinks description="is related to">
                            <issuelink>
            <issuekey id="12459100">LUCENE-2324</issuekey>
        </issuelink>
                    </inwardlinks>
                            </issuelinktype>
                    </issuelinks>
                <attachments>
                    <attachment id="12474885" name="LUCENE-2573.patch" size="106021" author="simonw" created="Tue, 29 Mar 2011 15:46:29 +0100" />
                    <attachment id="12474404" name="LUCENE-2573.patch" size="87364" author="simonw" created="Wed, 23 Mar 2011 15:28:18 +0000" />
                    <attachment id="12474007" name="LUCENE-2573.patch" size="70424" author="simonw" created="Fri, 18 Mar 2011 15:36:40 +0000" />
                    <attachment id="12473580" name="LUCENE-2573.patch" size="72341" author="simonw" created="Mon, 14 Mar 2011 15:02:36 +0000" />
                    <attachment id="12472932" name="LUCENE-2573.patch" size="62506" author="simonw" created="Tue, 8 Mar 2011 11:39:03 +0000" />
                    <attachment id="12472834" name="LUCENE-2573.patch" size="59512" author="simonw" created="Mon, 7 Mar 2011 16:51:07 +0000" />
                    <attachment id="12454055" name="LUCENE-2573.patch" size="19720" author="jasonrutherglen" created="Wed, 8 Sep 2010 00:42:33 +0100" />
                    <attachment id="12454034" name="LUCENE-2573.patch" size="19720" author="jasonrutherglen" created="Tue, 7 Sep 2010 19:09:42 +0100" />
                    <attachment id="12453960" name="LUCENE-2573.patch" size="17067" author="jasonrutherglen" created="Mon, 6 Sep 2010 20:00:57 +0100" />
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>9.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Wed, 28 Jul 2010 17:01:09 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>11260</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>25119</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                    <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                <customfieldname>Time in Status</customfieldname>
                <customfieldvalues>
                    
                </customfieldvalues>
            </customfield>
                            </customfields>
    </item>
</channel>
</rss>