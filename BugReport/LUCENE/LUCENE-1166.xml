<!-- 
RSS generated by JIRA (5.2.8#851-sha1:3262fdc28b4bc8b23784e13eadc26a22399f5d88) at Tue Jul 16 13:30:59 UTC 2013

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/LUCENE-1166/LUCENE-1166.xml?field=key&field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>5.2.8</version>
        <build-number>851</build-number>
        <build-date>26-02-2013</build-date>
    </build-info>

<item>
            <title>[LUCENE-1166] A tokenfilter to decompose compound words</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-1166</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;A tokenfilter to decompose compound words you find in many germanic languages (like German, Swedish, ...) into single tokens.&lt;/p&gt;

&lt;p&gt;An example: Donaudampfschiff would be decomposed to Donau, dampf, schiff so that you can find the word even when you only enter &quot;Schiff&quot;.&lt;/p&gt;

&lt;p&gt;I use the hyphenation code from the Apache XML project FOP (&lt;a href=&quot;http://xmlgraphics.apache.org/fop/&quot; class=&quot;external-link&quot;&gt;http://xmlgraphics.apache.org/fop/&lt;/a&gt;) to do the first step of decomposition. Currently I use the FOP jars directly. I only use a handful of classes from the FOP project.&lt;/p&gt;

&lt;p&gt;My question now:&lt;br/&gt;
Would it be OK to copy this classes over to the Lucene project (renaming the packages of course) or should I stick with the dependency to the FOP jars? The FOP code uses the ASF V2 license as well.&lt;/p&gt;

&lt;p&gt;What do you think?&lt;/p&gt;</description>
                <environment></environment>
            <key id="12387954">LUCENE-1166</key>
            <summary>A tokenfilter to decompose compound words</summary>
                <type id="2" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/newfeature.png">New Feature</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png">Resolved</status>
                    <resolution id="1">Fixed</resolution>
                                <assignee username="gsingers">Grant Ingersoll</assignee>
                                <reporter username="tpeuss">Thomas Peuss</reporter>
                        <labels>
                    </labels>
                <created>Wed, 6 Feb 2008 11:08:24 +0000</created>
                <updated>Sun, 18 May 2008 14:21:24 +0100</updated>
                    <resolved>Fri, 16 May 2008 13:28:41 +0100</resolved>
                                                            <component>modules/analysis</component>
                        <due></due>
                    <votes>0</votes>
                        <watches>1</watches>
                                                    <comments>
                    <comment id="12566084" author="tpeuss" created="Wed, 6 Feb 2008 11:09:00 +0000"  >&lt;p&gt;A preliminary version of the token filter.&lt;/p&gt;</comment>
                    <comment id="12566085" author="tpeuss" created="Wed, 6 Feb 2008 11:10:44 +0000"  >&lt;p&gt;A hyphenation grammar. You can download them from: &lt;a href=&quot;http://downloads.sourceforge.net/offo/offo-hyphenation.zip?modtime=1168687306&amp;amp;big_mirror=0&quot; class=&quot;external-link&quot;&gt;http://downloads.sourceforge.net/offo/offo-hyphenation.zip?modtime=1168687306&amp;amp;big_mirror=0&lt;/a&gt;&lt;/p&gt;</comment>
                    <comment id="12566086" author="tpeuss" created="Wed, 6 Feb 2008 11:11:12 +0000"  >&lt;p&gt;The DTD describing the hyphenation grammar XML files.&lt;/p&gt;</comment>
                    <comment id="12566188" author="steve_rowe" created="Wed, 6 Feb 2008 16:39:21 +0000"  >&lt;p&gt;Hi Thomas,&lt;/p&gt;

&lt;p&gt;Looking at &lt;a href=&quot;http://offo.sourceforge.net/hyphenation/licenses.html&quot; class=&quot;external-link&quot;&gt;http://offo.sourceforge.net/hyphenation/licenses.html&lt;/a&gt;, which seems to be the same information as in the off-hyphenation.zip file you attached to this issue, the license issue may be a problem - the hyphenation data is covered by different licenses on a per-language basis.  For example, there are two German data files, and both are licensed under a LaTeX license, as is the Danish file, and these two languages are the most likely targets for your TokenFilter.  IANAL, but unless Apache licenses can be secured for this data, I don&apos;t think the files can be incorporated directly into an Apache project.&lt;/p&gt;

&lt;p&gt;Also, I don&apos;t see Swedish among the hyphenation data licenses - is it covered in some other way?&lt;/p&gt;</comment>
                    <comment id="12566220" author="tpeuss" created="Wed, 6 Feb 2008 17:33:46 +0000"  >&lt;blockquote&gt;&lt;p&gt;Looking at &lt;a href=&quot;http://offo.sourceforge.net/hyphenation/licenses.html&quot; class=&quot;external-link&quot;&gt;http://offo.sourceforge.net/hyphenation/licenses.html&lt;/a&gt;, which seems to be the same information as in the off-hyphenation.zip file you attached to this issue, the license issue may be a problem - the hyphenation data is covered by different licenses on a per-language basis. For example, there are two German data files, and both are licensed under a LaTeX license, as is the Danish file, and these two languages are the most likely targets for your TokenFilter. IANAL, but unless Apache licenses can be secured for this data, I don&apos;t think the files can be incorporated directly into an Apache project.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This is true. And that&apos;s why I uploaded the two files without the ASF license grant. The FOP project does not have the files in the code base as well because of the licensing problem.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Also, I don&apos;t see Swedish among the hyphenation data licenses - is it covered in some other way?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;OFFO has no Swedish grammar file. We can generate a Swedish grammar file out of the LaTeX grammar files. I have a look into this tonight.&lt;/p&gt;

&lt;p&gt;All other hyphenation implementations I have found so far use them either directly or in an converted variant like the FOP code. What we can do of course is to ask the authors of the LaTeX files if they want to license their work under the ASF license as well. It is worth a try. But I suppose that many email addresses in the LaTeX files are not used anymore. I try to contact the authors of the German grammar files tomorrow.&lt;/p&gt;

&lt;p&gt;BTW: an example for those that don&apos;t want to try the patch:&lt;br/&gt;
&lt;ins&gt;Input token stream:&lt;/ins&gt;&lt;br/&gt;
Rindfleisch&#252;berwachungsgesetz Drahtschere abba&lt;/p&gt;

&lt;p&gt;&lt;ins&gt;Output token stream:&lt;/ins&gt;&lt;br/&gt;
(Rindfleisch&#252;berwachungsgesetz,0,29)&lt;br/&gt;
(Rind,0,4,posIncr=0)&lt;br/&gt;
(fleisch,4,11,posIncr=0)&lt;br/&gt;
(&#252;berwachung,11,22,posIncr=0)&lt;br/&gt;
(gesetz,23,29,posIncr=0)&lt;br/&gt;
(Drahtschere,30,41)&lt;br/&gt;
(Draht,30,35,posIncr=0)&lt;br/&gt;
(schere,35,41,posIncr=0)&lt;br/&gt;
(abba,42,46)&lt;/p&gt;</comment>
                    <comment id="12566568" author="tpeuss" created="Thu, 7 Feb 2008 12:22:51 +0000"  >&lt;p&gt;A Swedish hyphenation grammar is available at &lt;a href=&quot;http://www.peuss.de/node/64&quot; class=&quot;external-link&quot;&gt;http://www.peuss.de/node/64&lt;/a&gt;&lt;/p&gt;</comment>
                    <comment id="12568052" author="tpeuss" created="Tue, 12 Feb 2008 11:09:19 +0000"  >&lt;p&gt;Changes:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;added unittest&lt;/li&gt;
	&lt;li&gt;minor tweaks for getting the encoding of the XML files right&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="12568985" author="tpeuss" created="Thu, 14 Feb 2008 16:22:12 +0000"  >&lt;p&gt;Updated version:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;new dumb decomposition filter
	&lt;ul&gt;
		&lt;li&gt;uses a brute-force approach by generating substrings and checking them against the dictionary&lt;/li&gt;
		&lt;li&gt;seems to work better for languages that have no patterns file with a lot of special cases&lt;/li&gt;
		&lt;li&gt;Is roughly 3 times slower than the decomposition filter using hyphenation patterns&lt;/li&gt;
		&lt;li&gt;No licensing problems because of the hyphenation pattern files&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;Refactoring to have all methods used by both decomposition filters in one place&lt;/li&gt;
	&lt;li&gt;Minor performance improvements&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="12569641" author="otis" created="Sun, 17 Feb 2008 05:48:12 +0000"  >&lt;p&gt;I haven&apos;t looked at the patch.&lt;br/&gt;
But I&apos;m wondering if a similar approach could be used for, say, word segmentation in Chinese?&lt;br/&gt;
That is, iterate through a string of Chinese characters, buffering them and looking up the buffered string in a Chinese dictionary.  Once there is a dictionary match, and the addition of the following character results in a string that has no entry in the dictionary, that previous buffered string can be considered a word/token.&lt;/p&gt;

&lt;p&gt;I&apos;m not sure if your patch does something like this, but if it does, I am wondering if it is general enough that what you did can be used (as the basis of) word segmentation for Chinese, and thus for a Chinese Analyzer that&apos;s not just a dump n-gram Analyzer (which is what we have today).&lt;/p&gt;</comment>
                    <comment id="12569708" author="tpeuss" created="Sun, 17 Feb 2008 15:24:40 +0000"  >&lt;blockquote&gt;&lt;p&gt;But I&apos;m wondering if a similar approach could be used for, say, word segmentation in Chinese? That is, iterate through a string of Chinese characters, buffering them and looking up the buffered string in a Chinese dictionary. Once there is a dictionary match, and the addition of the following character results in a string that has no entry in the dictionary, that previous buffered string can be considered a word/token. I&apos;m not sure if your patch does something like this, but if it does, I am wondering if it is general enough that what you did can be used (as the basis of) word segmentation for Chinese, and thus for a Chinese Analyzer that&apos;s not just a dump n-gram Analyzer (which is what we have today).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Currently the code adds a token to the stream when an n-gram from the current token in the token stream matches a word in the dictionary (I am only speaking about the DumbCompoundWordTokenFilter because I doubt that there exist hyphenation patterns for Chinese languages). I don&apos;t know much about the structure of Chinese characters to answer this questions in detail. You can have a look at the test-case in the patch to see how the filters work.&lt;/p&gt;

</comment>
                    <comment id="12573508" author="otis" created="Thu, 28 Feb 2008 23:11:14 +0000"  >&lt;p&gt;Thomas, I think that might work for Chinese - going through the &quot;string&quot; of Chinese characters, one at a time, and looking up a dictionary after each additional character.  One you find a dictionary match, you look at one more character.  If that matches a dictionary entry, keep doing that until you keep matching dictionary entries (in order to grab the longest dictionary-matching string of characters).  If the next character does not match, then the previous/last character was the end of the dictionary entry.&lt;br/&gt;
That would work, no?&lt;/p&gt;

&lt;p&gt;As for the license info, I think you could take the approach where the required libraries are not included in the contribution in the ASF repo, but are downloaded on the fly, at build time, much like some other contributions.  Could you do that?&lt;/p&gt;</comment>
                    <comment id="12574418" author="tpeuss" created="Mon, 3 Mar 2008 10:33:40 +0000"  >&lt;blockquote&gt;&lt;p&gt;Thomas, I think that might work for Chinese - going through the &quot;string&quot; of Chinese characters, one at a time, and looking up a dictionary after each additional character. One you find a dictionary match, you look at one more character. If that matches a dictionary entry, keep doing that until you keep matching dictionary entries (in order to grab the longest dictionary-matching string of characters). If the next character does not match, then the previous/last character was the end of the dictionary entry. That would work, no?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I have started to look into this. I will add the constructor parameter &quot;onlyLongestMatch&quot; (default is false).&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;As for the license info, I think you could take the approach where the required libraries are not included in the contribution in the ASF repo, but are downloaded on the fly, at build time, much like some other contributions. Could you do that?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I pull the grammar files for the tests already. But I don&apos;t know if it makes sense to pull them on build time because the end-user can easily download them. I need the XML versions now - so the jar-file from Sourceforge does not help anymore (I have included the needed classes from the FOP project - they use the ASF license as well).&lt;/p&gt;</comment>
                    <comment id="12574604" author="tpeuss" created="Mon, 3 Mar 2008 16:35:35 +0000"  >&lt;p&gt;Updated patch according to Otis suggestions for longest match.&lt;/p&gt;

&lt;p&gt;Next step: move to contrib&lt;/p&gt;</comment>
                    <comment id="12581898" author="tpeuss" created="Tue, 25 Mar 2008 12:43:39 +0000"  >&lt;p&gt;Moved the compound word tokenfilter stuff to contrib.&lt;/p&gt;</comment>
                    <comment id="12581900" author="tpeuss" created="Tue, 25 Mar 2008 12:49:16 +0000"  >&lt;p&gt;Moved compound word token filter to contrib.&lt;/p&gt;</comment>
                    <comment id="12581903" author="tpeuss" created="Tue, 25 Mar 2008 12:56:27 +0000"  >&lt;p&gt;Dropped Java5 dependencies.&lt;/p&gt;</comment>
                    <comment id="12583303" author="tpeuss" created="Sat, 29 Mar 2008 11:04:16 +0000"  >&lt;p&gt;Fixed a compilation bug in the testcase.&lt;/p&gt;</comment>
                    <comment id="12591895" author="gsingers" created="Thu, 24 Apr 2008 02:14:00 +0100"  >&lt;blockquote&gt;
&lt;p&gt;I pull the grammar files for the tests already. But I don&apos;t know if it makes sense to pull them on build time because the end-user can easily download them. I need the XML versions now - so the jar-file from Sourceforge does not help anymore (I have included the needed classes from the FOP project - they use the ASF license as well).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think they have to download automatically, otherwise the automated tests, etc. will not run.  I applied the patch and ran &quot;ant test&quot; and it fails b/c I didn&apos;t download the files.&lt;/p&gt;

&lt;p&gt;Also, much of the code has author tags that are not you, I am assuming you got it from FOP per your comments above, but can you explicitly mark all the files as to there origin?&lt;/p&gt;</comment>
                    <comment id="12591930" author="tpeuss" created="Thu, 24 Apr 2008 07:13:16 +0100"  >&lt;p&gt;All files in the package &lt;em&gt;org.apache.lucene.analysis.compound.hyphenation&lt;/em&gt; are from the FOP project (as well ASF licensed). Should I add a comment to them to state from where they are? All other files are from me. I have to check why it fails when you run &quot;ant test&quot; by downloading a fresh copy of Lucene-trunk.&lt;/p&gt;</comment>
                    <comment id="12591936" author="tpeuss" created="Thu, 24 Apr 2008 07:51:40 +0100"  >&lt;p&gt;The error is&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;    [junit] Testsuite: org.apache.lucene.analysis.compound.TestCompoundWordTokenFilter
    [junit] Tests run: 4, Failures: 0, Errors: 2, Time elapsed: 2,139 sec
    [junit]
    [junit] Testcase: testHyphenationCompoundWordsDE(org.apache.lucene.analysis.compound.TestCompoundWordTokenFilter):  Caused an ERROR
    [junit] File not found: /home/thomas/projects/lucene-trunk-compound/hyphenation.dtd (No such file or directory)
    [junit] org.apache.lucene.analysis.compound.hyphenation.HyphenationException: File not found: /home/thomas/projects/lucene-trunk-compound/hyphenation.dtd (No such file or directory)
    [junit]     at org.apache.lucene.analysis.compound.hyphenation.PatternParser.parse(PatternParser.java:123)
    [junit]     at org.apache.lucene.analysis.compound.hyphenation.HyphenationTree.loadPatterns(HyphenationTree.java:138)
    [junit]     at org.apache.lucene.analysis.compound.HyphenationCompoundWordTokenFilter.getHyphenationTree(HyphenationCompoundWordTokenFilter.java:142)
    [junit]     at org.apache.lucene.analysis.compound.TestCompoundWordTokenFilter.testHyphenationCompoundWordsDE(TestCompoundWordTokenFilter.java:70)
    [junit]
    [junit]
    [junit] Testcase: testHyphenationCompoundWordsDELongestMatch(org.apache.lucene.analysis.compound.TestCompoundWordTokenFilter):      Caused an ERROR
    [junit] File not found: /home/thomas/projects/lucene-trunk-compound/hyphenation.dtd (No such file or directory)
    [junit] org.apache.lucene.analysis.compound.hyphenation.HyphenationException: File not found: /home/thomas/projects/lucene-trunk-compound/hyphenation.dtd (No such file or directory)
    [junit]     at org.apache.lucene.analysis.compound.hyphenation.PatternParser.parse(PatternParser.java:123)
    [junit]     at org.apache.lucene.analysis.compound.hyphenation.HyphenationTree.loadPatterns(HyphenationTree.java:138)
    [junit]     at org.apache.lucene.analysis.compound.HyphenationCompoundWordTokenFilter.getHyphenationTree(HyphenationCompoundWordTokenFilter.java:142)
    [junit]     at org.apache.lucene.analysis.compound.TestCompoundWordTokenFilter.testHyphenationCompoundWordsDELongestMatch(TestCompoundWordTokenFilter.java:96)
    [junit]
    [junit]
    [junit] Test org.apache.lucene.analysis.compound.TestCompoundWordTokenFilter FAILED
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;So it does not find the hyphenation.dtd. I have to investigate how I can make that DTD know to the parser without copying the hyphenation.dtd to Lucene&apos;s base directory.&lt;/p&gt;</comment>
                    <comment id="12591969" author="tpeuss" created="Thu, 24 Apr 2008 11:11:00 +0100"  >&lt;ul&gt;
	&lt;li&gt;Fixed the problem with the hyphenation.dtd file that was not found&lt;/li&gt;
	&lt;li&gt;Removed all @author tags&lt;/li&gt;
	&lt;li&gt;Added a note to all files I copied from the FOP project&lt;/li&gt;
	&lt;li&gt;Added package.html files (not very much in there - but credits for the FOP project)&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="12593194" author="gsingers" created="Wed, 30 Apr 2008 02:12:00 +0100"  >&lt;p&gt;So, why would I ever want to use a &quot;Dumb&quot; compound filter?  Any suggestions for a better name?  No need for a patch, I can just make the change.&lt;/p&gt;</comment>
                    <comment id="12593198" author="gsingers" created="Wed, 30 Apr 2008 02:20:44 +0100"  >&lt;p&gt;This looks pretty good Thomas.  I think the last bit that would be good is to add to the package docs an example of start to finish using it, kind of like in the test case.  You might want to explain a little bit about where to get the hyphenation files, etc. (if I am understanding them correctly). &lt;/p&gt;

&lt;p&gt;I think if we can finish that up, we can look to commit.&lt;/p&gt;

&lt;p&gt;The other interesting thing here, as an aside, is the Ternary Tree might be worth pulling up to a &quot;util&quot; package (no need to do so now, just thinking out loud), as it could be used for other interesting things.  For instance, see &lt;a href=&quot;http://www.javaworld.com/javaworld/jw-02-2001/jw-0216-ternary.html&quot; class=&quot;external-link&quot;&gt;http://www.javaworld.com/javaworld/jw-02-2001/jw-0216-ternary.html&lt;/a&gt;   The version we have needs a little work, but I have been thinking about how it might be used to improve spelling, etc.&lt;/p&gt;</comment>
                    <comment id="12593212" author="otis" created="Wed, 30 Apr 2008 04:16:53 +0100"  >&lt;p&gt;Ah, TST was in there, lovely!  +1 to what Grant said about getting it into util later.&lt;br/&gt;
Noticed a misspelling in javadoc while glancing at TST: hibrid -&amp;gt; hybrid&lt;/p&gt;</comment>
                    <comment id="12593216" author="gsingers" created="Wed, 30 Apr 2008 04:28:57 +0100"  >


&lt;p&gt;cool.  It needs some work, IMO to add more features, per that article  &lt;br/&gt;
I sent, but no biggie.&lt;/p&gt;


&lt;p&gt;I saw some of those, but purposely left in the FOP typos... There were  &lt;br/&gt;
more than just that one.&lt;/p&gt;


&lt;p&gt;--------------------------&lt;br/&gt;
Grant Ingersoll&lt;/p&gt;

&lt;p&gt;Lucene Helpful Hints:&lt;br/&gt;
&lt;a href=&quot;http://wiki.apache.org/lucene-java/BasicsOfPerformance&quot; class=&quot;external-link&quot;&gt;http://wiki.apache.org/lucene-java/BasicsOfPerformance&lt;/a&gt;&lt;br/&gt;
&lt;a href=&quot;http://wiki.apache.org/lucene-java/LuceneFAQ&quot; class=&quot;external-link&quot;&gt;http://wiki.apache.org/lucene-java/LuceneFAQ&lt;/a&gt;&lt;/p&gt;





</comment>
                    <comment id="12593238" author="tpeuss" created="Wed, 30 Apr 2008 07:49:40 +0100"  >&lt;blockquote&gt;&lt;p&gt;So, why would I ever want to use a &quot;Dumb&quot; compound filter? Any suggestions for a better name? No need for a patch, I can just make the change.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;A better name would be &lt;em&gt;DictionaryCompoundWordTokenFilter&lt;/em&gt;. I called it &quot;Dumb&quot; because it uses a brute-force approach. But &lt;em&gt;DictionaryCompoundWordTokenFilter&lt;/em&gt; characterizes it better.&lt;/p&gt;</comment>
                    <comment id="12593275" author="tpeuss" created="Wed, 30 Apr 2008 10:17:56 +0100"  >&lt;ul&gt;
	&lt;li&gt;Renamed &lt;em&gt;DumbCompoundWordTokenFilter&lt;/em&gt; to &lt;em&gt;DictionaryCompoundWordTokenFilter&lt;/em&gt;&lt;/li&gt;
	&lt;li&gt;Added more text to the package description file (package.html)&lt;/li&gt;
	&lt;li&gt;Removed some code that was necessary because of &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-1163&quot; title=&quot;CharArraySet.contains(char[] text, int off, int len) does not work&quot;&gt;&lt;del&gt;LUCENE-1163&lt;/del&gt;&lt;/a&gt; (in HyphenationCompoundWordTokenFilter and DictionaryCompoundWordTokenFilter&lt;br/&gt;
)&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="12593338" author="tpeuss" created="Wed, 30 Apr 2008 15:26:05 +0100"  >&lt;ul&gt;
	&lt;li&gt;Minor bugfix in DictionaryCompoundWordFilter: it was not using the &lt;em&gt;maxSubwordSize&lt;/em&gt; parameter&lt;/li&gt;
	&lt;li&gt;Major performance improvement for the DictionaryCompoundWordTokenFilter: we now convert all dictionary strings to lower case before adding them to the CharArraySet and set the &lt;em&gt;ignoreCase&lt;/em&gt; parameter of CharArraySet to false. The filter makes a lower case copy of the token before it starts working on it. This avoids many &lt;em&gt;toLowerCase()&lt;/em&gt; calls in CharArraySet.&lt;/li&gt;
	&lt;li&gt;Minor performance improvement for the HyphenationCompoundWordTokenFilter: see above&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="12594838" author="fterrier" created="Wed, 7 May 2008 10:48:16 +0100"  >&lt;p&gt;Is there any plan of integrating this patch in the official lucene libraries in the short term ? &lt;/p&gt;</comment>
                    <comment id="12594850" author="gsingers" created="Wed, 7 May 2008 11:42:57 +0100"  >&lt;p&gt;Yes.&lt;/p&gt;



&lt;p&gt;--------------------------&lt;br/&gt;
Grant Ingersoll&lt;/p&gt;

&lt;p&gt;Lucene Helpful Hints:&lt;br/&gt;
&lt;a href=&quot;http://wiki.apache.org/lucene-java/BasicsOfPerformance&quot; class=&quot;external-link&quot;&gt;http://wiki.apache.org/lucene-java/BasicsOfPerformance&lt;/a&gt;&lt;br/&gt;
&lt;a href=&quot;http://wiki.apache.org/lucene-java/LuceneFAQ&quot; class=&quot;external-link&quot;&gt;http://wiki.apache.org/lucene-java/LuceneFAQ&lt;/a&gt;&lt;/p&gt;





</comment>
                    <comment id="12596717" author="gsingers" created="Wed, 14 May 2008 11:43:50 +0100"  >&lt;p&gt;I&apos;m now getting:&lt;br/&gt;
 ..../lucene/java/lucene-clean/contrib/analyzers/src/test/org/apache/lucene/analysis/compound/TestCompoundWordTokenFilter.java:60: warning: unmappable character for encoding utf-8&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;javac&amp;#93;&lt;/span&gt;         &quot;Aufgabe&quot;, &quot;&#220;berwachung&quot; };&lt;/p&gt;


&lt;p&gt;Can you convert the classes in question to UTF-8 for the source?&lt;/p&gt;</comment>
                    <comment id="12597426" author="tpeuss" created="Fri, 16 May 2008 12:32:39 +0100"  >&lt;p&gt;UTF-8 problem fixed...&lt;/p&gt;</comment>
                    <comment id="12597446" author="gsingers" created="Fri, 16 May 2008 13:28:41 +0100"  >&lt;p&gt;Committed revision 657027.&lt;/p&gt;</comment>
                </comments>
                <issuelinks>
                        <issuelinktype id="10030">
                <name>Reference</name>
                                <outwardlinks description="relates to">
                            <issuelink>
            <issuekey id="12396279">LUCENE-1287</issuekey>
        </issuelink>
                    </outwardlinks>
                                            </issuelinktype>
                    </issuelinks>
                <attachments>
                    <attachment id="12382162" name="CompoundTokenFilter.patch" size="108557" author="tpeuss" created="Fri, 16 May 2008 12:32:39 +0100" />
                    <attachment id="12381187" name="CompoundTokenFilter.patch" size="108530" author="tpeuss" created="Wed, 30 Apr 2008 15:26:05 +0100" />
                    <attachment id="12381174" name="CompoundTokenFilter.patch" size="107126" author="tpeuss" created="Wed, 30 Apr 2008 10:17:55 +0100" />
                    <attachment id="12380832" name="CompoundTokenFilter.patch" size="101543" author="tpeuss" created="Thu, 24 Apr 2008 11:11:00 +0100" />
                    <attachment id="12378856" name="CompoundTokenFilter.patch" size="92647" author="tpeuss" created="Sat, 29 Mar 2008 11:04:16 +0000" />
                    <attachment id="12378565" name="CompoundTokenFilter.patch" size="92664" author="tpeuss" created="Tue, 25 Mar 2008 12:56:27 +0000" />
                    <attachment id="12378564" name="CompoundTokenFilter.patch" size="92996" author="tpeuss" created="Tue, 25 Mar 2008 12:49:16 +0000" />
                    <attachment id="12376987" name="CompoundTokenFilter.patch" size="92101" author="tpeuss" created="Mon, 3 Mar 2008 16:35:35 +0000" />
                    <attachment id="12375610" name="CompoundTokenFilter.patch" size="86926" author="tpeuss" created="Thu, 14 Feb 2008 16:22:12 +0000" />
                    <attachment id="12375343" name="CompoundTokenFilter.patch" size="77649" author="tpeuss" created="Tue, 12 Feb 2008 11:09:19 +0000" />
                    <attachment id="12374854" name="CompoundTokenFilter.patch" size="73171" author="tpeuss" created="Wed, 6 Feb 2008 11:08:59 +0000" />
                    <attachment id="12374855" name="de.xml" size="49189" author="tpeuss" created="Wed, 6 Feb 2008 11:10:43 +0000" />
                    <attachment id="12374856" name="hyphenation.dtd" size="2950" author="tpeuss" created="Wed, 6 Feb 2008 11:11:12 +0000" />
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>13.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Wed, 6 Feb 2008 16:39:21 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>12578</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10120"><![CDATA[Patch Available]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>26563</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                    <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                <customfieldname>Time in Status</customfieldname>
                <customfieldvalues>
                    
                </customfieldvalues>
            </customfield>
                            </customfields>
    </item>
</channel>
</rss>