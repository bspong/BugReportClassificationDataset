<!-- 
RSS generated by JIRA (5.2.8#851-sha1:3262fdc28b4bc8b23784e13eadc26a22399f5d88) at Tue Jul 16 13:13:00 UTC 2013

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/LUCENE-2373/LUCENE-2373.xml?field=key&field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>5.2.8</version>
        <build-number>851</build-number>
        <build-date>26-02-2013</build-date>
    </build-info>

<item>
            <title>[LUCENE-2373] Create a Codec to work with streaming and append-only filesystems</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-2373</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Since early 2.x times Lucene used a skip/seek/write trick to patch the length of the terms dict into a place near the start of the output data file. This however made it impossible to use Lucene with append-only filesystems such as HDFS.&lt;/p&gt;

&lt;p&gt;In the post-flex trunk the following code in StandardTermsDictWriter initiates this:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
    &lt;span class=&quot;code-comment&quot;&gt;// Count indexed fields up front
&lt;/span&gt;    CodecUtil.writeHeader(out, CODEC_NAME, VERSION_CURRENT); 

    out.writeLong(0);                             &lt;span class=&quot;code-comment&quot;&gt;// leave space &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; end index pointer&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;and completes this in close():&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
      out.seek(CodecUtil.headerLength(CODEC_NAME));
      out.writeLong(dirStart);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I propose to change this layout so that this pointer is stored simply at the end of the file. It&apos;s always 8 bytes long, and we known the final length of the file from Directory, so it&apos;s a single additional seek(length - 8) to read it, which is not much considering the benefits.&lt;/p&gt;</description>
                <environment></environment>
            <key id="12461317">LUCENE-2373</key>
            <summary>Create a Codec to work with streaming and append-only filesystems</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png">Closed</status>
                    <resolution id="1">Fixed</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="ab">Andrzej Bialecki </reporter>
                        <labels>
                    </labels>
                <created>Tue, 6 Apr 2010 23:56:48 +0100</created>
                <updated>Fri, 10 May 2013 11:43:12 +0100</updated>
                    <resolved>Fri, 9 Jul 2010 22:11:46 +0100</resolved>
                                            <fixVersion>4.0-ALPHA</fixVersion>
                                <component>core/index</component>
                        <due></due>
                    <votes>0</votes>
                        <watches>0</watches>
                                                    <comments>
                    <comment id="12854240" author="ab" created="Wed, 7 Apr 2010 00:03:09 +0100"  >&lt;p&gt;Just noticed that the same problem exists in SimpleStandardTermsIndexWriter, and I propose the same solution there.&lt;/p&gt;</comment>
                    <comment id="12854254" author="earwin" created="Wed, 7 Apr 2010 00:30:11 +0100"  >&lt;p&gt;And then IndexOutput.seek() can be deleted. Cool.&lt;/p&gt;</comment>
                    <comment id="12854409" author="mikemccand" created="Wed, 7 Apr 2010 11:10:12 +0100"  >&lt;p&gt;I would love to make Lucene truly write once (and moreve IndexOutput.seek), but... this approach makes me a little nervous...&lt;/p&gt;

&lt;p&gt;In some environments, relying on the length of the file to be accurate might be risky: it&apos;s metadata, that can be subject to different client-side caching than the file&apos;s contents.  EG on NFS I&apos;ve seen issues where the file length was stale yet the file contents were not.&lt;/p&gt;

&lt;p&gt;Maybe we could offer a separate codec that takes this approach, for use on filesystems like HDFS that can&apos;t seek during write?  We should refactor standard codec so that &quot;where this long gets stored&quot; can be easily overridden by a subclass.&lt;/p&gt;

&lt;p&gt;Or, alternatively, we could write this &quot;index of the index&quot; to a separate file?&lt;/p&gt;</comment>
                    <comment id="12855877" author="shaie" created="Mon, 12 Apr 2010 08:37:15 +0100"  >&lt;p&gt;I&apos;d rather not count on file length as well ... so a put/getTermDictSize method on Codec will allow one to implement it however one wants, if running on HDFS for example?&lt;/p&gt;</comment>
                    <comment id="12861214" author="lancenorskog" created="Tue, 27 Apr 2010 04:28:07 +0100"  >&lt;p&gt;Does this make it possible to add a good checksum? &lt;/p&gt;

&lt;p&gt;The Cloud and NRT architectures involve copying lots of segment files around, and disk&amp;amp;RAM&amp;amp;network bandwidth all have error rates. It would be great if the process of making an index file included, on the fly, the creation of a solid checksum that is then baked into the file at the last moment. It should also be in the segments.gen file, but it is more important that the file should have the checksum embedded such that walking the whole file gives a fixed value.&lt;/p&gt;</comment>
                    <comment id="12861281" author="ab" created="Tue, 27 Apr 2010 08:25:41 +0100"  >&lt;p&gt;Aggregated comments...&lt;/p&gt;

&lt;p&gt;Mike: I&apos;d hate to add yet another file just for this purpose. Long-term it&apos;s perhaps worth it. Short-term for HDFS use case it would be enough to provide a method to write a header and a trailer. Codecs that can seek/overwrite would just use the header, codecs that can&apos;t would use both. Codecs that operate on filesystems with unreliable fileLength could write a sync marker before the trailer, and there could be a back-tracking mechanism that starts from the reported fileLength and then tries to find the sync marker (reading back, and/or ahead).&lt;/p&gt;

&lt;p&gt;Shai: hm, but this would require a separate file that stores the header, right?&lt;/p&gt;

&lt;p&gt;Lance: yes. The original use case I had in mind was HDFS (Hadoop File System) which already implements on-the-fly checksums. If we go the way that Mike suggested, i.e. implementing a separate codec, then this should be a simple addition. We could also perhaps structure this as a codec wrapper so that this capability can be applied to other codecs too.&lt;/p&gt;</comment>
                    <comment id="12861356" author="mikemccand" created="Tue, 27 Apr 2010 11:47:06 +0100"  >&lt;blockquote&gt;
&lt;p&gt;Mike: I&apos;d hate to add yet another file just for this purpose. Long-term it&apos;s perhaps worth it. Short-term for HDFS use case it would be enough to provide a method to write a header and a trailer. Codecs that can seek/overwrite would just use the header, codecs that can&apos;t would use both.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I think that&apos;s a good plan &amp;#8211; abstract the header write/read methods so that another codec can easily subclass to change how/where these are written.  I think Lucene&apos;s default (standard) codec should continue to do what it does now?  And then HDFS can take the standard codec, and subclass StandardTermsDictWriter/Reader to put the header at the end.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Codecs that operate on filesystems with unreliable fileLength could write a sync marker before the trailer, and there could be a back-tracking mechanism that starts from the reported fileLength and then tries to find the sync marker (reading back, and/or ahead).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Can&apos;t we just use the current standard codec&apos;s approach by default?  Back-tracking seems dangerous.  Eg what if .fileLength() is too small on such filesystems?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Does this make it possible to add a good checksum?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;A codec could easily do this, today &amp;#8211; it&apos;s orthogonal to using HDFS.  EG Lucene already has a ChecksumIndexOutput/Input, so this should be a simple cutover in standard codec (though we would need to fix up the classes, eg to make &quot;get me the IndexOutput/Input&quot; method, so a subclass could override).&lt;/p&gt;</comment>
                    <comment id="12861361" author="ab" created="Tue, 27 Apr 2010 12:04:43 +0100"  >&lt;blockquote&gt;&lt;p&gt;I think that&apos;s a good plan - abstract the header write/read methods so that another codec can easily subclass to change how/where these are written. I think Lucene&apos;s default (standard) codec should continue to do what it does now? And then HDFS can take the standard codec, and subclass StandardTermsDictWriter/Reader to put the header at the end.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Assuming we add writeHeader/writeTrailer methods, the standard codec would write the header as it does today using writeHeader(), and in writeTrailer() it would just patch it the same way it does today.&lt;/p&gt;

&lt;blockquote&gt;&lt;blockquote&gt;&lt;p&gt;Codecs that operate on filesystems with unreliable fileLength could write a sync marker before the trailer, and there could be a back-tracking mechanism that starts from the reported fileLength and then tries to find the sync marker (reading back, and/or ahead).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Can&apos;t we just use the current standard codec&apos;s approach by default? Back-tracking seems dangerous. Eg what if .fileLength() is too small on such filesystems?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes, of course, I was just dreaming up a filesystem that is both append-only and with unreliable fileLength ... not that I know of any off-hand &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                    <comment id="12862407" author="lancenorskog" created="Thu, 29 Apr 2010 22:05:37 +0100"  >&lt;blockquote&gt;&lt;p&gt;Lance: yes. The original use case I had in mind was HDFS (Hadoop File System) which already implements on-the-fly checksums. If we go the way that Mike suggested, i.e. implementing a separate codec, then this should be a simple addition. We could also perhaps structure this as a codec wrapper so that this capability can be applied to other codecs too.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;+1 for in Lucene itself. Lots of large installations don&apos;t use HDFS to move shards around. Also, the HDFS checksum only counts after the file has touched down at the HDFS portal: there are error rates in local RAM, local hard disk, shared file systems and network I/O. Doing the checksum at the origin is more useful.&lt;/p&gt;</comment>
                    <comment id="12862409" author="lancenorskog" created="Thu, 29 Apr 2010 22:09:25 +0100"  >&lt;p&gt;Grid filesystems like larger blocksizes. HDFS uses a default blocksize of 128k right? At this size, is it worth doing a few merges/optimizes to make a segment fit? This pushes the problem of grid filesystems away from low-level indexing. I would want to index locally and push the index through a separate grid FS access manager.&lt;/p&gt;</comment>
                    <comment id="12862568" author="ab" created="Fri, 30 Apr 2010 08:22:13 +0100"  >&lt;p&gt;HDFS uses 64 or 128 _Mega_Byte blocks.&lt;/p&gt;</comment>
                    <comment id="12864954" author="lancenorskog" created="Thu, 6 May 2010 23:32:52 +0100"  >&lt;p&gt;Another reason to create files in a fully sequential mode is that SSD drives do not like random writes - they can get very slow. SSDs function well with sequential writes, sequential reads, and random reads, so if this issues is fixed, they should work well with Lucene.&lt;/p&gt;</comment>
                    <comment id="12864957" author="lancenorskog" created="Thu, 6 May 2010 23:36:54 +0100"  >&lt;p&gt;.bq HDFS uses 64 or 128 _Mega_Byte blocks. &lt;br/&gt;
Yet another reason to manage memory carefully. &lt;/p&gt;

&lt;p&gt;It should be possible to hit this watermark by using the NoMergePolicy and a RamBuffer size of 64M or 128M:. Hitting the RAMBuffer size causes a segment to flush to a file with little breakage (unused disk space), and it will never be merged again, cutting HDFS overheads. This should give a predictable and consistent segment writing overhead, right?&lt;/p&gt;</comment>
                    <comment id="12865152" author="ab" created="Fri, 7 May 2010 14:40:33 +0100"  >&lt;p&gt;Good point, Lance - though for larger indexes the number of blocks (hence the number of sub-readers in your scenario) would be substantial, maybe too high. Hadoop doesn&apos;t do much of local caching of remote blocks, but I implemented a HDFS Directory in Luke that uses ehcache, and it works quite well.&lt;/p&gt;</comment>
                    <comment id="12882761" author="ab" created="Sat, 26 Jun 2010 00:35:39 +0100"  >&lt;p&gt;This patch contains an implementation of AppendingCodec and necessary refactorings in  CodecProvider and SegmentInfos to support append-only filesystems. There is a unit test that illustrates the use of the codec and verifies that it works with append-only FS.&lt;/p&gt;

&lt;p&gt;Note 1: SegmentInfos write/read methods used the seek/rewrite trick to update the checksum, so it was necessary to extend CodecProvider with methods to provide custom implementations of SegmentInfosWriter/Reader (and default implementations thereof).&lt;/p&gt;

&lt;p&gt;Note 2: o.a.l.index.codecs.* doesn&apos;t have access to many package-level APIs from o.a.l.index.*, so I had to relax the visibility of some methods and fields. Perhaps this may be tightened back in a later revision...&lt;/p&gt;

&lt;p&gt;Patch is relative to the latest trunk (rev. 958137).&lt;/p&gt;</comment>
                    <comment id="12882782" author="rcmuir" created="Sat, 26 Jun 2010 01:27:50 +0100"  >&lt;blockquote&gt;
&lt;p&gt;Note 2: o.a.l.index.codecs.* doesn&apos;t have access to many package-level APIs from o.a.l.index.*, so I had to relax the visibility of some methods and fields. Perhaps this may be tightened back in a later revision...&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Hi, I wouldn&apos;t worry about this. In general Mike had this problem when moving things to the codec package, so we added a javadocs tag for consistent labeling: @lucene.internal&lt;/p&gt;

&lt;p&gt;This expands to the following text: NOTE: This API is for Lucene internal purposes only and might change in incompatible ways in the next release.&lt;/p&gt;

&lt;p&gt;Example usage: &lt;a href=&quot;http://svn.apache.org/repos/asf/lucene/dev/trunk/lucene/src/java/org/apache/lucene/index/IndexFileNames.java&quot; class=&quot;external-link&quot;&gt;http://svn.apache.org/repos/asf/lucene/dev/trunk/lucene/src/java/org/apache/lucene/index/IndexFileNames.java&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Additionally we added another tag: @lucene.experimental, which you can use for any new APIs you introduce that might not have the final stable API (most codecs use this already I think).&lt;/p&gt;

&lt;p&gt;This expands to the following text: WARNING: This API is experimental and might change in incompatible ways in the next release.&lt;/p&gt;

&lt;p&gt;Example usage:&lt;br/&gt;
&lt;a href=&quot;http://svn.apache.org/repos/asf/lucene/dev/trunk/lucene/src/java/org/apache/lucene/index/codecs/pulsing/PulsingCodec.java&quot; class=&quot;external-link&quot;&gt;http://svn.apache.org/repos/asf/lucene/dev/trunk/lucene/src/java/org/apache/lucene/index/codecs/pulsing/PulsingCodec.java&lt;/a&gt;&lt;/p&gt;</comment>
                    <comment id="12882791" author="ab" created="Sat, 26 Jun 2010 01:49:39 +0100"  >&lt;p&gt;Yup, I used @lucene.experimental in this patch.&lt;/p&gt;</comment>
                    <comment id="12882796" author="rcmuir" created="Sat, 26 Jun 2010 02:20:12 +0100"  >&lt;p&gt;Cool, I think @lucene.internal would be good for SegmentInfo etc that must become public.&lt;/p&gt;</comment>
                    <comment id="12883725" author="ab" created="Tue, 29 Jun 2010 23:01:40 +0100"  >&lt;p&gt;I would appreciate a review and a go/no-go from other committers. Especially regarding the part that changes CodecProvider API by adding SegmentInfoWriter/Reader.&lt;/p&gt;</comment>
                    <comment id="12884232" author="mikemccand" created="Thu, 1 Jul 2010 10:42:43 +0100"  >&lt;p&gt;This looks great Andrzej!  This gives codecs full control over reading/writing of SegmentInfo/s, which now empowers a Codec to store any per-segment info it needs to (eg, hasProx, which is now a hardwired bit in SegmentInfo, is really a codec level detail).  Probably the codec could return a (private to it) subclass of SegmentInfo to hold such extra info...&lt;/p&gt;

&lt;p&gt;Maybe we should provide default impls for CodecProvider.getSegmentInfosReader/Writer?  (Ie returning the Default impls)&lt;/p&gt;

&lt;p&gt;Also, should we factor out the &quot;leave space for index pointer&quot; (out.writeLong(0)) to the subclass?  (And, the reading of that dirOffset).  Because this is wasted now for the appending codec...&lt;/p&gt;</comment>
                    <comment id="12884291" author="ab" created="Thu, 1 Jul 2010 15:22:05 +0100"  >&lt;blockquote&gt;&lt;p&gt;Probably the codec could return a (private to it) subclass of SegmentInfo to hold such extra info... &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Nice idea, I didn&apos;t think about this - yes, this should be possible now.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Maybe we should provide default impls for CodecProvider.getSegmentInfosReader/Writer? (Ie returning the Default impls)&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;DefaultCodecProvider does exactly this. Or do you mean instead of using abstract methods in CodecProvider?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Also, should we factor out the &quot;leave space for index pointer&quot; (out.writeLong(0)) to the subclass? (And, the reading of that dirOffset). Because this is wasted now for the appending codec...&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The reading is already factored out, but the writing ... Well, it&apos;s just 8 bytes per segment ... the reason I didn&apos;t factor it out is that it would require additional before/after delegation, or a replication of larger sections of code...&lt;/p&gt;</comment>
                    <comment id="12885022" author="mikemccand" created="Sun, 4 Jul 2010 12:52:58 +0100"  >&lt;blockquote&gt;&lt;p&gt;DefaultCodecProvider does exactly this. Or do you mean instead of using abstract methods in CodecProvider?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Right, I meant move the default impls into CodecProvider, so an app with a custom CodecProvider need not implement the defaults.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;The reading is already factored out, but the writing ... Well, it&apos;s just 8 bytes per segment ... the reason I didn&apos;t factor it out is that it would require additional before/after delegation, or a replication of larger sections of code...&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I hear you, but it looks sort of hackish to factor out one part (seeking to the dir) but not the other part (writing/reading the dirOffset); but I&apos;m fine w/ committing it like this.  Maybe just add a comment in AppendingTermsDictReader.seekDir that dirOffset, which the writer had written into header of file, is ignored?&lt;/p&gt;</comment>
                    <comment id="12885023" author="ab" created="Sun, 4 Jul 2010 13:05:03 +0100"  >&lt;blockquote&gt;&lt;p&gt;I hear you, but it looks sort of hackish to factor out one part (seeking to the dir) but not the other part (writing/reading the dirOffset); but I&apos;m fine w/ committing it like this. Maybe just add a comment in AppendingTermsDictReader.seekDir that dirOffset, which the writer had written into header of file, is ignored?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I hear you too &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; I&apos;ll try to factor out the whole section, if this becomes too messy then I&apos;ll add a comment. Re: CodecProvider default impls - ok.&lt;/p&gt;</comment>
                    <comment id="12885255" author="ab" created="Mon, 5 Jul 2010 16:00:44 +0100"  >&lt;p&gt;It wasn&apos;t too messy after all - here&apos;s an updated patch that incorporates your suggestions.&lt;/p&gt;</comment>
                    <comment id="12885265" author="mikemccand" created="Mon, 5 Jul 2010 17:18:54 +0100"  >&lt;p&gt;Patch looks great!  Thanks Andrzej.&lt;/p&gt;

&lt;p&gt;I tweaked a few things &amp;#8211; added some missing copyrights, removed some unnecessary imports, etc.  I also strengthened the test a bit, by having it write 2 segments and then optimize them, which hit an exception because seek was called when building the compound file doc store (cfx) file.  So I fixed test to also disable that compound-file, and added explanation of this in AppendingCodec&apos;s jdocs.&lt;/p&gt;

&lt;p&gt;We still need a CHANGES entry, but... should this go into contrib/misc instead of core?  Few people need to use appending codec?&lt;/p&gt;</comment>
                    <comment id="12885275" author="ab" created="Mon, 5 Jul 2010 18:26:28 +0100"  >&lt;p&gt;contrib/misc is fine with me. I&apos;ll update the patch to include contrib/CHANGES.txt and move the content to contrib/misc.&lt;/p&gt;</comment>
                    <comment id="12886314" author="ab" created="Thu, 8 Jul 2010 13:55:47 +0100"  >&lt;p&gt;Updated patch. I added comments both in top-level CHANGES and in contrib/CHANGES, to account for two new areas of functionality - the customizable SegmentInfosWriter and the appending codec. If there are no objections I&apos;d like to commit it.&lt;/p&gt;</comment>
                    <comment id="12886671" author="mikemccand" created="Fri, 9 Jul 2010 11:21:17 +0100"  >&lt;p&gt;Looks great Andrzej!  +1 to commit.&lt;/p&gt;</comment>
                    <comment id="12886883" author="ab" created="Fri, 9 Jul 2010 22:11:46 +0100"  >&lt;p&gt;Committed to trunk in revision 962694. Thank you all for helping and reviewing this issue!&lt;/p&gt;</comment>
                </comments>
                <issuelinks>
                        <issuelinktype id="10001">
                <name>dependent</name>
                                                <inwardlinks description="is depended upon by">
                            <issuelink>
            <issuekey id="12463729">LUCENE-2446</issuekey>
        </issuelink>
                    </inwardlinks>
                            </issuelinktype>
                    </issuelinks>
                <attachments>
                    <attachment id="12448706" name="appending.patch" size="51870" author="ab" created="Mon, 5 Jul 2010 16:00:44 +0100" />
                    <attachment id="12448106" name="appending.patch" size="48916" author="ab" created="Sat, 26 Jun 2010 00:35:39 +0100" />
                    <attachment id="12448973" name="LUCENE-2372-2.patch" size="55921" author="ab" created="Thu, 8 Jul 2010 13:55:47 +0100" />
                    <attachment id="12448710" name="LUCENE-2373.patch" size="54749" author="mikemccand" created="Mon, 5 Jul 2010 17:18:54 +0100" />
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>4.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Tue, 6 Apr 2010 23:30:11 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>11426</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>25317</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                    <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                <customfieldname>Time in Status</customfieldname>
                <customfieldvalues>
                    
                </customfieldvalues>
            </customfield>
                            </customfields>
    </item>
</channel>
</rss>