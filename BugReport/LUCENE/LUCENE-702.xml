<!-- 
RSS generated by JIRA (5.2.8#851-sha1:3262fdc28b4bc8b23784e13eadc26a22399f5d88) at Tue Jul 16 13:31:31 UTC 2013

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/LUCENE-702/LUCENE-702.xml?field=key&field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>5.2.8</version>
        <build-number>851</build-number>
        <build-date>26-02-2013</build-date>
    </build-info>

<item>
            <title>[LUCENE-702] Disk full during addIndexes(Directory[]) can corrupt index</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-702</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;This is a spinoff of &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-555&quot; title=&quot;Index Corruption&quot;&gt;&lt;del&gt;LUCENE-555&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;If the disk fills up during this call then the committed segments file can reference segments that were not written.  Then the whole index becomes unusable.&lt;/p&gt;

&lt;p&gt;Does anyone know of any other cases where disk full could corrupt the index?&lt;/p&gt;

&lt;p&gt;I think disk full should worse lose the documents that were &quot;in flight&quot; at the time.  It shouldn&apos;t corrupt the index.&lt;/p&gt;</description>
                <environment></environment>
            <key id="12354184">LUCENE-702</key>
            <summary>Disk full during addIndexes(Directory[]) can corrupt index</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png">Closed</status>
                    <resolution id="1">Fixed</resolution>
                                <assignee username="mikemccand">Michael McCandless</assignee>
                                <reporter username="mikemccand">Michael McCandless</reporter>
                        <labels>
                    </labels>
                <created>Sat, 28 Oct 2006 01:08:45 +0100</created>
                <updated>Tue, 27 Feb 2007 18:10:35 +0000</updated>
                    <resolved>Mon, 18 Dec 2006 16:53:48 +0000</resolved>
                            <version>2.1</version>
                                <fixVersion>2.1</fixVersion>
                                <component>core/index</component>
                        <due></due>
                    <votes>0</votes>
                        <watches>0</watches>
                                                    <comments>
                    <comment id="12446307" author="ningli" created="Wed, 1 Nov 2006 17:19:18 +0000"  >&lt;p&gt;A possible solution to this issue is to check, when writing segment infos to &quot;segments&quot; in directory d,&lt;br/&gt;
whether dir of a segment info is d, and only write if it is. Suggestions?&lt;/p&gt;

&lt;p&gt;The following is my comment on this issue from the mailing list documenting how Lucene could&lt;br/&gt;
produce an inconsistent index if addIndexes(Directory[]) does not run to its completion.&lt;/p&gt;

&lt;p&gt;&quot;This makes me notice a bug in current addIndexes(Directory[]). In current addIndexes(Directory[]),&lt;br/&gt;
segment infos in S are added to T&apos;s &quot;segmentInfos&quot; upfront. Then segments in S are merged to T&lt;br/&gt;
several at a time. Every merge is committed with T&apos;s &quot;segmentInfos&quot;. So if a reader is opened on T&lt;br/&gt;
while addIndexes(Directory[]) is going on, it could see an inconsistent index.&quot;&lt;/p&gt;</comment>
                    <comment id="12446376" author="mikemccand" created="Wed, 1 Nov 2006 21:00:16 +0000"  >
&lt;p&gt;That seems like a reasonable approach?  At least the index would be&lt;br/&gt;
consistent (ie, loadable).&lt;/p&gt;

&lt;p&gt;Though, if you hit disk full part way through, then some of your&lt;br/&gt;
indexes were added and some where not.  How do you &quot;recover&quot; after you&lt;br/&gt;
free up your disk space?  If you just re-add all indexes then you have&lt;br/&gt;
duplicates in the index.&lt;/p&gt;

&lt;p&gt;Is it possible instead to not reference (in the written segments info&lt;br/&gt;
file) those segments that were carried over from the input&lt;br/&gt;
readers/directories?  This would make the operation transactional, so&lt;br/&gt;
that if we crashed part way through, then the index rolls back to&lt;br/&gt;
where it was before the addIndexes call.  And user could free up disk&lt;br/&gt;
space and try again, without creating dups.&lt;/p&gt;

&lt;p&gt;(One problem with this is that the orphan&apos;d segments that had been&lt;br/&gt;
written would not actually get deleted automatically; lockless commits&lt;br/&gt;
would fix that though because it recomputes on instantiating a reader&lt;br/&gt;
which files are unreferenced and then deletes them).&lt;/p&gt;</comment>
                    <comment id="12447968" author="mikemccand" created="Tue, 7 Nov 2006 23:19:58 +0000"  >&lt;p&gt;I think we should try to make all of the addIndexes calls (and more&lt;br/&gt;
generally any call to Lucene) &quot;transactional&quot;.  Meaning, if the call&lt;br/&gt;
is aborted (machine crashes, disk full, jvm killed, neutrino hits CPU,&lt;br/&gt;
etc.) then your index just &quot;rolls back&quot; to where it was at the start&lt;br/&gt;
of the call.  Ie, it is consistent and none of the incoming documents&lt;br/&gt;
were added.&lt;/p&gt;

&lt;p&gt;This way your index is fine after the crash, and, you can fix the&lt;br/&gt;
cause of the crash and re-run the addIndexes call and you won&apos;t get&lt;br/&gt;
duplicate documents.&lt;/p&gt;

&lt;p&gt;To achieve this, each of the three addIndexes methods would need to 1)&lt;br/&gt;
not commit a new segments file until the end, and 2) not delete any&lt;br/&gt;
segments referenced by the initial segments file (segmentInfos) until&lt;br/&gt;
the end.&lt;/p&gt;

&lt;p&gt;We have three methods now for addIndexes:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;For addIndexes(IndexReader[]): this method is I think already&lt;br/&gt;
    transactional.  We create a merger, add all readers to it, do the&lt;br/&gt;
    merge, and only at the end commit the new segments file &amp;amp; remove&lt;br/&gt;
    old segments.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;For addIndexes(Directory[]): this method can currently corrupt the&lt;br/&gt;
    index if aborted.  However, because all merging is done only on&lt;br/&gt;
    the newly added segments, I think the fix is simply to not commit&lt;br/&gt;
    the new segments file until the end?&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;For addIndexesNoOptimize(Directory[]): this method can also&lt;br/&gt;
    currently corrupt the index if aborted.  To fix this I think we&lt;br/&gt;
    need to not only prevent committing a new segments file until the&lt;br/&gt;
    end, but also to prevent deletion of any segments in the original&lt;br/&gt;
    segments file.  This is because it&apos;s able (I think?) to merge&lt;br/&gt;
    both old and new segments in its step 3.  This would normally&lt;br/&gt;
    result in deleting those old segments that were merged.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    Note that this will increase the temporary disk usage used during&lt;br/&gt;
    the call, because old segments must remain on disk even if they&lt;br/&gt;
    have been merged, but I think this is the right tradeoff&lt;br/&gt;
    (transactional vs temporary disk usage)?&lt;/p&gt;

&lt;p&gt;Also note that we would need the fixes from lockless &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-701&quot; title=&quot;Lock-less commits&quot;&gt;&lt;del&gt;LUCENE-701&lt;/del&gt;&lt;/a&gt; to&lt;br/&gt;
properly delete orphan&apos;d segments after an abort.  Without the&lt;br/&gt;
IndexFileDeleter I think they would stick around indefinitely.&lt;/p&gt;

&lt;p&gt;Does this approach sound reasonable/right?  Any feedback?&lt;/p&gt;</comment>
                    <comment id="12448006" author="ningli" created="Wed, 8 Nov 2006 01:55:12 +0000"  >&lt;p&gt;&amp;gt; I think we should try to make all of the addIndexes calls (and more&lt;br/&gt;
&amp;gt; generally any call to Lucene) &quot;transactional&quot;.&lt;/p&gt;

&lt;p&gt;Agree. A transactional semantics would be better.&lt;/p&gt;

&lt;p&gt;The approach you described for three addIndexes looks good.&lt;/p&gt;

&lt;p&gt;addIndexes(IndexReader[]) is transactional but has two commits: one&lt;br/&gt;
when existing segments are merged at the beginning, the other at the&lt;br/&gt;
end when all segment/readers are merged.&lt;/p&gt;

&lt;p&gt;addIndexes(Directory[]) can be fixed to have a similar behaviour:&lt;br/&gt;
first commit when existing segments are merged at the beginning, then&lt;br/&gt;
at the end when all old/new segments are merged.&lt;/p&gt;

&lt;p&gt;addIndexesNoOptimize(Directory[]), on the other hand, does not merge&lt;br/&gt;
existing segments at the beginning. So when fixed, it will only have&lt;br/&gt;
one commit at the end which captures all the changes.&lt;/p&gt;</comment>
                    <comment id="12457414" author="mikemccand" created="Mon, 11 Dec 2006 17:09:45 +0000"  >
&lt;p&gt;I&apos;ve attached a patch with changes as described below.  I will commit&lt;br/&gt;
in a few days if no one objects!&lt;/p&gt;

&lt;p&gt;All unit tests pass.&lt;/p&gt;

&lt;p&gt;This patch fixes addIndexes to 1) not corrupt the index on exception,&lt;br/&gt;
2) be transactional (either all or nothing was actually added), and 3)&lt;br/&gt;
to leave the writer instance in an consistent state (meaning, on an&lt;br/&gt;
exception, the segmentInfos state is restored to its starting point,&lt;br/&gt;
so that it matches what&apos;s actually in the index, and any unreferenced,&lt;br/&gt;
presumably partially written, files in the index are removed).&lt;/p&gt;

&lt;p&gt;I&apos;ve also fixed IndexWriter.mergeSegments() and IndexReader.commit()&lt;br/&gt;
to keep the instance &quot;consistent&quot; on exception as well (ie, even when&lt;br/&gt;
not being called from addIndexes).  Meaning, on an exception, any&lt;br/&gt;
changes to segmentInfos state are rolled back, any opened readers (in&lt;br/&gt;
the merger) are closed, and non-committed files are removed.&lt;/p&gt;

&lt;p&gt;I&apos;ve added two unit tests (one for IndexWriter.addIndexes* and one for&lt;br/&gt;
IndexReader.close) that expose these various cases as failures in the&lt;br/&gt;
current Lucene sources, which then pass with this patch.&lt;/p&gt;

&lt;p&gt;These unit tests use a new handy MockRAMDirectory that can simulate&lt;br/&gt;
disk full, inject random IOExceptions, measures peak disk usage, etc.&lt;/p&gt;

&lt;p&gt;Here is the summary of the approach:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;Added private methods startTransaction(), rollbackTransaction()&lt;br/&gt;
    and commitTransaction() to IndexWriter.  During a transaction&lt;br/&gt;
    (inTransaction = true), I block changes to the index that alter&lt;br/&gt;
    how it was at the start of the transaction: we don&apos;t write any new&lt;br/&gt;
    segments_N files (but, do update the segmentInfos in memory, and&lt;br/&gt;
    do write / merge new segment files); we are not allowed to delete&lt;br/&gt;
    any segment files that were in the index at the start (added&lt;br/&gt;
    &quot;protectedSegments&quot; to track this during a transaction).&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;For the addIndexes methods, I first call startTransaction(), then&lt;br/&gt;
    do the actual work in a try block, and in a finally block then&lt;br/&gt;
    call either commitTransaction() or rollbackTransaction().&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Fixed mergeSegments to respect whether it&apos;s in a transaction (and&lt;br/&gt;
    not commit/delete).  Also, fixed this method so that if it&apos;s not&lt;br/&gt;
    in a transaction (ie, being called by optimize or&lt;br/&gt;
    maybeMergeSegments) it still (in a try/finally) leaves&lt;br/&gt;
    segmentInfos &quot;consistent&quot; on hitting an exception, and removes any&lt;br/&gt;
    partial files that had been written.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Fixed IndexReader.commit with similar rollback logic to reset&lt;br/&gt;
    internal state if commit has failed.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;It is actually possible to get an IOException from&lt;br/&gt;
addIndexes(Directory[]) yet see that all docs were in fact added. This&lt;br/&gt;
happens when the disk full is hit in building the CFS file on the&lt;br/&gt;
final optimize.  In this case, the index is consistent, but that&lt;br/&gt;
segment will remain in non-CFS format and will show all docs as added.&lt;/p&gt;

&lt;p&gt;Various other small changes:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;Changed RAMDirectory, and its createOutput method, to be&lt;br/&gt;
    non-final.  Also changed private -&amp;gt; protected for some of its&lt;br/&gt;
    instance variables.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Created MockRAMDirectory subclass.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Moved the RAMDirectory.getRecomputedSizeInBytes() into&lt;br/&gt;
    MockRAMDirectory.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Changed IndexFileDeleter to use a HashSet (instead of Vector) to&lt;br/&gt;
    track pending files since with these changes the same file can be&lt;br/&gt;
    added to the pending set many times.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Added some javadocs around temporary disk usage by these methods&lt;br/&gt;
    (this came up on java-user recently).  Also included a check in&lt;br/&gt;
    one of the unit tests to assert this.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;In SegmentInfos, separately track (in memory only) which&lt;br/&gt;
    generation we will use for writing vs which generation was last&lt;br/&gt;
    successfully read.  These are almost always the same, but can&lt;br/&gt;
    differ when a commit failed while writing the next segments_N&lt;br/&gt;
    file.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Changed package protected IndexFileDeleter.commitPendingFiles() to&lt;br/&gt;
    actually delete the files (previously you also had to separately&lt;br/&gt;
    call deleteFiles).&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Added SegmentInfos.getNextSegmentFileName()&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Added SegmentInfos.clone() to also copy the contents of the vector&lt;br/&gt;
    (ie, each SegmentInfo).&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Added SegmentInfo.reset(), which is used to &quot;copy back&quot; a previous&lt;br/&gt;
    clone of a SegmentInfo (IndexReader uses this to rollback).&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Added package protected SegmentReader.getSegmentName()&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Fixed a small bug in IndexFileDeleter that was failing to remove&lt;br/&gt;
    un-referenced CFS file on a segment that wasn&apos;t successfully&lt;br/&gt;
    converted to a CFS file (and added case to TestIndexFileDelter to&lt;br/&gt;
    expose the bug first).&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Fixed a few minor typos.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="12457520" author="ningli" created="Tue, 12 Dec 2006 00:25:04 +0000"  >&lt;p&gt;It looks good. My two cents:&lt;/p&gt;

&lt;p&gt;1 In the two rollbacks in mergeSegments (where inTransaction is false), the segmentInfos&apos; generation is not always rolled back. So something like this could happen: two consecutive successful commits write segments_3 and segments_5, respectively. Nothing is broken, but it&apos;d be nice to roll back completely (even for the IndexWriter instance) when a commit fails.&lt;/p&gt;

&lt;p&gt;2 Code serving two purposes are (and has been) mixed in mergeSegments: one to merge segments and create compound file if necessary, the other to commit or roll back when inTransaction is false. It&apos;d be nice if the two could be separated: optimize and maybeMergeSegments call mergeSegmentsAndCommit, which creates a transaction, calls mergeSegments and commits or rolls back; mergeSegments doesn&apos;t deal with commit or rollback. However, currently the non-CFS version is committed first even if useCompoundFile is true. Until that&apos;s changed, mergeSegments probably has to continue serving both purposes.&lt;/p&gt;</comment>
                    <comment id="12457691" author="mikemccand" created="Tue, 12 Dec 2006 12:51:03 +0000"  >&lt;p&gt;Thanks for the review Ning!&lt;/p&gt;

&lt;p&gt;&amp;gt; 1 In the two rollbacks in mergeSegments (where inTransaction is&lt;br/&gt;
&amp;gt; false), the segmentInfos&apos; generation is not always rolled back. So&lt;br/&gt;
&amp;gt; something like this could happen: two consecutive successful commits&lt;br/&gt;
&amp;gt; write segments_3 and segments_5, respectively. Nothing is broken,&lt;br/&gt;
&amp;gt; but it&apos;d be nice to roll back completely (even for the IndexWriter&lt;br/&gt;
&amp;gt; instance) when a commit fails.&lt;/p&gt;

&lt;p&gt;This is actually intentional: I don&apos;t want to write to the same&lt;br/&gt;
segments_N filename, ever, on the possibility that a reader may be&lt;br/&gt;
reading it.  Admittedly, this should be quite rare (filling up disk&lt;br/&gt;
and then experiencing contention, only on Windows), but still I wanted&lt;br/&gt;
to keep &quot;write once&quot; even in this case.&lt;/p&gt;

&lt;p&gt;&amp;gt; 2 Code serving two purposes are (and has been) mixed in&lt;br/&gt;
&amp;gt; mergeSegments: one to merge segments and create compound file if&lt;br/&gt;
&amp;gt; necessary, the other to commit or roll back when inTransaction is&lt;br/&gt;
&amp;gt; false. It&apos;d be nice if the two could be separated: optimize and&lt;br/&gt;
&amp;gt; maybeMergeSegments call mergeSegmentsAndCommit, which creates a&lt;br/&gt;
&amp;gt; transaction, calls mergeSegments and commits or rolls back;&lt;br/&gt;
&amp;gt; mergeSegments doesn&apos;t deal with commit or rollback. However,&lt;br/&gt;
&amp;gt; currently the non-CFS version is committed first even if&lt;br/&gt;
&amp;gt; useCompoundFile is true. Until that&apos;s changed, mergeSegments&lt;br/&gt;
&amp;gt; probably has to continue serving both purposes.&lt;/p&gt;

&lt;p&gt;I agree, mergeSegments is doing two different things now: merging&lt;br/&gt;
segments, and, committing this change (in 2 steps, for the CFS case)&lt;br/&gt;
into the index (if not in a transaction).&lt;/p&gt;

&lt;p&gt;I had in fact tried putting a transaction up at the&lt;br/&gt;
optimize/maybeMergeSegments level, and it worked, but there was one&lt;br/&gt;
severe drawback: the max temp free space required would be &lt;span class=&quot;error&quot;&gt;&amp;#91;up to&amp;#93;&lt;/span&gt; 2X&lt;br/&gt;
the starting size of the segments-to-be-merged because the original&lt;br/&gt;
segments would be there (1X), the newly merged separate segment files&lt;br/&gt;
would be there (another 1X) and the just-created CFS segment file&lt;br/&gt;
would also be there (another 1X).&lt;/p&gt;

&lt;p&gt;Whereas the max temp space required now is 1X the starting size of&lt;br/&gt;
segments-to-be-merged.&lt;/p&gt;

&lt;p&gt;So I think this (doing 2 commits with the current source before my&lt;br/&gt;
patch) is intentional, to keep the temp free space required at 1X.&lt;/p&gt;

&lt;p&gt;It&apos;s also &lt;span class=&quot;error&quot;&gt;&amp;#91;relatively&amp;#93;&lt;/span&gt; OK to commit that first time, at least&lt;br/&gt;
functionally: the index is consistent and has all docs.  The only&lt;br/&gt;
downside is that if you hit disk full or other exception when creating&lt;br/&gt;
the CFS file then your index has one segment not in compound format (I&lt;br/&gt;
will call this out in the javadocs).&lt;/p&gt;

&lt;p&gt;OK I&apos;ve added a unit-test that explicitly tests max temp space&lt;br/&gt;
required by just optimize and asserts that it&apos;s at most 1X starting&lt;br/&gt;
index size.  Will attach a patch shortly!&lt;/p&gt;</comment>
                    <comment id="12457692" author="mikemccand" created="Tue, 12 Dec 2006 12:52:56 +0000"  >&lt;p&gt;OK I attached a new patch with changes to only javadocs &amp;amp; unit tests:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;Fixed the disk full unit test to use &quot;richer&quot; documents so indexes&lt;br/&gt;
    shrink less on merging/optimizing (ie make the test case &quot;harder&quot;&lt;br/&gt;
    to satisfy the disk usage check).&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Added new test case for temp disk usage of optimize.  Verified&lt;br/&gt;
    that it fails if we put a transaction around mergeSegments call in&lt;br/&gt;
    optimize (as described above).&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Fixed javadocs for addIndexes&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/star_yellow.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;: we actually require up to 2X the&lt;br/&gt;
    total input size of all indices.  Fixed unit test to assert this.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Fixed javadocs in IndexWriter&apos;s optimize, addIndexes&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/star_yellow.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;,&lt;br/&gt;
    addDocument to describe disk usage and index state after an&lt;br/&gt;
    IOException is thrown.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Improved how MockRAMDirectory tracks/enforces max usage.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Other small fixes to unit test.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="12457858" author="ningli" created="Tue, 12 Dec 2006 20:00:48 +0000"  >&lt;p&gt;&amp;gt; This is actually intentional: I don&apos;t want to write to the same&lt;br/&gt;
&amp;gt; segments_N filename, ever, on the possibility that a reader may be&lt;br/&gt;
&amp;gt; reading it.  Admittedly, this should be quite rare (filling up disk&lt;br/&gt;
&amp;gt; and then experiencing contention, only on Windows), but still I wanted&lt;br/&gt;
&amp;gt; to keep &quot;write once&quot; even in this case.&lt;/p&gt;

&lt;p&gt;In IndexWriter, the rollbackTransaction call in commitTransaction could&lt;br/&gt;
cause write to the same segment_N filename, right?&lt;/p&gt;

&lt;p&gt;The &quot;write once&quot; semantics is not kept for segment names or .delN. This&lt;br/&gt;
is ok because no reader will read the old versions.&lt;/p&gt;</comment>
                    <comment id="12457896" author="mikemccand" created="Tue, 12 Dec 2006 21:55:38 +0000"  >&lt;p&gt;&amp;gt; In IndexWriter, the rollbackTransaction call in commitTransaction could&lt;br/&gt;
&amp;gt; cause write to the same segment_N filename, right?&lt;/p&gt;

&lt;p&gt;Good catch &amp;#8211; you are correct!  OK, I fixed rollbackTransaction to do&lt;br/&gt;
the same in-place rollback of the segmentInfos that I do in&lt;br/&gt;
mergeSegments (patch attached).  This means if another attempt is made&lt;br/&gt;
to commit, using this same IndexWriter instance after it had received&lt;br/&gt;
an exception, it will write to a new segments_N file.&lt;/p&gt;

&lt;p&gt;&amp;gt; The &quot;write once&quot; semantics is not kept for segment names or&lt;br/&gt;
&amp;gt; .delN. This is ok because no reader will read the old versions.&lt;/p&gt;

&lt;p&gt;Right, I think these cases are less important because readers would never try to&lt;br/&gt;
open those partially written files (since no segments_N references them).&lt;/p&gt;</comment>
                    <comment id="12476270" author="mikemccand" created="Tue, 27 Feb 2007 18:10:35 +0000"  >&lt;p&gt;Closing all issues that were resolved for 2.1.&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12346929" name="LUCENE-702.patch" size="74994" author="mikemccand" created="Mon, 11 Dec 2006 17:09:45 +0000" />
                    <attachment id="12347001" name="LUCENE-702.take2.patch" size="78652" author="mikemccand" created="Tue, 12 Dec 2006 12:52:56 +0000" />
                    <attachment id="12347045" name="LUCENE-702.take3.patch" size="78885" author="mikemccand" created="Tue, 12 Dec 2006 21:55:38 +0000" />
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>3.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Wed, 1 Nov 2006 17:19:18 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>13050</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    <customfieldvalue key="10120"><![CDATA[Patch Available]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>27028</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                    <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                <customfieldname>Time in Status</customfieldname>
                <customfieldvalues>
                    
                </customfieldvalues>
            </customfield>
                            </customfields>
    </item>
</channel>
</rss>