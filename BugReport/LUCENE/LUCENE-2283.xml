<!-- 
RSS generated by JIRA (5.2.8#851-sha1:3262fdc28b4bc8b23784e13eadc26a22399f5d88) at Tue Jul 16 13:30:46 UTC 2013

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/LUCENE-2283/LUCENE-2283.xml?field=key&field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>5.2.8</version>
        <build-number>851</build-number>
        <build-date>26-02-2013</build-date>
    </build-info>

<item>
            <title>[LUCENE-2283] Possible Memory Leak in StoredFieldsWriter</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-2283</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;StoredFieldsWriter creates a pool of PerDoc instances&lt;/p&gt;

&lt;p&gt;this pool will grow but never be reclaimed by any mechanism&lt;/p&gt;

&lt;p&gt;furthermore, each PerDoc instance contains a RAMFile.&lt;br/&gt;
this RAMFile will also never be truncated (and will only ever grow) (as far as i can tell)&lt;/p&gt;

&lt;p&gt;When feeding documents with large number of stored fields (or one large dominating stored field) this can result in memory being consumed in the RAMFile but never reclaimed. Eventually, each pooled PerDoc could grow very large, even if large documents are rare.&lt;/p&gt;

&lt;p&gt;Seems like there should be some attempt to reclaim memory from the PerDoc[] instance pool (or otherwise limit the size of RAMFiles that are cached) etc&lt;/p&gt;</description>
                <environment></environment>
            <key id="12457225">LUCENE-2283</key>
            <summary>Possible Memory Leak in StoredFieldsWriter</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png">Closed</status>
                    <resolution id="1">Fixed</resolution>
                                <assignee username="mikemccand">Michael McCandless</assignee>
                                <reporter username="tsmith">Tim Smith</reporter>
                        <labels>
                    </labels>
                <created>Tue, 23 Feb 2010 21:41:19 +0000</created>
                <updated>Fri, 18 Jun 2010 09:03:56 +0100</updated>
                    <resolved>Sun, 30 May 2010 11:29:44 +0100</resolved>
                            <version>2.4.1</version>
                                <fixVersion>2.9.3</fixVersion>
                <fixVersion>3.0.2</fixVersion>
                <fixVersion>3.1</fixVersion>
                <fixVersion>4.0-ALPHA</fixVersion>
                                <component>core/index</component>
                        <due></due>
                    <votes>1</votes>
                        <watches>1</watches>
                                                    <comments>
                    <comment id="12837740" author="mikemccand" created="Wed, 24 Feb 2010 11:00:50 +0000"  >
&lt;p&gt;TermVectorsTermsWriter has the same issue.&lt;/p&gt;

&lt;p&gt;You&apos;re right: with &quot;irregular&quot; sized documents coming through, you can&lt;br/&gt;
end up with PerDoc instances that waste space, because the RAMFile has&lt;br/&gt;
buffers allocated from past huge docs that the latest tiny docs don&apos;t&lt;br/&gt;
use.&lt;/p&gt;

&lt;p&gt;Note that the number of outstanding PerDoc instances is a function of&lt;br/&gt;
how &quot;out of order&quot; the docs are being indexed, because the PerDoc&lt;br/&gt;
holds any state only until that doc can be written to the store files&lt;br/&gt;
(stored fields, term vectors).  It&apos;s transient.&lt;/p&gt;

&lt;p&gt;EG with a single thread, there will only be one PerDoc &amp;#8211; it&apos;s written&lt;br/&gt;
immediately.  With 2 threads, if you have a massive doc (which thread&lt;br/&gt;
1 get stuck indexing) and then zillions of tiny docs (which thread 2&lt;br/&gt;
burns through, while thread 1 is busy), then you can get a large&lt;br/&gt;
number of PerDocs created, waiting for their turn because thread 1&lt;br/&gt;
hasn&apos;t finished yet.&lt;/p&gt;

&lt;p&gt;But this process won&apos;t use unbounded RAM &amp;#8211; the RAM used by the&lt;br/&gt;
RAMFiles is accounted for, and once it gets too high (10% of the RAM&lt;br/&gt;
buffer size), we forcefully idle the incoming threads until the &quot;out&lt;br/&gt;
of orderness&quot; is resolved.  EG in this case, thread 2 will stall until&lt;br/&gt;
thread 1 has finished its doc.  That byte accounting does account for&lt;br/&gt;
the allocated but not used byte&lt;span class=&quot;error&quot;&gt;&amp;#91;1024&amp;#93;&lt;/span&gt; inside RAMFile (we use&lt;br/&gt;
RAMFile.sizeInBytes()).&lt;/p&gt;

&lt;p&gt;So... this is not really a memory leak.  But it is a potential&lt;br/&gt;
starvation issue, in that if your PerDoc instances all grow to large&lt;br/&gt;
RAMFiles over time (as each has had to service a very large document),&lt;br/&gt;
then it can mean the amount of concurrency that DW allows will become&lt;br/&gt;
&quot;pinched&quot;.  Especially if these docs are large relative to your&lt;br/&gt;
ram buffer size.&lt;/p&gt;

&lt;p&gt;Are you hitting this issue?  Ie seeing poor concurrency during&lt;br/&gt;
indexing despite using many threads, because DW is forcefully idleing&lt;br/&gt;
the threads?  It should only happen if you sometimes index docs&lt;br/&gt;
that are larger than RAMBufferSize/10/numberOrIndexingThreads.&lt;/p&gt;

&lt;p&gt;I&apos;ll work out  a fix.  I think we should fix RAMFile.reset to trim its&lt;br/&gt;
buffers using ArrayUtil.getShrinkSize.&lt;/p&gt;</comment>
                    <comment id="12837793" author="tsmith" created="Wed, 24 Feb 2010 14:05:39 +0000"  >&lt;p&gt;I came across this issue looking for a reported memory leak during indexing&lt;/p&gt;

&lt;p&gt;a yourkit snapshot showed that the PerDocs for an IndexWriter were using ~40M of memory (at which point i came across this potentially unbounded memory use in StoredFieldsWriter)&lt;br/&gt;
this snapshot seems more or less at a stable point (memory grows but then returns to a &quot;normal&quot; state), however i have reports that eventually the memory is completely exhausted resulting in out of memory errors.&lt;/p&gt;

&lt;p&gt;I so far have not found any other major culprit in the lucene indexing code.&lt;/p&gt;

&lt;p&gt;This index receives a routine mix of very large and very small documents (which would explain this situation)&lt;br/&gt;
The VM and system have more than ample amount of memory given the buffer size and what should be normal indexing RAM requirements.&lt;/p&gt;

&lt;p&gt;Also, a major difference between this leak not occurring and it showing up is that previously, the IndexWriter was closed when performing commits, now the IndexWriter remains open (just calling IndexWriter.commit()). So, if any memory is leaking during indexing, it is no longer being reclaimed during commit. As a side note, closing the index writer at commit time would sometimes fail, resulting in some following updates to fail because the index writer was locked and couldn&apos;t be reopened until the old index writer was garbage collected, so i don&apos;t want to go back to this for commits.&lt;/p&gt;

&lt;p&gt;Its possible there is a leak somewhere else (i currently do not have a snapshot right before out of memory issues occur, so currently the only thing that stands out is the PerDoc memory use)&lt;/p&gt;

&lt;p&gt;As far as a fix goes, wouldn&apos;t it be better to have the RAMFile&apos;s used for stored fields pull and return byte buffers from the byte block pool on the DocumentsWriter? This would allow the memory to be reclaimed based on the index writers buffer size (otherwise there is no configurable way to tune this memory use)&lt;/p&gt;
</comment>
                    <comment id="12837811" author="mikemccand" created="Wed, 24 Feb 2010 14:47:38 +0000"  >&lt;blockquote&gt;&lt;p&gt;a yourkit snapshot showed that the PerDocs for an IndexWriter were using ~40M of memory&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;What was IW&apos;s ramBufferSizeMB when you saw this?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;however i have reports that eventually the memory is completely exhausted resulting in out of memory errors.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Hmm, that makes me nervous, because I think in this case the use should be bounded.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;As a side note, closing the index writer at commit time would sometimes fail, resulting in some following updates to fail because the index writer was locked and couldn&apos;t be reopened until the old index writer was garbage collected, so i don&apos;t want to go back to this for commits.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;That doesn&apos;t sound good!  Can you post some details on this (eg an exception)?&lt;/p&gt;

&lt;p&gt;But, anyway, keeping the same IW open and just calling commit is (should be) fine.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;As far as a fix goes, wouldn&apos;t it be better to have the RAMFile&apos;s used for stored fields pull and return byte buffers from the byte block pool on the DocumentsWriter?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes, that&apos;s a great solution &amp;#8211; a single pool.  But that&apos;s a somewhat bigger change.  I guess we can pass a byte[] allocator to RAMFile.  It&apos;d have to be a new pool, too (DW&apos;s byte blocks are 32KB not the 1KB that RAMFile uses).&lt;/p&gt;</comment>
                    <comment id="12837821" author="tsmith" created="Wed, 24 Feb 2010 14:59:14 +0000"  >&lt;p&gt;ramBufferSizeMB is 64MB&lt;/p&gt;

&lt;p&gt;Here&apos;s the yourkit breakdown per class:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;DocumentsWriter - 256 MB
	&lt;ul&gt;
		&lt;li&gt;TermsHash - 38.7 MB&lt;/li&gt;
		&lt;li&gt;StoredFieldsWriter - 37.5 MB&lt;/li&gt;
		&lt;li&gt;DocumentsWriterThreadState - 36.2 MB&lt;/li&gt;
		&lt;li&gt;DocumentsWriterThreadState - 34.6 MB&lt;/li&gt;
		&lt;li&gt;DocumentsWriterThreadState - 33.8 MB&lt;/li&gt;
		&lt;li&gt;DocumentsWriterThreadState - 27.5 MB&lt;/li&gt;
		&lt;li&gt;DocumentsWriterThreadState - 13.4 MB&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I&apos;m starting to dig into the ThreadStates now to see if anything stands out here&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Hmm, that makes me nervous, because I think in this case the use should be bounded.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I should be getting a new profile dump at &quot;crash&quot; time soon, so hopefully that will make things clearer&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;That doesn&apos;t sound good! Can you post some details on this (eg an exception)?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;If i recall correctly, I think the exception was caused by an out of disk space situation (which would recover)&lt;br/&gt;
obviously not much that can be done about this other than adding more disk space, however the situation would recover, but docs would be lost in the interum&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;But, anyway, keeping the same IW open and just calling commit is (should be) fine.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yeah, this should be the way to go, especially as it results in the pooled buffers not needing to be reallocated/reclaimed/etc, however right now this is the only change i can currently think of that could result in memory issues.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Yes, that&apos;s a great solution - a single pool. But that&apos;s a somewhat bigger change. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Seems like this would be the best approach as it makes the memory bounded by the configuration of the engine, giving better reuse of byte blocks and better ability to reclaim memory (in DocumentsWriter.balanceRAM())&lt;/p&gt;

</comment>
                    <comment id="12837865" author="mikemccand" created="Wed, 24 Feb 2010 16:39:51 +0000"  >&lt;blockquote&gt;
&lt;p&gt;ramBufferSizeMB is 64MB&lt;/p&gt;

&lt;p&gt;Here&apos;s the yourkit breakdown per class:&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Hmm &amp;#8211; spooky.  With ram buffer @ 64MB, DocumentsWriter is using 256MB!?  Something is clearly amiss.  40 MB used by StoredFieldsWriter&apos;s PerDoc still leaves 152 MB unaccounted for... hmm.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;If i recall correctly, I think the exception was caused by an out of disk space situation (which would recover)&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Oh OK.  Though... closing the iW vs calling IW.commit should be not different in that regard.  Both should have the same transient disk space usage.  It&apos;s odd you&apos;d see out of disk for .close but not also for .commit.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Seems like this would be the best approach as it makes the memory bounded by the configuration of the engine, giving better reuse of byte blocks and better ability to reclaim memory (in DocumentsWriter.balanceRAM())&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I agree.  I&apos;ll mull over how to do it... unless you&apos;re planning on consing up a patch &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/wink.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;How many threads do you pass through IW?  Are the threads forever from a static pool, or do they come and go?  I&apos;d like to try to simulate your usage (huge docs &amp;amp; tiny docs)  in my dev area to see if I can provoke the same behavior.&lt;/p&gt;</comment>
                    <comment id="12837875" author="tsmith" created="Wed, 24 Feb 2010 16:50:10 +0000"  >&lt;blockquote&gt;&lt;p&gt;I agree. I&apos;ll mull over how to do it... unless you&apos;re planning on consing up a patch &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I&apos;d love to, but don&apos;t have the free cycles at the moment &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/sad.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;How many threads do you pass through IW?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;honestly don&apos;t 100% know about the origin of the threads i&apos;m given&lt;br/&gt;
In general, they should be from a static pool, but may be dynamically allocated if the static pool runs out&lt;/p&gt;

&lt;p&gt;One thought i had recently was to control this more tightly by having a limited number of static threads that called IndexWriter methods in case that was the issue (but that would be a pretty big change)&lt;/p&gt;</comment>
                    <comment id="12837881" author="tsmith" created="Wed, 24 Feb 2010 17:02:06 +0000"  >&lt;p&gt;latest profile dump has pointed out a non-lucene issue as causing some memory growth&lt;/p&gt;

&lt;p&gt;so feel free to drop down priority&lt;/p&gt;

&lt;p&gt;however it seems like using the bytepool for the stored fields would be good overall&lt;/p&gt;</comment>
                    <comment id="12837896" author="mikemccand" created="Wed, 24 Feb 2010 17:28:27 +0000"  >&lt;p&gt;Yeah it would be good to make the pool shared...&lt;/p&gt;

&lt;p&gt;It still bugs me that yourkit is claiming DW was using 256 MB when you&apos;ve got a 64 MB ram buffer.... that&apos;s spooky.&lt;/p&gt;</comment>
                    <comment id="12837919" author="tsmith" created="Wed, 24 Feb 2010 18:10:46 +0000"  >&lt;p&gt;another note is that this was on 64 bit vm&lt;/p&gt;

&lt;p&gt;i&apos;ve noticed that all the memsize calculations assume 4 byte pointers, so perhaps that can lead to more memory being used that would otherwise be expected (although 256 MB is still well over the 2X mem use that would potentially be expected in that case)&lt;/p&gt;
</comment>
                    <comment id="12838017" author="tsmith" created="Wed, 24 Feb 2010 22:04:11 +0000"  >&lt;p&gt;i&apos;m working up a patch for the shared byteblock pool for stored field buffers (found a few cycles)&lt;/p&gt;</comment>
                    <comment id="12838387" author="tsmith" created="Thu, 25 Feb 2010 16:01:44 +0000"  >&lt;p&gt;Here&apos;s a patch for using a pool for stored fields buffers&lt;/p&gt;</comment>
                    <comment id="12838975" author="mikemccand" created="Fri, 26 Feb 2010 18:34:53 +0000"  >&lt;p&gt;Patch looks great &amp;#8211; thanks Tim!&lt;/p&gt;

&lt;p&gt;A few fixes:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;I think you should pass false to allocator.getByteBlock in PerDocBuffer.newBuffer.  We don&apos;t want this allocator to track allocations because this storage used is transient (reset per doc).  We only pass true for things like the terms hash, that keeps allocating RAM until it&apos;s time to flush.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Can you also make the same changes to TermVectorsTermsWriter?  I think the same hazard applies to it.  It should just use the same pool.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Can you add a CHANGES entry?&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Thanks!&lt;/p&gt;</comment>
                    <comment id="12838976" author="tsmith" created="Fri, 26 Feb 2010 18:38:25 +0000"  >&lt;p&gt;I&apos;ll work up another patch&lt;/p&gt;

&lt;p&gt;might take me a few minutes to get my head wrapped around the TermVectorsTermsWriter stuff&lt;/p&gt;</comment>
                    <comment id="12838991" author="tsmith" created="Fri, 26 Feb 2010 19:26:24 +0000"  >&lt;p&gt;Here&apos;s a new patch with your suggestions&lt;/p&gt;</comment>
                    <comment id="12839010" author="mikemccand" created="Fri, 26 Feb 2010 20:13:52 +0000"  >&lt;p&gt;Looks great Tim, thanks!  I think it&apos;s ready to commit... I&apos;ll wait a day or two.&lt;/p&gt;</comment>
                    <comment id="12839488" author="mikemccand" created="Sun, 28 Feb 2010 19:50:33 +0000"  >&lt;p&gt;New rev attached with some small improvements:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;Changed RAMOutputStream.reset to set currentBuffer to null (frees up that 1 buffer), and call that instead of close.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Removed the separate PerDoc.recycle &amp;#8211; now I just do the recycle in the existing PerDoc.reset method&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Also, this patch changes the RAM accounting to count buffers allocated not bytes written to the file.  This is a good change, and I think may explain the too-much-memory-allocated problem you saw.  Ie if you write tiny docs, Lucene was thinking they consumed tiny memory (not the 1024 bytes that the 1 buffer uses) and thus was undercounting.  When mixed in with massive docs this can cause way too much RAM to be allocated.&lt;/p&gt;

&lt;p&gt;Have you been able to test if this patch resolves Lucene&apos;s part of the mem growth you were seeing?&lt;/p&gt;</comment>
                    <comment id="12839682" author="tsmith" created="Mon, 1 Mar 2010 14:07:57 +0000"  >&lt;p&gt;i haven&apos;t been able to fully replicate this issue in a unit test scenario, &lt;/p&gt;

&lt;p&gt;however it will definitely resolve that 40M of ram that was allocated and never released for the RAMFiles on the StoredFieldsWriter (keeping that bound to the configured memory size)&lt;/p&gt;</comment>
                    <comment id="12841378" author="mikemccand" created="Thu, 4 Mar 2010 16:46:20 +0000"  >&lt;p&gt;Thanks Tim!&lt;/p&gt;</comment>
                    <comment id="12864042" author="kimchy" created="Tue, 4 May 2010 23:08:16 +0100"  >&lt;p&gt;Is there a chance that this can also be applied to 3.0.2 / 3.1? It would be really helpful to get this as soon as possible in the next Lucene version.&lt;/p&gt;
</comment>
                    <comment id="12864066" author="mikemccand" created="Wed, 5 May 2010 00:24:17 +0100"  >&lt;p&gt;I&apos;ll merge back to 2.9.x / 3.0.x / 3.1.x.&lt;/p&gt;</comment>
                    <comment id="12864287" author="mikemccand" created="Wed, 5 May 2010 12:44:21 +0100"  >&lt;p&gt;Merged fix for this back to 29x, 30x.  It was already on 3x since we cut that branch after this landed.&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12437429" name="LUCENE-2283.patch" size="13357" author="mikemccand" created="Sun, 28 Feb 2010 19:50:33 +0000" />
                    <attachment id="12437230" name="LUCENE-2283.patch" size="13760" author="tsmith" created="Fri, 26 Feb 2010 19:26:24 +0000" />
                    <attachment id="12437013" name="LUCENE-2283.patch" size="12072" author="tsmith" created="Thu, 25 Feb 2010 16:01:44 +0000" />
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>3.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Wed, 24 Feb 2010 11:00:50 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>11509</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>25442</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                    <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                <customfieldname>Time in Status</customfieldname>
                <customfieldvalues>
                    
                </customfieldvalues>
            </customfield>
                            </customfields>
    </item>
</channel>
</rss>