<!-- 
RSS generated by JIRA (5.2.8#851-sha1:3262fdc28b4bc8b23784e13eadc26a22399f5d88) at Tue Jul 16 13:00:48 UTC 2013

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/LUCENE-1693/LUCENE-1693.xml?field=key&field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>5.2.8</version>
        <build-number>851</build-number>
        <build-date>26-02-2013</build-date>
    </build-info>

<item>
            <title>[LUCENE-1693] AttributeSource/TokenStream API improvements</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-1693</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;This patch makes the following improvements to AttributeSource and&lt;br/&gt;
TokenStream/Filter:&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;introduces interfaces for all Attributes. The corresponding&lt;br/&gt;
  implementations have the postfix &apos;Impl&apos;, e.g. TermAttribute and&lt;br/&gt;
  TermAttributeImpl. AttributeSource now has a factory for creating&lt;br/&gt;
  the Attribute instances; the default implementation looks for&lt;br/&gt;
  implementing classes with the postfix &apos;Impl&apos;. Token now implements&lt;br/&gt;
  all 6 TokenAttribute interfaces.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;new method added to AttributeSource:&lt;br/&gt;
  addAttributeImpl(AttributeImpl). Using reflection it walks up in the&lt;br/&gt;
  class hierarchy of the passed in object and finds all interfaces&lt;br/&gt;
  that the class or superclasses implement and that extend the&lt;br/&gt;
  Attribute interface. It then adds the interface-&amp;gt;instance mappings&lt;br/&gt;
  to the attribute map for each of the found interfaces.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;removes the set/getUseNewAPI() methods (including the standard&lt;br/&gt;
  ones). Instead it is now enough to only implement the new API,&lt;br/&gt;
  if one old TokenStream implements still the old API (next()/next(Token)),&lt;br/&gt;
  it is wrapped automatically. The delegation path is determined via&lt;br/&gt;
  reflection (the patch determines, which of the three methods was&lt;br/&gt;
  overridden).&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Token is no longer deprecated, instead it implements all 6 standard&lt;br/&gt;
  token interfaces (see above). The wrapper for next() and next(Token)&lt;br/&gt;
  uses this, to automatically map all attribute interfaces to one&lt;br/&gt;
  TokenWrapper instance (implementing all 6 interfaces), that contains&lt;br/&gt;
  a Token instance. next() and next(Token) exchange the inner Token&lt;br/&gt;
  instance as needed. For the new incrementToken(), only one&lt;br/&gt;
  TokenWrapper instance is visible, delegating to the currect reusable&lt;br/&gt;
  Token. This API also preserves custom Token subclasses, that maybe&lt;br/&gt;
  created by very special token streams (see example in Backwards-Test).&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;AttributeImpl now has a default implementation of toString that uses&lt;br/&gt;
  reflection to print out the values of the attributes in a default&lt;br/&gt;
  formatting. This makes it a bit easier to implement AttributeImpl,&lt;br/&gt;
  because toString() was declared abstract before.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Cloning is now done much more efficiently in&lt;br/&gt;
  captureState. The method figures out which unique AttributeImpl&lt;br/&gt;
  instances are contained as values in the attributes map, because&lt;br/&gt;
  those are the ones that need to be cloned. It creates a single&lt;br/&gt;
  linked list that supports deep cloning (in the inner class&lt;br/&gt;
  AttributeSource.State). AttributeSource keeps track of when this&lt;br/&gt;
  state changes, i.e. whenever new attributes are added to the&lt;br/&gt;
  AttributeSource. Only in that case will captureState recompute the&lt;br/&gt;
  state, otherwise it will simply clone the precomputed state and&lt;br/&gt;
  return the clone. restoreState(AttributeSource.State) walks the&lt;br/&gt;
  linked list and uses the copyTo() method of AttributeImpl to copy&lt;br/&gt;
  all values over into the attribute that the source stream&lt;br/&gt;
  (e.g. SinkTokenizer) uses. &lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Tee- and SinkTokenizer were deprecated, because they use&lt;br/&gt;
Token instances for caching. This is not compatible to the new API&lt;br/&gt;
using AttributeSource.State objects. You can still use the old&lt;br/&gt;
deprecated ones, but new features provided by new Attribute types&lt;br/&gt;
may get lost in the chain. A replacement is a new TeeSinkTokenFilter,&lt;br/&gt;
which has a factory to create new Sink instances, that have compatible&lt;br/&gt;
attributes. Sink instances created by one Tee can also be added to&lt;br/&gt;
another Tee, as long as the attribute implementations are compatible&lt;br/&gt;
(it is not possible to add a sink from a tee using one Token instance&lt;br/&gt;
to a tee using the six separate attribute impls). In this case UOE is thrown.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;The cloning performance can be greatly improved if not multiple&lt;br/&gt;
AttributeImpl instances are used in one TokenStream. A user can&lt;br/&gt;
e.g. simply add a Token instance to the stream instead of the individual&lt;br/&gt;
attributes. Or the user could implement a subclass of AttributeImpl that&lt;br/&gt;
implements exactly the Attribute interfaces needed. I think this&lt;br/&gt;
should be considered an expert API (addAttributeImpl), as this manual&lt;br/&gt;
optimization is only needed if cloning performance is crucial. I ran&lt;br/&gt;
some quick performance tests using Tee/Sink tokenizers (which do&lt;br/&gt;
cloning) and the performance was roughly 20% faster with the new&lt;br/&gt;
API. I&apos;ll run some more performance tests and post more numbers then.&lt;/p&gt;

&lt;p&gt;Note also that when we add serialization to the Attributes, e.g. for&lt;br/&gt;
supporting storing serialized TokenStreams in the index, then the&lt;br/&gt;
serialization should benefit even significantly more from the new API&lt;br/&gt;
than cloning. &lt;/p&gt;

&lt;p&gt;This issue contains one backwards-compatibility break:&lt;br/&gt;
TokenStreams/Filters/Tokenizers should normally be final&lt;br/&gt;
(see &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-1753&quot; title=&quot;Make not yet final core/contrib TokenStream/Filter implementations final&quot;&gt;&lt;del&gt;LUCENE-1753&lt;/del&gt;&lt;/a&gt; for the explaination). Some of these core classes are &lt;br/&gt;
not final and so one could override the next() or next(Token) methods.&lt;br/&gt;
In this case, the backwards-wrapper would automatically use&lt;br/&gt;
incrementToken(), because it is implemented, so the overridden&lt;br/&gt;
method is never called. To prevent users from errors not visible&lt;br/&gt;
during compilation or testing (the streams just behave wrong),&lt;br/&gt;
this patch makes all implementation methods final&lt;br/&gt;
(next(), next(Token), incrementToken()), whenever the class&lt;br/&gt;
itsself is not final. This is a BW break, but users will clearly see,&lt;br/&gt;
that they have done something unsupoorted and should better&lt;br/&gt;
create a custom TokenFilter with their additional implementation&lt;br/&gt;
(instead of extending a core implementation).&lt;/p&gt;

&lt;p&gt;For further changing contrib token streams the following procedere should be used:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;rewrite and replace next(Token)/next() implementations by new API&lt;/li&gt;
	&lt;li&gt;if the class is final, no next(Token)/next() methods needed (must be removed!!!)&lt;/li&gt;
	&lt;li&gt;if the class is non-final add the following methods to the class:
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
      /** @deprecated Will be removed in Lucene 3.0. This method is &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt;, as it should
       * not be overridden. Delegates to the backwards compatibility layer. */
      &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; Token next(&lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; Token reusableToken) &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; java.io.IOException {
        &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;super&lt;/span&gt;.next(reusableToken);
      }

      /** @deprecated Will be removed in Lucene 3.0. This method is &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt;, as it should
       * not be overridden. Delegates to the backwards compatibility layer. */
      &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; Token next() &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; java.io.IOException {
        &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;super&lt;/span&gt;.next();
      }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Also the incrementToken() method must be final in this case&lt;br/&gt;
(and the new method end() of &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-1448&quot; title=&quot;add getFinalOffset() to TokenStream&quot;&gt;&lt;del&gt;LUCENE-1448&lt;/del&gt;&lt;/a&gt;)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
                <environment></environment>
            <key id="12427984">LUCENE-1693</key>
            <summary>AttributeSource/TokenStream API improvements</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png">Closed</status>
                    <resolution id="1">Fixed</resolution>
                                <assignee username="michaelbusch">Michael Busch</assignee>
                                <reporter username="michaelbusch">Michael Busch</reporter>
                        <labels>
                    </labels>
                <created>Tue, 16 Jun 2009 10:45:15 +0100</created>
                <updated>Thu, 2 May 2013 03:29:26 +0100</updated>
                    <resolved>Fri, 24 Jul 2009 22:48:12 +0100</resolved>
                                            <fixVersion>2.9</fixVersion>
                                <component>modules/analysis</component>
                        <due></due>
                    <votes>0</votes>
                        <watches>1</watches>
                                                    <comments>
                    <comment id="12720023" author="michaelbusch" created="Tue, 16 Jun 2009 10:52:03 +0100"  >&lt;p&gt;Patch that includes all mentioned improvements, but needs cleanup, documentation improvements and junits.&lt;/p&gt;</comment>
                    <comment id="12720031" author="thetaphi" created="Tue, 16 Jun 2009 11:04:35 +0100"  >&lt;p&gt;Why do you add a new class &quot;SmallToken&quot;? I think it should be the good old deprecated &quot;Token&quot;.&lt;/p&gt;

&lt;p&gt;In my opinion, I would not deprecate the old Token, instead the default factory should always create Token instead of all these default *Impl attributes.&lt;/p&gt;

&lt;p&gt;What was your concusion about my idea yesterday, to pass the Token around even with the old API and copy it on demand, if return value by next() is not the same? In this case, Token should be the only implementation of all standard attributes.&lt;/p&gt;</comment>
                    <comment id="12720046" author="michaelbusch" created="Tue, 16 Jun 2009 11:28:48 +0100"  >&lt;blockquote&gt;
&lt;p&gt;Why do you add a new class &quot;SmallToken&quot;? I think it should be the good old deprecated &quot;Token&quot;. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I should have added a &quot;no commit&quot; comment to SmallToken. I just used it for performance tests, we don&apos;t have to commit it. It&apos;s an example of what an advanced user could do to speed up cloning. Token is still there and unchanged.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;In my opinion, I would not deprecate the old Token, instead the default factory should always create Token instead of all these default *Impl attributes.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yeah we shouldn&apos;t deprecate Token. Not sure about the default. It kind of depends on the use case. Cloning and memory is faster I think with only two or three Attributes compared to using the Token cloning is faster. But it&apos;d be simpler to not have all the Impl classes... hmm not sure... &lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;What was your concusion about my idea yesterday, to pass the Token around even with the old API and copy it on demand&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I don&apos;t think the indexer should know at all about Token? It should only use the interfaces so that we can maintain the full flexibility.&lt;br/&gt;
Also I don&apos;t really like the fact very much that some user might get a performance hit. I had the idea to throw the exception in incrementToken() to automatically being able to fallback to the old API. I think this is nice and gets rid of the explicit useNewAPI methods. The only drawback is still the fact that we have to implement both old and new APIs in Lucene&apos;s tokenizers and filters until we remove the old API. Grant won&apos;t like this &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; but I think this is better than the possible performance hit? Also we don&apos;t add new filters THAT often to Lucene and implementing both methods is often mostly copy&amp;amp;paste.&lt;/p&gt;

&lt;p&gt;Do you like the idea with the UnsupportedOperationException?&lt;/p&gt;</comment>
                    <comment id="12720052" author="thetaphi" created="Tue, 16 Jun 2009 11:42:46 +0100"  >&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;What was your concusion about my idea yesterday, to pass the Token around even with the old API and copy it on demand&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I don&apos;t think the indexer should know at all about Token? It should only use the interfaces so that we can maintain the full flexibility.&lt;br/&gt;
Also I don&apos;t really like the fact very much that some user might get a performance hit. I had the idea to throw the exception in incrementToken() to automatically being able to fallback to the old API. I think this is nice and gets rid of the explicit useNewAPI methods. The only drawback is still the fact that we have to implement both old and new APIs in Lucene&apos;s tokenizers and filters until we remove the old API. Grant won&apos;t like this  but I think this is better than the possible performance hit? Also we don&apos;t add new filters THAT often to Lucene and implementing both methods is often mostly copy&amp;amp;paste.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I did not mean to use Token in the indexer, I would like to remove the whole old API from the indexer (even the UOE). My idea would be the following:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Let the default interfaces implemented by Token, the default factory creates it for all requests to the default attributes&lt;/li&gt;
	&lt;li&gt;If somebody implements the new API, the indexer can use it without problems. If he doesn&apos;t, the default impl in TokenStream would call next(Token), using the token instance from AttributeSource. If the method returns with another Token instance (because it did not reuse, which is seldom I think), copy this returned token into the per instance AttributeSource Token.&lt;/li&gt;
	&lt;li&gt;The other way round, if one uses a TokenStream with the old API (own indexer, query parser,...), the TokenStream only implemented the new API, the deprectated old method would also have a default impl, ignoring the token supplied to next() and returning always the instance-token after calling incrementToken().&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Because of this, the indexer would always use the new API, the old API is wrapped. Core analyzers only need to implement the new methods (or could even stay with the old).&lt;/p&gt;

&lt;p&gt;There are two problems:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;If somebody does not implement either method, the call to incrementToken() will loop endless, there should be some early break with UOE in this case. Do not know how to implement this without inspecting the stack trace in the default methods.&lt;/li&gt;
	&lt;li&gt;Filters are a bit tricky: They could pass in both methods per default to the delegate. The problem, if one only implements one of the methods, the other passes down to the delegate, and doing nothing. But this could be fixed by delegating in the deprecated old method always to the new one (or vice versa).&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Hope this was clear, maybe I should create a patch.&lt;/p&gt;</comment>
                    <comment id="12720060" author="michaelbusch" created="Tue, 16 Jun 2009 11:56:12 +0100"  >&lt;blockquote&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;If somebody implements the new API, the indexer can use it without problems. If he doesn&apos;t, the default impl in TokenStream would call next(Token), using the token instance from AttributeSource. If the method returns with another Token instance (because it did not reuse, which is seldom I think), copy this returned token into the per instance AttributeSource Token.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;What if you currently have a filter that caches the token instance passed into next(Token) and returns a different one. If you then copy the values from the returned token into the per instance Attribute Token, you alter the cached one, because it&apos;s the same instance.&lt;/p&gt;

&lt;p&gt;Not sure if this is common or even allowed, but these are the sideeffects I&apos;m worried about.&lt;/p&gt;</comment>
                    <comment id="12720066" author="thetaphi" created="Tue, 16 Jun 2009 12:04:43 +0100"  >&lt;blockquote&gt;
&lt;p&gt;What if you currently have a filter that caches the token instance passed into next(Token) and returns a different one. If you then copy the values from the returned token into the per instance Attribute Token, you alter the cached one, because it&apos;s the same instance.&lt;/p&gt;

&lt;p&gt;Not sure if this is common or even allowed, but these are the sideeffects I&apos;m worried about.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;See:&lt;br/&gt;
&lt;a href=&quot;http://lucene.apache.org/java/2_4_1/api/org/apache/lucene/analysis/TokenStream.html&quot; class=&quot;external-link&quot;&gt;http://lucene.apache.org/java/2_4_1/api/org/apache/lucene/analysis/TokenStream.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;It states in next(Token): Also, the producer must make no assumptions about a Token after it has been returned: the caller may arbitrarily change it. If the producer needs to hold onto the token for subsequent calls, it must clone() it before storing it. Note that a TokenFilter is considered a consumer.&lt;/p&gt;

&lt;p&gt;So I see no problem with this.&lt;/p&gt;</comment>
                    <comment id="12720076" author="shaie" created="Tue, 16 Jun 2009 12:34:12 +0100"  >&lt;p&gt;I have a couple of TokenFilters that work that way - get a Token from the wrapped TokenStream, cache it (cloning) and return another Token instead. Especially as the Tokenizer fills the Token w/ characters, I think that cloning is the only option if you want to hold on to a Token that was returned by a wrapped TokenStream.&lt;/p&gt;</comment>
                    <comment id="12720081" author="thetaphi" created="Tue, 16 Jun 2009 12:51:08 +0100"  >&lt;p&gt;If you clone, you would not fall into the mentioned problem. The reuseable Token passed to next() is owned by the caller.&lt;/p&gt;</comment>
                    <comment id="12720196" author="michaelbusch" created="Tue, 16 Jun 2009 16:41:55 +0100"  >&lt;p&gt;But, the additional copying would affect performance in Shai&apos;s case. The same with CachingTokenFilter and Tee/Sink filer/tokenizer. I don&apos;t think anymore it&apos;s such a corner case to not return the reusable token.&lt;/p&gt;

&lt;p&gt;From a user&apos;s perspective, the UnsupportedOperationException solution and your solution should not be distinguishable. The advantage of your solution is cleaner, less cluttered code, and it removes the burden from the developers to implement both APIs in the transition period.The disadvantage is a possible performance hit in the cases mentioned above.&lt;br/&gt;
The advantage of the UOE solution is no performance hit for users of the old API, the disadvantage the need to implement both APIs.&lt;/p&gt;

&lt;p&gt;Even though I think your solution is very elegant, Uwe, I&apos;m in favor of the one that doesn&apos;t affect performance significantly. What do you or others think about the possible performance hit? Not a big deal? Or worth keeping the old code around for a while?&lt;/p&gt;</comment>
                    <comment id="12720198" author="gsingers" created="Tue, 16 Jun 2009 16:44:49 +0100"  >&lt;p&gt;Just linking some of these related issues together such that we can make sure we cover all of the bases with this new Token Stream stuff&lt;/p&gt;</comment>
                    <comment id="12720222" author="mikemccand" created="Tue, 16 Jun 2009 17:28:09 +0100"  >&lt;blockquote&gt;&lt;p&gt;What do you or others think about the possible performance hit?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I haven&apos;t been able to keep up with this, but this one caught my eye!&lt;/p&gt;

&lt;p&gt;I think performance of analysis, especially the common case (all my tokenizers are new, I&apos;m not cloning, etc.) is important.  It sounds like this performance hit was specifically on cloning... but still we should try not to lose performance if we can help it.&lt;/p&gt;</comment>
                    <comment id="12720227" author="gsingers" created="Tue, 16 Jun 2009 17:36:54 +0100"  >&lt;p&gt;I think cloning is more prevalent than people think.  For instance, the RemoveDuplicatesTokenFilter in Solr is in the default schema and is thus used, albeit naively, by a whole lot of people.  Furthermore, I&apos;ve seen quite a few apps that need to buffer tokens before spitting them out, things like phrase detection, n-grams, entity extraction, sentence detection, part of speech detection, parsing, etc.&lt;/p&gt;</comment>
                    <comment id="12720230" author="shaie" created="Tue, 16 Jun 2009 17:39:18 +0100"  >&lt;p&gt;Perhaps I&apos;m missing something, but I don&apos;t understand what is the performance hit that&apos;s been discussed. If you don&apos;t need to clone a Token in order to cache it, don&apos;t do it and I agree we shouldn&apos;t require it or anything.&lt;/p&gt;

&lt;p&gt;In my scenario, I do need to cache a Token, and then on subsequent calls I use it to generate new Tokens. In next(Token) I populate the given Token with information from the cached Token. That goes until it&apos;s consumed, and then I call super.next(Token), and copy its content into the cached Token (I don&apos;t use clone() since that allocates a new Token which i don&apos;t need).&lt;/p&gt;

&lt;p&gt;I don&apos;t suffer in my scenario, I&apos;m quite happy w/ how it works and expect to be able to do the exact same thing when I switch to the new API. If that will be the case, then it&apos;s fine with me.&lt;/p&gt;</comment>
                    <comment id="12720236" author="michaelbusch" created="Tue, 16 Jun 2009 17:54:07 +0100"  >&lt;blockquote&gt;
&lt;p&gt;Perhaps I&apos;m missing something, but I don&apos;t understand what is the performance hit that&apos;s been discussed.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;We&apos;re discussing here basically how we want to treat backwards-compatibility for the old API. In the patch attached here the default implementation of TokenStream.incrementToken() throws a UnsupportedOperationException. If the consumer, i.e. indexer, hits that exception it knows to fallback to the old API. This change has the same disadvantage as trunk currently has: for all streams/filters in the core and contrib we have to implement both the old next(Token) and the new incrementToken().&lt;/p&gt;

&lt;p&gt;To avoid this disadvantage Uwe is proposing an elegant change: each TokenStreams owns a reusableToken, which is shared with the consumer (indexer). That&apos;s easily possible, because the patch here changes Token to implement all TokenAttribute interfaces. The default implementation of incrementToken() then calls next(Token) and compares the Token next(Token) returns with the reusableToken. If it&apos;s not identical then we need to copy all values from the returned Token into the reusableToken, so that the consumer gets the values. This additional copying is what concerns me in terms of performance. &lt;br/&gt;
Currently e.g. SinkTokenizer always returns a different Token, which means we would always have to copy. We could change SinkTokenizer to not clone, but call Token.reinit(), which copies. But I think there might be a significant amount of filters out there that do similar things. &lt;/p&gt;</comment>
                    <comment id="12720237" author="thetaphi" created="Tue, 16 Jun 2009 17:55:23 +0100"  >&lt;p&gt;Grant: But with the new TokenStream API, you must always clone the attributes before caching, because the TokenFilter only has &lt;b&gt;one&lt;/b&gt; final &quot;working&quot; instance per Attribute whose contents change all the time.&lt;br/&gt;
In this case it would not be better than wrapping next(Token) with the new api and feeding in the current instance token and copy it back into the instance-one if next(Token) returns a different instance.&lt;/p&gt;</comment>
                    <comment id="12720239" author="thetaphi" created="Tue, 16 Jun 2009 18:02:00 +0100"  >&lt;blockquote&gt;&lt;p&gt;Currently e.g. SinkTokenizer always returns a different Token, which means we would always have to copy. We could change SinkTokenizer to not clone, but call Token.reinit(), which copies. But I think there might be a significant amount of filters out there that do similar things.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;If SinkTokenizer does this, it would not be very fast even with the old API because it creates a lot of new instances instead of reusing? So the problem here is in SinkTokenizer.&lt;/p&gt;</comment>
                    <comment id="12720240" author="shaie" created="Tue, 16 Jun 2009 18:03:35 +0100"  >&lt;p&gt;Thanks Michael. Given that, I don&apos;t think my private scenario will be affected in terms of performance, since my cached token is a different instance then the one that&apos;s been passed on the filters, and I just populate the instance passed on the filters with data from the cached token. So besides me copying char[], the consumer will still share the same instance.&lt;/p&gt;

&lt;p&gt;Perhaps we should document that as a best practice or something, in light of this proposal?&lt;/p&gt;</comment>
                    <comment id="12720244" author="gsingers" created="Tue, 16 Jun 2009 18:15:13 +0100"  >&lt;p&gt;Right, SinkTokenizer is only really cost effective when the number of Tokens that are &quot;teed&quot; is roughly less than 1/2, according to my basic tests.  After that, just doing a copy field approach is fine.&lt;/p&gt;</comment>
                    <comment id="12720247" author="thetaphi" created="Tue, 16 Jun 2009 18:20:12 +0100"  >&lt;p&gt;I wanted to add one additional advantage of my Proposal:&lt;br/&gt;
With it, it is possible (if correctly implemented on the Filter side, must think about it one more time), to mix Filters and TokenStreams together regardless if they implement the new or old API. With the UOE or the current trunk solution, all Filters/Streams in the chain must share the same setting useNewAPI or the same implementation state!&lt;br/&gt;
I would suggest to add a note in the JavaDocs of the deprecated next(Token) method, that it should for optimal performance always return the given Token or null.&lt;/p&gt;</comment>
                    <comment id="12720276" author="michaelbusch" created="Tue, 16 Jun 2009 19:15:45 +0100"  >&lt;p&gt;The in 2.4 released CachingTokenFilter and SinkTokenizer do the cloning, so it was kind of our recommended implementation. Chances are that people have similar implementations. They would see a possible performance hit. Do you think that&apos;s ok, Uwe?&lt;/p&gt;</comment>
                    <comment id="12720278" author="thetaphi" created="Tue, 16 Jun 2009 19:31:16 +0100"  >&lt;p&gt;The problem goes further: &lt;br/&gt;
If the users move from the old Token API to the new one, they will get the same performance decrease. We only have one token instance per tokenizer chain with the new API. So they must also copy the values into their cache and back or think about a completely new implementation of what they have done before.&lt;/p&gt;

&lt;p&gt;I will post a patch soon (based on your patch), that implements my thoughts and we could compare.&lt;/p&gt;</comment>
                    <comment id="12720279" author="michaelbusch" created="Tue, 16 Jun 2009 19:31:56 +0100"  >&lt;p&gt;Also what happens if a user starts using the new API and has a TokenStream that adds a different implementation of one or more of the TokenAttribute interfaces?&lt;/p&gt;

&lt;p&gt;I think for this case you&apos;re proposing that the TokenStream will always add Token right away to the AttributeSource? But that takes away some of the flexibility of the new API?&lt;/p&gt;</comment>
                    <comment id="12720281" author="michaelbusch" created="Tue, 16 Jun 2009 19:36:01 +0100"  >&lt;blockquote&gt;
&lt;p&gt;If the users move from the old Token API to the new one, they will get the same performance decrease.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I&apos;m seeing a ~20% performance gain with using the new approach with one Token instance. Because the new implementation clones once in captureState(), and copies then in restoreState() using copyTo(), which is for Token implemented as reinit(), which copies all members.&lt;/p&gt;

&lt;p&gt;The old approach of SinkTokenizer and CachingTokenFilter clones twice. The second clone could be avoided with switching to reinit() also. The old API allows you to do both (double-cloning and clone/copy), the new API forces you to use the more efficient approach.&lt;/p&gt;</comment>
                    <comment id="12720293" author="michaelbusch" created="Tue, 16 Jun 2009 19:47:16 +0100"  >&lt;blockquote&gt;
&lt;p&gt;Also what happens if a user starts using the new API and has a TokenStream that adds a different implementation of one or more of the TokenAttribute interfaces? &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;You could use the initialize() method I put in there and a user can overwrite it to change the default behavior of putting a Token into the AttributeSource.&lt;/p&gt;</comment>
                    <comment id="12720435" author="thetaphi" created="Wed, 17 Jun 2009 01:21:25 +0100"  >&lt;p&gt;Hallo Michael,&lt;/p&gt;

&lt;p&gt;I played a little bit around. This patch implements my proposal:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;removes all deprecated API and corresponding wrapper from DocInverter and QueryParser.&lt;/li&gt;
	&lt;li&gt;The default TokenStream calls in each of the three possible method the next best one, wrapping the tokens with cloning, as described. Please note: As before, if one extends TokenStream/TokenFilter but does not override one of these methods, indexing will loop and stack overflow. I could add a test in initialize, that tests, if at last one of the methods is overridden (which is a runtime check). An example how this could be done is currently commented out (but was used for some other test).&lt;/li&gt;
	&lt;li&gt;I played a little bit, trying to only register the &quot;big&quot; Token instance in the stream, if one does not override incrementToken (see code commented out). But the problem is, that this is then backwards compatible only in one direction: consumers calling incrmentToken succeed always, but some outdated consumers alling next() or next(Token) then fail, because the instance may not be Token, if all producers implement incrementToken and TokenStreams would not register Token as impl. Because this does not work, the TokenStreams and Filters register Token as Impl and use it for wrapping. Because of this, the simple TokenAttribute impls are now useless.&lt;/li&gt;
	&lt;li&gt;There is currently no possibility to change the default Token impl, I will think about it during the night!&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I tested the attached patch with core (all tests pass, so &quot;new&quot; token streams work correct) and contrib/analyzers (all tests pass, so also old token streams work correct). You can even use mixed impls (not fully tested yet, but you can have e.g. a new-style tokenstream and filter it with an old-style tokenfilter).&lt;/p&gt;

&lt;p&gt;With this patch, all core tokenstreams could be updated, to only implement the new API and remove all next(Token) impls.&lt;/p&gt;

&lt;p&gt;Uwe&lt;/p&gt;</comment>
                    <comment id="12720440" author="michaelbusch" created="Wed, 17 Jun 2009 01:33:18 +0100"  >&lt;p&gt;I haven&apos;t review the patch yet, but I have a quick question:&lt;/p&gt;

&lt;p&gt;is your patch backwards-compatible if a user has a TokenStream or -Filter which returns a custom subclass of Token? And then another one in the chain casts to that subclass? Note that Token is not final. Also not sure how common this scenario is, just came to my mind.&lt;/p&gt;

&lt;p&gt;Also, can a user still use the AttributeFactory to use something else but Token?&lt;/p&gt;</comment>
                    <comment id="12720508" author="thetaphi" created="Wed, 17 Jun 2009 06:54:32 +0100"  >&lt;blockquote&gt;&lt;p&gt;is your patch backwards-compatible if a user has a TokenStream or -Filter which returns a custom subclass of Token? And then another one in the chain casts to that subclass? Note that Token is not final. Also not sure how common this scenario is, just came to my mind.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This is no problem; I can explain:&lt;br/&gt;
Returning something other than Token from next() only makes sense, if the &lt;b&gt;direct&lt;/b&gt; consumer can handle it. So A TokenStream that returns these tokens must then be wrapped by a TokenFilter that can handle these Tokens. If there would be any other TokenFilter in between, it is not guaranteed, that this TokenFilter also returns the special Token. When you have this direct relation, the calls from the Filter to the prducers method are directly without wrapping (because both implement the old API). At the point where the indexer consumes the TokenFilter on top, the custom Token is uninteresting and can safely copied into a conventional Token (which is done because nextToken!=attributeInstance).&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Also, can a user still use the AttributeFactory to use something else but Token?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;As noted in my patch description: This is not possible. One can add additional attributes and use them in his chain (even when old filters are in between, which only handle the Token instance). TokenStream and TokenFilter creates a Token() instance on initialize() and call AddAttributeImpl(). After doing this, it checks, if all Attributes are then subclass of Token, and calls getAttribute(TermAttribute) is called and the result casted to Token (which then should be the same).&lt;/p&gt;

&lt;p&gt;One could change this behaviour if he overrides initialize() in one of his classes, but then another TokenSteam/Filter in the chain also initializing, will see, that one of the instances is &lt;b&gt;not&lt;/b&gt; Token and will throw a RuntimeException. I tried everything, to be able to handle both pathes (old -&amp;gt; new API, new -&amp;gt; old API), TokenStream and TokenFilter must have a Token instance. In 3.0 or later this can be removed and we will only use the factory to init the attributes.&lt;/p&gt;

&lt;p&gt;In my opinion, this is not a problem, because one could still add custom attributes to his chain and the best: he can mix old and new tokenstreams in one chain as he want. The missing flexibility in modifying the instances of Tokenattributes are in my opinion not important (and one instance initializes faster than 5).&lt;/p&gt;</comment>
                    <comment id="12720522" author="michaelbusch" created="Wed, 17 Jun 2009 07:44:29 +0100"  >&lt;p&gt;I quickly hacked a tool demonstrating my concerns.&lt;/p&gt;

&lt;p&gt;Running the attached tool on trunk+my patch yields the following output:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;new
tokenstream --&amp;gt; proper noun
api
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The output is identical if the tool is run on Lucene 2.4.&lt;/p&gt;


&lt;p&gt;Running the same tool using trunk+uwe&apos;s patch yields:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;new
tokenstream
api
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This tool might not make much sense, but it shows in what unexpected ways people might use these APIs. It doesn&apos;t break API compatibility, but changes runtime behavior - in this case if users have their own subclasses of Token.&lt;/p&gt;</comment>
                    <comment id="12720524" author="shaie" created="Wed, 17 Jun 2009 07:52:28 +0100"  >&lt;p&gt;Doesn&apos;t this mean that we need to change all our Core TokenStream/TokenFilter impls to use clone() instead of instantiating a Token themselves (i.e., using one of its ctors)? If we use clone() you shouldn&apos;t experience that problem because the type of the cloned Token will be your subclass, and any core filter/stream can still work with Token and just use its methods.&lt;/p&gt;

&lt;p&gt;We can also add a newToken() method to Token, and let the Token extensions return a new Token instance of their type. That is like clone() only it won&apos;t copy any characters and initialize fields.&lt;/p&gt;

&lt;p&gt;If we think Token should be extended, I think we should also add proper documentation to TokenStream that mentions this possible way of using and our recommended approach.&lt;/p&gt;</comment>
                    <comment id="12720529" author="michaelbusch" created="Wed, 17 Jun 2009 08:23:40 +0100"  >&lt;p&gt;I don&apos;t think we mention subclassing of Token really in the documentation. We also certainly don&apos;t prevent it. The tool I wrote works fine with 2.4, if you add other filters to the chain it might not work anymore. But since we don&apos;t promise that subclassing of Token works everywhere, that&apos;s probably fine.&lt;/p&gt;

&lt;p&gt;We&apos;re deprecating the old API anyway, so we shouldn&apos;t have to introduce new stuff to fully support subclassing Token.&lt;/p&gt;

&lt;p&gt;My point here is just that this is a very complex API (even though it looks pretty simple). When I wrote the new TokenStream API patch end of last year I thought about all these possibilities of making backwards compatibility more elegant. But I wanted to be certain to not break any runtime behavior or affect performance negatively. Therefore I decided to not mess with the old API, but rather put the burden of implementing both APIs on the committers during the transition phase. I know this is somewhat annoying, on the other hand, how often do we really add new TokenFilters to the core? Often implementing incrementToken() takes 10 minutes if you already have next() implemented. Just copy&amp;amp;paste and change a few things.&lt;/p&gt;
</comment>
                    <comment id="12720530" author="michaelbusch" created="Wed, 17 Jun 2009 08:25:22 +0100"  >&lt;p&gt;But I&apos;ll definitely buy Uwe a beer if he comes up with solution that is more elegant and doesn&apos;t have the mentioned disadvantages! &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                    <comment id="12720534" author="thetaphi" created="Wed, 17 Jun 2009 08:38:56 +0100"  >&lt;p&gt;Hi Michael,&lt;br/&gt;
in principle your test is invalid. It has other tokenfilters in the chain, which the user has no control on. With the two mentioned filters it may work, because they do not change the reuseableToken instance. But the API clearly states, that the reuseableToken must not be used and another one can be returned.&lt;br/&gt;
So this is really unsupported behaviour. If you remove the filters in between, it would work correct. And this could even fail with 2.4 if you put other tokenfilters in your chain.&lt;/p&gt;

&lt;p&gt;In my opinion, the advantages of the token reuse clearly overweigh the small problems with (unsupported) usage. The API does exactly, what is menthioned in the API Docs for 2.4.1.&lt;/p&gt;

&lt;p&gt;The main advantage is, that you can mix old and new filter instances and you loose nothing...&lt;/p&gt;</comment>
                    <comment id="12720538" author="michaelbusch" created="Wed, 17 Jun 2009 08:54:49 +0100"  >&lt;p&gt;OK, what about this sentence in Token.java:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
  When caching a reusable token, clone it. When injecting a cached token into a stream that can be reset, clone it again.
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This double-cloning is exactly what CachingTokenFilter and Tee/Sink do, so they preserve the actual Token class type.&lt;br/&gt;
You can easily construct an example similar to the tool I attached that uses these streams. &lt;/p&gt;
</comment>
                    <comment id="12720550" author="thetaphi" created="Wed, 17 Jun 2009 09:13:30 +0100"  >&lt;p&gt;OK, I have a solution:&lt;br/&gt;
I write a wrapper class (a reference) that implement all token attribute interfaces but pass this downto the wrapped Token/Subclass-of-Token. Instead of cloning the token when wrapping the return value of next(), I could simply put it into the wrapper. The instance keeps the same, only the delegate is different. Outside users or TokenStreams using the new API, will only see one instance that implements all interfaces.&lt;/p&gt;

&lt;p&gt;(in principle the same like your backwards-compatibility thing in the docinverter)&lt;/p&gt;

&lt;p&gt;Would this be an idea?&lt;/p&gt;</comment>
                    <comment id="12720562" author="michaelbusch" created="Wed, 17 Jun 2009 09:44:14 +0100"  >&lt;p&gt;For caching:&lt;br/&gt;
I guess you would have to implement the wrapper&apos;s clone() method such that it returns what delegate.clone() returns. This would put a clone of the original Token (or subclass) into the cache, instead a clone of the wrapper, which is good. Then the second clone also clones the original Token again and put&apos;s it into a second wrapper that the CachingTokenStream owns. Hmm complicated, but should work. &lt;/p&gt;

&lt;p&gt;Need to think more about if all mixes of old and new TokenSteams would work... and if this approach affects performance in any way or changes runtime behavior of corner cases...&lt;/p&gt;

&lt;p&gt;Gosh, this is like running a huge backwards-compatibility junit test suite in my head every time we consider a different approach. &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;I think you should try it out and see if you run into problems. This should not be much code to write. You might have to do tricks with Tee/Sink, if the sink is wrapped by a filter with the new API, but the tee wraps a stream with the old API, or vice versa.&lt;/p&gt;</comment>
                    <comment id="12720568" author="thetaphi" created="Wed, 17 Jun 2009 09:55:38 +0100"  >&lt;blockquote&gt;&lt;p&gt;I think you should try it out and see if you run into problems. This should not be much code to write. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I am working on that, I have a meeting now, after that. &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;You might have to do tricks with Tee/Sink, if the sink is wrapped by a filter with the new API, but the tee wraps a stream with the old API, or vice versa.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This is currently working without any problems, but I want to add a test-case, that explicitely chains some dummy-filters in deprecated and not-deprecated form and looks whats coming out. But it should work.&lt;/p&gt;</comment>
                    <comment id="12720571" author="michaelbusch" created="Wed, 17 Jun 2009 10:01:59 +0100"  >&lt;blockquote&gt;
&lt;p&gt;I am working on that, I have a meeting now, after that. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Good luck. I&apos;m off to bed...&lt;/p&gt;</comment>
                    <comment id="12720636" author="thetaphi" created="Wed, 17 Jun 2009 13:23:11 +0100"  >&lt;p&gt;Attached is a new patch, that implements the last idea:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;There is no more copying of Tokens, so the API should have the same speed (almost) as before.&lt;/li&gt;
	&lt;li&gt;Per default, the chain of TokenStreams/TokenFilters can be mixed completely (test that explicitely tests this is still missing), the drawback is, that there is only &lt;b&gt;one&lt;/b&gt; attribute instance called TokenWrapper (package private) that manages the exchange of the Token instance behind.&lt;/li&gt;
	&lt;li&gt;If the user knows, that all tokenizers in his JVM implement incrementToken and do not fallback to next(), he can increase speed by using the static setter setOnlyUseNewAPI(true). In this case, no single TokenWrapper is initialized and code will use the normal Attribute factory to generate the Attributes. If some old code is still available or your consumer calls next(), you will get an UOE during tokenization. The same happens, if you override initialize() and instantiate your attributes manually without super.initialize().&lt;/li&gt;
	&lt;li&gt;When the old API is removed, TokenWrapper and large parts inside TokenStream can be removed and incrementToken() made abstract. This is identical to setting onlyUseNewAPI to true.&lt;/li&gt;
	&lt;li&gt;the api setting can only be static, because the attribute instances are generated during construction of the streams and so a later downgrade to TokenWrapper is not possible.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Documentation inside this patch enforce, that at least all core tokenizers and consumers are conformant, so one must be able to set TokenStream.setOnlyUseNewAPI to true and then use StandardAnalyzer without any problem. When contrib is transformed, we can extend this to contrib.&lt;/p&gt;

&lt;p&gt;Because the code wraps the old API completely, all converted streams can be changed to only implement only incrementToken() using attributes. Super&apos;s TokenStream.next() and next(Token) manage the rest. There is no speed degradion by this, it is safe to remove (and all will be happy)!&lt;/p&gt;

&lt;p&gt;Uwe&lt;/p&gt;</comment>
                    <comment id="12720644" author="thetaphi" created="Wed, 17 Jun 2009 13:35:33 +0100"  >&lt;p&gt;Sorry, small bug in cloning inside next(): the POSToken-test was failing again. But now it works also correct.&lt;/p&gt;</comment>
                    <comment id="12720676" author="thetaphi" created="Wed, 17 Jun 2009 14:54:43 +0100"  >&lt;p&gt;Hi Michael,&lt;br/&gt;
I did not do any performance tests until now, I think you have the better knowledge about measuring tokenization performance. Important would be to compare perf of:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Old API with useNewAPI=true&lt;/li&gt;
	&lt;li&gt;Old API with useNewAPI=false&lt;/li&gt;
	&lt;li&gt;My impl with defaults (onlyUseNewAPI=false)&lt;/li&gt;
	&lt;li&gt;My impl with onlyUseNewAPI=true&lt;br/&gt;
For all tests, you should only use conformant streams (e.g. from core).&lt;br/&gt;
An good additional test would be to create a chain that has completely implemented incrementToken() and one only suplying next() for some chain entries.&lt;br/&gt;
Is this hard to do?&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="12720692" author="shaie" created="Wed, 17 Jun 2009 15:49:10 +0100"  >&lt;p&gt;You can run tokenize.alg which invokes the ReadTokenTask, which iterates on a TokenStream. You&apos;ll probably need to modify the .alg file to create a different analyzer/token stream each time, and I think this can be done by the &quot;rounds&quot; syntax in benchmark.&lt;/p&gt;</comment>
                    <comment id="12720794" author="michaelbusch" created="Wed, 17 Jun 2009 18:55:57 +0100"  >&lt;p&gt;I&apos;m looking at TokenStream.next():&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; Token next(&lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; Token reusableToken) &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; IOException {
    &lt;span class=&quot;code-comment&quot;&gt;// We don&apos;t actually use reusableToken, but still add &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;assert&lt;/span&gt;
&lt;/span&gt;    &lt;span class=&quot;code-keyword&quot;&gt;assert&lt;/span&gt; reusableToken != &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;;
    checkTokenWrapper();
    &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; next();
  }

  /** Returns the next token in the stream, or &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt; at EOS.
   *  @deprecated The returned Token is a &lt;span class=&quot;code-quote&quot;&gt;&quot;full &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt; copy&quot;&lt;/span&gt; (not
   *  re-used across calls to next()) but will be slower
   *  than calling {@link #next(Token)} instead.. */
  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; Token next() &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; IOException {
    checkTokenWrapper();
    &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (incrementToken()) {
      &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; Token token = (Token) tokenWrapper.delegate.clone();
      Payload p = token.getPayload();
      &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (p != &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;) {
        token.setPayload((Payload) p.clone());
      }
      &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; token;
    }
    &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;;
  }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This seems like a big performance hit for users of the old API, no? Now every single Token will be cloned, even if they implement next(Token), as soon as the users have one filter in the chain that doesn&apos;t implement the new API yet.&lt;/p&gt;</comment>
                    <comment id="12720809" author="thetaphi" created="Wed, 17 Jun 2009 19:20:07 +0100"  >&lt;p&gt;The code is almost identical to before, the old code also copied the token to make it a full private copy.&lt;br/&gt;
There are three modes of operation:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if incrementToken is implemented, docinverter will use it (the code always calls incrementToken, so no indirection)&lt;/li&gt;
	&lt;li&gt;if next(Token) is implemented, the docinverterwill call incrementToken which is forwarded to next(Token), which is cheap&lt;/li&gt;
	&lt;li&gt;if only next() is implemented, the docinverter will call incrementTojen, which forwards to next(Token) and this forwards to next(). But this is identical to before, only one indirection more: the old code got useNewAPI(false) and called next(Token) which forwarded to next()&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;So for indexing using the normal indexing components (docinverter), the code is never cloning more that with your code.&lt;/p&gt;

&lt;p&gt;There is one other case: if you have an old consumer calling nextToken(Token), the tokenizer only implemented incrementToken, then you will get a performance degradion. But this is not the indexing case, it is e.g. reusing the tokenizer in a very old e.g. QueryParser. I did not find a good way to pass directly for this special case to incrementToken(). The problem is also, that incrementToken uses the internal buffer and not the supplied buffer.&lt;/p&gt;</comment>
                    <comment id="12720811" author="thetaphi" created="Wed, 17 Jun 2009 19:26:26 +0100"  >&lt;p&gt;Ah I understand the problem: As I told, if a consumer (like a filter() calls next(Token) on the underlying filter), which does not implement this or implements the new API,  he will get a performance decrease because of cloning. I think, we should simply test this with the benchmarker. Mixing old and new API is always a performance decrease.&lt;/p&gt;</comment>
                    <comment id="12720820" author="michaelbusch" created="Wed, 17 Jun 2009 19:34:34 +0100"  >&lt;blockquote&gt;
&lt;p&gt;Ah I understand the problem: As I told, if a consumer (like a filter() calls next(Token) on the underlying filter), which does not implement this or implements the new API,  he will get a performance decrease because of cloning. I think, we should simply test this with the benchmarker. Mixing old and new API is always a performance decrease.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes that&apos;s what I mean. But I think this will almost be the most common use case: I would think most users have chains that mix core streams/filters with custom filters. Also I assume most users who need high performance switched from next() to next(Token) by now. These users will see a performance degradation, which I predict will be similar or worse as going back to using next(), unless they implement the new API in their filters right away.&lt;/p&gt;

&lt;p&gt;So those users will see a performance hit if they just do a drop-in replacement of the lucene jar. &lt;/p&gt;</comment>
                    <comment id="12720822" author="thetaphi" created="Wed, 17 Jun 2009 19:39:58 +0100"  >&lt;p&gt;I could change the calling chain:&lt;br/&gt;
incrementToken() calls next() calls next(Token), would this be better. next(Token) would per default set the delegate to the reuseable token. hmhm - thinking about it. Where is then the degradion?&lt;/p&gt;</comment>
                    <comment id="12720846" author="thetaphi" created="Wed, 17 Jun 2009 20:20:49 +0100"  >&lt;p&gt;I have a solution to build in some shortcuts:&lt;br/&gt;
in initialize I use reflection (see the earlier patch) to find out, which of the three methods is implemented (check if this.getClass().getMethod(name,params).getDeclaringClass() == TokenStream.class, when this is true, the method was &lt;b&gt;not&lt;/b&gt; overridden).&lt;br/&gt;
in incrementToken() the method checks if either next(Token) or next() is implemented and calls direct. The same in the other classes. next() should be ideally never called then.&lt;br/&gt;
I will post a patch later.&lt;/p&gt;</comment>
                    <comment id="12720849" author="markrmiller@gmail.com" created="Wed, 17 Jun 2009 20:29:09 +0100"  >&lt;p&gt;Should I wait to put in the Highlighter update till you guys are done here?&lt;/p&gt;</comment>
                    <comment id="12720854" author="thetaphi" created="Wed, 17 Jun 2009 20:53:29 +0100"  >&lt;blockquote&gt;&lt;p&gt;Should I wait to put in the Highlighter update till you guys are done here?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;You can start with highlighter, if this patch goes through, we can remove the next() methods from all tokenizers.&lt;br/&gt;
For consumers like the highlighter, there will be no need anymore to switch between old/new api. Just use the new API, it will also work with old tokenizers.&lt;/p&gt;
</comment>
                    <comment id="12720887" author="michaelbusch" created="Wed, 17 Jun 2009 21:54:49 +0100"  >&lt;p&gt;I&apos;m not convinced yet that we will be able to remove the implementations of next() and next(Token). &lt;br/&gt;
Mark, I&apos;m not familiar with what changes you need to make to the highlighter, but you should not rely yet on the fact that next() and nextToken() won&apos;t have to be implemented anymore.&lt;/p&gt;</comment>
                    <comment id="12720891" author="thetaphi" created="Wed, 17 Jun 2009 22:00:29 +0100"  >&lt;p&gt;Here my solution: The three default methods are now optimized to use the shortest path to the by subclasses implemented iteration method. The implemented iteration methods are determined by reflection in initialize().&lt;br/&gt;
Cloning now only done, if next() is directly called by a consumer, in all other cases the reuseableToken is used for passing the attributes around.&lt;/p&gt;

&lt;p&gt;The new TokenStream also checks in initialize, that one of the &quot;abstract&quot; methods is overridden. Because of this TestIndexWriter and the inverter singleton state was updated to at least have an empty incrementToken(). Because of this check, nobody can create a TokenStream, that loops indefinite after calling next() because no pseuso-abstract method was overridden. As incrementToken will be abstract in future, it must always be implemented, and this is what I have done.&lt;/p&gt;</comment>
                    <comment id="12720907" author="michaelbusch" created="Wed, 17 Jun 2009 22:27:00 +0100"  >&lt;p&gt;Slightly changes tool yields on 2.4 and identically on trunk + my patch:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;new
tokenstream --&amp;gt; proper noun
api
new
tokenstream --&amp;gt; proper noun
api
new
tokenstream
api
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;On trunk + your latest patch:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;new
tokenstream --&amp;gt; proper noun
api
new
tokenstream
api
Exception in thread &quot;main&quot; java.lang.ClassCastException: org.apache.lucene.util.AttributeSource$State
	at org.apache.lucene.analysis.SinkTokenizer.next(SinkTokenizer.java:97)
	at org.apache.lucene.analysis.TestCompatibility.consumeStream(TestCompatibility.java:97)
	at org.apache.lucene.analysis.TestCompatibility.main(TestCompatibility.java:90)

&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;It runs three tests. The first is good with your patch; the second doesn;t seem to preserve the right Token subclass; the third throws a ClassCastException. I haven&apos;t debugged why...&lt;/p&gt;</comment>
                    <comment id="12720913" author="michaelbusch" created="Wed, 17 Jun 2009 22:37:59 +0100"  >&lt;p&gt;You can probably fix CachingTokenFilter and tee/sink to behave correctly. But please remember that a user might have their own implementations of something like a CachingTokenFilter or tee/sink, which must keep working.&lt;/p&gt;</comment>
                    <comment id="12720919" author="michaelbusch" created="Wed, 17 Jun 2009 22:48:48 +0100"  >&lt;p&gt;Btw: SinkTokenizer in my patch has a small bug too. I need to throw a UOE in incrementToken() if it was filled using the old API.&lt;/p&gt;

&lt;p&gt;It should probably also throw a UOE when someone tries to fill it with both, old and new API streams. And that this is not allowed must be made clear in the javadocs.&lt;/p&gt;</comment>
                    <comment id="12720926" author="thetaphi" created="Wed, 17 Jun 2009 22:59:36 +0100"  >&lt;p&gt;Exactly: The problem is in SinkTokenizer. when calling next(Token)  the result is casted to Token, which does not work (the iterator only contains either Tokens or States, dependent on what was added. As SinkTokenizer and TeeTokenFilter may use different APIs it crahes.&lt;br/&gt;
The problem with the test is, that depending on chaining with old/new APIs the iter may conatin wron type. This can be fixed by removing next(Token) (preferred) or incrementToken() . The problem is that dependent on chaining it is not clear which method is called and the new/old API should not share the same state information.&lt;/p&gt;

&lt;p&gt;Because the problem is related new/old API, we should simply remove the old API from both filters, so they share the same instances in all cases! Then we do not need UOE.&lt;/p&gt;

&lt;p&gt;I will look into and check, why the Token in the second test is not preserverd&lt;/p&gt;</comment>
                    <comment id="12720952" author="thetaphi" created="Wed, 17 Jun 2009 23:19:09 +0100"  >&lt;p&gt;The second test does not work, because it always uses per default incrementToken.&lt;/p&gt;

&lt;p&gt;By the way, the APIdocs and behaviour changed with these three classes, TeeTokenFilter, SinkTokenizer and CachingTokenFilter: e.g. getTokens() does not return what is noted. For backwards-compatiblility we should deprecate the current versions of these class &lt;span class=&quot;error&quot;&gt;&amp;#91;and only let them implement next(Token)&amp;#93;&lt;/span&gt;. They can then be used even together with the new API, but they always work on Token instances. When I remove incrementToken from them your test passes complete.&lt;br/&gt;
For the new API there should be new classes, that use attributesource and restorestate to cache and so on.&lt;/p&gt;

&lt;p&gt;But for current backwards compatibility (you mentioned, somebody have written a similar thing): If the user&apos;s class only uses next(Token) it will work as before. The problem is mixed implementations of old/new API and different cache contents. This is not a problem of my proposal!&lt;/p&gt;

&lt;p&gt;Again: We should remove the double implementations everywhere. In these special cases with caches, where the cache should contain a specific class (Tokens or AttributeSource.State), two classes are needed, one deprecated.&lt;/p&gt;

&lt;p&gt;But: what do you think about my latest patch in general?&lt;/p&gt;</comment>
                    <comment id="12720978" author="thetaphi" created="Thu, 18 Jun 2009 00:10:59 +0100"  >&lt;p&gt;Small updates, before I go to sleep. This patch removes the incrementTokenAPI from the three caching classes. It also fixes the double cloning of the payload in next() when the token is cloned directly.&lt;br/&gt;
There is still one small problem, that your test &amp;#8211; I hate it... &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/sad.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; &amp;#8211; fails again, if I remove next(Token) from StopFilter or LowerCaseFilter.&lt;/p&gt;</comment>
                    <comment id="12720984" author="michaelbusch" created="Thu, 18 Jun 2009 00:19:13 +0100"  >&lt;p&gt;Go to bed, I&apos;ll review later... in meetings now...&lt;/p&gt;</comment>
                    <comment id="12721083" author="thetaphi" created="Thu, 18 Jun 2009 07:18:47 +0100"  >&lt;p&gt;Sorry, last patch was invalid (did not compile), I forgot to to revert some changes before posting.&lt;br/&gt;
Attached patch has still problems in TeeTokenStream, SinkTokenizer and CachingTokenFilter (see before), but fixes:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;double cloning of payloads&lt;/li&gt;
	&lt;li&gt;the first of your tests works correct, even if i remove next() from StopFilter and/or LowercaseFilter&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="12721135" author="michaelbusch" created="Thu, 18 Jun 2009 10:16:37 +0100"  >&lt;blockquote&gt;
&lt;p&gt;For backwards-compatiblility we should deprecate the current versions of these class &lt;span class=&quot;error&quot;&gt;&amp;#91;and only let them implement next(Token)&amp;#93;&lt;/span&gt;. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I agree. With my patch the Tee/Sink stuff doesn&apos;t work in all situations either, when the new API is used. We need to deprecate tee/sink and write a new class that implements the same functionality with the new API.&lt;/p&gt;</comment>
                    <comment id="12721142" author="thetaphi" created="Thu, 18 Jun 2009 10:39:00 +0100"  >&lt;p&gt;OK, we can merge our patches then! At the moement I see no real show-stoppers with the current aproach, have you tested thoroughly and measured performance? All tests from core and contrib/analyzers pass, the problems with your last TestCompatibility.java are Tee/Sink problems.&lt;br/&gt;
The interesting part (if we stay with my not-so-elegant-anymore solution because of reflections hacks), would be to remove the deprecated next(Token) methods from core streams, which would be a great code cleanup!&lt;/p&gt;</comment>
                    <comment id="12721158" author="thetaphi" created="Thu, 18 Jun 2009 11:02:19 +0100"  >&lt;p&gt;By the way, I tested Solr&apos;s token streams also after updating the lucene jar file. All tests pass (only some not related ones fail because of latest changes in Lucene trunk and some compile failures because of changes in no-released APIs).&lt;br/&gt;
Solrs TokenStreams are all programmed with the old-api, but they get inverted using incrementToken from our patch.&lt;br/&gt;
Also the solr query parser seems to work.&lt;/p&gt;</comment>
                    <comment id="12721214" author="thetaphi" created="Thu, 18 Jun 2009 13:05:25 +0100"  >&lt;p&gt;Again an update: Unified the reuseable tokens in the TokenWrapper.delegate. No it is always set after each action, so no state changes left out.&lt;/p&gt;</comment>
                    <comment id="12721281" author="gsingers" created="Thu, 18 Jun 2009 15:35:18 +0100"  >&lt;blockquote&gt;
&lt;p&gt;By the way, I tested Solr&apos;s token streams also after updating the lucene jar file. All tests pass (only some not related ones fail because of latest changes in Lucene trunk and some compile failures because of changes in no-released APIs).&lt;br/&gt;
Solrs TokenStreams are all programmed with the old-api, but they get inverted using incrementToken from our patch.&lt;br/&gt;
Also the solr query parser seems to work. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Did you look at the performance on this?&lt;/p&gt;</comment>
                    <comment id="12721426" author="thetaphi" created="Thu, 18 Jun 2009 20:27:11 +0100"  >&lt;p&gt;I only tested performance with the lucene benchmarker on the various standard analyzers. The tokenizer.alg produces after the patch the same results as before in almost the same time (time variations are bigger than differences). With an unmodified benchmarker, this is clear, benchmarkers tokenizer task call still the deprecated next(Token) and as all core analyzers still implement this directly, so there is no wrapping. I modified the tested tokenstreams and filters in core, that were used, and removed next(Token) and left only incrementToken() avalilable, in this case the speed difference was also not measureable in my configuration (Thinkpad T60, Core Duo, Win32). I also changed some of the filters to implement next(Token) only, others to only incrementToken(), to have a completely mixed old/new API chain, and still the same results (and same tokenization results, as seen in generated indexes for wikipedia). I also changed the benchmarker to use incrementToken(), which was also fine.&lt;/p&gt;

&lt;p&gt;To have a small speed incresase (but I was not able to measure it), I changed all tokenizers to use only incrementToken for the whole chain and changed the benchmarker to also use this method. In this case I was able to TokenStream.setOnlyUseNewAPI(true), which removed the backwards-compatibility-wrapper and the Token instance, so the chain only used the unwrapped simple attributes. In my opinion, tokenization was a little bit faster, faster than without any patch and next(Token). When the old API is completely removed, this will be the default behaviour.&lt;/p&gt;

&lt;p&gt;So I would suggest to review this patch, add some tests for heterogenous tokenizer chains and remove all next(...) implementations from all streams and filters and only implement incrementToken(). Contrib analyzers should then only be rewritten to the new API without the old API.&lt;/p&gt;

&lt;p&gt;The mentioned bugs with Tee/Sink are not related to this bug, but are more serious now, because the tokenizer chain is no longer fixed to on specfic API variant (it supports both mixed together).&lt;/p&gt;</comment>
                    <comment id="12721722" author="michaelbusch" created="Fri, 19 Jun 2009 11:16:04 +0100"  >&lt;p&gt;Sorry, Uwe, I was really busy today. I&apos;ll try to review the latest patch as soon as I can.&lt;/p&gt;</comment>
                    <comment id="12721769" author="thetaphi" created="Fri, 19 Jun 2009 13:41:04 +0100"  >&lt;p&gt;After committing TrieRange to core, here some updates to the patch (TrieRange used setUseNewAPI in its tests).&lt;br/&gt;
This patch now also has all next(Token) implementations removed in core (excluding Tee, Sink and CachingToken.)&lt;/p&gt;</comment>
                    <comment id="12721795" author="thetaphi" created="Fri, 19 Jun 2009 15:17:18 +0100"  >&lt;p&gt;During my tests I found a small problem with AttributeSource.getAttributesIterator() &amp;amp; Co.:&lt;br/&gt;
If you, for example, print TokenStream.toString(), but have all the 6 small interfaces like TermAttribute, FlagsAttribute,.. in the Token class implemented, you will get 6 times the same string, because the iterator enumerates the Token instance 6 times. The same happens when cloning all Attributes or doing a copyTo of all attributes, equals of AttributeSource: They are handled 6 times. So the list of AttributesImplementations should be unique. So in principle the instances should also put into a LinkedHashSet in parallel to the Interface-&amp;gt;Implementation mapping which is unique only on the interface side. The getAttributesIterator then should return the iterator on the implementation set. It would fix all double handling and would be faster for our new standard case for backwards compatibility, where e.g. Token is copyTo()&apos;ed 6 times on captureState and so on.&lt;/p&gt;</comment>
                    <comment id="12723345" author="markrmiller@gmail.com" created="Wed, 24 Jun 2009 00:05:33 +0100"  >&lt;p&gt;How is this patch coming?&lt;/p&gt;

&lt;p&gt;Do you think MemoryIndex will still need to be changed to check the TokenStreams API (al la the QueryParser), or is this going to clean it up?&lt;/p&gt;</comment>
                    <comment id="12723351" author="thetaphi" created="Wed, 24 Jun 2009 00:13:39 +0100"  >&lt;p&gt;Michael did not yet respond (I think he is busy).&lt;/p&gt;

&lt;p&gt;For me this patch works perfectly, all tests pass and performance is same as before. The big difference is, that you do not need to implement both APIs in your TokenStreams (the current patch removes the old API completely from core tokenizers). Consumers like the highlighter can simply only use the new API, but will also work when calling the old API. And you can even mix old an new API TokenFilters in one chain.&lt;/p&gt;

&lt;p&gt;In principle this is all we need. Maybe you can also do some tests!&lt;/p&gt;</comment>
                    <comment id="12723353" author="thetaphi" created="Wed, 24 Jun 2009 00:16:21 +0100"  >&lt;p&gt;By the way: Token is no longer deprecated, it is just an implementation of the six standard attributes in one class!&lt;/p&gt;</comment>
                    <comment id="12723362" author="markrmiller@gmail.com" created="Wed, 24 Jun 2009 00:35:38 +0100"  >&lt;p&gt;Ah, nice - I had to re-implement it for one or two spots, will be nice to just keep the old.&lt;/p&gt;

&lt;p&gt;That sounds great Uwe - I had thought thats where this was going but Michael seemed to have some doubts.&lt;/p&gt;

&lt;p&gt;I&apos;ll try it out with the Highlighter changes I have made.&lt;/p&gt;</comment>
                    <comment id="12723369" author="markrmiller@gmail.com" created="Wed, 24 Jun 2009 00:54:24 +0100"  >&lt;blockquote&gt;&lt;p&gt;The big difference is, that you do not need to implement both APIs in your TokenStreams (the current patch removes the old API completely from core tokenizers).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Thats a big back compat violation though right ? &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                    <comment id="12723471" author="michaelbusch" created="Wed, 24 Jun 2009 07:45:22 +0100"  >&lt;p&gt;Ok another version of my hated compatibility patch. What I did was to copy the exact 2.4 implementations of TeeTokenFilter and SinkTokenizer into the test class and use them in the test I previously had. It performs the test twice: using the old and the new API.&lt;/p&gt;

&lt;p&gt;On my patch it produces:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Testing old API...
new
tokenstream --&amp;gt; proper noun
api
new
tokenstream --&amp;gt; proper noun
api
new
tokenstream
api
Testing new API...
Exception in thread &quot;main&quot; org.apache.lucene.analysis.TokenStream$UnsupportedTokenStreamAPIException: This stream or filter does not support the new Lucene 2.9 TokenStream API yet.
	at org.apache.lucene.analysis.TokenStream.incrementToken(TokenStream.java:114)
	at org.apache.lucene.analysis.TestAPICompatibility.consumeStreamNewAPI(TestAPICompatibility.java:145)
	at org.apache.lucene.analysis.TestAPICompatibility.test2(TestAPICompatibility.java:119)
	at org.apache.lucene.analysis.TestAPICompatibility.main(TestAPICompatibility.java:83)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;On your patch, Uwe:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Testing old API...
This
TokenStream
api
This
TokenStream
api
This
TokenStream
api
Testing new API...
This
TokenStream
api
This
TokenStream
api
This
TokenStream
api
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
</comment>
                    <comment id="12723473" author="thetaphi" created="Wed, 24 Jun 2009 07:59:57 +0100"  >&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;The big difference is, that you do not need to implement both APIs in your TokenStreams (the current patch removes the old API completely from core tokenizers).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Thats a big back compat violation though right ? &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;No it isn&apos;t. It is not completely removed, the abstact TokenStream emulates the old next() methods, so even if all core tokenizers do not implement next(), they still export the next(Token) and next() methods from TokenStream.&lt;/p&gt;</comment>
                    <comment id="12723536" author="markrmiller@gmail.com" created="Wed, 24 Jun 2009 12:52:51 +0100"  >&lt;p&gt;Ah, okay. Sounds like some magic you have worked here Uwe - great stuff. Very nice to not have to impl twice everywhere.&lt;/p&gt;</comment>
                    <comment id="12723545" author="thetaphi" created="Wed, 24 Jun 2009 13:42:24 +0100"  >&lt;p&gt;Hi Michael,&lt;br/&gt;
this was a small bug in the translation next(Token) -&amp;gt; incrementToken(). If the call to incrementToken itsself calls other filters, that itsself delegate back to next() the wrapper.delegate can change during the incrementToken call. I fixed this, now it works:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
    [junit] ------------- Standard Output ---------------
    [junit]
    [junit] Test old API...
    [junit] &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt;
    [junit] tokenstream --&amp;gt; proper noun
    [junit] api
    [junit] &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt;
    [junit] tokenstream --&amp;gt; proper noun
    [junit] api
    [junit] &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt;
    [junit] tokenstream
    [junit] api
    [junit]
    [junit] Test &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; API...
    [junit] &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt;
    [junit] tokenstream --&amp;gt; proper noun
    [junit] api
    [junit] &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt;
    [junit] tokenstream --&amp;gt; proper noun
    [junit] api
    [junit] &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt;
    [junit] tokenstream
    [junit] api
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Are you happy now? I think Mark is also very interesting in this and fast progress.&lt;/p&gt;

&lt;p&gt;In my opinion, only the new Tee/Sink/Cache impls are needed and the fix for the duplicate attributes in toString().&lt;/p&gt;</comment>
                    <comment id="12723549" author="thetaphi" created="Wed, 24 Jun 2009 13:48:36 +0100"  >&lt;blockquote&gt;&lt;p&gt;But I&apos;ll definitely buy Uwe a beer if he comes up with solution that is more elegant and doesn&apos;t have the mentioned disadvantages!  &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;By the way: I want to have my beer!!!&lt;/p&gt;</comment>
                    <comment id="12723720" author="michaelbusch" created="Wed, 24 Jun 2009 21:11:20 +0100"  >&lt;p&gt;Sorry, I don&apos;t want to be the bad guy here, just trying to mention things that come to my mind. Maybe this one is negligible?&lt;/p&gt;

&lt;p&gt;If the user had extensions of streams/filters that in 2.4 didn&apos;t declare next() or next(Token) as final and calls super.next() or super.next(Token), then I think you&apos;ll get different behavior or exceptions. E.g.: &lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
  &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;static&lt;/span&gt; class ExtendedSinkTokenizer &lt;span class=&quot;code-keyword&quot;&gt;extends&lt;/span&gt; SinkTokenizer {
	  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; Token next() &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; IOException {
		  Token t = &lt;span class=&quot;code-keyword&quot;&gt;super&lt;/span&gt;.next();
		  &lt;span class=&quot;code-comment&quot;&gt;// &lt;span class=&quot;code-keyword&quot;&gt;do&lt;/span&gt; something with t
&lt;/span&gt;		  &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; t;
	  }
  }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                    <comment id="12723727" author="thetaphi" created="Wed, 24 Jun 2009 21:41:19 +0100"  >&lt;p&gt;In your example, you mean SinkTokenizer is from the core and only implements incrementToken(), the others are handled by the default in TokenStream (using this reflection-based redirect?)&lt;/p&gt;

&lt;p&gt;I tested this with overriding LowerCaseFilter. In my latest patch LowerCaseFilter only implements incrementToken(), next() and next(Token) are handled by the forwarder in TokenStream. If you override next(Token) and consume this stream using incrementToken(), then your overriden next(Token) is never called.&lt;/p&gt;

&lt;p&gt;This exact same backwards-compatibility problem was also there when changed from next() to next(Token). If some core filter only implemented next(Token) and a subclass overrides only next() (because older code-base) and calls super() there, the same happened: the tokenizer code called next(Token), but next() was never called. You have the same problem with our recent deprecations in DocIdSetIterator.&lt;/p&gt;

&lt;p&gt;I would no see this as a problem, this problem can be found everywhere in Lucene, where we deprecated (originally abstract) methods and replaced by newer ones calling the old ones as default.&lt;/p&gt;</comment>
                    <comment id="12723743" author="michaelbusch" created="Wed, 24 Jun 2009 22:12:49 +0100"  >&lt;p&gt;Not sure if we&apos;re talking about the same thing here. The problem is not that next() in the extended class doesn&apos;t get called. It gets called, but the super.next() call throws either a ClassCastException, when consuming using the old API, or produces a different result than before uwhen consuming the new API. &lt;/p&gt;

&lt;p&gt;This is the output on your patch:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Testing old API...
new
tokenstream --&amp;gt; proper noun
api
new
tokenstream --&amp;gt; proper noun
api
new
tokenstream
api
new
tokenstream --&amp;gt; proper noun
api
(new,12,15)
new
(tokenstream,16,27)
tokenstream
(api,28,31)
api
null
java.lang.ClassCastException: org.apache.lucene.util.AttributeSource$State
	at org.apache.lucene.analysis.SinkTokenizer.next(SinkTokenizer.java:97)
	at org.apache.lucene.analysis.TestCompatibility.consumeStreamOldAPI(TestCompatibility.java:194)
	at org.apache.lucene.analysis.TestCompatibility.test2(TestCompatibility.java:142)
	at org.apache.lucene.analysis.TestCompatibility.main(TestCompatibility.java:87)
Testing new API...
new
tokenstream --&amp;gt; proper noun
api
new
tokenstream --&amp;gt; proper noun
api
new
tokenstream
api
new
tokenstream --&amp;gt; proper noun
api
(new,12,15)
new
(tokenstream,16,27)
tokenstream
(api,28,31)
api
null
new
tokenstream
api
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;On my patch:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Testing old API...
new
tokenstream --&amp;gt; proper noun
api
new
tokenstream --&amp;gt; proper noun
api
new
tokenstream
api
new
tokenstream --&amp;gt; proper noun
api
(new,12,15)
new
(tokenstream,16,27)
tokenstream --&amp;gt; proper noun
(api,28,31)
api
null
new
tokenstream
api
Testing new API...
org.apache.lucene.analysis.TokenStream$UnsupportedTokenStreamAPIException: This stream or filter does not support the new Lucene 2.9 TokenStream API yet.
	at org.apache.lucene.analysis.TokenStream.incrementToken(TokenStream.java:114)
	at org.apache.lucene.analysis.TestAPICompatibility.consumeStreamNewAPI(TestAPICompatibility.java:210)
	at org.apache.lucene.analysis.TestAPICompatibility.test3(TestAPICompatibility.java:162)
	at org.apache.lucene.analysis.TestAPICompatibility.main(TestAPICompatibility.java:93)
org.apache.lucene.analysis.TokenStream$UnsupportedTokenStreamAPIException: This stream or filter does not support the new Lucene 2.9 TokenStream API yet.
	at org.apache.lucene.analysis.TokenStream.incrementToken(TokenStream.java:114)
	at org.apache.lucene.analysis.TestAPICompatibility.consumeStreamNewAPI(TestAPICompatibility.java:210)
	at org.apache.lucene.analysis.TestAPICompatibility.test4(TestAPICompatibility.java:183)
	at org.apache.lucene.analysis.TestAPICompatibility.main(TestAPICompatibility.java:98)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                    <comment id="12723749" author="thetaphi" created="Wed, 24 Jun 2009 22:27:17 +0100"  >&lt;blockquote&gt;&lt;p&gt;Not sure if we&apos;re talking about the same thing here. The problem is not that next() in the extended class doesn&apos;t get called. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think we are talking about the same. We have exact the same problem, if you override OpenBitSetIterator, but only implement next() instead of nextDoc(). next() is never called, because the new API (nextDoc()) is handling everything. It&apos;s exactly the same. You have this always, if you deprecate former abstract methods and make the new one call the old one. I a subclass overrides the new API, the old API is dead for this class. A further (3rd party) subclass then overrides the deprecated method, it is never called -&amp;gt; b&#228;ng. Other example: Filter.&lt;br/&gt;
We have this everywhere (DocIdSetIterator next() vs nextDoc(), Filter getBits() vs. getDocIdSet(), TokenStream very old API vs. old API).&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;It gets called, but the super.next() call throws either a ClassCastException, when consuming using the old API, or produces a different result than before uwhen consuming the new API.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;A ClassCastException? Why that? super.next() is always available (at least TokenStream on top of the hierarchy defines it).&lt;/p&gt;</comment>
                    <comment id="12723751" author="thetaphi" created="Wed, 24 Jun 2009 22:30:40 +0100"  >&lt;p&gt;One solution around this poblem would be to always implement both incrementToken() and next(Token) in non-final classes that tend to be overridden (CharTokenizer). In this case you are right. So I should revert the removal of next(Token). But very old code only overriding next() still fails.&lt;/p&gt;

&lt;p&gt;But final things like StandardFilter do not need to have both implementations.&lt;/p&gt;</comment>
                    <comment id="12723759" author="michaelbusch" created="Wed, 24 Jun 2009 22:49:51 +0100"  >&lt;blockquote&gt;
&lt;p&gt;By the way: I want to have my beer!!!&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I decided a while ago that you&apos;ll definitely get it, because of your admirable tenaciousness. &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                    <comment id="12726066" author="thetaphi" created="Wed, 1 Jul 2009 14:52:16 +0100"  >&lt;p&gt;After thinking one round about it again. Your original patch with the UnsupportedOperationException have the same problem. If one overrides next(Token) in one non-final TokenStream/Filter, the overriden method is never called.&lt;/p&gt;

&lt;p&gt;And it is also not enough to always override both methods in subclass-able streams/filters. With both batches we break BW compatibility for users that do not override TokenStream or TokenFilter directly.&lt;/p&gt;

&lt;p&gt;Even my last comment on java-dev is not correct. Somebody who overrides a Filter (not direct subclass of TokenStream/Filter/Tokenizer, or the in-between classes are also of this user and only override deprecated APIs) and only override next(Token) (e.g. subclass of CharTokenizer), the overriden method will never be called, if the Filter/Stream is top-level in the filter chain &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/sad.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;I think, we must have somewhere a BW break and clearly document it in the following way in CHANGES.txt:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;You can mix old and new filters/streams without problems.&lt;/li&gt;
	&lt;li&gt;Such mixed chains can be consumed with both new/old API&lt;/li&gt;
	&lt;li&gt;To have this working, always override the abstract TokenStream/TokenFilter/Tokenizer directly and see all other core classes as &quot;final&quot;. In all other cases your filters/token streams will have unpredicatable behaviour.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Uwe&lt;/p&gt;</comment>
                    <comment id="12728823" author="markrmiller@gmail.com" created="Wed, 8 Jul 2009 18:55:29 +0100"  >&lt;p&gt;Mr. Busch my friend, I&apos;ll buy both you and Uwe &lt;b&gt;many&lt;/b&gt; beers if you resolve this issue soon!&lt;/p&gt;</comment>
                    <comment id="12728879" author="michaelbusch" created="Wed, 8 Jul 2009 21:21:58 +0100"  >&lt;p&gt;Alright, I hope you are coming to Oakland in November! &lt;/p&gt;

&lt;p&gt;I had a few (literally) sleepless nights last week to meet some internal deadlines; but it looks like I&apos;ll now have time to work on Lucene, so I&apos;ll continue on this issue tonight!&lt;/p&gt;</comment>
                    <comment id="12729117" author="thetaphi" created="Thu, 9 Jul 2009 10:14:17 +0100"  >&lt;p&gt;Attached is a new patch to current trunk. I did a hard work to make it compatible to also test-tag (see the java-dev discussion last weekend: test-tag was not really testing against 2.4, the bw-branch was created in November from trunk, when the new TokenStream API was already committed):&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;All tests core, contrib and tag pass correctly (and no changes to bw-branch needed!)&lt;/li&gt;
	&lt;li&gt;Michaels latest TestCompatibility also passes&lt;/li&gt;
	&lt;li&gt;All non-final and abstract Tokenizers implement both next(Token) and incrementToken() for maximum compatibility (but there are still problems mentioned above). This problems are always there and cannot be worked around, even with Michael&apos;s initial patch and at other places in Lucene, too (Filters, HitCollector).&lt;/li&gt;
	&lt;li&gt;CachingTokenFilter, TeeTokenizer and SinkTokenizer and the corresponding tests are reverted to 2.4 API and deprecated. There is not yet an implementation in new classes using solely the new API available. This is the only thing on the TODO-list for this case. I suggest the following name: CachingAttributesFilter; but for TeeTokenizer and SinkTokenizer I have no idea &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/sad.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;In my opinion, this is ready to commit and the non-deprecated Caching/Sink/Tee things could be a separate issue.&lt;/p&gt;

&lt;p&gt;We also need to write some conclusion or howto for CHANGES.txt, mentioning the problems with the switch to the new API.&lt;/p&gt;</comment>
                    <comment id="12729565" author="michaelbusch" created="Fri, 10 Jul 2009 08:49:59 +0100"  >&lt;p&gt;Thanks for all your hard work here, Uwe.&lt;/p&gt;

&lt;p&gt;I think this patch is as good as it can be for achieving the goal of being able to combine the old and the new API.&lt;br/&gt;
And I agree that my patch that I posted here has the same potential backwards-compatibility problems regarding inheritance. &lt;/p&gt;

&lt;p&gt;I think in the majority of use cases we&apos;re fine here. Only the corner cases make me a bit nervous. I think the case I feel most uncomfortable with is when people use Lucene + some external analyzer package + their own subclasses. If they use Lucene 2.9, the external package is not upgraded to the new API yet, but they did upgrade their own classes already to the new API, then they might run into undefined problems. However, I don&apos;t even know how many of such &quot;external analyzer packages&quot; exist (well, I think Grant mentioned he was working on one...)&lt;/p&gt;

&lt;p&gt;And I still just have this not-going-away slightly bad feeling in my gut that there are still other corner case problems we haven&apos;t thought about yet. What makes this feeling worse is the fact that those problems might not result in exceptions, but in unexpected and hard-to-find search problems, because the wrong tokens were indexed.&lt;/p&gt;

&lt;p&gt;The current patch uses reflection extensively to figure out which of the three APIs the user has implemented. The comments above mention the possible problems. The solution is cool, but also a bit hack-ish (no offense Uwe, you called it that yourself &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/wink.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; )&lt;/p&gt;

&lt;p&gt;So, having said all this, I&apos;d like other people to chime in here and give their opinion. I&apos;m okay with committing this solution if everyone else is too.&lt;br/&gt;
I think the only solution to not break compatibility at all is to not touch the old API at all and provide APIs that switch on/off using the new API. That&apos;s what the code in trunk currently does. It has the major disadvantage that it doesn&apos;t allow combining the old and new API in the same chain, and that we have to implement both APIs in core Lucene until the old API is fully removed.&lt;/p&gt;

&lt;p&gt;So Mike, Grant, Mark, or others, could you please comment here?&lt;/p&gt;

&lt;p&gt;PS: Uwe, in any case, your solution is cool and I like how cleverly you solved the problems!!&lt;/p&gt;</comment>
                    <comment id="12729567" author="thetaphi" created="Fri, 10 Jul 2009 09:01:08 +0100"  >&lt;p&gt;Thanks!&lt;br/&gt;
Before going to bed yesterday, I remembered the fact, that the attributes iterators list the interfaces and not the instances. I will fix this in a further patch to also have the possibility to iterate over the instances and e.g. only clone instances.&lt;/p&gt;

&lt;p&gt;Currently when you clone or toString(), the big Token instance for all 6 attributes is cloned/printed 6 times. This is the only thing, I forgot yesterday. I will think about it and maybe add an attributesInstancesIterator method or something like that.&lt;/p&gt;

&lt;p&gt;What do we do with Tee/Sink/Cached?&lt;/p&gt;</comment>
                    <comment id="12729568" author="michaelbusch" created="Fri, 10 Jul 2009 09:06:59 +0100"  >&lt;p&gt;I agree that in any case we should deprecate those three classes and write new ones. I think we need to merge Tee/Sink into one actually. Maybe name it TeeSinkTokenFilter &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; And it gets a method like &lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; TokenStream getSinkTokenStream();
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;That way we can ensure that the sink tokenstream has exactly the same attributes as the TeeSinkTokenFilter. Otherwise, a user could e.g. create the SinkTokenizer first, add some attributes to it, before calling new TeeTokenFilter(in, sink).&lt;/p&gt;</comment>
                    <comment id="12729819" author="mikemccand" created="Fri, 10 Jul 2009 22:02:56 +0100"  >&lt;p&gt;I&apos;m still trying to wrap my brain around this issue &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;So to sum up where things &lt;span class=&quot;error&quot;&gt;&amp;#91;seem&amp;#93;&lt;/span&gt; to stand here:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;We are switching to separate interface + impl for token&lt;br/&gt;
    attributes.  This is nice because a single class (eg Token.java)&lt;br/&gt;
    can provide multiple attrs, it makes cloning more efficient, etc.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;We are keeping good old Token.java around; it simply implements all&lt;br/&gt;
    the attrs, and is very convenient to use.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I think this is a great improvement to the new tokenStream API.  It&apos;s&lt;br/&gt;
also a sizable perf gain to clone only one impl that has only the&lt;br/&gt;
attrs you care about.&lt;/p&gt;

&lt;p&gt;Uwe then extended the original patch:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;Use reflection on init&apos;ing a TokenStream to determine which of the&lt;br/&gt;
    3 APIs (old, newer, newest) it implements&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Indexer (and in general any consumer) now only has to consume the&lt;br/&gt;
    new API.  Any parts of the chain that aren&apos;t new are automatically&lt;br/&gt;
    wrapped.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Core tokenizers/filters (and contrib, when we get to them) only&lt;br/&gt;
    implement the new API; old API is &quot;wrapped on demand&quot; if needed&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;One can mix &amp;amp; match old, new tokenizers, though at some perf cost.&lt;br/&gt;
    But that&apos;s actually OK since the original patch it&apos;s &quot;all or&lt;br/&gt;
    none&quot;, ie your chain must be entirely new or old&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;I think &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/help_16.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; a chain of all-old-tokenizer/filters and all-new&lt;br/&gt;
    wouldn&apos;t see a perf hit?  Wrapping only happens when there&apos;s a&lt;br/&gt;
    mismatch b/w two filters in the chain?&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I&apos;m tentatively optimistic about the extended patch... (though these&lt;br/&gt;
back-compat, corner cases, etc., are real hard to think about).  I&lt;br/&gt;
agree it&apos;s doing alot of &quot;magic&quot; under the hood to figure out how to&lt;br/&gt;
best wrap things, but the appeal of only implementing the new api in&lt;br/&gt;
core/contrib tokenizers, only consuming new, being free to mix&amp;amp;match,&lt;br/&gt;
etc, is strong.&lt;/p&gt;

&lt;p&gt;One concern is: TokenStream.initialize looks spookily heavy weight; eg&lt;br/&gt;
I don&apos;t know the &quot;typical&quot; cost of reflection.  I think there are&lt;br/&gt;
likely many apps out there that make a new TokenStream for ever field&lt;br/&gt;
that&apos;s analyzed (ie implement Analyzer.tokenStream not&lt;br/&gt;
reusableTokenStream) and this (introspection every time) could be a&lt;br/&gt;
sizable perf hit.  Uwe was this included in your perf tests?&lt;/p&gt;</comment>
                    <comment id="12729823" author="yseeley@gmail.com" created="Fri, 10 Jul 2009 22:14:26 +0100"  >&lt;blockquote&gt;&lt;p&gt;One concern is: TokenStream.initialize looks spookily heavy weight; eg I don&apos;t know the &quot;typical&quot; cost of reflection. I think there are likely many apps out there that make a new TokenStream for ever field that&apos;s analyzed&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;At this point, I&apos;d bet that&apos;s still the norm.&lt;/p&gt;</comment>
                    <comment id="12729992" author="thetaphi" created="Sat, 11 Jul 2009 15:50:04 +0100"  >&lt;p&gt;Some updates:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Added missing License headers&lt;/li&gt;
	&lt;li&gt;Differentiate between iteration over attributes or attribute impls. The probleme here is, that the unique attribute impls are not known, so a set must be created (which is done on-the fly). As the attributes member is currently protected and can be modified (even by TokenFilters), there is no way around this. Michael: Do you have any idea about that? During cloning, state capturing and toString() only the unique instances should be visited.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Michael: Some more questions: The superinterface Attribute does not contain clone(), but the interface TokenAttribute does. Why these two interfaces (because AttributeImpl has clone). In my opinion, clone() should be part of Attribute and TokenAttribute can be removed.&lt;/p&gt;

&lt;p&gt;To the performance questions:&lt;br/&gt;
The new Token API uses reflection very intensive, even without the changes from me. Everytime, when you add an attribute, the instance is checked for all implemented interfaces (via reflection), see addAttributeImpl(). This means, creating ne instances of TokenStreams is much more costly than with the old API (with the new &quot;smart&quot; TokenStreams and also without). I will prepare a small performance comparison (something like &quot;time to create one million WhitespaceAnalyzer&apos;s TokenStreams&quot; with old API, new API (as in trunk) and newest API (this patch)).&lt;/p&gt;</comment>
                    <comment id="12730003" author="markrmiller@gmail.com" created="Sat, 11 Jul 2009 17:00:25 +0100"  >&lt;p&gt;Token is still deprecated in the latest patch - we are not going to deprecate it though, right?&lt;/p&gt;</comment>
                    <comment id="12730306" author="thetaphi" created="Mon, 13 Jul 2009 13:02:45 +0100"  >&lt;p&gt;Mike implemented a nice idea to solve the problems with tokenstreams overriding deprecated methods in &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-1678&quot; title=&quot;Deprecate Analyzer.tokenStream&quot;&gt;&lt;del&gt;LUCENE-1678&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I will try this out here and also fix the problems with # of attribute instances != # of attributes and the iterator problems because of this.&lt;/p&gt;</comment>
                    <comment id="12730990" author="thetaphi" created="Tue, 14 Jul 2009 18:36:51 +0100"  >&lt;p&gt;New patch with cleanup and fixing the last problems and inconsistencies in the&lt;br/&gt;
AttributeSource with captureState and toString and so on, because attributes&lt;br/&gt;
can now be implemented by more than one impl and one impl can implement more&lt;br/&gt;
than one attribute (best example: Token). Renamed some methods for that and made sure, that the two Maps holding attributes and instances are private and cannot be modified (to prevent users from making bad things). There are only the public methods and iterators (unmodifiable) available (with changed semantics).&lt;/p&gt;

&lt;p&gt;Missing is still the new CachingAttributesFilter and the new TeeSinkTokenizer-combi for the new API (the 3 old ones are already deprecated).&lt;/p&gt;

&lt;p&gt;I also removed some of the &quot;old&quot; captureState methods with AttributeSource. Also added testcases for that.&lt;/p&gt;

&lt;p&gt;I also removed the TokenAttribute and moved clear into Attribute, as the base class AttributeImpl always implements this method.&lt;/p&gt;

&lt;p&gt;After that 1693 needs only the idea from &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-1678&quot; title=&quot;Deprecate Analyzer.tokenStream&quot;&gt;&lt;del&gt;LUCENE-1678&lt;/del&gt;&lt;/a&gt;, to detect if in non-final&lt;br/&gt;
classes, any subclass overrides deprecated methods. Problematic core token&lt;br/&gt;
streams are:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;ISOLatin1Filter (no-final and deprecated, so maybe it should not implement&lt;br/&gt;
incrementToken at all) -&amp;gt; would be fixed then.&lt;/li&gt;
	&lt;li&gt;KeywordTokenizer should normally be final, but is not &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/sad.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; -&amp;gt; needs this&lt;br/&gt;
special trick&lt;/li&gt;
	&lt;li&gt;StandardTokenizer is the same, should be final, but isn&apos;t&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Contrib analyzers could have this prob, too, but this must be checked when doing the&lt;br/&gt;
transformation there. If the backwards wrapper is available, we could do&lt;br/&gt;
this like with the analyzers in &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-1678&quot; title=&quot;Deprecate Analyzer.tokenStream&quot;&gt;&lt;del&gt;LUCENE-1678&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;</comment>
                    <comment id="12730993" author="thetaphi" created="Tue, 14 Jul 2009 18:40:05 +0100"  >&lt;p&gt;I forgot:&lt;br/&gt;
I also added the TestCompatibilty class as a testcase and implemented ASCIIFoldingFilter with new API.&lt;/p&gt;</comment>
                    <comment id="12731286" author="michaelbusch" created="Wed, 15 Jul 2009 06:46:03 +0100"  >&lt;p&gt;Hi Uwe,&lt;/p&gt;

&lt;p&gt;I modified the compatibility test so that it is now a junit and I&apos;d like to commit it too.&lt;/p&gt;

&lt;p&gt;It&apos;s failing right now, because it&apos;s testing a lot of different combinations. I need to check if all of those different tests are actually valid, because we&apos;re saying you can&apos;t use Tee/Sink with the new API anymore. &lt;/p&gt;

&lt;p&gt;I&apos;m also working on a new TeeSinkTokenizer that will be the replacement. However, we have to make sure that everyone who is upgrading to 2.9 (without code modifications, just after 2.x -&amp;gt; 2.9 jar replacement) and using the old Tee/Sink doesn&apos;t see a different behavior.&lt;/p&gt;</comment>
                    <comment id="12731289" author="michaelbusch" created="Wed, 15 Jul 2009 06:49:57 +0100"  >&lt;p&gt;So if people are using streams/filters that implement next(Token) I think the performance should be comparable - even though there&apos;s also a (hopefully small) performance hit to expect because of more method calls and if checks.&lt;/p&gt;

&lt;p&gt;However, if people are using next() in a chain with core streams/filters, then every single token will now be cloned, possibly multiple times, right?&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
  /** Returns the next token in the stream, or &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt; at EOS.
   *  @deprecated The returned Token is a &lt;span class=&quot;code-quote&quot;&gt;&quot;full &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt; copy&quot;&lt;/span&gt; (not
   *  re-used across calls to next()) but will be slower
   *  than calling {@link #next(Token)} instead. */
  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; Token next() &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; IOException {
    checkTokenWrapper();
    
    &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (hasIncrementToken) {
      &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; incrementToken() ? ((Token) tokenWrapper.delegate.clone()) : &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;;
    } &lt;span class=&quot;code-keyword&quot;&gt;else&lt;/span&gt; {
      &lt;span class=&quot;code-keyword&quot;&gt;assert&lt;/span&gt; hasReusableNext;
      &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; Token token = next(tokenWrapper.delegate);
      &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (token == &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;) &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;;
      tokenWrapper.delegate = token;
      &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; (Token) token.clone();
    }
  }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                    <comment id="12731295" author="thetaphi" created="Wed, 15 Jul 2009 07:11:43 +0100"  >&lt;blockquote&gt;&lt;p&gt;However, if people are using next() in a chain with core streams/filters, then every single token will now be cloned, possibly multiple times, right?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;That was a similar problem in 2.4, whenever you call next() to consume a stream, every token is a new instance. Only cloning WAS not needed but its now needed for BW compatibility (&quot;full private copy&quot;).&lt;/p&gt;

&lt;p&gt;In my opinion, this is neglectible, as indexing speed is important, and the indexer always uses incrementToken(). If somebody written an own query parser that uses next() to consume speed is not really important. And it is deprecated and in the docs (even in 2.4) stands: &quot;but will be slower than calling next(Token)&quot;.&lt;/p&gt;

&lt;p&gt;There is one case, when it also affects you (during indexing). If you have an old-style tokenfilter that calls next() on the next stream that is new-api, it would clone. In my opinion, the speed is about the same like before:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;the 2.4 code created a new uninitialized token instance and the filter then filled it with data, you have to initialize the char arrays and so on.&lt;/li&gt;
	&lt;li&gt;here the token from the TokenWrapper-Attribute is reused (no allocation costs for arrays and so on), but you have to clone the Token (to be full private).&lt;/li&gt;
&lt;/ul&gt;


&lt;blockquote&gt;&lt;p&gt;So if people are using streams/filters that implement next(Token) I think the performance should be comparable - even though there&apos;s also a (hopefully small) performance hit to expect because of more method calls and if checks.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I found no performance hit, it is about same speed. The varieties between tests is bigger than a measureable performance impact. The other sensitive thing (TokenWrapper): The wrapping using TokenWrapper was in the original indexing code, too (this BackwardsCompatibilityStream private class).&lt;/p&gt;

&lt;p&gt;The if checks are all on final variables, so can be optimized away by the JVM. The method calls are inlined, as far as I have seen.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;It&apos;s failing right now, because it&apos;s testing a lot of different combinations. I need to check if all of those different tests are actually valid, because we&apos;re saying you can&apos;t use Tee/Sink with the new API anymore. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Have you seen my backwards compatibility test, too? It is a copy of yours (with some variation)? The Lucene24* classes were removed, because Tee/SinkTokenizer werde reverted to their original 2.4 status in the patch (only implement old API).&lt;br/&gt;
As far as I see (not yet tried out), you try to test new-style-API streams with the old Tee/Sink tokenizer, that is deprecated. You were not able to do this before 2.9 (no new API) and so the bw problem is not there. If you rewrite your streams with new API, you should use TeeSinkTokenizer, too.&lt;/p&gt;</comment>
                    <comment id="12731312" author="thetaphi" created="Wed, 15 Jul 2009 08:18:48 +0100"  >&lt;p&gt;I changed TokenStream to use real final boolean variables to help the optimizer to optimize away the if checks.&lt;/p&gt;

&lt;p&gt;By the way: The problem with initialize() and lateInitialize() &lt;span class=&quot;error&quot;&gt;&amp;#91;new&amp;#93;&lt;/span&gt; is the typical example, why you should never call non-private or non-final methods from a ctor (Sun has a big warning in the documentation). The problem: During initialize() the final fields are not yet initialized, because initialize() is called before the ctor of the actual class (which initializes the fields) is called. Very nasty. So I moved the checks, which methods are available to a private lateInitialize().&lt;/p&gt;

&lt;p&gt;In my opinion, we should remove the protected initialize() from AttributeSource and let every subclass do its initialization using a (possibly) private/final method on its own (must be called in two ctors). What was the idea behind that?&lt;/p&gt;</comment>
                    <comment id="12731315" author="michaelbusch" created="Wed, 15 Jul 2009 08:27:14 +0100"  >&lt;blockquote&gt;
&lt;p&gt;There is one case, when it also affects you (during indexing). If you have an old-style tokenfilter that calls next() on the next stream that is new-api, it would clone. In my opinion, the speed is about the same like before:&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes this is exactly what I mean. Not sure if cloning in this case is slower than creating a new empty instance; if yes, it&apos;s probably not significant.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;As far as I see (not yet tried out), you try to test new-style-API streams with the old Tee/Sink tokenizer, that is deprecated. You were not able to do this before 2.9 (no new API) and so the bw problem is not there. If you rewrite your streams with new API, you should use TeeSinkTokenizer, too.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;You are right - it fails because it uses a new attribute that will not be cached in the Tee/Sink. So I agree that this is not a valid test if we say that Tee/Sink only supports the old API. I will remove the cases that are invalid.&lt;/p&gt;

&lt;p&gt;Thinking out loud here again: What if a user uses Lucene in combination with a third-party jar containing TokenStreams, and also own implementations. Are there use cases where it would be necessary for us to provide a switch to run in only-old-API mode?&lt;/p&gt;</comment>
                    <comment id="12731317" author="michaelbusch" created="Wed, 15 Jul 2009 08:32:05 +0100"  >&lt;blockquote&gt;
&lt;p&gt;In my opinion, we should remove the protected initialize() from AttributeSource and let every subclass do its initialization using a (possibly) private/final method on its own (must be called in two ctors). What was the idea behind that?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Definitely. I should start using the nocommit comments Mike puts into early patches. I never wanted the initialize() method to stay for similar reason that you mentioned here. I added it for a test, because I was lazy.&lt;/p&gt;

&lt;p&gt;What we really should have is an AttributeSource constructor that takes an AttributeFactory. You need to be able to set the factory in the ctor, because TokenStreams usually add attributes in their ctor.&lt;/p&gt;</comment>
                    <comment id="12731330" author="thetaphi" created="Wed, 15 Jul 2009 08:52:45 +0100"  >&lt;blockquote&gt;&lt;p&gt;What we really should have is an AttributeSource constructor that takes an AttributeFactory. You need to be able to set the factory in the ctor, because TokenStreams usually add attributes in their ctor.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I have done this here locally. The ctor that takes an other AttSource uses the factory of the delegate, do we need also a ctor with another AttSource and the factory? The case may be&lt;br/&gt;
I also added this ctor to TokenStream, but not TokenFilter (as it uses the factory from the filtered stream).&lt;/p&gt;

&lt;p&gt;I also cleaned up the initialization of TokenStream. Now the tokenWrapper and all the booleans are final.&lt;/p&gt;

&lt;p&gt;If you are interested I could post my current patch.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Thinking out loud here again: What if a user uses Lucene in combination with a third-party jar containing TokenStreams, and also own implementations. Are there use cases where it would be necessary for us to provide a switch to run in only-old-API mode?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;There is still the problem with a TokenStream overriding a deprecated method of a core filter that will be never be called anymore (see &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-1678&quot; title=&quot;Deprecate Analyzer.tokenStream&quot;&gt;&lt;del&gt;LUCENE-1678&lt;/del&gt;&lt;/a&gt; which faces the same problem). I will try to fix this here using the same mechanism.&lt;/p&gt;

&lt;p&gt;I tested with mixing contrib tokenfilters and core filters. I have seen no problems.&lt;/p&gt;</comment>
                    <comment id="12731332" author="michaelbusch" created="Wed, 15 Jul 2009 09:03:47 +0100"  >&lt;blockquote&gt;
&lt;p&gt;There is still the problem with a TokenStream overriding a deprecated method of a core filter that will be never be called anymore (see &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-1678&quot; title=&quot;Deprecate Analyzer.tokenStream&quot;&gt;&lt;del&gt;LUCENE-1678&lt;/del&gt;&lt;/a&gt; which faces the same problem). I will try to fix this here using the same mechanism.&lt;/p&gt;

&lt;p&gt;I tested with mixing contrib tokenfilters and core filters. I have seen no problems.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yeah that is a good fix for overriding the non-final methods of the core filters.&lt;/p&gt;

&lt;p&gt;I guess what I meant here is that my invalid use case could happen in the field: Let&apos;s say something like tee/sink lived in the third-party jar and the user upgrades to Lucene 2.9 and also upgrades the own streams/filters, but a version of the third-party jar that has the new implementations is not available yet. The user couldn&apos;t simply implement both the new and old API and use such a filter then with the not-updated third party jar, unless there was a only-old-api switch.&lt;/p&gt;

&lt;p&gt;But I&apos;m not sure how realistic this scenario is. I guess we&apos;ll find it out sooner or later &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                    <comment id="12731334" author="michaelbusch" created="Wed, 15 Jul 2009 09:08:56 +0100"  >&lt;blockquote&gt;
&lt;p&gt;Have you seen my backwards compatibility test, too?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Oh cool... I guess I should have checked that before... &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;I think if I remove the invalid tests it looks pretty similar to yours, so let&apos;s keep the one you have in the patch.&lt;/p&gt;

&lt;p&gt;I&apos;m going to bed now... will add the new tee/sink stuff tomorrow.&lt;/p&gt;</comment>
                    <comment id="12731337" author="thetaphi" created="Wed, 15 Jul 2009 09:16:36 +0100"  >&lt;blockquote&gt;&lt;p&gt;But I&apos;m not sure how realistic this scenario is. I guess we&apos;ll find it out sooner or later &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think in 2.9 we have a lot of BW breaks (see the long list in CHANGES.txt at the begin). This not so realistic case is just one more &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; There should be a BW note in CHANGES.txt about the new TokenStream API and possible traps (something like: &quot;we did our best to make it bw-compatible, but there may be the following problems: &amp;lt;list&amp;gt;. You Should upgrade all your TokenStreams as soon as possible, especially if a strange behaviour occurs.&quot;)&lt;/p&gt;

&lt;p&gt;Do you have a example of TeeSink and CachingAttributesFilter?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Yeah that is a good fix for overriding the non-final methods of the core filters&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I would try to do this now.&lt;/p&gt;</comment>
                    <comment id="12731555" author="thetaphi" created="Wed, 15 Jul 2009 17:53:28 +0100"  >&lt;p&gt;After the whole day thinking about a solution for overriding deprecated methods, I came to one conclusion/solution, that I would create a &quot;visible&quot; backwards break (to be noted in CHANGES.txt).&lt;/p&gt;

&lt;p&gt;Mike&apos;s idea from &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-1678&quot; title=&quot;Deprecate Analyzer.tokenStream&quot;&gt;&lt;del&gt;LUCENE-1678&lt;/del&gt;&lt;/a&gt; is good, but very complicated for this issue and may lead to unpredicted behavior. And what makes me think, that this will not be a problem for developers, is the fact that there is no JIRA issue about a similar break in the past: When Lucene switched from next() to next(reusableToken), we also had a compatibility method in TokenStream that delegates to next(new Token()). Core streams did &lt;b&gt;not&lt;/b&gt; implement the old method and the indexer code only called next(Token). If somebody would have overridden only the old next() method of a core tokenstream, this method would have been never called -&amp;gt; bumm we have a break, but nobody realized it. With the new patch, we have the same in 2.9 for incrementToken vs. next(Token) and also next(). In principle the same issue like in &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-1678&quot; title=&quot;Deprecate Analyzer.tokenStream&quot;&gt;&lt;del&gt;LUCENE-1678&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The good thing is, that most TokenStreams in core are final, except the following ones:&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;ISOLatin1Filter&lt;/li&gt;
	&lt;li&gt;KeywordTokenizer&lt;/li&gt;
	&lt;li&gt;StandardTokenizer&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;...and last but not least the whole structure of subclasses of CharTokenizer. The good thing is (and thanks to the developer!), they are correctly implemented, making their methods incrementToken, next(Token) &lt;b&gt;final&lt;/b&gt;. Haha, nobody could override them, so the class is not final, but the affected methods. So all subclasses of CharTokenizer are also not affected.&lt;/p&gt;

&lt;p&gt;My latest patch also includes this &lt;b&gt;final&lt;/b&gt; modifier for the abstract CharTokenizer:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; Token next(&lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; Token reusableToken) &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; IOException {
 &lt;span class=&quot;code-comment&quot;&gt;// Overriding &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; method to make it &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; as before has no effect &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; the reflection-wrapper in TokenStream.
&lt;/span&gt; &lt;span class=&quot;code-comment&quot;&gt;// TokenStream.hasReusableNext is &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt; because of &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;, but it is never used, as incrementToken() has preference.
&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;super&lt;/span&gt;.next(reusableToken);
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;So it is not overrideable and is still compatible (code calling next(Token) will be delegated to incrementToken() by the superclass). For complete correctness also next() should be similar overridden. In both cases the super&apos;s method always delegates preferably to incrementToken() so iven that a subclass of TokenStream overrides this method and so hasNext == true and hasReusableNext == true, incrementToken() is still preferred, so everything works.&lt;/p&gt;

&lt;p&gt;This prevents users from overriding next() or next(Token) of core or contrib tokenstreams (which in my opinion nobody has ever done, because if yes, we would have a bug report regarding the last transition).&lt;/p&gt;

&lt;p&gt;For those people, that really have done it (they used one of the tree classes above as super for their own class), the error would not be to detectable. Their TokenStream would simply not work, as next()/next(Token) is never called. To produce a compile error for them (or a runtime error, when they instantiate such a class), I suggest to include this backwards-break (which is better than failing silently). All non-final TokenStreams/Tokenizers/TokenFilters should simply include the code snipplet above to redeclare next() &lt;b&gt;and&lt;/b&gt; next(Token) as final (only delegating to super) in the first subclass that implements incrementToken(). Instead of failing silently, users will get runtime linker errors (when they replace the lucene jar) or compile errors. We have done a similar change in TokenFilter, because we made the delegate stream final to prevent disturbing the attributes (Mike have done this in &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-1636&quot; title=&quot;TokenFilters with a null value in the constructor fail&quot;&gt;&lt;del&gt;LUCENE-1636&lt;/del&gt;&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;CHANGES.txt would contain this as BW-break together with the other breaks.&lt;/p&gt;

&lt;p&gt;Any comments? Michael, what do you think?&lt;/p&gt;</comment>
                    <comment id="12731859" author="michaelbusch" created="Thu, 16 Jul 2009 09:42:43 +0100"  >&lt;p&gt;I like the cleanup you did! Good that initialize() is gone now. The only small performance improvement we should probably make is to avoid checking which method in TokenStream is overridden when onlyUseNewAPI==true.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;To produce a compile error for them (or a runtime error, when they instantiate such a class), I suggest to include this backwards-break (which is better than failing silently). All non-final TokenStreams/Tokenizers/TokenFilters should simply include the code snipplet above to redeclare next() and next(Token) as final (only delegating to super) in the first subclass that implements incrementToken().&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;+1. I think this backwards-compatibility break is acceptable and makes sense. Most likely the final was just forgotten in these three classes in the first place - all the other core classes declare these methods correctly as final. So we can kind of consider this as a bug fix.&lt;/p&gt;

&lt;p&gt;And I like that they will get a compile or link error, instead of seeing undefined behavior.&lt;/p&gt;</comment>
                    <comment id="12731860" author="michaelbusch" created="Thu, 16 Jul 2009 09:48:34 +0100"  >&lt;p&gt;This is basically your last patch with these changes:&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;I removed AttributeSource.setAttributeFactory(factory). Since we have the constructor now that takes the factory as an arg, there should be no need to ever change the factory after a TokenStream was created. It would also lead to problems regarding e.g. Tee/Sink: a user could add attributes to the Tee, then change the factory, then create the sink. How could we then create the same attribute impls for the sink? So I think the right thing to do is to not allow changing the factory after the stream is instantiated.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;I added the initial (untested) version of TeeSinkTokenFilter to demonstrate how I think it should work now. I&apos;ll finish tomorrow or Friday (add more javadocs and unit test). I&apos;ll also add the CachingAttributeTokenFilter, which is essentially almost the same as the new inner class of TeeSinkTokenFilter. When I have CATF the inner class can probably just extend it.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="12731893" author="thetaphi" created="Thu, 16 Jul 2009 11:36:50 +0100"  >&lt;p&gt;Ok looks good. I think you will go to bed now, so the work would not collide. If you start to program again, ask me, that I will post a patch (which makes merging simplier). TortoiseSVN has a problem with merging added files, so when applying your patch I have to remove them first &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/sad.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;Some comments:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;TeeSinkTokenFilter looks good, I think we should also add a test for it (in principle the version of TestTeeTokenFilter from current trunk, not the one reverted to old API from the current patch)&lt;/li&gt;
	&lt;li&gt;I do not understand completely why this WeakReference is needed between Tee and Sink? If it is needed, the code may fail with NPE, when Reference.get() returns null. The idea is, that one can create a Sink for the Tee and throw the Sink away. Tee would then simply not pass the attributes anymore to the sink? If this is the case, the check for Reference.get()==null is really missing.&lt;/li&gt;
	&lt;li&gt;Should I implement CachingAttributesFilter as replacement for CachingTokenFilter, or will you do it together with TeeSink?&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I will now start to add all the finals to the missing core analyzers.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;The only small performance improvement we should probably make is to avoid checking which method in TokenStream is overridden when onlyUseNewAPI==true&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I could disable this for next() and next(Token). In the case of incrementToken, it should really check, that it is enabled, because not doing so would fail hard create endless loops. So the check should be there in all cases. But if onlyUseNewAPI is enabled, I could simply define hasNext and hasReusableNext=false. I will do this.&lt;/p&gt;</comment>
                    <comment id="12731896" author="gsingers" created="Thu, 16 Jul 2009 11:44:47 +0100"  >&lt;p&gt;Favor to ask, when this is ready to commit, can you give a few days notice so that the rest of us can look at it before committing?  I&apos;ve been keeping up with the comments, but not the patches.&lt;/p&gt;</comment>
                    <comment id="12731945" author="thetaphi" created="Thu, 16 Jul 2009 15:02:39 +0100"  >&lt;p&gt;New patch with some more work. First the phantastic news:&lt;/p&gt;

&lt;p&gt;As CachingTokenFilter has no API to access the cached attributes/tokens directly, it does not need to be deprecated, it just switched the internal and hidden impl to incrementToken() and attributes. I also added an additional test in the BW-Testcase, that checks if the caching also works for your strange POSTokens. And it works! You can even mix the consumers, e.g. first use new API to cache tokens and then replay using the old API. really cool. The problem, why the POSToken was not preserved in the past was an error in TokenWrapper.copyTo(). This method created a new Token and copied the contents into it using reinit(). Now it simply creates a clone and let delegate point to it (this is how the caching worked before).&lt;/p&gt;

&lt;p&gt;In principle Tee/SinkTokenizer could also work like this, the only problem with this class is the fact, that it has a public API that exposes the Token instances to the outside. Because of that, there is no way around deprecating.&lt;/p&gt;

&lt;p&gt;Your new TeeSinkTokenFilter looks good, it only had one problem:&lt;br/&gt;
It used addAttributeImpl to add the attribute of the Tee to the new created Sink. Because of this, the sink got the same instance as the parent added. With useOnlyNewAPI, this does not have an effect for the standard attributes, as the ctor already created a Token instance as implementation and added it to the stream, so addAttributeImpl had no effect.&lt;br/&gt;
I changed this to use the getAttributeClassesIterator and added a new attribute instance for each attribute using addAttribute to the sink. As the factory is the same, the attributes are generated in the same way. TeeSinkTokenizer would only &lt;b&gt;not&lt;/b&gt; work correctly if somebody addes an custom instance using addAttributeImpl in one ctor of another filter in the chain. In this case, the factory would create another impl and restoreState throws IAE. In backwards compatibility mode (default) the new created sink and also the tee have always the default TokenWrapper implementation, so state restoring also works. You only have a problem if you change useOnlyNewAPIU inbetween (which would always create corrupt chains).&lt;/p&gt;

&lt;p&gt;Another idea would be to clone all attribute impls and then add them to the sink - the factory would then not be used?&lt;/p&gt;

&lt;p&gt;I started to create a test for the new TeeSinkTokenFilter, but there is one thing missing: The original test created a subclass of SinkTokenizer, overriding add() to filter the tokens added to the sink. This functionality is missing with the new API. The correct workaround would be to plug a filter around the sink and filter the tokens there? The problem is then, that the cache always contains also non-needed tokens (the old impl would not store them in the sink).&lt;/p&gt;

&lt;p&gt;Maybe we add the filter to the TeeSinkTokenFilter (getting a State, which would not work, as contents of state pkg-private?). Somehow else? Or leave it as it is and let the user plug the filter on top of the sink (I prefer this)?&lt;/p&gt;</comment>
                    <comment id="12731947" author="thetaphi" created="Thu, 16 Jul 2009 15:04:00 +0100"  >&lt;p&gt;I forgot: I also implemented the final next() methods in all non-final classes.&lt;/p&gt;</comment>
                    <comment id="12731949" author="thetaphi" created="Thu, 16 Jul 2009 15:08:37 +0100"  >&lt;blockquote&gt;&lt;p&gt;Favor to ask, when this is ready to commit, can you give a few days notice so that the rest of us can look at it before committing? I&apos;ve been keeping up with the comments, but not the patches. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;No problem. I want to finish this until the weekend and then you have time to review it. My holidays start next week on monday, so I have only limited time after that.&lt;/p&gt;</comment>
                    <comment id="12732407" author="michaelbusch" created="Fri, 17 Jul 2009 09:36:15 +0100"  >&lt;blockquote&gt;
&lt;p&gt;As CachingTokenFilter has no API to access the cached attributes/tokens directly,&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Oh true &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; well, scratch it from the TODO list...&lt;/p&gt;

&lt;p&gt;We knew it&apos;d work conceptually the same for AttributeSource.State; unlike Tee/Sink, which wouldn&apos;t even be save to use with the new API if it hadn&apos;t the getTokens() method for the reasons I explained above.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Another idea would be to clone all attribute impls and then add them to the sink - the factory would then not be used?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes, I thought about this for a while. It would be nice to have this (i. e. cloning an AttributeSource) in general: you could reduce the costs for initializing the TokenStreams with onlyUseNewAPI=false. We just need to keep a static AttributeSource around, that contains the wrapper and the mappings from the 6 default interfaces. Then instead of constructing it every time we just clone that AttributeSource for new TokenStreams.&lt;/p&gt;

&lt;p&gt;The query parser could do the same to keep initialization costs of the TokenStreams minimal, because it always needs the same attributes.&lt;/p&gt;

&lt;p&gt;I think it should be easy? We just need to implement clone() for AttributeSource. &lt;/p&gt;</comment>
                    <comment id="12732408" author="michaelbusch" created="Fri, 17 Jul 2009 09:39:05 +0100"  >&lt;p&gt;I made these changes:&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;added clone() to AttributeSource and changed TeeSinkTokenFilter to use it.&lt;/li&gt;
	&lt;li&gt;added a SinkFilter as inner interface of TeeSinkTokenFilter that adds the missing functionality you mentioned, Uwe.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="12732450" author="thetaphi" created="Fri, 17 Jul 2009 11:25:32 +0100"  >&lt;p&gt;Patch looks good.&lt;br/&gt;
Only one thing: If you clone a TokenStream you will not get a TokenStream, only an AttributeSource instance (if TokenStream does not override). For our use case it is ok, because we only want to have the attributes and impls cloned, but it is strange.&lt;br/&gt;
A real clone() method should call super.clone() and then create new maps and copy the old maps into them. Not sure. Or we do not call the method clone() and call it cloneAttributes not returning Object but AttributeSource. E.g. &lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; AttributeSource cloneAttributes()&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I will now rewrite the TeeSink-Test to use the new interface and the test should then pass as before with Tee/Sink separate (but I let both tests available, one that tests the old ones and one that tests the new class). I also add a test for the cloning to TestAttributeSource.&lt;/p&gt;

&lt;p&gt;The cloning also speeds up the case with useOnlyNewAPI=true, because the addAttribute-call also uses reflection to find out what interfaces are implemented. In my opinion this cost (the while loop with getSuperclass() and so on) is much more costy than the simple check of the declaring class for a method.&lt;/p&gt;

&lt;p&gt;The patch is still missing some javadocs. If we finished this, are there any other things to do? The optimizations in QueryParser to clone and so on are not really part of this issue, so could be done separately.&lt;/p&gt;</comment>
                    <comment id="12732458" author="michaelbusch" created="Fri, 17 Jul 2009 11:59:55 +0100"  >&lt;blockquote&gt;
&lt;p&gt;Or we do not call the method clone() and call it cloneAttributes not returning Object but AttributeSource.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;+1. Let&apos;s do that.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;If we finished this, are there any other things to do? The optimizations in QueryParser to clone and so on are not really part of this issue, so could be done separately.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I agree. I don&apos;t think these optimizations are critical at this point. I think updating the javadocs should be the only remaining thing here. (given that everyone else is ok with this patch)&lt;/p&gt;

&lt;p&gt;The other related issues I think will be straightforward... except &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-1448&quot; title=&quot;add getFinalOffset() to TokenStream&quot;&gt;&lt;del&gt;LUCENE-1448&lt;/del&gt;&lt;/a&gt;, I have the feeling this will cause some headaches too.... not sure if you read the discussions in 1448 yet, Uwe?&lt;/p&gt;</comment>
                    <comment id="12732478" author="thetaphi" created="Fri, 17 Jul 2009 12:38:13 +0100"  >&lt;blockquote&gt;&lt;p&gt;+1. Let&apos;s do that.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;OK, I change this locally. And I remove the Cloneable interface again. In principle, this method cloneAttributes() should only be used, to create a new TokenStream that should use the same attributes, but needs different instances. TeeSink is currently the only example for this, but more may follow.&lt;/p&gt;

&lt;p&gt;About the TokenStream clones in QueryParser: I think, this will not work, as the TokenStream then needs to be really cloned with also setting a new Reader for the input. In my opinion, the reusableTokenStream method of Analyzer should handle this and not QueryParser.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;The other related issues I think will be straightforward... except &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-1448&quot; title=&quot;add getFinalOffset() to TokenStream&quot;&gt;&lt;del&gt;LUCENE-1448&lt;/del&gt;&lt;/a&gt;, I have the feeling this will cause some headaches too.... not sure if you read the discussions in 1448 yet, Uwe?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Wrrrrr, this one is hard. This final offset is not really fitting very good into the current attributes API, it could be an new extra attribute that is only updated at the end of the stream (but the problem is, that it needs to be done when incrementToken returns false.&lt;/p&gt;

&lt;p&gt;...and Mike said all others are trivial &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/sad.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;I will now update as noted before&lt;/p&gt;</comment>
                    <comment id="12732483" author="markrmiller@gmail.com" created="Fri, 17 Jul 2009 12:43:07 +0100"  >&lt;blockquote&gt;&lt;p&gt;...and Mike said all others are trivial &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;He also said hes willing to skip that one for 2.9 though. &lt;/p&gt;

&lt;p&gt;I&apos;d rather not if we can help it though - I started looking at it last night, but I got side tracked before I got very far thinking about it.&lt;/p&gt;</comment>
                    <comment id="12732733" author="thetaphi" created="Fri, 17 Jul 2009 21:59:46 +0100"  >&lt;p&gt;Here my latest patch before I go to bed. I had not much time today, but I implemented parts of the TeeSinkTokenFilter test. The first test and also the performance test are implemented. The performance test is almost as fast as the old Tee/Sink combi (good news). I found a small bug in the new Sink (it did not lazyly created the iterator), but it is fixed and it works as exspected (without calling reset() on the sink first).&lt;/p&gt;

&lt;p&gt;The second test is not implementable with TeeSinkTokenizer and this is a limitation: You are not able to combine different sources into one sink (and this is what the second test does). I am not sure, how you could implement this at all with the new API. It would only work if both tee streams have exactly same attributes, so they could feed their attributes into the same sink.&lt;/p&gt;

&lt;p&gt;Michael, any idea? The functionality to feed tokens from two streams into one sink is nice, but how to do it? Or is it just a useless theoretical demonstration in the test?&lt;/p&gt;

&lt;p&gt;Good night, Uwe&lt;/p&gt;</comment>
                    <comment id="12732749" author="michaelbusch" created="Fri, 17 Jul 2009 22:53:42 +0100"  >&lt;blockquote&gt;
&lt;p&gt;Michael, any idea? The functionality to feed tokens from two streams into one sink is nice, but how to do it? Or is it just a useless theoretical demonstration in the test?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I have personally never used the Tee/Sink stuff, so I&apos;m not sure in what kind of ways people use it.&lt;/p&gt;

&lt;p&gt;Also Tee/Sink does not use any internal-only APIs, so everyone could implement it outside of Lucene themselves. Of course this kind of stuff is a bit more tricky with the new API. &lt;/p&gt;

&lt;p&gt;If we want to support multiple tees feeding one sink it think it needs to be based on the Attribute interfaces themselves, not on the impls. Then it doesn&apos;t really matter, what kind of impls the sink has. The tee would just try to copy the Attribute&apos;s value over. But in this case we&apos;d need to implement the copyTo() method a bit differently, so that you can copy values per Attribute interface. This would probably be a bit slower compared to the current TeeSinkTokenFilter approach.&lt;/p&gt;</comment>
                    <comment id="12732757" author="yseeley@gmail.com" created="Fri, 17 Jul 2009 23:16:56 +0100"  >&lt;blockquote&gt;&lt;p&gt;I have personally never used the Tee/Sink stuff, so I&apos;m not sure in what kind of ways people use it.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Does anyone use it?  Given the difficulty of using it, esp since Lucene has been sorting fields before analysis (hence you have to name the fields properly to get one to be indexed before the other), maybe no one is using it.&lt;/p&gt;</comment>
                    <comment id="12732762" author="thetaphi" created="Fri, 17 Jul 2009 23:30:30 +0100"  >&lt;p&gt;In my opinion, this test that I was not able to reimplement using the new TeeSink is very theoretical. In my opinion someone who consumes two streams could always implement this very simple using the two aproaches:&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;consume the first stream completely, the  the other&lt;/li&gt;
	&lt;li&gt;consume one token from the first, then one token from the next and so on&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Who invented the whole Tee/Sink originally for what? Maybe we should simply ask on java-user who have ever used it. I have never heard of Tee/Sink before Michael confronted me with his strange POSToken tests &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                    <comment id="12732767" author="thetaphi" created="Fri, 17 Jul 2009 23:42:42 +0100"  >&lt;p&gt;So the simple case of an AppendingTokenStream could easily implemented in the same way (just a filter with not only one but an iterator of delegates). Just have an empty constructor and then add TokenStreams to it (that adds all attributes of each added tokenstream to itsself). Then each call to incrementToken() consumes each added stream until its exhausted. But I do not know if we really need to add this just for 1% of users that could also do it themselves.&lt;/p&gt;

&lt;p&gt;In my opinion the whole Tee/Sink/Appending is very theoretical (look at this test, it is very strange). Whoever would create such a ModuloTokenStream?&lt;/p&gt;</comment>
                    <comment id="12732769" author="gsingers" created="Fri, 17 Jul 2009 23:44:58 +0100"  >&lt;blockquote&gt;&lt;p&gt;Who invented the whole Tee/Sink originally for what?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yonik and I did. &lt;/p&gt;

&lt;p&gt;I have used it and would like to use it more.  For the case where the number of tokens captured is roughly less than 50% of the original number of tokens, it is faster than reanalyzing.&lt;/p&gt;</comment>
                    <comment id="12732772" author="gsingers" created="Fri, 17 Jul 2009 23:48:29 +0100"  >&lt;p&gt;It is not theoretical at all, even if the test is.  Imagine a stream that identifies phrases, captures them and then tees them to a second field.   Phrases happen infrequently relative to the number of overall tokens, so it is much faster to just buffer those few tokens that are part of the phrase and tee them to your &quot;phrases&quot; field.  &lt;/p&gt;

&lt;p&gt;The fact that it may not be used much now is probably due to lack of marketing more so than lack of functionality. &lt;/p&gt;</comment>
                    <comment id="12732773" author="michaelbusch" created="Fri, 17 Jul 2009 23:51:59 +0100"  >&lt;blockquote&gt;
&lt;p&gt;I have never heard of Tee/Sink before Michael confronted me with his strange POSToken tests &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Hey, stop insulting my awesome backwards-compatibility tests!! &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/biggrin.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                    <comment id="12732781" author="thetaphi" created="Sat, 18 Jul 2009 00:05:06 +0100"  >&lt;blockquote&gt;&lt;p&gt;Hey, stop insulting my awesome backwards-compatibility tests!! &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;They are phantastic. But I would use a simple FlagsAttribute/Token.flags() to annotate my tokens instead of creating a POSToken subclass...&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;It is not theoretical at all, even if the test is. Imagine a stream that identifies phrases, captures them and then tees them to a second field. Phrases happen infrequently relative to the number of overall tokens, so it is much faster to just buffer those few tokens that are part of the phrase and tee them to your &quot;phrases&quot; field. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;For this the new TeeSinkTokenFilter is working like a before (only better):&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
TeeSinkTokenFilter tee = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; TeeSinkTokenFilter(stream);
doc.add(&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; Field(&lt;span class=&quot;code-quote&quot;&gt;&quot;everything&quot;&lt;/span&gt;, tee));
doc.add(&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; Field(&lt;span class=&quot;code-quote&quot;&gt;&quot;parts1&quot;&lt;/span&gt;, &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; DoSomethingTokenFilter(tee.newSinkTokenStream()));
doc.add(&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; Field(&lt;span class=&quot;code-quote&quot;&gt;&quot;parts2&quot;&lt;/span&gt;, &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; DoSomethingOtherTokenFilter(tee.newSinkTokenStream(optionalFilter)));
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Our problem was more to have two Streams fed into the same Sink. This is not possible anymore, because Sinks are generated by a factory now and are always bound to one Tee.&lt;/p&gt;</comment>
                    <comment id="12732869" author="mikemccand" created="Sat, 18 Jul 2009 10:01:30 +0100"  >&lt;blockquote&gt;&lt;p&gt;Given the difficulty of using it, esp since Lucene has been sorting fields before analysis (hence you have to name the fields properly to get one to be indexed before the other), maybe no one is using it.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Can&apos;t we fix Tee/Sink so that whichever &quot;tee&quot; is pulled from first, does the caching, and then the 2nd one pulls from the cache?&lt;/p&gt;

&lt;p&gt;Ie right now when you create them you are forced to commit to which is &quot;primary&quot; and which is &quot;secondary&quot;, but if we relax that then it wouldn&apos;t be sensitive to the order in which Lucene indexed its fields.&lt;/p&gt;

&lt;p&gt;Of course, someday Lucene may index fields concurrently, then Tee/Sink&apos;ll get really interesting &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/wink.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                    <comment id="12732878" author="thetaphi" created="Sat, 18 Jul 2009 12:04:19 +0100"  >&lt;p&gt;In this case we should rename TeeSink to something like SplitTokenStream (which does not extend TokenStream). One could get then any number of &quot;sinks&quot; from it:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
SplitTokenFilter splitter=&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; SplitTokenStream(stream); &lt;span class=&quot;code-comment&quot;&gt;// does not extend TokenStream!!!
&lt;/span&gt;TokenStream stream1=splitter.newSinkTokenStream();
TokenStream stream2=splitter.newSinkTokenStream();
...
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In this case the caching would be done directly in the splitter and the sinks are only consumers. The first sink that calls to get the attribute states forces the splitter to harvest and cache the input stream (exactly like CachingTokenStream does it). In principle it would be the same like a CachingTokenStream.&lt;/p&gt;

&lt;p&gt;But on the other hand: You can always create a CachingTokenFilter and reuse the same instance for different fields. Because the indexer always calls reset() before consuming, you could re-read it easily. Any additional filters could then plugged in front for each field. In this case the order is not important:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
TokenStream stream=&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; CachingTokenFilter(input);
doc.add(&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; Field(&lt;span class=&quot;code-quote&quot;&gt;&quot;xyz&quot;&lt;/span&gt;, &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; DoSomethingTokenFilter(stream)));
doc.add(&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; Field(&lt;span class=&quot;code-quote&quot;&gt;&quot;abc&quot;&lt;/span&gt;, &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; DoSometingOtherTokenFilter(stream)));
...
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This would not work, if the indexer can consume the different fields in parallel. But with the current state it would even not work with Tee/Sink (not multithread compatible).&lt;/p&gt;</comment>
                    <comment id="12733068" author="thetaphi" created="Sun, 19 Jul 2009 22:23:25 +0100"  >&lt;p&gt;New and final patch. I will be in holidays from tomorrow and have limited time for Lucene. I will respond to comments and if there are major faults with the new API.&lt;/p&gt;

&lt;p&gt;The new patch has some imporvements:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;TeeSinkTokenizer is now able to also feed multiple tees to one sink. You create first a TeeSinkTokenizer, retrieve a TeeTokenStream from it (newSinkTokenStream()). After that you can add this TeeTokenStream to another tee (addSinkTokenStream()). The test (now similar to the old test) and javadocs demonstrates this.&lt;/li&gt;
	&lt;li&gt;Reflection performance was greatly improved by using caches. Most time was used in AttributeSource.addAttributeImpl() because it iterates through all interfaces of the supplied instance. It caches the found interfaces using a IdentityHashMap&amp;lt;Class&amp;lt;AttributeImpl&amp;gt;,LinkedList&amp;lt;Class&amp;lt;Attribute&amp;gt;&amp;gt;&amp;gt; keyed by the implementation class. Also the default AttributeFactory uses a cache (IdentityHashMap) for the mapping &amp;lt;Class&amp;lt;Attribute&amp;gt;,Class&amp;lt;AttributeImpl&amp;gt;&amp;gt;. So the number of Class.forName() is drastically reduced.&lt;/li&gt;
	&lt;li&gt;Also fixed a bug in addAttributeImpl after refactoring for the cache.&lt;/li&gt;
	&lt;li&gt;TokenStream now has a separate AttributeFactory available, that creates a TokenWrapper for the 6 default attributes. This is now a more clear implementation. The extra checks in next() default impls were removed because of this. Filters now also reuse the tokenWrapper instance already resolved by the input stream.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I did some performance tests with the final impl, analyzing the lorem ipsum text 100000 times with new instances for each time, using reused instances, old/new API for the trunk with latest patch, current trunk and lucene-2.4 (old api only):&lt;/p&gt;

&lt;p&gt;The results (but these test are not very representative due to a variance of +/- 4 sec per run):&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
Testing trunk w/ newest API...
Time &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; 100000 runs with &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; instances (old API): 27.344s
Time &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; 100000 runs with reused stream (old API): 21.828s
Time &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; 100000 runs with &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; instances (&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; API only): 27.297s
Time &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; 100000 runs with reused stream (&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; API only): 24.484s
Testing trunk w/o newest API...
Time &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; 100000 runs with &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; instances (old API): 22.485s
Time &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; 100000 runs with reused stream (old API): 19.047s
Time &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; 100000 runs with &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; instances (&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; API only): 26.89s
Time &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; 100000 runs with reused stream (&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; API only): 23.719s
Testing 2.4...
Time &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; 100000 runs with &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; instances (old API): 18.984s
Time &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; 100000 runs with reused stream (old API): 18.75s
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The cost of creating 100000 new instances on my 32 bit Thinkpad T60 is about 5 sec (no difference between new api and old api). The cost is not caused by reflection, it is caused by building the LinkedHashMaps for the attributes on creation. A little bit faster was the current trunk, because it uses only one LinkedHasMap.&lt;/p&gt;

&lt;p&gt;One interesting thing: Using &lt;b&gt;only the new api&lt;/b&gt; is little slower during tokenization, because it seems faster to use only &lt;b&gt;one&lt;/b&gt; instance (Token) instead of 6 instances.&lt;/p&gt;

&lt;p&gt;The cost of creating new instances is smallest with Lucene 2.4, because no attributes are used (in 2.9 it always creates the LinkedHashMaps, even if only the old API was used in current trunk).&lt;/p&gt;</comment>
                    <comment id="12733070" author="thetaphi" created="Sun, 19 Jul 2009 22:26:32 +0100"  >&lt;p&gt;Michael: can you wait for comments and commit &amp;amp; close then? Or should I do this on Wednesday? After closing this, there are some more issues to be notified/closed, because some of the core tokenstreams were already upgraded to latest API.&lt;/p&gt;</comment>
                    <comment id="12733081" author="michaelbusch" created="Sun, 19 Jul 2009 23:10:38 +0100"  >&lt;p&gt;Hi Uwe,&lt;/p&gt;

&lt;p&gt;I haven&apos;t reviewed the latest patch yet, but it sounds like great improvements you made! &lt;/p&gt;

&lt;p&gt;I can commit this after waiting a few days, and will also review during the time. And then I&apos;ll work on the related issues, just a bit worried still about 1448. &lt;/p&gt;</comment>
                    <comment id="12733083" author="thetaphi" created="Sun, 19 Jul 2009 23:26:49 +0100"  >&lt;p&gt;OK!&lt;/p&gt;

&lt;p&gt;I forgot to mention: I replaced the lucene JARs in trunk Solr and ran all the tests there, all of them pass! No speed degradion in solr, and all analyzers and query parsers work (and they still use the old API!). Also all Lucene core, Lucene contrib and backwards tests pass.&lt;/p&gt;

&lt;p&gt;By the way: the additional speed degradion even when using the old API with the current trunk version (which should have no speed degradion there when reusing stream) is caused by the latest CharFilters added to the Tokenizers. So please do not say, they come from the new API! &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                    <comment id="12733087" author="thetaphi" created="Mon, 20 Jul 2009 00:01:27 +0100"  >&lt;p&gt;Forgot CHANGES.txt entries with the backwards-break note and other changes.&lt;/p&gt;

&lt;p&gt;When starting to transform the contrib streams, we should not forget to do the following (maybe add this note to the JIRA issues):&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;rewrite and replace next(Token)/next() implementations by new API&lt;/li&gt;
	&lt;li&gt;if the class is final, no next(Token)/next() methods needed &lt;b&gt;(must be removed!!!)&lt;/b&gt;&lt;/li&gt;
	&lt;li&gt;if the class is non-final add the following methods to the class:
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
/** @deprecated Will be removed in Lucene 3.0. This method is &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt;, as it should
 * not be overridden. Delegates to the backwards compatibility layer. */
&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; Token next(&lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; Token reusableToken) &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; java.io.IOException {
  &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;super&lt;/span&gt;.next(reusableToken);
}

/** @deprecated Will be removed in Lucene 3.0. This method is &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt;, as it should
 * not be overridden. Delegates to the backwards compatibility layer. */
&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; Token next() &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; java.io.IOException {
  &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;super&lt;/span&gt;.next();
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Also the incrementToken() method must be final in this case.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="12733091" author="michaelbusch" created="Mon, 20 Jul 2009 00:19:35 +0100"  >&lt;p&gt;I like the cache you added to AttributeSource for speedup!&lt;/p&gt;

&lt;p&gt;One thing we could improve is to add this check, which avoids iterating the foundInterfaces list in case the AttributeImpl has been added previously:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
  /** Adds a custom AttributeImpl instance with one or more Attribute interfaces. */
  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; void addAttributeImpl(&lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; AttributeImpl att) {
    &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;Class&lt;/span&gt; clazz = att.getClass();
    &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (attributeImpls.containsKey(clazz)) &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt;;

&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                    <comment id="12733095" author="thetaphi" created="Mon, 20 Jul 2009 00:43:54 +0100"  >&lt;p&gt;Yes, good idea. I added it locally, works good. This is also a speed improvement (the synchronized access to the cache is also removed by this).&lt;/p&gt;

&lt;p&gt;I think a new patch is not needed, just add it to your checkout, too!&lt;/p&gt;</comment>
                    <comment id="12733096" author="michaelbusch" created="Mon, 20 Jul 2009 01:10:42 +0100"  >&lt;p&gt;yep, did already and all tests pass!&lt;/p&gt;</comment>
                    <comment id="12733580" author="mikemccand" created="Tue, 21 Jul 2009 11:41:34 +0100"  >&lt;p&gt;OK I looked at the latest patch!  It looks good &amp;#8211; only these questions:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;Are we going to keep Token.java or not?  (Current patch still has&lt;br/&gt;
    it deprecated).&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Typo (occure) in CHANGES.txt&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;It looks like we are making all core tokenizers/filters final (to&lt;br/&gt;
    foreclose future back-compat problems if we need to change the&lt;br/&gt;
    API)?  If so, we should also make CachingTokenFilter final?&lt;/li&gt;
&lt;/ul&gt;


</comment>
                    <comment id="12733604" author="gsingers" created="Tue, 21 Jul 2009 13:50:26 +0100"  >&lt;p&gt;One of the things that works really well in Solr is that any time some significant JIRA issue is undertaken, a Wiki page is also generated that effectively documents the ideas in the patch, as well as how to use it and thus results in the final page effectively becoming the documentation.  I know Mike and Uwe have done a ton of work on this, but would it be too much trouble to ask for a Wiki page that describes the current state of the patch?  It is really hard to follow, in JIRA, all the different threads and which ones are still valid and which are not.&lt;/p&gt;</comment>
                    <comment id="12733761" author="thetaphi" created="Tue, 21 Jul 2009 19:11:10 +0100"  >&lt;p&gt;Mark Miller on java-dev:&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;* Are we going to keep Token.java or not?  (Current patch still has it deprecated).&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I need to know this as well - I have to make a new Token class for the Highlighter package if this one is deprecated. It would seem a convenience to keep it around.&lt;/p&gt;</comment>
                    <comment id="12733765" author="thetaphi" created="Tue, 21 Jul 2009 19:16:51 +0100"  >&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;* Are we going to keep Token.java or not?  (Current patch still has it deprecated).&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I need to know this as well - I have to make a new Token class for the Highlighter package if this one is deprecated. It would seem a convenience to keep it around.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I would also keep it non-deprecated. It is useful also as a convenience class and can be used as one attribute instance for all 6 base attributes (the backwards compatibility layer of the current patch already does this).&lt;/p&gt;</comment>
                    <comment id="12733845" author="thetaphi" created="Tue, 21 Jul 2009 22:59:44 +0100"  >&lt;p&gt;Updated patch that has the following changes:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Updated QueryParser.jj instead of .java and rebuilt the parser&lt;/li&gt;
	&lt;li&gt;added final to incrementToken() in CachingTokenFilter&lt;/li&gt;
	&lt;li&gt;added Michael&apos;s AttributeSource improvement&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="12733857" author="michaelbusch" created="Tue, 21 Jul 2009 23:23:23 +0100"  >&lt;p&gt;I agree  - we should keep Token and not deprecate it.&lt;/p&gt;</comment>
                    <comment id="12733930" author="michaelbusch" created="Wed, 22 Jul 2009 02:06:20 +0100"  >&lt;p&gt;Uwe&apos;s latest patch with these changes:&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Token is not deprecated anymore.&lt;/li&gt;
	&lt;li&gt;Updated the examples in the &apos;New TokenStream API&quot; section in package.html (analysis package) to reflect the changes this patch introduces (Attribute vs. AttributeImpl.)&lt;/li&gt;
	&lt;li&gt;All new files have svn:eol-style=native set now.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;test-core, test-contrib and test-tag all pass. I think this is ready to commit now. &lt;/p&gt;

&lt;p&gt;We might want to improve some javadocs a bit, especially the &quot;expert&quot; APIs, such as AttributeFactory. Also  another section about how to improve caching performance would be good. But I think we should open a separate issue for the doc improvements to get the other related issues going. The main APIs are pretty well documented I think.&lt;/p&gt;</comment>
                    <comment id="12733931" author="michaelbusch" created="Wed, 22 Jul 2009 02:08:39 +0100"  >&lt;p&gt;I think the points listed in the description of this issue are still mostly valid. Uwe, do you want to add the changes you made in addition?&lt;/p&gt;</comment>
                    <comment id="12734041" author="thetaphi" created="Wed, 22 Jul 2009 09:58:56 +0100"  >&lt;p&gt;Updated issue description/summary to reflect last state.&lt;/p&gt;</comment>
                    <comment id="12734060" author="thetaphi" created="Wed, 22 Jul 2009 11:20:24 +0100"  >&lt;p&gt;Forgot the new TeeSinkTokenFilter and reformatted the text to use NLs.&lt;/p&gt;</comment>
                    <comment id="12734274" author="howlingdawg" created="Wed, 22 Jul 2009 20:40:56 +0100"  >&lt;p&gt;My first post to the list, it appears i should comment here in the JIRA, not reply to email, apologize if i did this wrong:&lt;/p&gt;

&lt;p&gt;I&apos;ve been following this AttributeSource/TokenStream patch thread and reviewing the changes/backwards compatibility issues and the changes.  &lt;br/&gt;
extremely interesting problem/solution.&lt;/p&gt;

&lt;p&gt;while looking at Uwe&apos;s PerfTest3 I noticed an unused allocation in the last run for &quot;reused stream new API only&quot;&lt;/p&gt;

&lt;p&gt;     for (int i = 0; i &amp;lt; c; i++) {&lt;br/&gt;
        if (i==1000) t = System.currentTimeMillis();&lt;br/&gt;
        tz.reset(new StringReader(text));&lt;br/&gt;
        // Token reusableToken=new Token();   &amp;lt;&amp;lt;&amp;lt;&amp;lt;     This one&lt;br/&gt;
        int num=0;&lt;br/&gt;
        while (tok.incrementToken()) &lt;/p&gt;
{
          num++;
        }
&lt;p&gt;      }&lt;/p&gt;


&lt;p&gt;just a small cost, but makes the new reusable api slightly faster&lt;/p&gt;

&lt;p&gt;With extra alloc:&lt;/p&gt;

&lt;p&gt;Time for 100000 runs with new instances (old API): 12.75s&lt;br/&gt;
Time for 100000 runs with reused stream (old API): 9.969s&lt;br/&gt;
Time for 100000 runs with new instances (new API only): 13.969s&lt;br/&gt;
Time for 100000 runs with reused stream (new API only): 11.735s  &amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&lt;/p&gt;


&lt;p&gt;Without extra alloc (changes only the last line&apos;s time):&lt;/p&gt;

&lt;p&gt;Time for 100000 runs with new instances (old API): 12.593s&lt;br/&gt;
Time for 100000 runs with reused stream (old API): 9.578s&lt;br/&gt;
Time for 100000 runs with new instances (new API only): 13.75s&lt;br/&gt;
Time for 100000 runs with reused stream (new API only): 11.453s     &amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&lt;/p&gt;



&lt;p&gt;dave&lt;/p&gt;</comment>
                    <comment id="12734289" author="michaelbusch" created="Wed, 22 Jul 2009 21:14:34 +0100"  >&lt;p&gt;Thanks, Dave... I&apos;ll remove that unused allocation before committing.&lt;/p&gt;</comment>
                    <comment id="12734291" author="michaelbusch" created="Wed, 22 Jul 2009 21:15:34 +0100"  >&lt;p&gt;OK, I think we&apos;re finally ready to commit here!&lt;/p&gt;

&lt;p&gt;I&apos;ll wait until Friday - if nobody objects until then, I will commit the latest patch.&lt;/p&gt;</comment>
                    <comment id="12734867" author="gsingers" created="Fri, 24 Jul 2009 02:35:56 +0100"  >&lt;p&gt;Checking now.&lt;/p&gt;</comment>
                    <comment id="12734888" author="gsingers" created="Fri, 24 Jul 2009 04:30:07 +0100"  >&lt;blockquote&gt;&lt;p&gt;Token is no longer deprecated, instead it implements all 6 standard&lt;br/&gt;
token interfaces (see above). The wrapper for next() and next(Token)&lt;br/&gt;
uses this, to automatically map all attribute interfaces to one&lt;br/&gt;
TokenWrapper instance (implementing all 6 interfaces), that contains&lt;br/&gt;
a Token instance. next() and next(Token) exchange the inner Token&lt;br/&gt;
instance as needed. For the new incrementToken(), only one&lt;br/&gt;
TokenWrapper instance is visible, delegating to the currect reusable&lt;br/&gt;
Token. This API also preserves custom Token subclasses, that maybe&lt;br/&gt;
created by very special token streams (see example in Backwards-Test)&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Token is no longer deprecated, but all the methods that return it are, right?  I think they are, but it is late, and I may have missed something.  So, what happens in 3.0?  What good does a Token do at that point?&lt;/p&gt;

&lt;p&gt;Also, in looking at incrementToken(), nearly the entire implementation seems to be based on deprecated stuff, doesn&apos;t that mean it has to all be re-implemented in 3.0?  Something about that doesn&apos;t feel right.&lt;/p&gt;

&lt;p&gt;More later...&lt;/p&gt;
</comment>
                    <comment id="12734955" author="michaelbusch" created="Fri, 24 Jul 2009 08:50:04 +0100"  >&lt;blockquote&gt;
&lt;p&gt;Token is no longer deprecated, but all the methods that return it are, right? I think they are, but it is late, and I may have missed something. So, what happens in 3.0? What good does a Token do at that point?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;It&apos;s what you often asked for: if you don&apos;t want to deal with multiple Attributes you can simply add a Token to the AttributeSource and cache the Token reference locally in your stream/filter, because Token now implements all core token attributes.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Also, in looking at incrementToken(), nearly the entire implementation seems to be based on deprecated stuff, doesn&apos;t that mean it has to all be re-implemented in 3.0? Something about that doesn&apos;t feel right.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Ideally there wouldn&apos;t be an incrementToken() implementation and it would be abstract. However, that&apos;s of course not backwards-compatible. Everything is deprecated in the implementation because this code was only written to support backwards-compatibility with the old API.&lt;/p&gt;</comment>
                    <comment id="12734959" author="thetaphi" created="Fri, 24 Jul 2009 08:59:57 +0100"  >&lt;p&gt;And in 3.0 all my phantastic backwards-compatibility stuff is completely removed - and incrementToken() is abstract - What a pity &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/sad.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; The default impl in incrementToken and all other code parts in TokenStream.java will be removed (this is why also new code parts are marked deprectated, it makes it easier to be removed). The code will be about 1/3 of the current size when the backwards layer is removed &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;br/&gt;
The biggest problem of the migration to the new API is the backwards stuff, because we have now 3 different TokenStream APIs (next(), next(Token), incrementToken()). Also TokenWrapper is backwards stuff and will be removed.&lt;/p&gt;</comment>
                    <comment id="12734963" author="thetaphi" created="Fri, 24 Jul 2009 09:11:28 +0100"  >&lt;blockquote&gt;&lt;p&gt;if you don&apos;t want to deal with multiple Attributes you can simply add a Token to the AttributeSource and cache the Token reference locally in your stream/filter, because Token now implements all core token attributes.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;See the test case for AttributeSource, which tests this.&lt;br/&gt;
But I think it is not as easy if you have a chain of TokenFilters. Only the first one can add the Token Impl to the AttSource (when the attributes are not yet added). So if one TokenStream adds a TermAttribute and later a Token impl is added, the Token will handle all attributes except the TermAttribute.&lt;br/&gt;
To force a whole chain to use Token as AttributeImpl, the first created TokenStream (normally the Tokenizer) should set an AttributeFactory, that creates a Token. All filters will then get it from the parent.&lt;br/&gt;
So in general you can add an Token instance to the AttributeSource but should still reference the attributes by the interfaces.&lt;/p&gt;</comment>
                    <comment id="12734982" author="michaelbusch" created="Fri, 24 Jul 2009 09:48:53 +0100"  >&lt;blockquote&gt;
&lt;p&gt;So in general you can add an Token instance to the AttributeSource but should still reference the attributes by the interfaces.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I completely agree. We should discourage users to reference Token and rather use the Attribute interfaces. That&apos;s the whole beauty and flexibility about this new API.&lt;/p&gt;

&lt;p&gt;However, using Token as the actual implementing instance can be convenient to optimize caching or serialization performance.&lt;/p&gt;</comment>
                    <comment id="12735114" author="michaelbusch" created="Fri, 24 Jul 2009 19:25:00 +0100"  >&lt;p&gt;Grant, are you still reviewing? I was going to commit this today... shall I wait?&lt;/p&gt;</comment>
                    <comment id="12735119" author="gsingers" created="Fri, 24 Jul 2009 19:34:26 +0100"  >&lt;p&gt;Go ahead, I&apos;m satisfied.&lt;/p&gt;</comment>
                    <comment id="12735123" author="michaelbusch" created="Fri, 24 Jul 2009 19:50:12 +0100"  >&lt;p&gt;Cool thanks for reviewing.&lt;/p&gt;

&lt;p&gt;I&apos;ll commit later this afternoon.&lt;/p&gt;</comment>
                    <comment id="12735124" author="rcmuir" created="Fri, 24 Jul 2009 19:56:47 +0100"  >&lt;blockquote&gt;
&lt;p&gt;I completely agree. We should discourage users to reference Token and rather use the Attribute interfaces. That&apos;s the whole beauty and flexibility about this new API.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Has there been any thought into reconsidering the new API&apos;s &quot;experimental&quot; status then?&lt;br/&gt;
I don&apos;t think the WARNING: encourages users to use these interfaces!&lt;/p&gt;

&lt;p&gt;or maybe a compromise: maybe modify the javadocs to be a little less scary: does this text have to be FF0000 (red) ?&lt;/p&gt;</comment>
                    <comment id="12735125" author="gsingers" created="Fri, 24 Jul 2009 19:57:49 +0100"  >&lt;p&gt;Actually, one thing I still don&apos;t get:&lt;/p&gt;

&lt;p&gt;What happens to the attributes that have traditionally been thrown away during indexing?  ie offset, type?  How would one add them into the index like other attributes?  Or, for that matter, exclude them.&lt;/p&gt;

&lt;p&gt;I seem to recall there being a loop over attributes somewhere in the posting process, but I can no longer find that code.&lt;/p&gt;</comment>
                    <comment id="12735133" author="michaelbusch" created="Fri, 24 Jul 2009 20:29:20 +0100"  >&lt;blockquote&gt;
&lt;p&gt;I don&apos;t think the WARNING: encourages users to use these interfaces!&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yeah we should improve that. Let me commit the patch as is and open a new issue for improving the warnings.&lt;br/&gt;
I don&apos;t want to touch this patch anymore, it&apos;s so big.&lt;/p&gt;</comment>
                    <comment id="12735134" author="michaelbusch" created="Fri, 24 Jul 2009 20:31:11 +0100"  >&lt;blockquote&gt;
&lt;p&gt;What happens to the attributes that have traditionally been thrown away during indexing? ie offset, type? How would one add them into the index like other attributes? Or, for that matter, exclude them.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The default index format does not make use of some attributes, e.g. type, just as before.&lt;br/&gt;
Flexible indexing will allow to customize the format; then you will be able to store whatever attribute you like in the index.&lt;/p&gt;</comment>
                    <comment id="12735176" author="michaelbusch" created="Fri, 24 Jul 2009 22:48:12 +0100"  >&lt;p&gt;Committed revision 797665.&lt;/p&gt;

&lt;p&gt;Thanks, Uwe, for all your hard work!!&lt;br/&gt;
And thanks to everyone else who helped reviewing here.&lt;/p&gt;</comment>
                    <comment id="12735210" author="markrmiller@gmail.com" created="Sat, 25 Jul 2009 00:56:52 +0100"  >&lt;p&gt;Not sure what issue it stems from, but Token has a bunch of constructors that are deprecated, but that don&apos;t point you to something new.&lt;/p&gt;

&lt;p&gt;edit&lt;/p&gt;

&lt;p&gt;must have come from the setBuffer stuff &lt;/p&gt;</comment>
                    <comment id="12735215" author="michaelbusch" created="Sat, 25 Jul 2009 01:10:42 +0100"  >&lt;blockquote&gt;
&lt;p&gt;Not sure what issue it stems from, but Token has a bunch of constructors that are deprecated, but that don&apos;t point you to something new. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This is already the case in Lucene 2.4; unrelated to this issue.&lt;/p&gt;</comment>
                    <comment id="12735249" author="thetaphi" created="Sat, 25 Jul 2009 10:05:06 +0100"  >&lt;p&gt;This is a small improvement, related to Grant&apos;s comments:&lt;/p&gt;

&lt;p&gt;The TokenStream ctor can have a AttributeFactory, so you can create a subclass of TokenStream that uses a specific AttributeFacory (e.g. using Token instances). Filters do not need this (as they use the factory of the input stream). &lt;br/&gt;
The factory must therefore be set on the root stream. This is normally a subclass of Tokenizer. The problem: Tokenizer does not have ctors for AttributeFacory, so you are not able to create any Tokenizer using a custom factory, e.g. for using Token as impl.&lt;/p&gt;

&lt;p&gt;I will commit this patch shortly.&lt;/p&gt;</comment>
                    <comment id="12735253" author="thetaphi" created="Sat, 25 Jul 2009 10:23:32 +0100"  >&lt;p&gt;Committed revision: 797727&lt;/p&gt;</comment>
                </comments>
                <issuelinks>
                        <issuelinktype id="10032">
                <name>Blocker</name>
                                <outwardlinks description="blocks">
                            <issuelink>
            <issuekey id="12428010">LUCENE-1696</issuekey>
        </issuelink>
                    </outwardlinks>
                                            </issuelinktype>
                        <issuelinktype id="10030">
                <name>Reference</name>
                                <outwardlinks description="relates to">
                            <issuelink>
            <issuekey id="12427999">LUCENE-1695</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12428012">LUCENE-1697</issuekey>
        </issuelink>
                    </outwardlinks>
                                            </issuelinktype>
                        <issuelinktype id="10001">
                <name>dependent</name>
                                                <inwardlinks description="is depended upon by">
                            <issuelink>
            <issuekey id="12408785">LUCENE-1460</issuekey>
        </issuelink>
                    </inwardlinks>
                            </issuelinktype>
                    </issuelinks>
                <attachments>
                    <attachment id="12414170" name="lucene-1693.patch" size="229469" author="michaelbusch" created="Wed, 22 Jul 2009 02:06:20 +0100" />
                    <attachment id="12413783" name="lucene-1693.patch" size="198912" author="michaelbusch" created="Fri, 17 Jul 2009 09:39:05 +0100" />
                    <attachment id="12413662" name="lucene-1693.patch" size="177929" author="michaelbusch" created="Thu, 16 Jul 2009 09:48:34 +0100" />
                    <attachment id="12410775" name="lucene-1693.patch" size="113350" author="michaelbusch" created="Tue, 16 Jun 2009 10:52:03 +0100" />
                    <attachment id="12414148" name="LUCENE-1693.patch" size="224244" author="thetaphi" created="Tue, 21 Jul 2009 22:59:44 +0100" />
                    <attachment id="12413967" name="LUCENE-1693.patch" size="216188" author="thetaphi" created="Mon, 20 Jul 2009 00:01:27 +0100" />
                    <attachment id="12413875" name="LUCENE-1693.patch" size="203605" author="thetaphi" created="Fri, 17 Jul 2009 21:59:46 +0100" />
                    <attachment id="12413689" name="LUCENE-1693.patch" size="200415" author="thetaphi" created="Thu, 16 Jul 2009 15:02:39 +0100" />
                    <attachment id="12413531" name="LUCENE-1693.patch" size="176174" author="thetaphi" created="Wed, 15 Jul 2009 08:55:44 +0100" />
                    <attachment id="12413529" name="LUCENE-1693.patch" size="176190" author="thetaphi" created="Wed, 15 Jul 2009 08:18:48 +0100" />
                    <attachment id="12413451" name="LUCENE-1693.patch" size="176299" author="thetaphi" created="Tue, 14 Jul 2009 18:36:51 +0100" />
                    <attachment id="12413203" name="LUCENE-1693.patch" size="145064" author="thetaphi" created="Sat, 11 Jul 2009 15:50:04 +0100" />
                    <attachment id="12412979" name="LUCENE-1693.patch" size="143456" author="thetaphi" created="Thu, 9 Jul 2009 10:14:17 +0100" />
                    <attachment id="12411640" name="LUCENE-1693.patch" size="128082" author="thetaphi" created="Wed, 24 Jun 2009 13:42:24 +0100" />
                    <attachment id="12411221" name="LUCENE-1693.patch" size="129142" author="thetaphi" created="Fri, 19 Jun 2009 13:41:04 +0100" />
                    <attachment id="12411069" name="LUCENE-1693.patch" size="111288" author="thetaphi" created="Thu, 18 Jun 2009 13:05:25 +0100" />
                    <attachment id="12410977" name="LUCENE-1693.patch" size="111366" author="thetaphi" created="Wed, 17 Jun 2009 22:00:29 +0100" />
                    <attachment id="12410927" name="LUCENE-1693.patch" size="108911" author="thetaphi" created="Wed, 17 Jun 2009 13:35:33 +0100" />
                    <attachment id="12410874" name="LUCENE-1693.patch" size="98228" author="thetaphi" created="Wed, 17 Jun 2009 01:21:25 +0100" />
                    <attachment id="12414506" name="LUCENE-1693-TokenizerAttrFactory.patch" size="1393" author="thetaphi" created="Sat, 25 Jul 2009 10:05:06 +0100" />
                    <attachment id="12413965" name="PerfTest3.java" size="4364" author="thetaphi" created="Sun, 19 Jul 2009 22:23:25 +0100" />
                    <attachment id="12413526" name="TestAPIBackwardsCompatibility.java" size="17749" author="michaelbusch" created="Wed, 15 Jul 2009 06:46:03 +0100" />
                    <attachment id="12411706" name="TestCompatibility.java" size="7767" author="michaelbusch" created="Wed, 24 Jun 2009 22:12:49 +0100" />
                    <attachment id="12411610" name="TestCompatibility.java" size="7767" author="michaelbusch" created="Wed, 24 Jun 2009 07:45:22 +0100" />
                    <attachment id="12410983" name="TestCompatibility.java" size="2964" author="michaelbusch" created="Wed, 17 Jun 2009 22:27:00 +0100" />
                    <attachment id="12410906" name="TestCompatibility.java" size="2604" author="michaelbusch" created="Wed, 17 Jun 2009 07:44:29 +0100" />
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>26.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Tue, 16 Jun 2009 10:04:35 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>12065</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    <customfieldvalue key="10120"><![CDATA[Patch Available]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>26033</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                    <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                <customfieldname>Time in Status</customfieldname>
                <customfieldvalues>
                    
                </customfieldvalues>
            </customfield>
                            </customfields>
    </item>
</channel>
</rss>