<!-- 
RSS generated by JIRA (5.2.8#851-sha1:3262fdc28b4bc8b23784e13eadc26a22399f5d88) at Tue Jul 16 13:17:25 UTC 2013

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/LUCENE-2205/LUCENE-2205.xml?field=key&field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>5.2.8</version>
        <build-number>851</build-number>
        <build-date>26-02-2013</build-date>
    </build-info>

<item>
            <title>[LUCENE-2205] Rework of the TermInfosReader class to remove the Terms[], TermInfos[], and the index pointer long[] and create a more memory efficient data structure.</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-2205</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Basically packing those three arrays into a byte array with an int array as an index offset.  &lt;/p&gt;

&lt;p&gt;The performance benefits are stagering on my test index (of size 6.2 GB, with ~1,000,000 documents and ~175,000,000 terms), the memory needed to load the terminfos into memory were reduced to 17% of there original size.  From 291.5 MB to 49.7 MB.  The random access speed has been made better by 1-2%, load time of the segments are ~40% faster as well, and full GC&apos;s on my JVM were made 7 times faster.&lt;/p&gt;

&lt;p&gt;I have already performed the work and am offering this code as a patch.  Currently all test in the trunk pass with this new code enabled.  I did write a system property switch to allow for the original implementation to be used as well.&lt;/p&gt;

&lt;p&gt;-Dorg.apache.lucene.index.TermInfosReader=default or small&lt;/p&gt;

&lt;p&gt;I have also written a blog about this patch here is the link.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.nearinfinity.com/blogs/aaron_mccurry/my_first_lucene_patch.html&quot; class=&quot;external-link&quot;&gt;http://www.nearinfinity.com/blogs/aaron_mccurry/my_first_lucene_patch.html&lt;/a&gt;&lt;/p&gt;


</description>
                <environment>&lt;p&gt;Java5&lt;/p&gt;</environment>
            <key id="12445384">LUCENE-2205</key>
            <summary>Rework of the TermInfosReader class to remove the Terms[], TermInfos[], and the index pointer long[] and create a more memory efficient data structure.</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png">Closed</status>
                    <resolution id="1">Fixed</resolution>
                                <assignee username="mikemccand">Michael McCandless</assignee>
                                <reporter username="amccurry">Aaron McCurry</reporter>
                        <labels>
                    </labels>
                <created>Wed, 13 Jan 2010 08:28:34 +0000</created>
                <updated>Sun, 27 Nov 2011 12:29:23 +0000</updated>
                    <resolved>Thu, 27 Oct 2011 21:47:38 +0100</resolved>
                                            <fixVersion>3.5</fixVersion>
                <fixVersion>4.0-ALPHA</fixVersion>
                                <component>core/index</component>
                        <due></due>
                    <votes>5</votes>
                        <watches>11</watches>
                                                    <comments>
                    <comment id="12799651" author="amccurry" created="Wed, 13 Jan 2010 08:29:43 +0000"  >&lt;p&gt;All unit tests passed.&lt;/p&gt;</comment>
                    <comment id="12799657" author="amccurry" created="Wed, 13 Jan 2010 08:41:05 +0000"  >&lt;p&gt;This is the program that used to get performance metrics.  I have also attached the raw output of the test.&lt;/p&gt;</comment>
                    <comment id="12799676" author="mikemccand" created="Wed, 13 Jan 2010 10:10:26 +0000"  >&lt;p&gt;We&apos;ve done something very similar to this, on the &quot;flex&quot; branch at &lt;a href=&quot;https://svn.apache.org/repos/asf/lucene/java/branches/flex_1458&quot; class=&quot;external-link&quot;&gt;https://svn.apache.org/repos/asf/lucene/java/branches/flex_1458&lt;/a&gt; (which should land for Lucene 3.1), eg have a look at the oal.index.codecs.standard.SimpleStandardTermsIndexReader, in the inner CoreFieldIndex class, where we use single arrays to track the value for each indexed term.&lt;/p&gt;</comment>
                    <comment id="12799772" author="amccurry" created="Wed, 13 Jan 2010 14:48:34 +0000"  >&lt;p&gt;I took a look at that class, and it does look somewhat similar.  Has anyone run any numbers against it to see that savings or performance of it?  I&apos;m at work now, but I may try it out in my tests when I get home.&lt;/p&gt;</comment>
                    <comment id="12799825" author="mikemccand" created="Wed, 13 Jan 2010 16:56:16 +0000"  >&lt;p&gt;I don&apos;t think anyone&apos;s run specific numbers on the terms dict &amp;#8211; that&apos;d be great if you get a chance to do so!&lt;/p&gt;

&lt;p&gt;Note that we plan also to cutover those arrays to packed ints (&lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-1990&quot; title=&quot;Add unsigned packed int impls in oal.util&quot;&gt;&lt;del&gt;LUCENE-1990&lt;/del&gt;&lt;/a&gt;) since eg using a long for fileOffset and blockPointer is very wasteful in general.  Even a short to record the length of each term is wasteful.&lt;/p&gt;</comment>
                    <comment id="12799835" author="amccurry" created="Wed, 13 Jan 2010 17:33:49 +0000"  >&lt;p&gt;My in memory implementation uses Vints and Vlongs, so they aren&apos;t that bad for storing the fileoffsets and blockpointer&apos;s.&lt;/p&gt;</comment>
                    <comment id="12799862" author="mikemccand" created="Wed, 13 Jan 2010 18:35:47 +0000"  >&lt;p&gt;That&apos;s very interesting &amp;#8211; I like that approach.  So it  presumably has higher index/enumeration CPU cost but lower RAM cost.  I think this is an OK tradeoff with terms dict, since &quot;typically&quot; the cost of the term lookup is dwarfed by the cost of iterating the postings (though, for TermRangeQuery on many low-freq terms, this doesn&apos;t hold true).&lt;/p&gt;

&lt;p&gt;In flex you could just implement StandardTermsIndexReader &amp;#8211; the purpose of that abstract class is to allow pluggability of the terms index.  Also, implementing this new approach there would be a good test case &lt;span class=&quot;error&quot;&gt;&amp;#91;that it&amp;#39;s usable, generic enough, etc.&amp;#93;&lt;/span&gt;.  You should be able to plug it in (temporarily make it the default, for the standard codec), then pass all unit tests...&lt;/p&gt;</comment>
                    <comment id="12799869" author="amccurry" created="Wed, 13 Jan 2010 18:53:56 +0000"  >&lt;p&gt;Sure I can rework things for that.  Not sure if read through my blog, but it&lt;br/&gt;
seems as if the packing of the bytes as vints and such are actually faster&lt;br/&gt;
than doing the java reference lookups.  At least for the test cases that I&lt;br/&gt;
have come up with.  Plus the JVM&apos;s GC is much happier without having to&lt;br/&gt;
traverse the object graph of terms.&lt;/p&gt;

&lt;p&gt;Over the next few days I will rework the api to StandardTermsIndexReader and&lt;br/&gt;
repost a patch.  Thanks.&lt;/p&gt;

&lt;p&gt;On Wed, Jan 13, 2010 at 1:36 PM, Michael McCandless (JIRA)&lt;/p&gt;
</comment>
                    <comment id="12799906" author="mikemccand" created="Wed, 13 Jan 2010 19:57:35 +0000"  >&lt;p&gt;Another benefit doing this with flex is you can also change the index file format, ie write the vints to disk (so &quot;build&quot; is done at index time, not reader startup time), so the init time would be even faster.&lt;/p&gt;

&lt;p&gt;Hmm... it&apos;s surprising you&apos;re seeing faster decode time &amp;#8211; it looks like you read a vint per character of each index term compared, during the binary search?  Vs String.compareTo done by trunk.  (Though, if those characters are simple ascii, then the vint is always a single byte read).&lt;/p&gt;

&lt;p&gt;Actually, couldn&apos;t you simply compare the utf8 bytes (plus a &quot;fixup&quot;, to match UTF16 sort order), which would require no per-character vint decode?  (flex does this, since it holds the term data as utf8 bytes in memory).&lt;/p&gt;</comment>
                    <comment id="12799923" author="amccurry" created="Wed, 13 Jan 2010 20:42:26 +0000"  >&lt;p&gt;Well to be honest, I spent a lot of time making the uni-code UTF-8/16/32 compare work, and work faster than the default implementation of TermInfosReader.  I thought the same thing, but it didn&apos;t seem to work faster.  I think that now that I have a working version as a baseline, I will go back and try some different things in the term.text compare.&lt;/p&gt;

&lt;p&gt;As far as &quot;Another benefit doing this with flex is you can also change the index file format, ie write the vints to disk (so &quot;build&quot; is done at index time, not reader startup time), so the init time would be even faster.&quot;&lt;/p&gt;

&lt;p&gt;I actually have that implemented in our production system to give us an &quot;instant on&quot; capability when our huge indexes have to be reloaded.  But I thought I would start simple for my contribution.  &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                    <comment id="12802015" author="kalradeepak" created="Tue, 19 Jan 2010 01:08:18 +0000"  >&lt;p&gt;Hi Aaron&lt;/p&gt;

&lt;p&gt;I would like to test this patch on our environment. It seems patch you have supplied does not contain full version of Term.java&lt;/p&gt;

&lt;p&gt;Can you please suppy this class Term.java?&lt;/p&gt;

&lt;p&gt;Regards&lt;br/&gt;
D&lt;/p&gt;</comment>
                    <comment id="12802023" author="amccurry" created="Tue, 19 Jan 2010 01:33:55 +0000"  >&lt;p&gt;Sure, I will post it all in an hour or so.&lt;/p&gt;

&lt;p&gt;Aaron&lt;/p&gt;



</comment>
                    <comment id="12802535" author="kalradeepak" created="Tue, 19 Jan 2010 23:30:39 +0000"  >&lt;p&gt;Hi Aaron&lt;/p&gt;

&lt;p&gt;I hope you will be able to post the files today&lt;/p&gt;

&lt;p&gt;Regards&lt;br/&gt;
D&lt;/p&gt;</comment>
                    <comment id="12802631" author="amccurry" created="Wed, 20 Jan 2010 02:55:05 +0000"  >&lt;p&gt;The patch as it exists now.  It no longer needs any mods to the Term.java file.&lt;/p&gt;</comment>
                    <comment id="12802632" author="amccurry" created="Wed, 20 Jan 2010 02:56:44 +0000"  >&lt;p&gt;Here&apos;s the last file.  I have also back patched 3.0.0 and 2.9.1 and placed them on my blog incase you want to have a drop in replacement to try out.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.nearinfinity.com/blogs/aaron_mccurry/low_memory_patch_for_lucene.html&quot; class=&quot;external-link&quot;&gt;http://www.nearinfinity.com/blogs/aaron_mccurry/low_memory_patch_for_lucene.html&lt;/a&gt;&lt;/p&gt;</comment>
                    <comment id="12802787" author="mikemccand" created="Wed, 20 Jan 2010 10:42:45 +0000"  >&lt;p&gt;Aaron, are you also working on a version for the flex branch?&lt;/p&gt;</comment>
                    <comment id="12802801" author="amccurry" created="Wed, 20 Jan 2010 11:58:56 +0000"  >&lt;p&gt;Yes&lt;/p&gt;

&lt;p&gt;Sent from my iPhone&lt;/p&gt;

&lt;p&gt;On Jan 20, 2010, at 5:42 AM, &quot;Michael McCandless (JIRA)&quot; &amp;lt;jira@apache.org &lt;/p&gt;
</comment>
                    <comment id="13104976" author="cutting" created="Wed, 14 Sep 2011 23:57:52 +0100"  >&lt;p&gt;A few comments on the patch:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;It&apos;d probably be better not to make TermInfosReaderIndex and its subclasses public, to reduce the APIs that must be supported long-term.&lt;/li&gt;
	&lt;li&gt;Could you use BufferedIndexInput directly instead of re-implementing readVInt, readVLong, etc?&lt;/li&gt;
	&lt;li&gt;The code uses tabs for indentation.  Lucene&apos;s standard is 2-spaces per level, no tabs. &lt;a href=&quot;http://wiki.apache.org/lucene-java/HowToContribute&quot; class=&quot;external-link&quot;&gt;http://wiki.apache.org/lucene-java/HowToContribute&lt;/a&gt;&lt;/li&gt;
	&lt;li&gt;It would be good to add some tests, perhaps running some existing set of test searches with a reader  configured to use the new TermInfosReaderIndex implementation.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Probably the &quot;Fix-version&quot; of this patch should be 3.5, since it&apos;s not fixing a regression.&lt;/p&gt;</comment>
                    <comment id="13104982" author="cutting" created="Thu, 15 Sep 2011 00:06:24 +0100"  >&lt;p&gt;Also have you tried specifying termInfosIndexDivisor?  I added that feature many years ago to address the memory footprint of the terms index.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://lucene.apache.org/java/3_0_3/api/core/org/apache/lucene/index/IndexReader.html#open(org.apache.lucene.store.Directory&quot; class=&quot;external-link&quot;&gt;http://lucene.apache.org/java/3_0_3/api/core/org/apache/lucene/index/IndexReader.html#open(org.apache.lucene.store.Directory&lt;/a&gt;, org.apache.lucene.index.IndexDeletionPolicy, boolean, int)&lt;/p&gt;

&lt;p&gt;If this is 2 then the memory use is halved, but the compute cost of looking up each search term is doubled.  It would be interesting to compare the performance of the two approaches, since the approach of this patch probably increases lookup cost somewhat too.&lt;/p&gt;</comment>
                    <comment id="13105017" author="amccurry" created="Thu, 15 Sep 2011 00:55:17 +0100"  >&lt;p&gt;I will update my patch taking your comments into account and re-submit.&lt;/p&gt;

&lt;p&gt;I have tried the termInfosIndexDivisor and it does help with memory consumption but it typically costs access time (if I remember since I last tried changing the values to tune the index).  Since I started running with the patch above I haven&apos;t had any memory issues relating to index size, so I can&apos;t really comment it&apos;s effect.&lt;/p&gt;</comment>
                    <comment id="13105255" author="mikemccand" created="Thu, 15 Sep 2011 11:22:55 +0100"  >&lt;p&gt;Realistically I don&apos;t think we should commit such a large change to how the terms dict/index works, on the 3.x (stable) branch.&lt;/p&gt;

&lt;p&gt;In trunk, with flex indexing, we&apos;ve substantially improved how indexed terms are maintained; we now use an FST (a single byte[]) to hold the prefix trie for all terms.  This is very compact: I recently checked on a large Wikipedia index segment and it was ~0.35 bytes of RAM in the terms index, per term in the terms dict, with the default allowed25 - 48 terms per block.&lt;/p&gt;</comment>
                    <comment id="13105554" author="cutting" created="Thu, 15 Sep 2011 18:52:03 +0100"  >&lt;p&gt;Michael, thanks for responding.  Great to hear that this is likely greatly improved in 4.0!&lt;/p&gt;

&lt;p&gt;This patch is a minor refactoring to add an extension point and a new implementation of that extension point that&apos;s not used by default.  There appear to be both refactorings (&lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-3201&quot; title=&quot;improved compound file handling&quot;&gt;&lt;del&gt;LUCENE-3201&lt;/del&gt;&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-3360&quot; title=&quot;Move FieldCache to IndexReader&quot;&gt;&lt;del&gt;LUCENE-3360&lt;/del&gt;&lt;/a&gt;) and new features scheduled for 3.5.  Do you feel this refactoring is riskier than those?&lt;/p&gt;

&lt;p&gt;It should certainly also be demonstrated that this provides significant advantages over termInfosIndexDivisor before it is committed.&lt;/p&gt;</comment>
                    <comment id="13105624" author="mikemccand" created="Thu, 15 Sep 2011 20:32:15 +0100"  >
&lt;p&gt;OK, thinking more here... the fact that this won&apos;t change the index&lt;br/&gt;
format, and only replaces the low-level representation &amp;amp; methods for&lt;br/&gt;
how indexed terms are held in RAM and accessed, means that the risk&lt;br/&gt;
here is actually quite low.&lt;/p&gt;

&lt;p&gt;And the gains are tremendous (much lower RAM usage; must less GC load;&lt;br/&gt;
faster IR init time).  Users shouldn&apos;t have to wait for 4.0 to get&lt;br/&gt;
these improvements.&lt;/p&gt;

&lt;p&gt;Seek cost will go up, but this likely doesn&apos;t often matter (3.x&lt;br/&gt;
doesn&apos;t have any super-seek-intensive queries).  Maybe the primary-key&lt;br/&gt;
lookup case is the worst-case, so we should measure that?&lt;/p&gt;

&lt;p&gt;I think we can port back some help from trunk to support this, eg&lt;br/&gt;
ByteArrayDataInput (to get readVInt/readVLong/etc. on a byte[]).&lt;/p&gt;

&lt;p&gt;I don&apos;t think we need to make this switchable with a system prop;&lt;br/&gt;
let&apos;s just do a hard cutover to the new impl?&lt;/p&gt;

&lt;p&gt;Instead of writing the term data as vLong, can we just write the UTF8&lt;br/&gt;
bytes?  On seek we can convert incoming term&apos;s text to UTF8, and then&lt;br/&gt;
use trunk&apos;s UTF8SortedAsUTF16Comparator to do the compares in the&lt;br/&gt;
binary search (so we keep 3.x&apos;s UTF16 term sort order).&lt;/p&gt;

&lt;p&gt;We should also remember on commit merge this to 4.0&apos;s preflex codec...&lt;/p&gt;

&lt;p&gt;Aaron, on future iterations, could you use &quot;svn diff&quot; to produce a&lt;br/&gt;
single patch file (instead of separate files as attachments)?  This&lt;br/&gt;
way I (and others) can easily apply it to a local checkout for testing...&lt;/p&gt;</comment>
                    <comment id="13105635" author="amccurry" created="Thu, 15 Sep 2011 20:49:10 +0100"  >&lt;p&gt;I agree with the back porting of ByteArrayDataInput and the UTF8 storing of string data.  I will work getting the changes that have been discussed here reworked and submit a new (single patch).&lt;/p&gt;</comment>
                    <comment id="13105639" author="mikemccand" created="Thu, 15 Sep 2011 20:52:50 +0100"  >&lt;p&gt;Thanks Aaron, and sorry this process has taken so long!  This is a great improvement.&lt;/p&gt;</comment>
                    <comment id="13109413" author="amccurry" created="Wed, 21 Sep 2011 12:21:26 +0100"  >&lt;p&gt;I have reimplemented the patch using the UTF8SortedAsUTF16Comparator as well as ByteArrayDataInput.  The patch also contains a unit test and I have run all the current tests of the core plus the contribs and everything passes.  As a plus the code has gotten much simpler.&lt;/p&gt;

&lt;p&gt;During my functional testing I created a test index with small but very diverse terms.  Roughly 50 terms per document with 50 million documents.  So there are approximately 2.5 billion terms in this index.&lt;/p&gt;

&lt;p&gt;The current 3x branch produces:&lt;br/&gt;
50000000 documents at a heap size of 598902872.&lt;/p&gt;

&lt;p&gt;The patched version produces:&lt;br/&gt;
50000000 documents at a heap size of 282526224.&lt;/p&gt;

&lt;p&gt;The random access performance of this index goes to the patch.  Running 200 passes of a collection of randomly sampled queries (queries changes each time) produces the following:&lt;/p&gt;

&lt;p&gt;The current 3x branch produces:&lt;br/&gt;
4186.0225 avg response time in ms&lt;/p&gt;

&lt;p&gt;The patched version produces:&lt;br/&gt;
2930.1371 avg response time in ms&lt;/p&gt;

&lt;p&gt;NOTE: The hard drive I was using is a very slow drive.  While using smaller indexes the patch and the current branch are very close to the same performance.  Depending on the pass the either one was faster.&lt;/p&gt;</comment>
                    <comment id="13109567" author="mikemccand" created="Wed, 21 Sep 2011 16:09:15 +0100"  >&lt;p&gt;Thanks Aaron, I&apos;ll have a look at the patch.&lt;/p&gt;

&lt;p&gt;It&apos;s interesting that the patch was faster in your testing (I thought it should be somewhat slower, while using much less RAM) &amp;#8211; when you picked randomly sampled queries, was it deterministically random for patch &amp;amp; 3.x?  Ie, patch and 3.x ran the same randomly picked set of queries?&lt;/p&gt;

&lt;p&gt;Also, I would expect the RAM reduction to be even more than you saw in the heap sizes above, since you entirely avoid object overhead (headers, pointers), ints become vInts, etc.&lt;/p&gt;</comment>
                    <comment id="13109630" author="amccurry" created="Wed, 21 Sep 2011 17:31:49 +0100"  >&lt;p&gt;I would agree on the heap size, I&apos;m will do more analysis on that tonight.  As far the speed, it took a bit of time to get the performance basically the same.  I had to change a few methods inside TermInfosReader to reuse resources.&lt;/p&gt;

&lt;p&gt;The random access test sampled 100,000 terms from the index and stored it in a file.  Then at when I run the test it pulls all of the terms into memory and random selects terms to use in TermQueries.  Then the test times the search in nanotime and averages it.  I will attach my test programs tonight if you want.  While running a MMAPDirectory on a small ~1,000,000 documents the performance is basically the same between the patch and no patch, if there is a difference the current implementation (no patch) is slightly faster, as you would think.&lt;/p&gt;</comment>
                    <comment id="13109780" author="amccurry" created="Wed, 21 Sep 2011 20:02:42 +0100"  >&lt;p&gt;I found a major bug in my test.  I was using keyword analyzer instead of whitespace or standard, thus it was turning everyone of my sentences that contained 50 randomly generated words into 1 huge token.  This helps to explain why the heap space results are not that stellar, because the fewer terms there are (as well as the larger they are), the less the patch helps reduce space.  I&apos;m retesting now.&lt;/p&gt;</comment>
                    <comment id="13109787" author="mikemccand" created="Wed, 21 Sep 2011 20:08:23 +0100"  >
&lt;p&gt;Patch looks great Aaron!  Very much simplified... some comments:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;Instead of separate build method, could we have&lt;br/&gt;
    TermInfosReaderIndex&apos;s ctor take all the args?  Then we can make&lt;br/&gt;
    its private fields final?&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;I think the index and indexLength can be final, in&lt;br/&gt;
    TermInfosReader?&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Can you put the GrowableByteArrayDataOutput as a separate source&lt;br/&gt;
    file in oal.store?  Seems useful!&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Hmm should indexToTermsArray be a long[]...?  I wonder how large&lt;br/&gt;
    your index would have to be to overflow 2.1GB of the byte[]&lt;br/&gt;
    format...&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;We could further reduce the RAM usage by using packed ints&lt;br/&gt;
    (oal.util.packed) for the indexToTerms array; this way each&lt;br/&gt;
    indexed term would only use as many bits are actually required to&lt;br/&gt;
    address the byte[] (and, this would solve the int[]/long[] problem&lt;br/&gt;
    since packed ints are logically a long[]).&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;I think we should just always trim?  (Ie we don&apos;t need the&lt;br/&gt;
    &lt;tt&gt;private boolean trim&lt;/tt&gt;)&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Could you add comment &quot;Just for testing&quot; to&lt;br/&gt;
    TermInfosReaderIndex.getTerm?&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;For the compareTo methods, can you add to the jdocs that this&lt;br/&gt;
    &quot;compares term to index term&quot;, ie it returns negative N when term&lt;br/&gt;
    is less than index term?&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Hmm... I wonder if memory fragmentation will cause problems for&lt;br/&gt;
    the allocating/growing the single byte[].  Also, a single byte[]&lt;br/&gt;
    can &quot;only&quot; address 2.1B bytes (the same overflow problem as&lt;br/&gt;
    above).  Maybe we should port back PagedBytes (from trunk&lt;br/&gt;
    oal.util) and use that instead?  If we did that, then we could&lt;br/&gt;
    create a simple DataInput impl that reads from that.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Could you please remove the @author tags?  Thanks. It&apos;s Apache&apos;s&lt;br/&gt;
    policy (or at least discouraged) to not commit author tags...&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="13109788" author="mikemccand" created="Wed, 21 Sep 2011 20:09:59 +0100"  >&lt;blockquote&gt;&lt;p&gt;I was using keyword analyzer instead of whitespace or standard&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Aha! Good catch &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;I&apos;m also building up a 2B terms index (using Test2BTerms), and then I&apos;ll compare patch/3.x on that index.&lt;/p&gt;</comment>
                    <comment id="13109834" author="amccurry" created="Wed, 21 Sep 2011 21:02:37 +0100"  >&lt;p&gt;Memory fragmentation aside, if an index segment contained 2.1 B terms and a default term index interval of 128, each term would need to about 16M for the byte[] to run out of space.  However due to the growing array the likely hood of it landing on 2.1 exactly is probably not likely.  So it would probably error out sometime before that.&lt;/p&gt;

&lt;p&gt;I can back port PagedBytes instead if you think it&apos;s really needed.&lt;/p&gt;

&lt;p&gt;I&apos;m working on the PackedInts suggestion now, all the others have already been corrected.&lt;/p&gt;</comment>
                    <comment id="13109856" author="amccurry" created="Wed, 21 Sep 2011 21:35:15 +0100"  >&lt;p&gt;Ok retested things with the same test as before except this time I used a standard analyzer.&lt;/p&gt;

&lt;p&gt;During my functional testing I created a test index with small but very diverse terms. Roughly 50 terms per document with 5 million documents. So there are approximately 250 million terms in this index.&lt;/p&gt;

&lt;p&gt;The current 3x branch produces:&lt;br/&gt;
5000000 documents at a heap size of 259581864.&lt;/p&gt;

&lt;p&gt;The patched version produces:&lt;br/&gt;
5000000 documents at a heap size of 48991336.&lt;/p&gt;

&lt;p&gt;The random access performance of this index goes to the patch. Running 1000 passes of a collection of randomly sampled queries (queries changes each time) produces the following:&lt;/p&gt;

&lt;p&gt;The current 3x branch produces:&lt;br/&gt;
113.485454 avg response time in ms per collection&lt;/p&gt;

&lt;p&gt;The patched version produces:&lt;br/&gt;
118.926365 avg response time in ms per collection&lt;/p&gt;

&lt;p&gt;Each collection is 200 queries.  Also it&apos;s seems like the jvm is a slower to hotspot compile the patched version.  In a server implementation this shouldn&apos;t be a big concern.&lt;/p&gt;</comment>
                    <comment id="13112448" author="mikemccand" created="Thu, 22 Sep 2011 10:56:45 +0100"  >
&lt;blockquote&gt;&lt;p&gt;However due to the growing array the likely hood of it landing on 2.1 exactly is probably not likely. So it would probably error out sometime before that.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Actually ArrayUtil.grow is careful about this limit: on that final&lt;br/&gt;
grow() it&apos;ll go right up to Java&apos;s max allowed array size.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I&apos;m also building up a 2B terms index (using Test2BTerms), and then I&apos;ll compare patch/3.x on that index.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;OK this finished &amp;#8211; the test passed with the patch (good news!), and&lt;br/&gt;
3.x (phew!).&lt;/p&gt;

&lt;p&gt;With 3.x, IR.open takes 43.69 seconds and uses 2955 MB of heap.&lt;/p&gt;

&lt;p&gt;With the patch, IR.open takes 9.94 seconds (4.4X faster) and uses 505&lt;br/&gt;
MB of heap (5.9X less): AWESOME!&lt;/p&gt;

&lt;p&gt;The test then does a lookup of a random set of terms.  3.x does this&lt;br/&gt;
in 51.2 sec; patch does it in 48.5 sec, good!  (Same set of terms).&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I can back port PagedBytes instead if you think it&apos;s really needed.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think we should cutover to PagedBytes.  Today the number of terms we&lt;br/&gt;
can support is 2.1B times index interval (default 128), so ~274.9 B&lt;br/&gt;
terms. &lt;/p&gt;

&lt;p&gt;But with the current patch, we can roughly estimate bytes per indexed&lt;br/&gt;
term:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;1 byte for fieldCounter&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;15 bytes for term UTF8 bytes (non-English content)&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;1 byte for docFreq (vast majority of terms are &amp;lt; 128 df)&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;1 byte for skipOffset (vast majority of terms have no skip).&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;5 bytes for freqOffset&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;5 bytes for proxOffset&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;5 bytes for indexOffset&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;4 bytes for indexToTerms entry&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;So total ~37 bytes per indexed term, which means ~58.0 M indexed terms&lt;br/&gt;
can fit in the 2.1B byte[] limit, or 7.4 B total terms at the default&lt;br/&gt;
128 index interval.  This makes me a little nervous... we&apos;ve already&lt;br/&gt;
seen have apps that are well over 2.1 B terms.&lt;/p&gt;

&lt;p&gt;Even before the 2.1B limit, it makes me nervous relying on the JRE to&lt;br/&gt;
allocate such a large contiguous chunk of RAM.&lt;/p&gt;

&lt;p&gt;A couple other random things I noticed:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;When we estimate the initial size of the byte[] (based on .tii&lt;br/&gt;
    file size), I think we should divide by indexDivisor?&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;We should conditionally write the skipOffset, only when docFreq is&lt;br/&gt;
    &amp;gt;= skipInterval.  Since most terms won&apos;t have skip data we can&lt;br/&gt;
    save 1 byte for them...&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="13112467" author="mikemccand" created="Thu, 22 Sep 2011 11:51:33 +0100"  >&lt;p&gt;I ported luceneutil&apos;s PKLookupTest to 3.x (see&lt;br/&gt;
&lt;a href=&quot;http://code.google.com/a/apache-extras.org/p/luceneutil/source/browse/perf/PKLookupPerfTest3X.java&quot; class=&quot;external-link&quot;&gt;http://code.google.com/a/apache-extras.org/p/luceneutil/source/browse/perf/PKLookupPerfTest3X.java&lt;/a&gt;),&lt;br/&gt;
and ran a test w/ 100M docs, spread across 25 segs, doing 100K PK&lt;br/&gt;
lookups.  I temporarily disabled the terms lookup cache.&lt;/p&gt;

&lt;p&gt;3.x:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Reader=ReadOnlyDirectoryReader(segments_1 _d6(3.5):C18018000 _3x(3.5):C18018000 _70(3.5):C18018000 _a3(3.5):C18018000 _bh(3.5):C1801800 _g9(3.5):C18018000 _fp(3.5):C1801800 _gk(3.5):C1801800 _g1(3.5):C180180 _g2(3.5):C180180 _gu(3.5):C1801800 _g7(3.5):C180180 _gd(3.5):C180180 _ge(3.5):C180180 _gj(3.5):C180180 _gl(3.5):C180180 _gp(3.5):C180180 _gv(3.5):C180180 _gw(3.5):C180180 _gx(3.5):C180180 _gy(3.5):C180180 _gz(3.5):C180180 _h0(3.5):C180180 _h1(3.5):C180180 _h2(3.5):C100)
Cycle: warm
  Lookup...
  WARM: 10428 msec for 100000 lookups (104.28 us per lookup)
Cycle: test
  Lookup...
  10309 msec for 100000 lookups (103.09 us per lookup)
Cycle: test
  Lookup...
  10333 msec for 100000 lookups (103.33 us per lookup)
Cycle: test
  Lookup...
  10333 msec for 100000 lookups (103.33 us per lookup)
Cycle: test
  Lookup...
  10506 msec for 100000 lookups (105.06 us per lookup)
Cycle: test
  Lookup...
  10499 msec for 100000 lookups (104.99 us per lookup)
Cycle: test
  Lookup...
  10297 msec for 100000 lookups (102.97 us per lookup)
Cycle: test
  Lookup...
  10345 msec for 100000 lookups (103.45 us per lookup)
Cycle: test
  Lookup...
  10396 msec for 100000 lookups (103.96 us per lookup)
Cycle: test
  Lookup...
  10302 msec for 100000 lookups (103.02 us per lookup)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;


&lt;p&gt;Patch:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Reader=ReadOnlyDirectoryReader(segments_1 _d6(3.5):C18018000 _3x(3.5):C18018000 _70(3.5):C18018000 _a3(3.5):C18018000 _bh(3.5):C1801800 _g9(3.5):C18018000 _fp(3.5):C1801800 _gk(3.5):C1801800 _g1(3.5):C180180 _g2(3.5):C180180 _gu(3.5):C1801800 _g7(3.5):C180180 _gd(3.5):C180180 _ge(3.5):C180180 _gj(3.5):C180180 _gl(3.5):C180180 _gp(3.5):C180180 _gv(3.5):C180180 _gw(3.5):C180180 _gx(3.5):C180180 _gy(3.5):C180180 _gz(3.5):C180180 _h0(3.5):C180180 _h1(3.5):C180180 _h2(3.5):C100)
Cycle: warm
  Lookup...
  WARM: 11164 msec for 100000 lookups (111.64 us per lookup)
Cycle: test
  Lookup...
  10838 msec for 100000 lookups (108.38 us per lookup)
Cycle: test
  Lookup...
  10882 msec for 100000 lookups (108.82 us per lookup)
Cycle: test
  Lookup...
  10873 msec for 100000 lookups (108.73 us per lookup)
Cycle: test
  Lookup...
  10871 msec for 100000 lookups (108.71 us per lookup)
Cycle: test
  Lookup...
  10870 msec for 100000 lookups (108.7 us per lookup)
Cycle: test
  Lookup...
  10896 msec for 100000 lookups (108.96 us per lookup)
Cycle: test
  Lookup...
  10840 msec for 100000 lookups (108.4 us per lookup)
Cycle: test
  Lookup...
  10860 msec for 100000 lookups (108.6 us per lookup)
Cycle: test
  Lookup...
  10847 msec for 100000 lookups (108.47 us per lookup)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;


&lt;p&gt;So net/net patch is a bit (~5%) slower, as expected since PKLookup is&lt;br/&gt;
the worst case here, but I think the enormous gains in RAM reduction /&lt;br/&gt;
startup time / GC load make this tradeoff acceptable.&lt;/p&gt;</comment>
                    <comment id="13114804" author="mikemccand" created="Mon, 26 Sep 2011 18:18:20 +0100"  >&lt;p&gt;Aaron, if it would help, I can backport PagedBytes to 3.x?&lt;/p&gt;</comment>
                    <comment id="13114838" author="amccurry" created="Mon, 26 Sep 2011 18:39:00 +0100"  >&lt;p&gt;I have done the PagedBytes back port already.  It was a simple copy of the class (assuming that&apos;s what you want me to do).  As for the oal.util.packed package for the packed ints, I think they should be modified to work against the DataInput and DataOutput instead of the IndexInput and IndexOutput.  If so, should a separate issue in JIRA be opened to modify them in the trunk and then I will back port that version?  What do you think?&lt;/p&gt;

&lt;p&gt;Also I will probably work on this patch again in the next few days and get a final patch (hopefully) at some point this week.&lt;/p&gt;</comment>
                    <comment id="13115741" author="mikemccand" created="Tue, 27 Sep 2011 18:41:06 +0100"  >&lt;blockquote&gt;&lt;p&gt;I have done the PagedBytes back port already. It was a simple copy of the class (assuming that&apos;s what you want me to do).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Excellent, I&apos;m glad it was straightforward.  We also need a DataInput impl that reads from the PagedBytes... I can help on that if you want.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;As for the oal.util.packed package for the packed ints, I think they should be modified to work against the DataInput and DataOutput instead of the IndexInput and IndexOutput.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I agree &amp;#8211; I committed this to trunk.&lt;/p&gt;</comment>
                    <comment id="13118724" author="amccurry" created="Sat, 1 Oct 2011 12:36:41 +0100"  >&lt;p&gt;This patch includes fixes for all the previous comments, however in my small tests it seems to produce more memory overhead then  I would have expected.  I will be on vacation for the next week, so I wanted to attach the current state of the patch incase someone else sees my mistake causing the extra memory to be used.  Also I have not done any performance testing with this patch.&lt;/p&gt;</comment>
                    <comment id="13120504" author="mikemccand" created="Tue, 4 Oct 2011 22:51:58 +0100"  >&lt;p&gt;Looking great Aaron!&lt;/p&gt;

&lt;p&gt;It&apos;s spooky that PagedBytesDataInput calls fillSlice for every&lt;br/&gt;
.readByte &amp;#8211; can&apos;t we have it hold the current block and then only&lt;br/&gt;
switch to a new block in .readByte() if it&apos;s at the end of current&lt;br/&gt;
block?&lt;/p&gt;

&lt;p&gt;Same for PagedBytesDataOutput?&lt;/p&gt;

&lt;p&gt;We can have these DataInput/Output impls be private to PagedBytes (so&lt;br/&gt;
they can access the pages directly)?&lt;/p&gt;

&lt;p&gt;You should be able to use PackedInts.GrowableWriter, to append the&lt;br/&gt;
ints directly, instead of first writing to the indexToTermsArray and&lt;br/&gt;
then separately to the packed ints?  Saves the added transient RAM&lt;br/&gt;
usage and 2nd pass.&lt;/p&gt;

&lt;p&gt;I don&apos;t think you need to write the indexToTerms packed ints into a&lt;br/&gt;
PagedBytesDataOutput (if you use GrowableWriter it just uses a byte[]&lt;br/&gt;
under the hood, and resizes as needed)?  This array will be small&lt;br/&gt;
enough, since it&apos;s the packed int byte address of every 128th term, I&lt;br/&gt;
think (but dataOutput does need to be paged bytes).&lt;/p&gt;</comment>
                    <comment id="13131554" author="mikemccand" created="Thu, 20 Oct 2011 13:27:35 +0100"  >&lt;p&gt;Hi Aaron, any progress here?  If you want I can iterate from your last patch and do the remaining changes... I think we are close here, and this will be an awesome improvement.&lt;/p&gt;</comment>
                    <comment id="13131628" author="amccurry" created="Thu, 20 Oct 2011 14:41:34 +0100"  >&lt;p&gt;Sorry that I haven&apos;t gotten back to you yet, life has a way of slowing down development.&lt;/p&gt;

&lt;p&gt;As for you comments from a couple of weeks ago.  I agree with the PageBytes comments, I knew there was going to be more work there.  But I may not understand how the GrowableWriter will help.  I understand that it allows me to append more values as I go, but when I start writing them I have no idea what bit size to choose for the packing.  Can you explain?&lt;/p&gt;

&lt;p&gt;Also if you would like to finish this patch that would fine with me.  Let me know if you want me to continue or if you are going to work on it.  Thanks!&lt;/p&gt;
</comment>
                    <comment id="13131857" author="mikemccand" created="Thu, 20 Oct 2011 19:09:20 +0100"  >&lt;blockquote&gt;&lt;p&gt;But I may not understand how the GrowableWriter will help. I understand that it allows me to append more values as I go, but when I start writing them I have no idea what bit size to choose for the packing. Can you explain?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;It actually grows in &quot;both&quot; dimensions &amp;#8211; it tracks the max value so far and internally will &quot;upgrade&quot; to a bigger bits-per-value as needed.  So eg you could start with small bitsPerValue (maybe 4 or something) and then let it grow itself.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Also if you would like to finish this patch that would fine with me. Let me know if you want me to continue or if you are going to work on it. Thanks!&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;OK thanks Aaron... I&apos;ll take a crack at the next iteration.&lt;/p&gt;</comment>
                    <comment id="13131941" author="amccurry" created="Thu, 20 Oct 2011 20:40:51 +0100"  >&lt;p&gt;Wow GrowableWriter is very cool!  Thanks for explaining!&lt;/p&gt;</comment>
                    <comment id="13134559" author="mikemccand" created="Mon, 24 Oct 2011 23:09:32 +0100"  >&lt;p&gt;New patch, iterated from Aaron&apos;s last patch.&lt;/p&gt;

&lt;p&gt;I moved the DataInput/Output impls into PagedBytes, so they can directly operate on the byte[] blocks.  I also don&apos;t write skipOffset unless df &amp;gt;= skipInterval.&lt;/p&gt;

&lt;p&gt;I think this is ready!&lt;/p&gt;</comment>
                    <comment id="13134717" author="amccurry" created="Tue, 25 Oct 2011 04:20:25 +0100"  >&lt;p&gt;Awesome!  Good job!&lt;/p&gt;

&lt;p&gt;Thank you for working on this with me!&lt;/p&gt;</comment>
                    <comment id="13134982" author="mikemccand" created="Tue, 25 Oct 2011 13:35:28 +0100"  >&lt;p&gt;Well thank you Aaron for doing all the hard parts, and, persisting!  Sorry this took so long.&lt;/p&gt;

&lt;p&gt;This will be an enormous improvement for 3.5.0.&lt;/p&gt;</comment>
                    <comment id="13136799" author="rcmuir" created="Thu, 27 Oct 2011 07:03:43 +0100"  >&lt;p&gt;+1, nice work.&lt;/p&gt;

&lt;p&gt;just some tiny tweaks: added a missing license header, randomized the test, and when binary-searching bytesref.grow() versus making new ones in each comparison (this seems to help the pk-lookup perf test on my computer).&lt;/p&gt;

&lt;p&gt;When committing can we also add the packed ints test to 3.x, and also merge this improvement forward to 4.x&apos;s preflex codec?&lt;/p&gt;</comment>
                    <comment id="13137143" author="mikemccand" created="Thu, 27 Oct 2011 14:42:49 +0100"  >&lt;p&gt;New patch looks great, thanks Robert!&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;When committing can we also add the packed ints test to 3.x, and also merge this improvement forward to 4.x&apos;s preflex codec?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yup I&apos;ll do that.  I&apos;ll also merge forward the improvements to PagedBytes.&lt;/p&gt;

&lt;p&gt;I also ran Test2BTerms (it passed); I think this is ready to go in!&lt;/p&gt;</comment>
                    <comment id="13137527" author="mikemccand" created="Thu, 27 Oct 2011 21:47:38 +0100"  >&lt;p&gt;Finally resolved; thanks Aaron!&lt;/p&gt;</comment>
                    <comment id="13157741" author="thetaphi" created="Sun, 27 Nov 2011 12:29:23 +0000"  >&lt;p&gt;Bulk close after release of 3.5&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12495343" name="lowmemory_w_utf8_encoding.patch" size="20849" author="amccurry" created="Wed, 21 Sep 2011 12:23:52 +0100" />
                    <attachment id="12497279" name="lowmemory_w_utf8_encoding.v4.patch" size="94682" author="amccurry" created="Sat, 1 Oct 2011 12:36:41 +0100" />
                    <attachment id="12501036" name="LUCENE-2205.patch" size="92564" author="rcmuir" created="Thu, 27 Oct 2011 07:03:43 +0100" />
                    <attachment id="12500558" name="LUCENE-2205.patch" size="91460" author="mikemccand" created="Mon, 24 Oct 2011 23:09:32 +0100" />
                    <attachment id="12430112" name="patch-final.txt" size="18407" author="amccurry" created="Wed, 13 Jan 2010 08:29:42 +0000" />
                    <attachment id="12430113" name="RandomAccessTest.java" size="4169" author="amccurry" created="Wed, 13 Jan 2010 08:41:05 +0000" />
                    <attachment id="12430114" name="rawoutput.txt" size="7718" author="amccurry" created="Wed, 13 Jan 2010 08:41:05 +0000" />
                    <attachment id="12430835" name="TermInfosReaderIndexDefault.java" size="1691" author="amccurry" created="Wed, 20 Jan 2010 02:55:05 +0000" />
                    <attachment id="12430834" name="TermInfosReaderIndex.java" size="483" author="amccurry" created="Wed, 20 Jan 2010 02:55:05 +0000" />
                    <attachment id="12430836" name="TermInfosReaderIndexSmall.java" size="9652" author="amccurry" created="Wed, 20 Jan 2010 02:56:44 +0000" />
                    <attachment id="12430833" name="TermInfosReader.java" size="9432" author="amccurry" created="Wed, 20 Jan 2010 02:55:05 +0000" />
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>11.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Wed, 13 Jan 2010 10:10:26 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>2273</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    <customfieldvalue key="10120"><![CDATA[Patch Available]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>25520</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                    <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                <customfieldname>Time in Status</customfieldname>
                <customfieldvalues>
                    
                </customfieldvalues>
            </customfield>
                            </customfields>
    </item>
</channel>
</rss>