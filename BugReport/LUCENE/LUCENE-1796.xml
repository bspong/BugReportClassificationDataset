<!-- 
RSS generated by JIRA (5.2.8#851-sha1:3262fdc28b4bc8b23784e13eadc26a22399f5d88) at Tue Jul 16 13:21:07 UTC 2013

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/LUCENE-1796/LUCENE-1796.xml?field=key&field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>5.2.8</version>
        <build-number>851</build-number>
        <build-date>26-02-2013</build-date>
    </build-info>

<item>
            <title>[LUCENE-1796] Speed up repeated TokenStream init</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-1796</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt; by caching isMethodOverridden results&lt;/p&gt;</description>
                <environment></environment>
            <key id="12432692">LUCENE-1796</key>
            <summary>Speed up repeated TokenStream init</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png">Closed</status>
                    <resolution id="1">Fixed</resolution>
                                <assignee username="thetaphi">Uwe Schindler</assignee>
                                <reporter username="markrmiller@gmail.com">Mark Miller</reporter>
                        <labels>
                    </labels>
                <created>Mon, 10 Aug 2009 16:14:44 +0100</created>
                <updated>Thu, 13 Aug 2009 00:08:33 +0100</updated>
                    <resolved>Mon, 10 Aug 2009 22:35:35 +0100</resolved>
                                            <fixVersion>2.9</fixVersion>
                                        <due></due>
                    <votes>0</votes>
                        <watches>0</watches>
                                                    <comments>
                    <comment id="12741360" author="markrmiller@gmail.com" created="Mon, 10 Aug 2009 16:15:09 +0100"  >&lt;p&gt;I&apos;ve got my test environ all setup, so I&apos;ll be happy to test&lt;/p&gt;</comment>
                    <comment id="12741362" author="rcmuir" created="Mon, 10 Aug 2009 16:16:52 +0100"  >&lt;p&gt;me too (not a real benchmark but i think average sized docs)&lt;/p&gt;</comment>
                    <comment id="12741373" author="rcmuir" created="Mon, 10 Aug 2009 16:35:38 +0100"  >&lt;p&gt;unrelated to TokenStream init, but what appears to be creating additional slowdown is this:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;TRACE 300197:
	org.apache.lucene.util.AttributeSource.getAttributeImplsIterator(AttributeSource.java:140)
	org.apache.lucene.util.AttributeSource.clearAttributes(AttributeSource.java:233)
	org.apache.lucene.analysis.CharTokenizer.incrementToken(CharTokenizer.java:56)
	org.apache.lucene.index.DocInverterPerField.processFields(DocInverterPerField.java:189)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;even with reusableTokenStreams, I am still seeing a slowdown and with fairly granular profiling, this is consistently at the top.&lt;br/&gt;
is there a way to optimize clear() in any way?&lt;/p&gt;
</comment>
                    <comment id="12741374" author="thetaphi" created="Mon, 10 Aug 2009 16:35:59 +0100"  >&lt;p&gt;Here a poatch. About the naming of the private internal class we can discuss, but it seems to work. No real performance tests until now.&lt;/p&gt;

&lt;p&gt;Could you please test and compare the results with before?&lt;/p&gt;</comment>
                    <comment id="12741382" author="thetaphi" created="Mon, 10 Aug 2009 16:44:07 +0100"  >&lt;blockquote&gt;&lt;p&gt;is there a way to optimize clear() in any way?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The cost is creating the iterator. As it is a LinkedHashMap, there must be used an iterator (because direct access is slow), but for extra safety the iterator is made unmodifable, this wrapper around could be removed.&lt;/p&gt;</comment>
                    <comment id="12741387" author="thetaphi" created="Mon, 10 Aug 2009 16:52:18 +0100"  >&lt;p&gt;New patch, that also removes the Collections.unmodifiableXxx() calls. If somebody removes elements from the iterator its not our problem &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                    <comment id="12741391" author="rcmuir" created="Mon, 10 Aug 2009 16:57:30 +0100"  >&lt;p&gt;ok lemme restart my benchmark... the performance difference i was seeing with your first patch so far looked minor, but consistent.&lt;/p&gt;

&lt;p&gt;here&apos;s some unscientific numbers from your first patch indexing hamshari (about 500MB corpus, 165k docs)&lt;br/&gt;
analyzer with no reusableTS() (without patch):&lt;br/&gt;
52988 ms&lt;br/&gt;
52902 ms&lt;br/&gt;
53035 ms&lt;br/&gt;
53116 ms&lt;br/&gt;
52637 ms&lt;/p&gt;

&lt;p&gt;analyzer with no reusableTS() (with patch):&lt;br/&gt;
51916 ms&lt;br/&gt;
51969 ms&lt;br/&gt;
51872 ms&lt;br/&gt;
52438 ms&lt;br/&gt;
51710 ms&lt;/p&gt;</comment>
                    <comment id="12741393" author="markrmiller@gmail.com" created="Mon, 10 Aug 2009 16:59:34 +0100"  >&lt;p&gt;Test on the first patch:&lt;/p&gt;

&lt;p&gt;Almost brings things back to par with Yoniks short solr indexing test (previous results were like 90 some seconds):&lt;/p&gt;


&lt;p&gt;Before TokenStream revamp:&lt;br/&gt;
iter=100000 time=44173 throughput=2263&lt;br/&gt;
iter=100000 time=44403 throughput=2252&lt;/p&gt;

&lt;p&gt;After TokenStream revamp:&lt;br/&gt;
iter=100000 time=46720 throughput=2140&lt;br/&gt;
iter=100000 time=47038 throughput=2125&lt;/p&gt;


&lt;p&gt;That method inspection was like the second hotest method (see the profiling results), and this must just take it right out of there.&lt;/p&gt;

&lt;p&gt;By the way, I havn&apos;t looked if it slows down the single use case at all or not.&lt;/p&gt;</comment>
                    <comment id="12741395" author="markrmiller@gmail.com" created="Mon, 10 Aug 2009 17:03:09 +0100"  >&lt;p&gt;No noticeable diff in the second patch for me.&lt;/p&gt;</comment>
                    <comment id="12741397" author="rcmuir" created="Mon, 10 Aug 2009 17:06:21 +0100"  >&lt;p&gt;here are my large doc numbers with the second patch, same setup, no reusableTS()&lt;/p&gt;

&lt;p&gt;51347ms&lt;br/&gt;
49917ms&lt;br/&gt;
50676ms&lt;br/&gt;
50010ms&lt;br/&gt;
49261ms&lt;/p&gt;

&lt;p&gt;seems to help a bit. i will turn back on reusableTS and see if the iterator still shows up in profiling.&lt;/p&gt;</comment>
                    <comment id="12741400" author="thetaphi" created="Mon, 10 Aug 2009 17:09:02 +0100"  >&lt;p&gt;Mark: I do not understand your comment completely, do you mean with before/after revamp the comparison between old Lucene before-Attributes TokenStreams compared to the patched Attribute ones? And the 90 seconds is the unpatched Attributes case? (looks like half speed)&lt;/p&gt;

&lt;p&gt;If so, the problem was really this reflection calls.&lt;/p&gt;</comment>
                    <comment id="12741402" author="markrmiller@gmail.com" created="Mon, 10 Aug 2009 17:14:38 +0100"  >&lt;p&gt;Sorry - to be a bit more clear:&lt;/p&gt;

&lt;p&gt;Lucene trunk without your patch was like 90 seconds - twice as slow as a previous version of Lucene on Yoniks short doc test.&lt;/p&gt;

&lt;p&gt;The numbers reported above are for Lucene pre the TokenStream improvements, and post with your patch - so now the numbers are much closer - though its still a tad slower.&lt;/p&gt;

&lt;p&gt;You second patch didnt change things for me from your first patch - same perf in this test.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;If so, the problem was really this reflection calls.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes - which you can clearly see from the profiling results I got yesterday:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://myhardshadow.com/images/before.png&quot; class=&quot;external-link&quot;&gt;http://myhardshadow.com/images/before.png&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://myhardshadow.com/images/after.png&quot; class=&quot;external-link&quot;&gt;http://myhardshadow.com/images/after.png&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;isMethodOverriden was called way too often for its speed - its a highly inefficient call if you trace through it.&lt;/p&gt;</comment>
                    <comment id="12741404" author="thetaphi" created="Mon, 10 Aug 2009 17:18:19 +0100"  >&lt;p&gt;If it is still a little bit slower (in my opinion its a little bit, 2 seconds of 45s is 4%), could it be, because the Solr TokenFilters only implement next(Token) and not incrementToken()? What is your distribution of TokenStreams/Filters in your test. Only new ones, only old ones, both? %&lt;/p&gt;</comment>
                    <comment id="12741408" author="markrmiller@gmail.com" created="Mon, 10 Aug 2009 17:23:37 +0100"  >&lt;p&gt;Yes indeed, its very close now.&lt;/p&gt;

&lt;p&gt;The filters are:&lt;/p&gt;

&lt;p&gt;          &amp;lt;tokenizer class=&quot;solr.WhitespaceTokenizerFactory&quot;/&amp;gt;&lt;br/&gt;
          &amp;lt;filter class=&quot;solr.WordDelimiterFilterFactory&quot; generateWordParts=&quot;1&quot; generateNumberParts=&quot;0&quot; catenateWords=&quot;0&quot; catenateNumbers=&quot;0&quot; catenateAll=&quot;0&quot;/&amp;gt;&lt;br/&gt;
          &amp;lt;filter class=&quot;solr.LowerCaseFilterFactory&quot;/&amp;gt;&lt;/p&gt;

&lt;p&gt;So I guess its 1 and 1? Lowercase comes from Lucene and implements increment, but WordDelim just does next. The other hotspot was (though much less of one) TokenStream&amp;lt;init&amp;gt;(AttributeSource) - if implementing increment relieves that, it might be the same speed after.&lt;/p&gt;</comment>
                    <comment id="12741411" author="rcmuir" created="Mon, 10 Aug 2009 17:24:28 +0100"  >&lt;p&gt;Uwe, your patch seems to help my large doc case, although as you can see, the numbers are still very different if i implement reusableTS() than if i do not.&lt;/p&gt;

&lt;p&gt;With patch (reusableTS)&lt;br/&gt;
Total time:43373&lt;br/&gt;
Total time:43428&lt;br/&gt;
Total time:43536&lt;br/&gt;
Total time:42857&lt;br/&gt;
Total time:42835&lt;/p&gt;

&lt;p&gt;Without patch (reusableTS)&lt;br/&gt;
Total time:44613&lt;br/&gt;
Total time:45720&lt;br/&gt;
Total time:45592&lt;br/&gt;
Total time:45445&lt;br/&gt;
Total time:45090&lt;/p&gt;

&lt;p&gt;Also, with your patch the org.apache.lucene.util.AttributeSource.getAttributeImplsIterator from clearAttributes() is ranked 8 instead of 1 or 2 in profiling.&lt;/p&gt;</comment>
                    <comment id="12741416" author="thetaphi" created="Mon, 10 Aug 2009 17:37:58 +0100"  >&lt;p&gt;Mark: The only hotspot could be initTokenWrapper(), which does some checks, but normally shortcuts in filters to the input TokenStream&apos;s TokenWrapper instance.&lt;/p&gt;

&lt;p&gt;Using incrementToken() would not help here, only if you switch of using the old API complete using TokenStream.setOnlyUseNewAPI(true), which can only be done globally, so all Solr TokenStreams must implement incrementToken().&lt;/p&gt;</comment>
                    <comment id="12741426" author="thetaphi" created="Mon, 10 Aug 2009 18:07:17 +0100"  >&lt;p&gt;Attached is a patch that removes the clearAttributes() from CharTokenizer (see discussion with Yonik on java-dev) and also removes clear() calls where not really needed.&lt;/p&gt;</comment>
                    <comment id="12741430" author="rcmuir" created="Mon, 10 Aug 2009 18:17:33 +0100"  >&lt;p&gt;Uwe, removal of the CharTokenizer clearAttributes() makes a difference for me (compare to my last numbers):&lt;/p&gt;

&lt;p&gt;38729ms&lt;br/&gt;
40201ms&lt;br/&gt;
40085ms&lt;br/&gt;
40238ms&lt;br/&gt;
40169ms&lt;/p&gt;</comment>
                    <comment id="12741444" author="michaelbusch" created="Mon, 10 Aug 2009 18:51:02 +0100"  >&lt;p&gt;Another good cache, Uwe! &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;AttributeSource.clearAttributes() could use the State (which is also used for cloning) to iterate the AttributeImpls faster.&lt;/p&gt;</comment>
                    <comment id="12741445" author="thetaphi" created="Mon, 10 Aug 2009 18:53:43 +0100"  >&lt;p&gt;But if you use the State and there is no state already created, it would have the cost of capturing the state (cloning!) for no real use...&lt;/p&gt;</comment>
                    <comment id="12741447" author="thetaphi" created="Mon, 10 Aug 2009 18:59:09 +0100"  >&lt;p&gt;I have another idea:&lt;br/&gt;
Why not make the AttributeImpls itsself a linked list. So each AttributeImpl gets a member called &quot;nextAttributeImpl&quot; for the linked list. Whenever an AttributeImpl is added to an AttributeSource, the last added AttributeImpl&apos;s next is set to the added one. By this iterating over AttributeImpls is just a simple for-loop from the first attribute. The AttributeSource must hold a reference to the first and last Attribute, but only one-direction linkage from first to last.&lt;/p&gt;

&lt;p&gt;As Attributes can only added to one AttributeSource and not to multiple ones, I see no problem with it. And AttributeImpls can also not removed, so no problem at all.&lt;/p&gt;</comment>
                    <comment id="12741457" author="michaelbusch" created="Mon, 10 Aug 2009 19:11:22 +0100"  >&lt;p&gt;You don&apos;t have to call captureState and clone. You just need to call computeCurrentState() one time to create the internal state. That is basically a linked list.&lt;/p&gt;

&lt;p&gt;If then someone adds more attributes to the source the state is invalidated and you would have to call computeCurrentState() again.&lt;/p&gt;</comment>
                    <comment id="12741459" author="thetaphi" created="Mon, 10 Aug 2009 19:14:23 +0100"  >&lt;p&gt;Ah, you are right! I will try this out. The Iterator returned by getAttributeImplsIterator() could then also be implemented using the State. This iterator would not be used internally by AttributeSource (there the State is iterated directly), but e.g. in TeeSinkTokenStream. So supplying an iterator is still needed, but internally the faster direct State iteration can be used.&lt;/p&gt;

&lt;p&gt;I will try this out and re-enable clearAttribute() in the Tokenizers again.&lt;/p&gt;</comment>
                    <comment id="12741511" author="thetaphi" created="Mon, 10 Aug 2009 20:47:52 +0100"  >&lt;p&gt;New patch that optimizes the iteration over the AttributeImpls using the computed State linked list. This also adds the default buffer size to KeywordTokenizer, that got lost during the move to the new API.&lt;/p&gt;

&lt;p&gt;To test performance, I reactivated the clearAttributes() call in CharTokenizer.&lt;/p&gt;

&lt;p&gt;If this is now all ok, I would like to fix this issue as soon as possible to be able to do more perf testing with the optimized impls. The big hammer of isMethodOverridden is now removed and speed came back to the original one (with some small slowdown caused by mixing old and new TokenFilters together).&lt;/p&gt;</comment>
                    <comment id="12741514" author="markrmiller@gmail.com" created="Mon, 10 Aug 2009 20:52:41 +0100"  >&lt;p&gt;Nice work Uwe!&lt;/p&gt;</comment>
                    <comment id="12741520" author="markrmiller@gmail.com" created="Mon, 10 Aug 2009 20:58:53 +0100"  >&lt;p&gt;The latest patch appears to hurt the Solr use case a bit - went from 46-47 seconds to 51-52 (remember its 43-45 with the pre reflection stuff)&lt;/p&gt;</comment>
                    <comment id="12741523" author="thetaphi" created="Mon, 10 Aug 2009 21:07:50 +0100"  >&lt;p&gt;Hm, and with the termAtt.clear() instead of clearAttributes? Was the 46-47 with the clearAttributes call or without? You always have the problem with very short TokenStreams that are not reused, that the initialization and State linked-list construction needs some time. In the reused case, it should be faster with the latest patch or without clearAttributes at all.&lt;/p&gt;

&lt;p&gt;Where is the hotSpot? Do you have a figure like before the patch with method execution times?&lt;/p&gt;</comment>
                    <comment id="12741524" author="markrmiller@gmail.com" created="Mon, 10 Aug 2009 21:08:58 +0100"  >&lt;p&gt;I was getting 46-47 with both of the first two patches. I can double check a little later though.&lt;/p&gt;</comment>
                    <comment id="12741525" author="rcmuir" created="Mon, 10 Aug 2009 21:11:27 +0100"  >&lt;p&gt;uwe in my case the latest patch performs approx the same as your patch where CharTokenizer clearAttributes() was removed (avg 40893ms). Thanks.&lt;/p&gt;</comment>
                    <comment id="12741532" author="markrmiller@gmail.com" created="Mon, 10 Aug 2009 21:30:33 +0100"  >&lt;p&gt;And mine was a misreport - sorry - a wine program was eating one of my 4 cores and I didn&apos;t notice - its testing at about 48 s now, so just about the same as the first 2 patches. &lt;/p&gt;</comment>
                    <comment id="12741534" author="thetaphi" created="Mon, 10 Aug 2009 21:42:47 +0100"  >&lt;p&gt;OK. Last patch, I only added a test in TestAttributeSource that verifies the correctness of the returned getAttributeImplsIterator() &lt;span class=&quot;error&quot;&gt;&amp;#91;important, because iterator logic is not so simple to understand...&amp;#93;&lt;/span&gt;. TestTeeSinkTokenFilter would also fail with wrong Iterator...&lt;/p&gt;

&lt;p&gt;I think I commit this shortly? Any complaints?&lt;/p&gt;</comment>
                    <comment id="12741545" author="markrmiller@gmail.com" created="Mon, 10 Aug 2009 21:59:56 +0100"  >&lt;p&gt;Just to complete my report:&lt;/p&gt;

&lt;p&gt;The tests I reported in this issue were done with a little more beef in the documents - I had added about 4 lines from a newspaper article. The result is that we are only about 4-5% slower using those documents now. However, with Yonik&apos;s original test, with very short docs:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
 &lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;[] fields = {&lt;span class=&quot;code-quote&quot;&gt;&quot;text&quot;&lt;/span&gt;,&lt;span class=&quot;code-quote&quot;&gt;&quot;simple&quot;&lt;/span&gt;
            ,&lt;span class=&quot;code-quote&quot;&gt;&quot;text&quot;&lt;/span&gt;,&lt;span class=&quot;code-quote&quot;&gt;&quot;test&quot;&lt;/span&gt;
            ,&lt;span class=&quot;code-quote&quot;&gt;&quot;text&quot;&lt;/span&gt;,&lt;span class=&quot;code-quote&quot;&gt;&quot;how now brown cow&quot;&lt;/span&gt;
            ,&lt;span class=&quot;code-quote&quot;&gt;&quot;text&quot;&lt;/span&gt;,&lt;span class=&quot;code-quote&quot;&gt;&quot;what&apos;s that?&quot;&lt;/span&gt;
            ,&lt;span class=&quot;code-quote&quot;&gt;&quot;text&quot;&lt;/span&gt;,&lt;span class=&quot;code-quote&quot;&gt;&quot;radical!&quot;&lt;/span&gt;
            ,&lt;span class=&quot;code-quote&quot;&gt;&quot;text&quot;&lt;/span&gt;,&lt;span class=&quot;code-quote&quot;&gt;&quot;what&apos;s all &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; about, anyway?&quot;&lt;/span&gt;
            ,&lt;span class=&quot;code-quote&quot;&gt;&quot;text&quot;&lt;/span&gt;,&lt;span class=&quot;code-quote&quot;&gt;&quot;just how fast is &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; text indexing?&quot;&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;...we are 10% behind. This is a mix of TokenStreams - you can see the tokenfilters used in the profile pics. This is huge improvement from before - that was 50-60% slower with this test.&lt;/p&gt;

&lt;p&gt;All profiling pics are from Yoniks original small doc test with 100000 iterations.&lt;/p&gt;

&lt;p&gt;I&apos;ll attach:&lt;/p&gt;

&lt;p&gt;before the reflection token stream stuff&lt;br/&gt;
after (trunk)&lt;br/&gt;
after with this patch&lt;/p&gt;</comment>
                    <comment id="12741564" author="rcmuir" created="Mon, 10 Aug 2009 22:30:50 +0100"  >&lt;p&gt;I just want to say I think that 10% test case might be a worst case: very short documents and no reusableTS.&lt;/p&gt;

&lt;p&gt;I ran a bunch of iterations on a corpus (regular sized docs though) and found this:&lt;br/&gt;
Lucene 2.4.1 (CzechAnalyzer, does not implement reusableTS) avg 48290.4ms&lt;br/&gt;
HEAD+&lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-1796&quot; title=&quot;Speed up repeated TokenStream init&quot;&gt;&lt;del&gt;LUCENE-1796&lt;/del&gt;&lt;/a&gt; (CzechAnalyzer, does not implement reusableTS) avg 49943.8ms &amp;lt;-- a bit slower&lt;br/&gt;
HEAD+&lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-1796&quot; title=&quot;Speed up repeated TokenStream init&quot;&gt;&lt;del&gt;LUCENE-1796&lt;/del&gt;&lt;/a&gt;+&lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-1794&quot; title=&quot;implement reusableTokenStream for all contrib analyzers&quot;&gt;&lt;del&gt;LUCENE-1794&lt;/del&gt;&lt;/a&gt; (CzechAnalyzer, implements reusableTS) avg 47846.1ms &amp;lt;-- a bit faster&lt;/p&gt;

&lt;p&gt;So I think reusableTS in Solr combined with this patch can mitigate any remaining construction overhead, and maybe be faster overall compared to the last release.&lt;/p&gt;</comment>
                    <comment id="12741567" author="thetaphi" created="Mon, 10 Aug 2009 22:32:07 +0100"  >&lt;p&gt;The shorter the text, the more the construction cost increases. This is what I exspect. For normal text length like abstracts, newspaper articles and so on, the speed is equal than before.&lt;/p&gt;

&lt;p&gt;I think I commit this now and leave all other things to the current discussion on java-dev.&lt;/p&gt;</comment>
                    <comment id="12741570" author="thetaphi" created="Mon, 10 Aug 2009 22:35:35 +0100"  >&lt;p&gt;Committed revision: 802930&lt;br/&gt;
(I only removed the clearAttributes() call again, which is unneeded for CharTokenizer)&lt;/p&gt;</comment>
                    <comment id="12741571" author="michaelbusch" created="Mon, 10 Aug 2009 22:39:40 +0100"  >&lt;blockquote&gt;
&lt;p&gt;I think I commit this now and leave all other things to the current discussion on java-dev.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;+1.&lt;br/&gt;
Good work Uwe! Thanks.&lt;/p&gt;</comment>
                    <comment id="12741590" author="yseeley@gmail.com" created="Mon, 10 Aug 2009 23:11:13 +0100"  >&lt;blockquote&gt;&lt;p&gt;(I only removed the clearAttributes() call again, which is unneeded for CharTokenizer) &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I thought that Tokenizers had to clear all attributes?&lt;/p&gt;</comment>
                    <comment id="12741594" author="thetaphi" created="Mon, 10 Aug 2009 23:15:15 +0100"  >&lt;p&gt;We had no conclusion on this. I think we should create a new issue out of it and change &lt;b&gt;all&lt;/b&gt; Tokenizers to clear all Attributes in incrementToken(). This requirement then should also be noted in the JavaDocs of Tokenizer.&lt;/p&gt;

&lt;p&gt;Currently &lt;b&gt;no&lt;/b&gt; Tokenizer clears the attributes... AttributeSource.clearAttributes is never called at the moment.&lt;/p&gt;</comment>
                    <comment id="12741595" author="michaelbusch" created="Mon, 10 Aug 2009 23:19:55 +0100"  >&lt;p&gt;I think Token.reset() wasn&apos;t called before either. So I think we always had this potential problem?&lt;/p&gt;</comment>
                    <comment id="12741952" author="yseeley@gmail.com" created="Tue, 11 Aug 2009 17:39:52 +0100"  >&lt;p&gt;Token.clear() used to be called by the consumer... but then it was switched to the producer here:&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-1101&quot; class=&quot;external-link&quot;&gt;https://issues.apache.org/jira/browse/LUCENE-1101&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I don&apos;t know if all of the Tokenizers in lucene were ever changed, but in any case it looks like at least some of these bugs were introduced with the switch to the attribute API - for example StandardTokenizer did clear it&apos;s reusableToken... and now it doesn&apos;t.&lt;/p&gt;</comment>
                    <comment id="12741957" author="thetaphi" created="Tue, 11 Aug 2009 17:52:32 +0100"  >&lt;blockquote&gt;&lt;p&gt;I don&apos;t know if all of the Tokenizers in lucene were ever changed, but in any case it looks like at least some of these bugs were introduced with the switch to the attribute API - for example StandardTokenizer did clear it&apos;s reusableToken... and now it doesn&apos;t.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;No one is calling clearAttributes() in trunk code, only some of them clear attributes before filling data in.&lt;/p&gt;

&lt;p&gt;OK, I open another issue later and change all Tokenizers in core and contrib to call clearAttributes() as first call inside incrementToken()?&lt;/p&gt;

&lt;p&gt;But in principle we could also change the indexer to call clear before each incrementToken() removing the need to do it in every Tokenizer.&lt;/p&gt;</comment>
                    <comment id="12742293" author="yseeley@gmail.com" created="Wed, 12 Aug 2009 12:27:53 +0100"  >&lt;blockquote&gt;&lt;p&gt;But in principle we could also change the indexer to call clear before each incrementToken() removing the need to do it in every Tokenizer.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Doron brought up a good reason for not doing that in &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-1101&quot; title=&quot;TokenStream.next(Token) reuse &amp;#39;policy&amp;#39;: calling Token.clear() should be responsibility of producer.&quot;&gt;&lt;del&gt;LUCENE-1101&lt;/del&gt;&lt;/a&gt;.&lt;br/&gt;
A tokenizer (or other token producer) could produce multiple tokens before one made it to the ultimate consumer (because of stop filters, etc).  So it looks like producers should do the clear.&lt;/p&gt;</comment>
                    <comment id="12742644" author="thetaphi" created="Thu, 13 Aug 2009 00:08:33 +0100"  >&lt;p&gt;I opened &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-1801&quot; title=&quot;Tokenizers (which are the source of Tokens) should call AttributeSource.clearAttributes() first&quot;&gt;&lt;del&gt;LUCENE-1801&lt;/del&gt;&lt;/a&gt; for that. A patch is available and will be committed soon.&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12416119" name="afterAndLucene1796.png" size="57059" author="markrmiller@gmail.com" created="Mon, 10 Aug 2009 22:00:22 +0100" />
                    <attachment id="12416118" name="after.png" size="56815" author="markrmiller@gmail.com" created="Mon, 10 Aug 2009 22:00:22 +0100" />
                    <attachment id="12416117" name="before.png" size="58079" author="markrmiller@gmail.com" created="Mon, 10 Aug 2009 22:00:22 +0100" />
                    <attachment id="12416110" name="LUCENE-1796.patch" size="15354" author="thetaphi" created="Mon, 10 Aug 2009 21:42:47 +0100" />
                    <attachment id="12416104" name="LUCENE-1796.patch" size="13812" author="thetaphi" created="Mon, 10 Aug 2009 20:47:52 +0100" />
                    <attachment id="12416088" name="LUCENE-1796.patch" size="8712" author="thetaphi" created="Mon, 10 Aug 2009 18:07:17 +0100" />
                    <attachment id="12416081" name="LUCENE-1796.patch" size="7076" author="thetaphi" created="Mon, 10 Aug 2009 16:52:18 +0100" />
                    <attachment id="12416077" name="LUCENE-1796.patch" size="5840" author="thetaphi" created="Mon, 10 Aug 2009 16:35:58 +0100" />
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>8.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Mon, 10 Aug 2009 15:16:52 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>11966</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>25930</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                    <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                <customfieldname>Time in Status</customfieldname>
                <customfieldvalues>
                    
                </customfieldvalues>
            </customfield>
                            </customfields>
    </item>
</channel>
</rss>