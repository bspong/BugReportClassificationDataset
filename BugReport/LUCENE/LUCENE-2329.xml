<!-- 
RSS generated by JIRA (5.2.8#851-sha1:3262fdc28b4bc8b23784e13eadc26a22399f5d88) at Tue Jul 16 13:13:25 UTC 2013

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/LUCENE-2329/LUCENE-2329.xml?field=key&field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>5.2.8</version>
        <build-number>851</build-number>
        <build-date>26-02-2013</build-date>
    </build-info>

<item>
            <title>[LUCENE-2329] Use parallel arrays instead of PostingList objects</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-2329</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;This is Mike&apos;s idea that was discussed in &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2293&quot; title=&quot;IndexWriter has hard limit on max concurrency&quot;&gt;&lt;del&gt;LUCENE-2293&lt;/del&gt;&lt;/a&gt; and &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2324&quot; title=&quot;Per thread DocumentsWriters that write their own private segments&quot;&gt;&lt;del&gt;LUCENE-2324&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In order to avoid having very many long-living PostingList objects in TermsHashPerField we want to switch to parallel arrays.  The termsHash will simply be a int[] which maps each term to dense termIDs.&lt;/p&gt;

&lt;p&gt;All data that the PostingList classes currently hold will then we placed in parallel arrays, where the termID is the index into the arrays.  This will avoid the need for object pooling, will remove the overhead of object initialization and garbage collection.  Especially garbage collection should benefit significantly when the JVM runs out of memory, because in such a situation the gc mark times can get very long if there is a big number of long-living objects in memory.&lt;/p&gt;

&lt;p&gt;Another benefit could be to build more efficient TermVectors.  We could avoid the need of having to store the term string per document in the TermVector.  Instead we could just store the segment-wide termIDs.  This would reduce the size and also make it easier to implement efficient algorithms that use TermVectors, because no term mapping across documents in a segment would be necessary.  Though this improvement we can make with a separate jira issue.&lt;/p&gt;</description>
                <environment></environment>
            <key id="12459482">LUCENE-2329</key>
            <summary>Use parallel arrays instead of PostingList objects</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png">Closed</status>
                    <resolution id="1">Fixed</resolution>
                                <assignee username="michaelbusch">Michael Busch</assignee>
                                <reporter username="michaelbusch">Michael Busch</reporter>
                        <labels>
                    </labels>
                <created>Thu, 18 Mar 2010 05:57:17 +0000</created>
                <updated>Mon, 29 Aug 2011 03:12:32 +0100</updated>
                    <resolved>Tue, 4 May 2010 18:18:06 +0100</resolved>
                                            <fixVersion>3.1</fixVersion>
                <fixVersion>4.0-ALPHA</fixVersion>
                                <component>core/index</component>
                        <due></due>
                    <votes>0</votes>
                        <watches>1</watches>
                                                    <comments>
                    <comment id="12846807" author="mikemccand" created="Thu, 18 Mar 2010 09:21:21 +0000"  >&lt;p&gt;This would be great!&lt;/p&gt;

&lt;p&gt;But, note that term vectors today do not store the term char[] again &amp;#8211; they piggyback on the term char[] already stored for the postings.  Though, I believe they store &quot;int textStart&quot; (increments by term length per unique term), which is less compact than the termID would be (increments +1 per unique term), so if eg we someday use packed ints we&apos;d be more RAM efficient by storing termIDs...&lt;/p&gt;</comment>
                    <comment id="12846936" author="ab" created="Thu, 18 Mar 2010 14:39:31 +0000"  >&lt;p&gt;Slightly off-topic ... Having a facility to obtain termID-s per segment (or better yet per index) would greatly benefit Solr&apos;s UnInverted field creation, which currently needs to assign term ids by linear scanning.&lt;/p&gt;</comment>
                    <comment id="12846946" author="mikemccand" created="Thu, 18 Mar 2010 15:01:24 +0000"  >&lt;p&gt;This issue is just about how IndexWriter&apos;s RAM buffer stores its terms...&lt;/p&gt;

&lt;p&gt;But, the flex API adds long ord() and seek(long ord) to the TermsEnum API.&lt;/p&gt;</comment>
                    <comment id="12847024" author="michaelbusch" created="Thu, 18 Mar 2010 17:51:36 +0000"  >&lt;blockquote&gt;&lt;p&gt;This issue is just about how IndexWriter&apos;s RAM buffer stores its terms... &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Actually, when I talked about the TermVectors I meant we should explore to store the termIDs on &lt;b&gt;disk&lt;/b&gt;, rather than the strings.  It would help things like similarity search and facet counting.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;But, note that term vectors today do not store the term char[] again - they piggyback on the term char[] already stored for the postings.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yeah I think I&apos;m familiar with that part (secondary entry point in TermsHashPerField, hashes based on termStart).  Haven&apos;t looked much into how the &quot;rest&quot; of the TermVector in-memory data structures are working.  &lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Though, I believe they store &quot;int textStart&quot; (increments by term length per unique term), which is less compact than the termID would be (increments +1 per unique term)&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Actually we wouldn&apos;t need a second hashtable for the secondary TermsHash anymore, right?  It would just have like the primary TermsHash a parallel array with the things that the TermVectorsTermsWriter.Postinglist class currently contains (freq, lastOffset, lastPosition)?  And the index into that array would be the termID of course.&lt;/p&gt;

&lt;p&gt;This would be a nice simplification, because no hash collisions, no hash table resizing based on load factor, etc. would be necessary for non-primary TermsHashes?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;so if eg we someday use packed ints we&apos;d be more RAM efficient by storing termIDs...&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;How does the read performance of packed ints compare to &quot;normal&quot; int[] arrays?  I think nowadays RAM is less of an issue?  And with a searchable RAM buffer we might want to sacrifice a bit more RAM for higher search performance?  Oh man, will we need flexible indexing for the in-memory index too? &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; &lt;/p&gt;</comment>
                    <comment id="12847058" author="mikemccand" created="Thu, 18 Mar 2010 18:36:36 +0000"  >&lt;blockquote&gt;&lt;p&gt;Actually, when I talked about the TermVectors I meant we should explore to store the termIDs on disk, rather than the strings. It would help things like similarity search and facet counting.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Ahhhh that would be great!&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Actually we wouldn&apos;t need a second hashtable for the secondary TermsHash anymore, right? It would just have like the primary TermsHash a parallel array with the things that the TermVectorsTermsWriter.Postinglist class currently contains (freq, lastOffset, lastPosition)? And the index into that array would be the termID of course.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Hmm the challenge is that the tracking done for term vectors is just within a single doc.  Ie the hash used for term vectors only holds the terms for that one doc (so it&apos;s much smaller), vs the primary hash that holds terms for all docs in the current RAM buffer.  So we&apos;d be burning up much more RAM if we also key into the term vector&apos;s parallel arrays using the primary term id?&lt;/p&gt;

&lt;p&gt;But I do think we should cutover to parallel arrays for TVTW, too.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;How does the read performance of packed ints compare to &quot;normal&quot; int[] arrays? I think nowadays RAM is less of an issue? And with a searchable RAM buffer we might want to sacrifice a bit more RAM for higher search performance?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;It&apos;s definitely slower to read/write to/from packed ints, and I agree, indexing and searching speed trumps RAM efficiency.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Oh man, will we need flexible indexing for the in-memory index too?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;EG custom attrs appearing in the TokenStream?  Yes we will need to... but hopefully once we get serialization working cleanly for the attrs this&apos;ll be easy?  With ByteSliceWriter/Reader you just .writeBytes and .readBytes...&lt;/p&gt;

&lt;p&gt;I don&apos;t think we should allow Codecs to be used in the RAM buffer anytime soon though... &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/wink.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;
</comment>
                    <comment id="12847068" author="michaelbusch" created="Thu, 18 Mar 2010 18:57:28 +0000"  >&lt;blockquote&gt;&lt;p&gt;Hmm the challenge is that the tracking done for term vectors is just within a single doc.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Duh! Of course you&apos;re right.&lt;/p&gt;</comment>
                    <comment id="12848052" author="michaelbusch" created="Mon, 22 Mar 2010 08:05:17 +0000"  >&lt;p&gt;Changes the indexer to use parallel arrays instead of PostingList objects (for both FreqProx* and TermVectors*).&lt;/p&gt;

&lt;p&gt;All core &amp;amp; contrib &amp;amp; bw tests pass.  I haven&apos;t done performance tests yet.  &lt;/p&gt;

&lt;p&gt;I&apos;m wondering how to manage the size of the parallel array?  I started with an initial size for the parallel array equal to the size of the postingsHash array.  When it&apos;s full then I allocate a new one with 1.5x size.  When shrinkHash() is called I also shrink the parallel array to the same size as postingsHash.  How does that sound?&lt;/p&gt;</comment>
                    <comment id="12848058" author="michaelbusch" created="Mon, 22 Mar 2010 08:28:39 +0000"  >&lt;p&gt;One change I should make to the patch is how to track the memory consumption.  When the parallel array is allocated or grown then bytesAllocated() should be called?  And when a new termID is added, should only then bytesUsed() be called?&lt;/p&gt;</comment>
                    <comment id="12848063" author="michaelbusch" created="Mon, 22 Mar 2010 08:56:11 +0000"  >&lt;p&gt;Made the memory tracking changes as described in my previous comment.&lt;/p&gt;

&lt;p&gt;All tests still pass.&lt;/p&gt;</comment>
                    <comment id="12848085" author="mikemccand" created="Mon, 22 Mar 2010 11:19:23 +0000"  >&lt;p&gt;Looks great Michael!&lt;/p&gt;

&lt;p&gt;I think *ParallelPostingsArray.reset do not need to zero-fill the arrays &amp;#8211; these are overwritten when that termID is first used, right?&lt;/p&gt;</comment>
                    <comment id="12848161" author="michaelbusch" created="Mon, 22 Mar 2010 15:21:34 +0000"  >&lt;blockquote&gt;&lt;p&gt;I think *ParallelPostingsArray.reset do not need to zero-fill the arrays - these are overwritten when that termID is first used, right?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Good point!  I&apos;ll remove the reset() methods.&lt;/p&gt;</comment>
                    <comment id="12848179" author="michaelbusch" created="Mon, 22 Mar 2010 16:04:17 +0000"  >&lt;p&gt;Removed reset().  All tests still pass.&lt;/p&gt;</comment>
                    <comment id="12848475" author="michaelbusch" created="Tue, 23 Mar 2010 00:48:19 +0000"  >&lt;p&gt;I did some performance experiments:&lt;/p&gt;

&lt;p&gt;I indexed 1M wikipedia documents using the cheap WhiteSpaceAnalyzer, no cfs files, disabled any merging,  RAM buffer size = 200MB, single writer thread, TermVectors enabled.  &lt;/p&gt;

&lt;p&gt;Test machine: MacBook Pro, 2.53 GHz Intel Core 2 Duo, 4 GB 1067 MHz DDR3, MacOS X 10.5.8.&lt;/p&gt;

&lt;h4&gt;&lt;a name=&quot;ResultswithXmx2000m%3A&quot;&gt;&lt;/a&gt;Results with -Xmx2000m:&lt;/h4&gt;

&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;&amp;nbsp;&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt; Write performance &lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt; Gain &lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt; Number of segments &lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; trunk &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; 833 docs/sec &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;&amp;nbsp;&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;  41 &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; trunk + parallel arrays &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; 869 docs/sec &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt; + 4.3% &lt;/font&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; 32&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;



&lt;h4&gt;&lt;a name=&quot;ResultswithXmx256m%3A&quot;&gt;&lt;/a&gt;Results with -Xmx256m:&lt;/h4&gt;

&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;&amp;nbsp;&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt; Write performance &lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt; Gain &lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt; Number of segments &lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; trunk &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; 467 docs/sec &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;&amp;nbsp;&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; 41 &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; trunk + parallel arrays &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; 871 docs/sec &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt; +86.5% &lt;/font&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; 32&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;


&lt;p&gt;So I think these results are interesting and roughly as expected.  4.3% is a nice small performance gain.&lt;br/&gt;
But running the tests with a low heap shows how much cheaper the garbage collection becomes.  Setting IW&apos;s RAM buffer to 200MB and the overall heap to 256MB forces the gc to run frequently.  The mark times are much more costly if we have all long-living PostingList objects in memory compared to parallel arrays.&lt;/p&gt;

&lt;p&gt;So this is probably not a huge deal for &quot;normal&quot; indexing.  But once we can search on the RAM buffer it becomes much more attractive to fill up the RAM as much as you can.  And exactly in that case we safe a lot with this improvement. &lt;/p&gt;

&lt;p&gt;Also note that the number of segments decreased by 22% (from 41 to 32).  This shows that the parallel-array approach needs less RAM, thus flushes less often and will cause less segment merges in the long run.  So a longer test with actual segment merges would show even bigger gains (with both big and small heaps).&lt;/p&gt;

&lt;p&gt;So overall, I&apos;m very happy with these results!&lt;/p&gt;</comment>
                    <comment id="12848626" author="mikemccand" created="Tue, 23 Mar 2010 09:07:04 +0000"  >&lt;p&gt;Sweet, this looks great Michael!  Less RAM used and faster indexing (much less GC load) &amp;#8211; win/win.&lt;/p&gt;

&lt;p&gt;It&apos;s a little surprising that the segment count dropped from 41 -&amp;gt; 32?  Ie, how much less RAM do the parallel arrays take?  They save the object header per-unique-term, and 4 bytes on 64bit JREs since the &quot;pointer&quot; is now an int and not a real pointer?  But, other things use RAM (the docIDs in the postings themselves, norms, etc.) so it&apos;s surprising the savings was so much that you get 22% fewer segments... are you sure there isn&apos;t a bug in the RAM usage accounting?&lt;/p&gt;</comment>
                    <comment id="12848748" author="michaelbusch" created="Tue, 23 Mar 2010 15:16:41 +0000"  >&lt;blockquote&gt;
&lt;p&gt;so it&apos;s surprising the savings was so much that you get 22% fewer segments... are you sure there isn&apos;t a bug in the RAM usage accounting?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yeah it seems a bit suspicious.  I&apos;ll investigate.  But, keep in mind that TermVectors were enabled too.  And the number of &quot;unique terms&quot; in the 2nd TermsHash is higher, i.e. if you summed up numPostings from the 2nd TermsHash in each round that sum should be higher than numPostings from the first TermsHash. &lt;/p&gt;</comment>
                    <comment id="12848767" author="mikemccand" created="Tue, 23 Mar 2010 15:54:47 +0000"  >&lt;blockquote&gt;&lt;p&gt;But, keep in mind that TermVectors were enabled too.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;OK, but, RAM used by TermVectors* shouldn&apos;t participate in the accounting... ie it only holds RAM for the one doc, at a time.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;And the number of &quot;unique terms&quot; in the 2nd TermsHash is higher, i.e. if you summed up numPostings from the 2nd TermsHash in each round that sum should be higher than numPostings from the first TermsHash.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;1st TermsHash = current trunk and 2nd TermsHash = this patch?  Ie, it has more unique terms at flush time (because it&apos;s more RAM efficient)?  If so, then yes, I agree &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;  But 22% fewer still seems too good to be true...&lt;/p&gt;</comment>
                    <comment id="12848782" author="michaelbusch" created="Tue, 23 Mar 2010 16:19:28 +0000"  >&lt;blockquote&gt;
&lt;p&gt;OK, but, RAM used by TermVectors* shouldn&apos;t participate in the accounting... ie it only holds RAM for the one doc, at a time.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Man, my brain is lacking the TermVector synapses...&lt;/p&gt;</comment>
                    <comment id="12848827" author="michaelbusch" created="Tue, 23 Mar 2010 18:04:10 +0000"  >&lt;blockquote&gt;
&lt;p&gt;They save the object header per-unique-term, and 4 bytes on 64bit JREs since the &quot;pointer&quot; is now an int and not a real pointer?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;We actually save on 64bit JVMs (which I used for my tests) 28 bytes per unique-term:&lt;/p&gt;

&lt;h4&gt;&lt;a name=&quot;Trunk%3A&quot;&gt;&lt;/a&gt;Trunk:&lt;/h4&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
    &lt;span class=&quot;code-comment&quot;&gt;// Why + 4*POINTER_NUM_BYTE below?
&lt;/span&gt;    &lt;span class=&quot;code-comment&quot;&gt;//   +1: Posting is referenced by postingsFreeList array
&lt;/span&gt;    &lt;span class=&quot;code-comment&quot;&gt;//   +3: Posting is referenced by hash, which
&lt;/span&gt;    &lt;span class=&quot;code-comment&quot;&gt;//       targets 25-50% fill factor; approximate &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;
&lt;/span&gt;    &lt;span class=&quot;code-comment&quot;&gt;//       as 3X # pointers
&lt;/span&gt;    bytesPerPosting = consumer.bytesPerPosting() + 4*DocumentsWriter.POINTER_NUM_BYTE;

...

  @Override
  &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; bytesPerPosting() {
    &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; RawPostingList.BYTES_SIZE + 4 * DocumentsWriter.INT_NUM_BYTE;
  }

...
&lt;span class=&quot;code-keyword&quot;&gt;abstract&lt;/span&gt; class RawPostingList {
  &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; BYTES_SIZE = DocumentsWriter.OBJECT_HEADER_BYTES + 3*DocumentsWriter.INT_NUM_BYTE;

...

  &lt;span class=&quot;code-comment&quot;&gt;// Coarse estimates used to measure RAM usage of buffered deletes
&lt;/span&gt;  &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; OBJECT_HEADER_BYTES = 8;
  &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; POINTER_NUM_BYTE = Constants.JRE_IS_64BIT ? 8 : 4;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This needs 8 bytes + 3 * 4 bytes + 4 * 4 bytes + 4 * 8 bytes = 68 bytes. &lt;/p&gt;

&lt;h4&gt;&lt;a name=&quot;2329%3A&quot;&gt;&lt;/a&gt;2329:&lt;/h4&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
    &lt;span class=&quot;code-comment&quot;&gt;//   +3: Posting is referenced by hash, which
&lt;/span&gt;    &lt;span class=&quot;code-comment&quot;&gt;//       targets 25-50% fill factor; approximate &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;
&lt;/span&gt;    &lt;span class=&quot;code-comment&quot;&gt;//       as 3X # pointers
&lt;/span&gt;    bytesPerPosting = consumer.bytesPerPosting() + 3*DocumentsWriter.INT_NUM_BYTE;

...

  @Override
  &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; bytesPerPosting() {
    &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; ParallelPostingsArray.BYTES_PER_POSTING + 4 * DocumentsWriter.INT_NUM_BYTE;
  }

...

&lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; BYTES_PER_POSTING = 3 * DocumentsWriter.INT_NUM_BYTE;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This needs 3 * 4 bytes + 4 * 4 bytes + 3 * 4 bytes = 40 bytes.&lt;/p&gt;


&lt;p&gt;I checked how many bytes were allocated for postings when the first segment was flushed.  Trunk flushed after 6400 docs and had 103MB allocated for PostingList objects.  2329 flushed after 8279 docs and had 94MB allocated for the parallel arrays, and 74MB out of the 94MB were actually used.&lt;/p&gt;

&lt;p&gt;The first docs in the wikipedia dataset seem pretty large with many unique terms.&lt;/p&gt;

&lt;p&gt;I think this sounds reasonable?&lt;/p&gt;</comment>
                    <comment id="12848833" author="mikemccand" created="Tue, 23 Mar 2010 18:20:47 +0000"  >&lt;p&gt;OK indeed it does sounds reasonable!  Sweet &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;  I think you should commit it!  Make sure you &quot;svn switch&quot; your checkout first &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;  And pass Solr&apos;s tests!&lt;/p&gt;</comment>
                    <comment id="12848855" author="michaelbusch" created="Tue, 23 Mar 2010 19:02:56 +0000"  >&lt;p&gt;Cool, will do!  Thanks for the review and good questions... and the whole idea! &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                    <comment id="12848917" author="michaelbusch" created="Tue, 23 Mar 2010 21:25:52 +0000"  >&lt;p&gt;Committed revision 926791.&lt;/p&gt;</comment>
                    <comment id="12850884" author="mikemccand" created="Mon, 29 Mar 2010 11:58:00 +0100"  >&lt;p&gt;I think we need to fix how RAM is managed for this... right now, if&lt;br/&gt;
you turn on IW&apos;s infoStream you&apos;ll see a zillion prints where IW tries&lt;br/&gt;
to balance RAM (it &quot;runs hot&quot;), but, nothing can be freed.  We do this&lt;br/&gt;
per-doc, after the parallel arrays resize themselves to net/net over&lt;br/&gt;
our allowed RAM buffer.&lt;/p&gt;

&lt;p&gt;A few ideas on how we can fix:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;I think we have to change when we flush.  It&apos;s now based on RAM&lt;br/&gt;
    used (not alloc&apos;d), but I think we should switch it to use RAM&lt;br/&gt;
    alloc&apos;d after we&apos;ve freed all we can.  Ie if we free things up and&lt;br/&gt;
    we&apos;ve still alloc&apos;d over the limit, we flush.  This&apos;ll fix the&lt;br/&gt;
    running hot we now see...&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;TermsHash.freeRAM is now a no-op right?  We have to fix this to&lt;br/&gt;
    actually free something when it can because you can imagine&lt;br/&gt;
    indexing docs that are postings heavy but then switching to docs&lt;br/&gt;
    that are byte[] block heavy.  On that switch you have to balance&lt;br/&gt;
    the allocations (ie, shrink your postings).  I think we should&lt;br/&gt;
    walk the threads/fields and use ArrayUtil.shrink to shrink down,&lt;br/&gt;
    but, don&apos;t shrink by much at a time (to avoid running hot) &amp;#8211; IW&lt;br/&gt;
    will invoke this method again if more shrinkage is needed.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Also, shouldn&apos;t we use ArrayUtil.grow to increase?  Instead of&lt;br/&gt;
    always a 50% growth?  Because with such a large growth you can&lt;br/&gt;
    easily have horrible RAM efficiency... ie that 50% growth can&lt;br/&gt;
    suddenly put you over the limit and then you flush, having&lt;br/&gt;
    effectively used only half of the allowed RAM buffer in the worst&lt;br/&gt;
    case.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="12850885" author="mikemccand" created="Mon, 29 Mar 2010 12:00:03 +0100"  >&lt;p&gt;Reopening to fix the RAM balancing problems...&lt;/p&gt;</comment>
                    <comment id="12850989" author="michaelbusch" created="Mon, 29 Mar 2010 17:34:03 +0100"  >&lt;p&gt;Good catch!&lt;/p&gt;

&lt;p&gt;Thanks for the thorough explanation and suggestions.  I think it all makes sense.  Will work on a patch.&lt;/p&gt;</comment>
                    <comment id="12852195" author="michaelbusch" created="Thu, 1 Apr 2010 01:32:37 +0100"  >&lt;p&gt;This patch:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Changes DocumentsWriter to trigger the flush using bytesAllocated instead of bytesUsed to improve the &quot;running hot&quot; issue Mike&apos;s seeing&lt;/li&gt;
	&lt;li&gt;Improves the ParallelPostingsArray to grow using ArrayUtil.oversize()&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;In IRC we discussed changing TermsHashPerField to shrink the parallel arrays in freeRAM(), but that involves tricky thread-safety changes, because one thread could call DocumentsWriter.balanceRAM(), which triggers freeRAM() across &lt;b&gt;all&lt;/b&gt; thread states, while other threads keep indexing.&lt;/p&gt;

&lt;p&gt;We decided to leave it the way it currently works: we discard the whole parallel array during flush and don&apos;t reuse it.  This is not as optimal as it could be, but once &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2324&quot; title=&quot;Per thread DocumentsWriters that write their own private segments&quot;&gt;&lt;del&gt;LUCENE-2324&lt;/del&gt;&lt;/a&gt; is done this won&apos;t be an issue anymore anyway.&lt;/p&gt;

&lt;p&gt;Note that this new patch is against the flex branch: I thought we&apos;d switch it over soon anyway?  I can also create a patch for trunk if that&apos;s preferred.&lt;/p&gt;</comment>
                    <comment id="12852479" author="mikemccand" created="Thu, 1 Apr 2010 18:58:54 +0100"  >&lt;p&gt;Fixed a couple of issues on the last patch:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;We weren&apos;t notifying DW that we freed up RAM when setting postingsArray to null&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;We can shrink postingsArray separately from the termsHash, and, instead of nulling it, we can simply downsize it&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Tests pass, and indexing perf on first 10M 1 KB sized wikipedia docs is a bit faster though probably in the noise (1296 sec on current flex head vs 1238 sec with this patch).&lt;/p&gt;</comment>
                    <comment id="12852608" author="mikemccand" created="Fri, 2 Apr 2010 00:49:19 +0100"  >&lt;p&gt;New patch attached:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;Does away with separate tracking of used vs alloc, in IndexWriter.&lt;br/&gt;
    This distinction added much complexity and only saved a small&lt;br/&gt;
    number of free/alloc&apos;s per flush cycle, especially now that&lt;br/&gt;
    postings realloc only in big chunks (parallel arrays).&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Fixed some over-counting of bytes used.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;The indexing throughput is basically unchanged after this (on first&lt;br/&gt;
10M 1KB Wikipedia docs), so I think this is a good simplification.&lt;/p&gt;</comment>
                    <comment id="12852625" author="michaelbusch" created="Fri, 2 Apr 2010 01:54:59 +0100"  >&lt;p&gt;Looks great!  I like the removal of bytesAlloc - nice simplification.&lt;/p&gt;</comment>
                    <comment id="12852759" author="mikemccand" created="Fri, 2 Apr 2010 10:14:11 +0100"  >&lt;p&gt;Thanks Michael... I&apos;ll commit shortly.  It&apos;s a good simplification.&lt;/p&gt;</comment>
                    <comment id="12852858" author="michaelbusch" created="Fri, 2 Apr 2010 17:23:08 +0100"  >&lt;p&gt;Thanks!  I think we can resolve this now?&lt;/p&gt;</comment>
                    <comment id="12852863" author="mikemccand" created="Fri, 2 Apr 2010 17:43:32 +0100"  >&lt;p&gt;Woops, right!&lt;/p&gt;</comment>
                    <comment id="12853499" author="mikemccand" created="Mon, 5 Apr 2010 20:21:08 +0100"  >&lt;p&gt;Reopening &amp;#8211; this fixed causes an intermittent deadlock in&lt;br/&gt;
TestStressIndexing2.&lt;/p&gt;

&lt;p&gt;It&apos;s actually a pre-existing issue, whereby if a flush happens only&lt;br/&gt;
because of deletions (ie no indexed docs), and you&apos;re using multiple&lt;br/&gt;
threads, it&apos;s possible some idled threads would fail to be notified&lt;br/&gt;
to wake up and continue indexing once the flush completes.&lt;/p&gt;

&lt;p&gt;The fix here increased the chance of hitting that bug because the RAM&lt;br/&gt;
accounting has a bug whereby it overly-aggressively flushes because of&lt;br/&gt;
deletions, ie, rather than free up RAM allocated but not used for&lt;br/&gt;
indexing, it flushes.&lt;/p&gt;

&lt;p&gt;I first fixed the deadlock case (need to clear DW&apos;s flushPending when&lt;br/&gt;
we only flush deletes).&lt;/p&gt;

&lt;p&gt;Then I fixed the shared deletes/indexing RAM by:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;Not reusing the RAM for postings arrays &amp;#8211; we now null this out&lt;br/&gt;
    for every field after flushing&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Calling balanceRAM when deletes have filled up RAM before deciding&lt;br/&gt;
    to flush, because this can free RAM up, making more space for&lt;br/&gt;
    deletes.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I also further simplified things &amp;#8211; no more separate call to&lt;br/&gt;
doBalanceRAM, and added a fun unit test that randomly alternates&lt;br/&gt;
between pure indexing and pure deleting, asserting that the flushing&lt;br/&gt;
doesn&apos;t &quot;run hot&quot; on any of those transitions.&lt;/p&gt;</comment>
                    <comment id="12853509" author="michaelbusch" created="Mon, 5 Apr 2010 20:49:34 +0100"  >&lt;p&gt;We could move the if (postingsArray == null) check to start(), then we don&apos;t have to check for every new term?&lt;/p&gt;
</comment>
                    <comment id="12853815" author="mikemccand" created="Tue, 6 Apr 2010 10:11:23 +0100"  >&lt;blockquote&gt;&lt;p&gt;We could move the if (postingsArray == null) check to start(), then we don&apos;t have to check for every new term?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Excellent, I&apos;ll do that!&lt;/p&gt;</comment>
                    <comment id="12853824" author="mikemccand" created="Tue, 6 Apr 2010 10:29:25 +0100"  >&lt;p&gt;New patch, init&apos;ing the postings arrays in THPF.start instead of per term.&lt;/p&gt;</comment>
                    <comment id="12854146" author="mikemccand" created="Tue, 6 Apr 2010 20:57:16 +0100"  >&lt;p&gt;Third time&apos;s a charm?&lt;/p&gt;</comment>
                    <comment id="12863857" author="thetaphi" created="Tue, 4 May 2010 16:54:09 +0100"  >&lt;p&gt;This is broken now in stable branch. We should fix it, hudsons clover tests are hung in benchmark.&lt;/p&gt;</comment>
                    <comment id="12863865" author="michaelbusch" created="Tue, 4 May 2010 17:16:42 +0100"  >&lt;p&gt;I probably missed something here.  What exactly is broken and how is it related to this patch?&lt;/p&gt;</comment>
                    <comment id="12863879" author="mikemccand" created="Tue, 4 May 2010 17:47:04 +0100"  >&lt;p&gt;Michael I&apos;ll take care of it &amp;#8211; we just need to merge all commits under this issue, to stable.  Only the 1st commit made it... I&apos;m working on it now.&lt;/p&gt;</comment>
                    <comment id="12863899" author="mikemccand" created="Tue, 4 May 2010 18:18:06 +0100"  >&lt;p&gt;OK I merged all commits to 3x.&lt;/p&gt;</comment>
                    <comment id="12863914" author="michaelbusch" created="Tue, 4 May 2010 18:49:49 +0100"  >&lt;p&gt;Ah got it.  Thanks for taking care of it!&lt;/p&gt;</comment>
                    <comment id="13013272" author="gsingers" created="Wed, 30 Mar 2011 16:49:50 +0100"  >&lt;p&gt;Bulk close for 3.1&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12440427" name="lucene-2329-2.patch" size="10939" author="michaelbusch" created="Thu, 1 Apr 2010 01:32:37 +0100" />
                    <attachment id="12439470" name="lucene-2329.patch" size="49072" author="michaelbusch" created="Mon, 22 Mar 2010 16:04:17 +0000" />
                    <attachment id="12439439" name="lucene-2329.patch" size="49771" author="michaelbusch" created="Mon, 22 Mar 2010 08:56:11 +0000" />
                    <attachment id="12439436" name="lucene-2329.patch" size="49142" author="michaelbusch" created="Mon, 22 Mar 2010 08:05:17 +0000" />
                    <attachment id="12440867" name="LUCENE-2329.patch" size="9043" author="mikemccand" created="Tue, 6 Apr 2010 10:29:25 +0100" />
                    <attachment id="12440786" name="LUCENE-2329.patch" size="9184" author="mikemccand" created="Mon, 5 Apr 2010 20:21:39 +0100" />
                    <attachment id="12440557" name="LUCENE-2329.patch" size="21684" author="mikemccand" created="Fri, 2 Apr 2010 00:49:19 +0100" />
                    <attachment id="12440530" name="LUCENE-2329.patch" size="11522" author="mikemccand" created="Thu, 1 Apr 2010 18:58:54 +0100" />
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>8.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Thu, 18 Mar 2010 09:21:21 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>11465</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>25396</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                    <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                <customfieldname>Time in Status</customfieldname>
                <customfieldvalues>
                    
                </customfieldvalues>
            </customfield>
                            </customfields>
    </item>
</channel>
</rss>