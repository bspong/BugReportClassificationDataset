<!-- 
RSS generated by JIRA (5.2.8#851-sha1:3262fdc28b4bc8b23784e13eadc26a22399f5d88) at Tue Jul 16 13:30:28 UTC 2013

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/LUCENE-843/LUCENE-843.xml?field=key&field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>5.2.8</version>
        <build-number>851</build-number>
        <build-date>26-02-2013</build-date>
    </build-info>

<item>
            <title>[LUCENE-843] improve how IndexWriter uses RAM to buffer added documents</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-843</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;I&apos;m working on a new class (MultiDocumentWriter) that writes more than&lt;br/&gt;
one document directly into a single Lucene segment, more efficiently&lt;br/&gt;
than the current approach.&lt;/p&gt;

&lt;p&gt;This only affects the creation of an initial segment from added&lt;br/&gt;
documents.  I haven&apos;t changed anything after that, eg how segments are&lt;br/&gt;
merged.&lt;/p&gt;

&lt;p&gt;The basic ideas are:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;Write stored fields and term vectors directly to disk (don&apos;t&lt;br/&gt;
    use up RAM for these).&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Gather posting lists &amp;amp; term infos in RAM, but periodically do&lt;br/&gt;
    in-RAM merges.  Once RAM is full, flush buffers to disk (and&lt;br/&gt;
    merge them later when it&apos;s time to make a real segment).&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Recycle objects/buffers to reduce time/stress in GC.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Other various optimizations.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Some of these changes are similar to how KinoSearch builds a segment.&lt;br/&gt;
But, I haven&apos;t made any changes to Lucene&apos;s file format nor added&lt;br/&gt;
requirements for a global fields schema.&lt;/p&gt;

&lt;p&gt;So far the only externally visible change is a new method&lt;br/&gt;
&quot;setRAMBufferSize&quot; in IndexWriter (and setMaxBufferedDocs is&lt;br/&gt;
deprecated) so that it flushes according to RAM usage and not a fixed&lt;br/&gt;
number documents added.&lt;/p&gt;</description>
                <environment></environment>
            <key id="12365595">LUCENE-843</key>
            <summary>improve how IndexWriter uses RAM to buffer added documents</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png">Closed</status>
                    <resolution id="1">Fixed</resolution>
                                <assignee username="mikemccand">Michael McCandless</assignee>
                                <reporter username="mikemccand">Michael McCandless</reporter>
                        <labels>
                    </labels>
                <created>Thu, 22 Mar 2007 17:05:11 +0000</created>
                <updated>Fri, 25 Jan 2008 03:23:51 +0000</updated>
                    <resolved>Sun, 12 Aug 2007 16:48:41 +0100</resolved>
                            <version>2.2</version>
                                <fixVersion>2.3</fixVersion>
                                <component>core/index</component>
                        <due></due>
                    <votes>5</votes>
                        <watches>4</watches>
                                                    <comments>
                    <comment id="12483223" author="mikemccand" created="Thu, 22 Mar 2007 17:06:45 +0000"  >&lt;p&gt;I&apos;m attaching a patch with my current state.  NOTE: this is very rough&lt;br/&gt;
and very much a work in progress and nowhere near ready to commit!  I&lt;br/&gt;
wanted to get it out there sooner rather than later to get feedback,&lt;br/&gt;
maybe entice some daring early adopters, iterate, etc.&lt;/p&gt;

&lt;p&gt;It passes all unit tests except the disk-full tests.&lt;/p&gt;

&lt;p&gt;There are some big issues yet to resolve:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;Merge policy has problems when you &quot;flush by RAM&quot; (this is true&lt;br/&gt;
    even before my patch).  Not sure how to fix yet.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Thread safety and thread concurrency aren&apos;t there yet.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Norms are not flushed (just use up RAM until you close the&lt;br/&gt;
    writer).&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Many other things on my TODO list &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/li&gt;
&lt;/ul&gt;

</comment>
                    <comment id="12483940" author="mikemccand" created="Sun, 25 Mar 2007 15:30:36 +0100"  >&lt;p&gt;New rev of the patch:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;Fixed at least one data corruption case&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Added more asserts (run with &quot;java -ea&quot; so asserts run)&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Some more small optimizations&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Updated to current trunk so patch applies cleanly&lt;/li&gt;
&lt;/ul&gt;

</comment>
                    <comment id="12484853" author="mikemccand" created="Wed, 28 Mar 2007 13:49:18 +0100"  >
&lt;p&gt;Another rev of the patch:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;Got thread concurrency working: removed &quot;synchronized&quot; from entire&lt;br/&gt;
    call to MultiDocWriter.addDocument and instead synchronize two&lt;br/&gt;
    quick steps (init/finish) addDocument leaving the real work&lt;br/&gt;
    (processDocument) unsynchronized.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Fixed bug that was failing to delete temp files from index&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Reduced memory usage of Posting by inlining positions, start&lt;br/&gt;
    offset, end offset into a single int array.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Enabled IndexLineFiles.java (tool I use for local benchmarking) to&lt;br/&gt;
    run multiple threads&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Other small optimizations&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;BTW, one of the nice side effects of this patch is it cleans up the&lt;br/&gt;
mergeSegments method of IndexWriter by separating out &quot;flush&quot; of added&lt;br/&gt;
docs &amp;amp; deletions because it&apos;s no longer a merge, from the &quot;true&quot;&lt;br/&gt;
mergeSegments whose purpose is then to merge disk segments.&lt;br/&gt;
Previously mergeSegments was getting rather confusing with the&lt;br/&gt;
different cases/combinations of added docs or not, deleted docs or&lt;br/&gt;
not, any merges or not.&lt;/p&gt;
</comment>
                    <comment id="12486025" author="mikemccand" created="Mon, 2 Apr 2007 15:43:21 +0100"  >&lt;p&gt;Another rev of the patch.  All tests pass except disk full tests.  The&lt;br/&gt;
code is still rather &quot;dirty&quot; and not well commented.&lt;/p&gt;

&lt;p&gt;I think I&apos;m close to finishing optimizing and now I will focus on&lt;br/&gt;
error handling (eg disk full), adding some deeper unit tests, more&lt;br/&gt;
testing on corner cases like massive docs or docs with massive terms,&lt;br/&gt;
etc., flushing pending norms to disk, cleaning up / commenting the&lt;br/&gt;
code and various other smaller items.&lt;/p&gt;

&lt;p&gt;Here are the changes in this rev:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;A proposed backwards compatible change to the Token API to also&lt;br/&gt;
    allow the term text to be delivered as a slice (offset &amp;amp; length)&lt;br/&gt;
    into a char[] array instead of String.  With an analyzer/tokenizer&lt;br/&gt;
    that takes advantage of this, this was a decent performance gain&lt;br/&gt;
    in my local testing.  I&apos;ve created a SimpleSpaceAnalyzer that only&lt;br/&gt;
    splits words at the space character to test this.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Added more asserts (run java -ea to enable asserts).  The asserts&lt;br/&gt;
    are quite useful and now often catch a bug I&apos;ve introduced before&lt;br/&gt;
    the unit tests do.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Changed to custom int[] block buffering for postings to store&lt;br/&gt;
    freq, prox&apos;s and offsets.  With this buffering we no longer have&lt;br/&gt;
    to double the size of int[] arrays while adding positions, nor do&lt;br/&gt;
    we have to copy ints whenever we needs more space for these&lt;br/&gt;
    arrays.  Instead I allocate larger slices out of the shared int[]&lt;br/&gt;
    arrays.  This reduces memory and improves performance.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Changed to custom char[] block buffering for postings to store&lt;br/&gt;
    term text.  This also reduces memory and improves performance.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Changed to single file for RAM &amp;amp; flushed partial segments (was 3&lt;br/&gt;
    separate files before)&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Changed how I merge flushed partial segments to match what&apos;s&lt;br/&gt;
    described in &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-854&quot; title=&quot;Create merge policy that doesn&amp;#39;t periodically inadvertently optimize&quot;&gt;&lt;del&gt;LUCENE-854&lt;/del&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Reduced memory usage when indexing large docs (25 MB plain text&lt;br/&gt;
    each).  I&apos;m still consuming more RAM in this case than the&lt;br/&gt;
    baseline (trunk) so I&apos;m still working on this one ...&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Fixed a slow memory leak when building large (20+ GB) indices&lt;/li&gt;
&lt;/ul&gt;

</comment>
                    <comment id="12486292" author="mikemccand" created="Tue, 3 Apr 2007 11:20:01 +0100"  >&lt;p&gt;Some details on how I measure RAM usage: both the baseline (current&lt;br/&gt;
lucene trunk) and my patch have two general classes of RAM usage.&lt;/p&gt;

&lt;p&gt;The first class, &quot;document processing RAM&quot;, is RAM used while&lt;br/&gt;
processing a single doc. This RAM is re-used for each document (in the&lt;br/&gt;
trunk, it&apos;s GC&apos;d and new RAM is allocated; in my patch, I explicitly&lt;br/&gt;
re-use these objects) and how large it gets is driven by how big each&lt;br/&gt;
document is.&lt;/p&gt;

&lt;p&gt;The second class, &quot;indexed documents RAM&quot;, is the RAM used up by&lt;br/&gt;
previously indexed documents.  This RAM grows with each added&lt;br/&gt;
document and how large it gets is driven by the number and size of&lt;br/&gt;
docs indexed since the last flush.&lt;/p&gt;

&lt;p&gt;So when I say the writer is allowed to use 32 MB of RAM, I&apos;m only&lt;br/&gt;
measuring the &quot;indexed documents RAM&quot;.  With trunk I do this by&lt;br/&gt;
calling ramSizeInBytes(), and with my patch I do the analagous thing&lt;br/&gt;
by measuring how many RAM buffers are held up storing previously&lt;br/&gt;
indexed documents.&lt;/p&gt;

&lt;p&gt;I then define &quot;RAM efficiency&quot; (docs/MB) as how many docs we can hold&lt;br/&gt;
in &quot;indexed documents RAM&quot; per MB RAM, at the point that we flush to&lt;br/&gt;
disk.  I think this is an important metric because it drives how large&lt;br/&gt;
your initial (level 0) segments are.  The larger these segments are&lt;br/&gt;
then generally the less merging you need to do, for a given # docs in&lt;br/&gt;
the index.&lt;/p&gt;

&lt;p&gt;I also measure overall RAM used in the JVM (using&lt;br/&gt;
MemoryMXBean.getHeapMemoryUsage().getUsed()) just prior to each flush&lt;br/&gt;
except the last, to also capture the &quot;document processing RAM&quot;, object&lt;br/&gt;
overhead, etc.&lt;/p&gt;</comment>
                    <comment id="12486293" author="mikemccand" created="Tue, 3 Apr 2007 11:20:22 +0100"  >&lt;p&gt;To do the benchmarking I created a simple standalone tool&lt;br/&gt;
(demo/IndexLineFiles, in the last patch) that indexes one line at a&lt;br/&gt;
time from a large previously created file, optionally using multiple&lt;br/&gt;
threads.  I do it this way to minimize IO cost of pulling the document&lt;br/&gt;
source because I want to measure just indexing time as much as possible.&lt;/p&gt;

&lt;p&gt;Each line is read and a doc is created with field &quot;contents&quot; that is&lt;br/&gt;
not stored, is tokenized, and optionally has term vectors with&lt;br/&gt;
position+offsets.  I also optionally add two small only-stored fields&lt;br/&gt;
(&quot;path&quot; and &quot;modified&quot;).  I think these are fairly trivial documents&lt;br/&gt;
compared to typical usage of Lucene.&lt;/p&gt;

&lt;p&gt;For the corpus, I took Europarl&apos;s &quot;en&quot; content, stripped tags, and&lt;br/&gt;
processed into 3 files: one with 100 tokens per line (= ~550 bytes),&lt;br/&gt;
one with 1000 tokens per line (= ~5,500 bytes) and with 10000 tokens&lt;br/&gt;
per line (= ~55,000 bytes) plain text per line.&lt;/p&gt;

&lt;p&gt;All settings (mergeFactor, compound file, etc.) are left at defaults.&lt;br/&gt;
I don&apos;t optimize the index in the end.  I&apos;m using my new&lt;br/&gt;
SimpleSpaceAnalyzer (just splits token on the space character and&lt;br/&gt;
creates token text as slice into a char[] array instead of new&lt;br/&gt;
String(...)) to minimize the cost of tokenization.&lt;/p&gt;

&lt;p&gt;I ran the tests with Java 1.5 on a Mac Pro quad (2 Intel CPUs, each&lt;br/&gt;
dual core) OS X box with 2 GB RAM.  I give java 1 GB heap (-Xmx1024m).&lt;/p&gt;</comment>
                    <comment id="12486332" author="mikemccand" created="Tue, 3 Apr 2007 13:11:07 +0100"  >&lt;p&gt;A couple more details on the testing: I run java -server to get all&lt;br/&gt;
optimizations in the JVM, and the IO system is a local OS X RAID 0 of&lt;br/&gt;
4 SATA drives.&lt;/p&gt;

&lt;p&gt;Using the above tool I ran an initial set of benchmarks comparing old&lt;br/&gt;
(= Lucene trunk) vs new (= this patch), varying document size (~550&lt;br/&gt;
bytes to ~5,500 bytes to ~55,000 bytes of plain text from Europarl&lt;br/&gt;
&quot;en&quot;).&lt;/p&gt;

&lt;p&gt;For each document size I run 4 combinations of whether term vectors&lt;br/&gt;
and stored fields are on or off and whether autoCommit is true or&lt;br/&gt;
false.  I measure net docs/sec (= total # docs indexed divided by&lt;br/&gt;
total time taken), RAM efficiency (= avg # docs flushed with each&lt;br/&gt;
flush divided by RAM buffer size), and avg HEAP RAM usage before each&lt;br/&gt;
flush.&lt;/p&gt;

&lt;p&gt;Here are the results for the 10K tokens (= ~55,000 bytes plain text)&lt;br/&gt;
per document:&lt;/p&gt;

&lt;p&gt;  20000 DOCS @ ~55,000 bytes plain text&lt;br/&gt;
  RAM = 32 MB&lt;br/&gt;
  NUM THREADS = 1&lt;br/&gt;
  MERGE FACTOR = 10&lt;/p&gt;


&lt;p&gt;    No term vectors nor stored fields&lt;/p&gt;

&lt;p&gt;      AUTOCOMMIT = true (commit whenever RAM is full)&lt;/p&gt;

&lt;p&gt;        old&lt;br/&gt;
          20000 docs in 200.3 secs&lt;br/&gt;
          index size = 358M&lt;/p&gt;

&lt;p&gt;        new&lt;br/&gt;
          20000 docs in 126.0 secs&lt;br/&gt;
          index size = 356M&lt;/p&gt;

&lt;p&gt;        Total Docs/sec:             old    99.8; new   158.7 [   59.0% faster]&lt;br/&gt;
        Docs/MB @ flush:            old    24.2; new    49.1 [  102.5% more]&lt;br/&gt;
        Avg RAM used (MB) @ flush:  old    74.5; new    36.2 [   51.4% less]&lt;/p&gt;


&lt;p&gt;      AUTOCOMMIT = false (commit only once at the end)&lt;/p&gt;

&lt;p&gt;        old&lt;br/&gt;
          20000 docs in 202.7 secs&lt;br/&gt;
          index size = 358M&lt;/p&gt;

&lt;p&gt;        new&lt;br/&gt;
          20000 docs in 120.0 secs&lt;br/&gt;
          index size = 354M&lt;/p&gt;

&lt;p&gt;        Total Docs/sec:             old    98.7; new   166.7 [   69.0% faster]&lt;br/&gt;
        Docs/MB @ flush:            old    24.2; new    48.9 [  101.7% more]&lt;br/&gt;
        Avg RAM used (MB) @ flush:  old    74.3; new    37.0 [   50.2% less]&lt;/p&gt;



&lt;p&gt;    With term vectors (positions + offsets) and 2 small stored fields&lt;/p&gt;

&lt;p&gt;      AUTOCOMMIT = true (commit whenever RAM is full)&lt;/p&gt;

&lt;p&gt;        old&lt;br/&gt;
          20000 docs in 374.7 secs&lt;br/&gt;
          index size = 1.4G&lt;/p&gt;

&lt;p&gt;        new&lt;br/&gt;
          20000 docs in 236.1 secs&lt;br/&gt;
          index size = 1.4G&lt;/p&gt;

&lt;p&gt;        Total Docs/sec:             old    53.4; new    84.7 [   58.7% faster]&lt;br/&gt;
        Docs/MB @ flush:            old    10.2; new    49.1 [  382.8% more]&lt;br/&gt;
        Avg RAM used (MB) @ flush:  old   129.3; new    36.6 [   71.7% less]&lt;/p&gt;


&lt;p&gt;      AUTOCOMMIT = false (commit only once at the end)&lt;/p&gt;

&lt;p&gt;        old&lt;br/&gt;
          20000 docs in 385.7 secs&lt;br/&gt;
          index size = 1.4G&lt;/p&gt;

&lt;p&gt;        new&lt;br/&gt;
          20000 docs in 182.8 secs&lt;br/&gt;
          index size = 1.4G&lt;/p&gt;

&lt;p&gt;        Total Docs/sec:             old    51.9; new   109.4 [  111.0% faster]&lt;br/&gt;
        Docs/MB @ flush:            old    10.2; new    48.9 [  380.9% more]&lt;br/&gt;
        Avg RAM used (MB) @ flush:  old    76.0; new    37.3 [   50.9% less]&lt;/p&gt;
</comment>
                    <comment id="12486334" author="mikemccand" created="Tue, 3 Apr 2007 13:15:31 +0100"  >&lt;p&gt;Here are the results for &quot;normal&quot; sized docs (1K tokens = ~5,500 bytes plain text each):&lt;/p&gt;

&lt;p&gt;  200000 DOCS @ ~5,500 bytes plain text&lt;br/&gt;
  RAM = 32 MB&lt;br/&gt;
  NUM THREADS = 1&lt;br/&gt;
  MERGE FACTOR = 10&lt;/p&gt;


&lt;p&gt;    No term vectors nor stored fields&lt;/p&gt;

&lt;p&gt;      AUTOCOMMIT = true (commit whenever RAM is full)&lt;/p&gt;

&lt;p&gt;        old&lt;br/&gt;
          200000 docs in 397.6 secs&lt;br/&gt;
          index size = 415M&lt;/p&gt;

&lt;p&gt;        new&lt;br/&gt;
          200000 docs in 167.5 secs&lt;br/&gt;
          index size = 411M&lt;/p&gt;

&lt;p&gt;        Total Docs/sec:             old   503.1; new  1194.1 [  137.3% faster]&lt;br/&gt;
        Docs/MB @ flush:            old    81.6; new   406.2 [  397.6% more]&lt;br/&gt;
        Avg RAM used (MB) @ flush:  old    87.3; new    35.2 [   59.7% less]&lt;/p&gt;


&lt;p&gt;      AUTOCOMMIT = false (commit only once at the end)&lt;/p&gt;

&lt;p&gt;        old&lt;br/&gt;
          200000 docs in 394.6 secs&lt;br/&gt;
          index size = 415M&lt;/p&gt;

&lt;p&gt;        new&lt;br/&gt;
          200000 docs in 168.4 secs&lt;br/&gt;
          index size = 408M&lt;/p&gt;

&lt;p&gt;        Total Docs/sec:             old   506.9; new  1187.7 [  134.3% faster]&lt;br/&gt;
        Docs/MB @ flush:            old    81.6; new   432.2 [  429.4% more]&lt;br/&gt;
        Avg RAM used (MB) @ flush:  old   126.6; new    36.9 [   70.8% less]&lt;/p&gt;



&lt;p&gt;    With term vectors (positions + offsets) and 2 small stored fields&lt;/p&gt;

&lt;p&gt;      AUTOCOMMIT = true (commit whenever RAM is full)&lt;/p&gt;

&lt;p&gt;        old&lt;br/&gt;
          200000 docs in 754.2 secs&lt;br/&gt;
          index size = 1.7G&lt;/p&gt;

&lt;p&gt;        new&lt;br/&gt;
          200000 docs in 304.9 secs&lt;br/&gt;
          index size = 1.7G&lt;/p&gt;

&lt;p&gt;        Total Docs/sec:             old   265.2; new   656.0 [  147.4% faster]&lt;br/&gt;
        Docs/MB @ flush:            old    46.7; new   406.2 [  769.6% more]&lt;br/&gt;
        Avg RAM used (MB) @ flush:  old    92.9; new    35.2 [   62.1% less]&lt;/p&gt;


&lt;p&gt;      AUTOCOMMIT = false (commit only once at the end)&lt;/p&gt;

&lt;p&gt;        old&lt;br/&gt;
          200000 docs in 743.9 secs&lt;br/&gt;
          index size = 1.7G&lt;/p&gt;

&lt;p&gt;        new&lt;br/&gt;
          200000 docs in 244.3 secs&lt;br/&gt;
          index size = 1.7G&lt;/p&gt;

&lt;p&gt;        Total Docs/sec:             old   268.9; new   818.7 [  204.5% faster]&lt;br/&gt;
        Docs/MB @ flush:            old    46.7; new   432.2 [  825.2% more]&lt;br/&gt;
        Avg RAM used (MB) @ flush:  old    93.0; new    36.6 [   60.6% less]&lt;/p&gt;


</comment>
                    <comment id="12486335" author="mikemccand" created="Tue, 3 Apr 2007 13:16:20 +0100"  >
&lt;p&gt;Last is the results for small docs (100 tokens = ~550 bytes plain text each):&lt;/p&gt;

&lt;p&gt;  2000000 DOCS @ ~550 bytes plain text&lt;br/&gt;
  RAM = 32 MB&lt;br/&gt;
  NUM THREADS = 1&lt;br/&gt;
  MERGE FACTOR = 10&lt;/p&gt;


&lt;p&gt;    No term vectors nor stored fields&lt;/p&gt;

&lt;p&gt;      AUTOCOMMIT = true (commit whenever RAM is full)&lt;/p&gt;

&lt;p&gt;        old&lt;br/&gt;
          2000000 docs in 886.7 secs&lt;br/&gt;
          index size = 438M&lt;/p&gt;

&lt;p&gt;        new&lt;br/&gt;
          2000000 docs in 230.5 secs&lt;br/&gt;
          index size = 435M&lt;/p&gt;

&lt;p&gt;        Total Docs/sec:             old  2255.6; new  8676.4 [  284.7% faster]&lt;br/&gt;
        Docs/MB @ flush:            old   128.0; new  4194.6 [ 3176.2% more]&lt;br/&gt;
        Avg RAM used (MB) @ flush:  old   107.3; new    37.7 [   64.9% less]&lt;/p&gt;


&lt;p&gt;      AUTOCOMMIT = false (commit only once at the end)&lt;/p&gt;

&lt;p&gt;        old&lt;br/&gt;
          2000000 docs in 888.7 secs&lt;br/&gt;
          index size = 438M&lt;/p&gt;

&lt;p&gt;        new&lt;br/&gt;
          2000000 docs in 239.6 secs&lt;br/&gt;
          index size = 432M&lt;/p&gt;

&lt;p&gt;        Total Docs/sec:             old  2250.5; new  8348.7 [  271.0% faster]&lt;br/&gt;
        Docs/MB @ flush:            old   128.0; new  4146.8 [ 3138.9% more]&lt;br/&gt;
        Avg RAM used (MB) @ flush:  old   108.1; new    38.9 [   64.0% less]&lt;/p&gt;



&lt;p&gt;    With term vectors (positions + offsets) and 2 small stored fields&lt;/p&gt;

&lt;p&gt;      AUTOCOMMIT = true (commit whenever RAM is full)&lt;/p&gt;

&lt;p&gt;        old&lt;br/&gt;
          2000000 docs in 1480.1 secs&lt;br/&gt;
          index size = 2.1G&lt;/p&gt;

&lt;p&gt;        new&lt;br/&gt;
          2000000 docs in 462.0 secs&lt;br/&gt;
          index size = 2.1G&lt;/p&gt;

&lt;p&gt;        Total Docs/sec:             old  1351.2; new  4329.3 [  220.4% faster]&lt;br/&gt;
        Docs/MB @ flush:            old    93.1; new  4194.6 [ 4405.7% more]&lt;br/&gt;
        Avg RAM used (MB) @ flush:  old   296.4; new    38.3 [   87.1% less]&lt;/p&gt;


&lt;p&gt;      AUTOCOMMIT = false (commit only once at the end)&lt;/p&gt;

&lt;p&gt;        old&lt;br/&gt;
          2000000 docs in 1489.4 secs&lt;br/&gt;
          index size = 2.1G&lt;/p&gt;

&lt;p&gt;        new&lt;br/&gt;
          2000000 docs in 347.9 secs&lt;br/&gt;
          index size = 2.1G&lt;/p&gt;

&lt;p&gt;        Total Docs/sec:             old  1342.8; new  5749.4 [  328.2% faster]&lt;br/&gt;
        Docs/MB @ flush:            old    93.1; new  4146.8 [ 4354.5% more]&lt;br/&gt;
        Avg RAM used (MB) @ flush:  old   297.1; new    38.6 [   87.0% less]&lt;/p&gt;



&lt;p&gt;  200000 DOCS @ ~5,500 bytes plain text&lt;/p&gt;


&lt;p&gt;    No term vectors nor stored fields&lt;/p&gt;

&lt;p&gt;      AUTOCOMMIT = true (commit whenever RAM is full)&lt;/p&gt;

&lt;p&gt;        old&lt;br/&gt;
          200000 docs in 397.6 secs&lt;br/&gt;
          index size = 415M&lt;/p&gt;

&lt;p&gt;        new&lt;br/&gt;
          200000 docs in 167.5 secs&lt;br/&gt;
          index size = 411M&lt;/p&gt;

&lt;p&gt;        Total Docs/sec:             old   503.1; new  1194.1 [  137.3% faster]&lt;br/&gt;
        Docs/MB @ flush:            old    81.6; new   406.2 [  397.6% more]&lt;br/&gt;
        Avg RAM used (MB) @ flush:  old    87.3; new    35.2 [   59.7% less]&lt;/p&gt;


&lt;p&gt;      AUTOCOMMIT = false (commit only once at the end)&lt;/p&gt;

&lt;p&gt;        old&lt;br/&gt;
          200000 docs in 394.6 secs&lt;br/&gt;
          index size = 415M&lt;/p&gt;

&lt;p&gt;        new&lt;br/&gt;
          200000 docs in 168.4 secs&lt;br/&gt;
          index size = 408M&lt;/p&gt;

&lt;p&gt;        Total Docs/sec:             old   506.9; new  1187.7 [  134.3% faster]&lt;br/&gt;
        Docs/MB @ flush:            old    81.6; new   432.2 [  429.4% more]&lt;br/&gt;
        Avg RAM used (MB) @ flush:  old   126.6; new    36.9 [   70.8% less]&lt;/p&gt;



&lt;p&gt;    With term vectors (positions + offsets) and 2 small stored fields&lt;/p&gt;

&lt;p&gt;      AUTOCOMMIT = true (commit whenever RAM is full)&lt;/p&gt;

&lt;p&gt;        old&lt;br/&gt;
          200000 docs in 754.2 secs&lt;br/&gt;
          index size = 1.7G&lt;/p&gt;

&lt;p&gt;        new&lt;br/&gt;
          200000 docs in 304.9 secs&lt;br/&gt;
          index size = 1.7G&lt;/p&gt;

&lt;p&gt;        Total Docs/sec:             old   265.2; new   656.0 [  147.4% faster]&lt;br/&gt;
        Docs/MB @ flush:            old    46.7; new   406.2 [  769.6% more]&lt;br/&gt;
        Avg RAM used (MB) @ flush:  old    92.9; new    35.2 [   62.1% less]&lt;/p&gt;


&lt;p&gt;      AUTOCOMMIT = false (commit only once at the end)&lt;/p&gt;

&lt;p&gt;        old&lt;br/&gt;
          200000 docs in 743.9 secs&lt;br/&gt;
          index size = 1.7G&lt;/p&gt;

&lt;p&gt;        new&lt;br/&gt;
          200000 docs in 244.3 secs&lt;br/&gt;
          index size = 1.7G&lt;/p&gt;

&lt;p&gt;        Total Docs/sec:             old   268.9; new   818.7 [  204.5% faster]&lt;br/&gt;
        Docs/MB @ flush:            old    46.7; new   432.2 [  825.2% more]&lt;br/&gt;
        Avg RAM used (MB) @ flush:  old    93.0; new    36.6 [   60.6% less]&lt;/p&gt;


</comment>
                    <comment id="12486339" author="mikemccand" created="Tue, 3 Apr 2007 13:21:06 +0100"  >&lt;p&gt;A few notes from these results:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;A real Lucene app won&apos;t see these gains because frequently the&lt;br/&gt;
    retrieval of docs from the content source, and the tokenization,&lt;br/&gt;
    take substantial amounts of time whereas for this test I&apos;ve&lt;br/&gt;
    intentionally minimized the cost of those steps but they are very&lt;br/&gt;
    low for this test because I&apos;m 1) pulling one line at a time from a&lt;br/&gt;
    big text file, and 2) using my simplistic SimpleSpaceAnalyzer&lt;br/&gt;
    which just breaks tokens at the space character.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Best speedup is ~4.3X faster, for tiny docs (~550 bytes) with term&lt;br/&gt;
    vectors and stored fields enabled and using autoCommit=false.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Least speedup is still ~1.6X faster, for large docs (~55,000&lt;br/&gt;
    bytes) with autoCommit=true.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;The autoCommit=false cases are a little unfair to the new patch&lt;br/&gt;
    because with the new patch, you get a single-segment (optimized)&lt;br/&gt;
    index in the end, but with existing Lucene trunk, you don&apos;t.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;With term vectors and/or stored fields, autoCommit=false is quite&lt;br/&gt;
    a bit faster with the patch, because we never pay the price to&lt;br/&gt;
    merge them since they are written once.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;With term vectors and/or stored fields, the new patch has&lt;br/&gt;
    substantially better RAM efficiency.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;The patch is especially faster and has better RAM efficiency with&lt;br/&gt;
    smaller documents.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;The actual HEAP RAM usage is quite a bit more stable with the&lt;br/&gt;
    patch, especially with term vectors &amp;amp; stored fields enabled.  I&lt;br/&gt;
    think this is because the patch creates far less garbage for GC to&lt;br/&gt;
    periodically reclaim.  I think this also means you could push your&lt;br/&gt;
    RAM buffer size even higher to get better performance.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="12486373" author="creamyg" created="Tue, 3 Apr 2007 15:23:33 +0100"  >&lt;p&gt;&amp;gt; The actual HEAP RAM usage is quite a bit more &lt;br/&gt;
&amp;gt; stable with the  patch, especially with term vectors &lt;br/&gt;
&amp;gt; &amp;amp; stored fields enabled. I think this is because the &lt;br/&gt;
&amp;gt; patch creates far less garbage for GC to periodically &lt;br/&gt;
&amp;gt; reclaim. I think this also means you could push your &lt;br/&gt;
&amp;gt; RAM buffer size even higher to get better performance. &lt;/p&gt;

&lt;p&gt;For KinoSearch, the sweet spot seems to be a buffer of around 16 MB when benchmarking with the Reuters corpus on my G4 laptop. Larger than that and things actually slow down, unless the buffer is large enough that it never needs flushing. My hypothesis is that RAM fragmentation is slowing down malloc/free.  I&apos;ll be interested as to whether you see the same effect.&lt;/p&gt;</comment>
                    <comment id="12486385" author="mikemccand" created="Tue, 3 Apr 2007 16:33:58 +0100"  >
&lt;p&gt;&amp;gt;&amp;gt; The actual HEAP RAM usage is quite a bit more&lt;br/&gt;
&amp;gt;&amp;gt; stable with the patch, especially with term vectors&lt;br/&gt;
&amp;gt;&amp;gt; &amp;amp; stored fields enabled. I think this is because the&lt;br/&gt;
&amp;gt;&amp;gt; patch creates far less garbage for GC to periodically&lt;br/&gt;
&amp;gt;&amp;gt; reclaim. I think this also means you could push your&lt;br/&gt;
&amp;gt;&amp;gt; RAM buffer size even higher to get better performance.&lt;br/&gt;
&amp;gt;&lt;br/&gt;
&amp;gt; For KinoSearch, the sweet spot seems to be a buffer of around 16 MB&lt;br/&gt;
&amp;gt; when benchmarking with the Reuters corpus on my G4 laptop. Larger&lt;br/&gt;
&amp;gt; than that and things actually slow down, unless the buffer is large&lt;br/&gt;
&amp;gt; enough that it never needs flushing. My hypothesis is that RAM&lt;br/&gt;
&amp;gt; fragmentation is slowing down malloc/free. I&apos;ll be interested as to&lt;br/&gt;
&amp;gt; whether you see the same effect.&lt;/p&gt;

&lt;p&gt;Interesting.  OK I will run the benchmark across increasing RAM sizes&lt;br/&gt;
to see where the sweet spot seems to be!&lt;/p&gt;</comment>
                    <comment id="12486942" author="mikemccand" created="Thu, 5 Apr 2007 14:21:55 +0100"  >
&lt;p&gt;OK I ran old (trunk) vs new (this patch) with increasing RAM buffer&lt;br/&gt;
sizes up to 96 MB.&lt;/p&gt;

&lt;p&gt;I used the &quot;normal&quot; sized docs (~5,500 bytes plain text), left stored&lt;br/&gt;
fields and term vectors (positions + offsets) on, and&lt;br/&gt;
autoCommit=false.&lt;/p&gt;

&lt;p&gt;Here&apos;re the results:&lt;/p&gt;

&lt;p&gt;NUM THREADS = 1&lt;br/&gt;
MERGE FACTOR = 10&lt;br/&gt;
With term vectors (positions + offsets) and 2 small stored fields&lt;br/&gt;
AUTOCOMMIT = false (commit only once at the end)&lt;/p&gt;


&lt;p&gt;1 MB&lt;/p&gt;

&lt;p&gt;  old&lt;br/&gt;
    200000 docs in 862.2 secs&lt;br/&gt;
    index size = 1.7G&lt;/p&gt;

&lt;p&gt;  new&lt;br/&gt;
    200000 docs in 297.1 secs&lt;br/&gt;
    index size = 1.7G&lt;/p&gt;

&lt;p&gt;  Total Docs/sec:             old   232.0; new   673.2 [  190.2% faster]&lt;br/&gt;
  Docs/MB @ flush:            old    47.2; new   278.4 [  489.6% more]&lt;br/&gt;
  Avg RAM used (MB) @ flush:  old    34.5; new     3.4 [   90.1% less]&lt;/p&gt;



&lt;p&gt;2 MB&lt;/p&gt;

&lt;p&gt;  old&lt;br/&gt;
    200000 docs in 828.7 secs&lt;br/&gt;
    index size = 1.7G&lt;/p&gt;

&lt;p&gt;  new&lt;br/&gt;
    200000 docs in 279.0 secs&lt;br/&gt;
    index size = 1.7G&lt;/p&gt;

&lt;p&gt;  Total Docs/sec:             old   241.3; new   716.8 [  197.0% faster]&lt;br/&gt;
  Docs/MB @ flush:            old    47.0; new   322.4 [  586.7% more]&lt;br/&gt;
  Avg RAM used (MB) @ flush:  old    37.9; new     4.5 [   88.0% less]&lt;/p&gt;



&lt;p&gt;4 MB&lt;/p&gt;

&lt;p&gt;  old&lt;br/&gt;
    200000 docs in 840.5 secs&lt;br/&gt;
    index size = 1.7G&lt;/p&gt;

&lt;p&gt;  new&lt;br/&gt;
    200000 docs in 260.8 secs&lt;br/&gt;
    index size = 1.7G&lt;/p&gt;

&lt;p&gt;  Total Docs/sec:             old   237.9; new   767.0 [  222.3% faster]&lt;br/&gt;
  Docs/MB @ flush:            old    46.8; new   363.1 [  675.4% more]&lt;br/&gt;
  Avg RAM used (MB) @ flush:  old    33.9; new     6.5 [   80.9% less]&lt;/p&gt;



&lt;p&gt;8 MB&lt;/p&gt;

&lt;p&gt;  old&lt;br/&gt;
    200000 docs in 678.8 secs&lt;br/&gt;
    index size = 1.7G&lt;/p&gt;

&lt;p&gt;  new&lt;br/&gt;
    200000 docs in 248.8 secs&lt;br/&gt;
    index size = 1.7G&lt;/p&gt;

&lt;p&gt;  Total Docs/sec:             old   294.6; new   803.7 [  172.8% faster]&lt;br/&gt;
  Docs/MB @ flush:            old    46.8; new   392.4 [  739.1% more]&lt;br/&gt;
  Avg RAM used (MB) @ flush:  old    60.3; new    10.7 [   82.2% less]&lt;/p&gt;



&lt;p&gt;16 MB&lt;/p&gt;

&lt;p&gt;  old&lt;br/&gt;
    200000 docs in 660.6 secs&lt;br/&gt;
    index size = 1.7G&lt;/p&gt;

&lt;p&gt;  new&lt;br/&gt;
    200000 docs in 247.3 secs&lt;br/&gt;
    index size = 1.7G&lt;/p&gt;

&lt;p&gt;  Total Docs/sec:             old   302.8; new   808.7 [  167.1% faster]&lt;br/&gt;
  Docs/MB @ flush:            old    46.7; new   415.4 [  788.8% more]&lt;br/&gt;
  Avg RAM used (MB) @ flush:  old    47.1; new    19.2 [   59.3% less]&lt;/p&gt;



&lt;p&gt;24 MB&lt;/p&gt;

&lt;p&gt;  old&lt;br/&gt;
    200000 docs in 658.1 secs&lt;br/&gt;
    index size = 1.7G&lt;/p&gt;

&lt;p&gt;  new&lt;br/&gt;
    200000 docs in 243.0 secs&lt;br/&gt;
    index size = 1.7G&lt;/p&gt;

&lt;p&gt;  Total Docs/sec:             old   303.9; new   823.0 [  170.8% faster]&lt;br/&gt;
  Docs/MB @ flush:            old    46.7; new   430.9 [  822.2% more]&lt;br/&gt;
  Avg RAM used (MB) @ flush:  old    70.0; new    27.5 [   60.8% less]&lt;/p&gt;



&lt;p&gt;32 MB&lt;/p&gt;

&lt;p&gt;  old&lt;br/&gt;
    200000 docs in 714.2 secs&lt;br/&gt;
    index size = 1.7G&lt;/p&gt;

&lt;p&gt;  new&lt;br/&gt;
    200000 docs in 239.2 secs&lt;br/&gt;
    index size = 1.7G&lt;/p&gt;

&lt;p&gt;  Total Docs/sec:             old   280.0; new   836.0 [  198.5% faster]&lt;br/&gt;
  Docs/MB @ flush:            old    46.7; new   432.2 [  825.2% more]&lt;br/&gt;
  Avg RAM used (MB) @ flush:  old    92.5; new    36.7 [   60.3% less]&lt;/p&gt;



&lt;p&gt;48 MB&lt;/p&gt;

&lt;p&gt;  old&lt;br/&gt;
    200000 docs in 640.3 secs&lt;br/&gt;
    index size = 1.7G&lt;/p&gt;

&lt;p&gt;  new&lt;br/&gt;
    200000 docs in 236.0 secs&lt;br/&gt;
    index size = 1.7G&lt;/p&gt;

&lt;p&gt;  Total Docs/sec:             old   312.4; new   847.5 [  171.3% faster]&lt;br/&gt;
  Docs/MB @ flush:            old    46.7; new   438.5 [  838.8% more]&lt;br/&gt;
  Avg RAM used (MB) @ flush:  old   138.9; new    52.8 [   62.0% less]&lt;/p&gt;



&lt;p&gt;64 MB&lt;/p&gt;

&lt;p&gt;  old&lt;br/&gt;
    200000 docs in 649.3 secs&lt;br/&gt;
    index size = 1.7G&lt;/p&gt;

&lt;p&gt;  new&lt;br/&gt;
    200000 docs in 238.3 secs&lt;br/&gt;
    index size = 1.7G&lt;/p&gt;

&lt;p&gt;  Total Docs/sec:             old   308.0; new   839.3 [  172.5% faster]&lt;br/&gt;
  Docs/MB @ flush:            old    46.7; new   441.3 [  844.7% more]&lt;br/&gt;
  Avg RAM used (MB) @ flush:  old   302.6; new    72.7 [   76.0% less]&lt;/p&gt;



&lt;p&gt;80 MB&lt;/p&gt;

&lt;p&gt;  old&lt;br/&gt;
    200000 docs in 670.2 secs&lt;br/&gt;
    index size = 1.7G&lt;/p&gt;

&lt;p&gt;  new&lt;br/&gt;
    200000 docs in 227.2 secs&lt;br/&gt;
    index size = 1.7G&lt;/p&gt;

&lt;p&gt;  Total Docs/sec:             old   298.4; new   880.5 [  195.0% faster]&lt;br/&gt;
  Docs/MB @ flush:            old    46.7; new   446.2 [  855.2% more]&lt;br/&gt;
  Avg RAM used (MB) @ flush:  old   231.7; new    94.3 [   59.3% less]&lt;/p&gt;



&lt;p&gt;96 MB&lt;/p&gt;

&lt;p&gt;  old&lt;br/&gt;
    200000 docs in 683.4 secs&lt;br/&gt;
    index size = 1.7G&lt;/p&gt;

&lt;p&gt;  new&lt;br/&gt;
    200000 docs in 226.8 secs&lt;br/&gt;
    index size = 1.7G&lt;/p&gt;

&lt;p&gt;  Total Docs/sec:             old   292.7; new   882.0 [  201.4% faster]&lt;br/&gt;
  Docs/MB @ flush:            old    46.7; new   448.0 [  859.1% more]&lt;br/&gt;
  Avg RAM used (MB) @ flush:  old   274.5; new   112.7 [   59.0% less]&lt;/p&gt;


&lt;p&gt;Some observations:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;Remember the test is already biased against &quot;new&quot; because with the&lt;br/&gt;
    patch you get an optimized index in the end but with &quot;old&quot; you&lt;br/&gt;
    don&apos;t.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Sweet spot for old (trunk) seems to be 48 MB: that is the peak&lt;br/&gt;
    docs/sec @ 312.4.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;New (with patch) seems to just get faster the more memory you give&lt;br/&gt;
    it, though gradually.  The peak was 96 MB (the largest I ran).  So&lt;br/&gt;
    no sweet spot (or maybe I need to give more memory, but, above 96&lt;br/&gt;
    MB the trunk was starting to swap on my test env).&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;New gets better and better RAM efficiency, the more RAM you give.&lt;br/&gt;
    This makes sense: it&apos;s better able to compress the terms dict, the&lt;br/&gt;
    more docs are merged in RAM before having to flush to disk.  I&lt;br/&gt;
    would also expect this curve to be somewhat content dependent.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="12492650" author="mikemccand" created="Mon, 30 Apr 2007 11:39:19 +0100"  >&lt;p&gt;I attached a new iteration of the patch.  It&apos;s quite different from&lt;br/&gt;
the last patch.&lt;/p&gt;

&lt;p&gt;After discussion on java-dev last time, I decided to retry the&lt;br/&gt;
&quot;persistent hash&quot; approach, where the Postings hash lasts across many&lt;br/&gt;
docs and then a single flush produces a partial segment containing all&lt;br/&gt;
of those docs.  This is in contrast to the previous approach where&lt;br/&gt;
each doc makes its own segment and then they are merged.&lt;/p&gt;

&lt;p&gt;It turns out this is even faster than my previous approach, especially&lt;br/&gt;
for smaller docs and especially when term vectors are off (because no&lt;br/&gt;
quicksort() is needed until the segment is flushed).  I will attach&lt;br/&gt;
new benchmark results.&lt;/p&gt;

&lt;p&gt;Other changes:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;Changed my benchmarking tool / testing (IndexLineFiles):&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;I turned off compound file (to reduce time NOT spent on&lt;br/&gt;
      indexing).&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;I noticed I was not downcasing the terms, so I fixed that&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;I now do my own line processing to reduce GC cost of&lt;br/&gt;
      &quot;BufferedReader.readLine&quot; (to reduct time NOT spent on&lt;br/&gt;
      indexing).&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Norms now properly flush to disk in the autoCommit=false case&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;All unit tests pass except disk full&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;I turned on asserts for unit tests (jvm arg -ea added to junit ant&lt;br/&gt;
    task).  I think we should use asserts when running tests.  I have&lt;br/&gt;
    quite a few asserts now.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;With this new approach, as I process each term in the document I&lt;br/&gt;
immediately write the prox/freq in their compact (vints) format into&lt;br/&gt;
shared byte[] buffers, rather than accumulating int[] arrays that then&lt;br/&gt;
need to be re-processed into the vint encoding.  This speeds things up&lt;br/&gt;
because we don&apos;t double-process the postings.  It also uses less&lt;br/&gt;
per-document RAM overhead because intermediate postings are stored as&lt;br/&gt;
vints not as ints.&lt;/p&gt;

&lt;p&gt;When enough RAM is used by the Posting entries plus the byte[]&lt;br/&gt;
buffers, I flush them to a partial RAM segment.  When enough of these&lt;br/&gt;
RAM segments have accumulated I flush to a real Lucene segment&lt;br/&gt;
(autoCommit=true) or to on-disk partial segments (autoCommit=false)&lt;br/&gt;
which are then merged in the end to create a real Lucene segment.&lt;/p&gt;</comment>
                    <comment id="12492655" author="creamyg" created="Mon, 30 Apr 2007 12:44:28 +0100"  >&lt;p&gt;How are you writing the frq data in compressed format?  The  works fine for&lt;br/&gt;
prx data, because the deltas are all within a single doc &amp;#8211; but for  the freq&lt;br/&gt;
data, the deltas are tied up in doc num deltas, so you have to decompress it&lt;br/&gt;
when performing merges.  &lt;/p&gt;

&lt;p&gt;To continue our discussion from java-dev... &lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;I haven&apos;t been able to come up with a file format tweak that&lt;br/&gt;
   gets around this doc-num-delta-decompression problem to enhance the speed&lt;br/&gt;
   of frq data merging. I toyed with splitting off the freq from the&lt;br/&gt;
   doc_delta, at the price of increasing the file size in the common case of&lt;br/&gt;
   freq == 1, but went back to the old design.  It&apos;s not worth the size&lt;br/&gt;
   increase for what&apos;s at best a minor indexing speedup.&lt;/li&gt;
	&lt;li&gt;I&apos;ve added a custom MemoryPool class to KS which grabs memory in 1 meg&lt;br/&gt;
   chunks, allows resizing (downwards) of only the last allocation, and can&lt;br/&gt;
   only release everything at once.  From one of these pools, I&apos;m allocating&lt;br/&gt;
   RawPosting objects, each of which is a doc_num, a freq, the term_text, and&lt;br/&gt;
   the pre-packed prx data (which varies based on which Posting subclass&lt;br/&gt;
   created the RawPosting object).  I haven&apos;t got things 100% stable yet, but&lt;br/&gt;
   preliminary results seem to indicate that this technique, which is a riff&lt;br/&gt;
   on your persistent arrays, improves indexing speed by about 15%.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="12492658" author="mikemccand" created="Mon, 30 Apr 2007 13:14:37 +0100"  >
&lt;p&gt;&amp;gt; How are you writing the frq data in compressed format? The works fine for&lt;br/&gt;
&amp;gt; prx data, because the deltas are all within a single doc &amp;#8211; but for the freq&lt;br/&gt;
&amp;gt; data, the deltas are tied up in doc num deltas, so you have to decompress it&lt;br/&gt;
&amp;gt; when performing merges.&lt;/p&gt;

&lt;p&gt;For each Posting I keep track of the last docID that its term occurred&lt;br/&gt;
in; when this differs from the current docID I record the &quot;delta code&quot;&lt;br/&gt;
that needs to be written and then I later write it with the final freq&lt;br/&gt;
for this document.&lt;/p&gt;

&lt;p&gt;&amp;gt; * I haven&apos;t been able to come up with a file format tweak that&lt;br/&gt;
&amp;gt;   gets around this doc-num-delta-decompression problem to enhance the speed&lt;br/&gt;
&amp;gt;   of frq data merging. I toyed with splitting off the freq from the&lt;br/&gt;
&amp;gt;   doc_delta, at the price of increasing the file size in the common case of&lt;br/&gt;
&amp;gt;   freq == 1, but went back to the old design. It&apos;s not worth the size&lt;br/&gt;
&amp;gt;   increase for what&apos;s at best a minor indexing speedup.&lt;/p&gt;

&lt;p&gt;I&apos;m just doing the &quot;stitching&quot; approach here: it&apos;s only the very first&lt;br/&gt;
docCode (&amp;amp; freq when freq==1) that must be re-encoded on merging.  The&lt;br/&gt;
one catch is you must store the last docID of the previous segment so&lt;br/&gt;
you can compute the new delta at the boundary.  Then I do a raw&lt;br/&gt;
&quot;copyBytes&quot; for the remainder of the freq postings.&lt;/p&gt;

&lt;p&gt;Note that I&apos;m only doing this for the &quot;internal&quot; merges (of partial&lt;br/&gt;
RAM segments and flushed partial segments) I do before creating a real&lt;br/&gt;
Lucene segment.  I haven&apos;t changed how the &quot;normal&quot; Lucene segment&lt;br/&gt;
merging works (though I think we should look into it &amp;#8211; I opened a&lt;br/&gt;
separate issue): it still re-interprets and then re-encodes all&lt;br/&gt;
docID/freq&apos;s.&lt;/p&gt;

&lt;p&gt;&amp;gt; * I&apos;ve added a custom MemoryPool class to KS which grabs memory in 1 meg&lt;br/&gt;
&amp;gt;   chunks, allows resizing (downwards) of only the last allocation, and can&lt;br/&gt;
&amp;gt;   only release everything at once. From one of these pools, I&apos;m allocating&lt;br/&gt;
&amp;gt;    RawPosting objects, each of which is a doc_num, a freq, the term_text, and&lt;br/&gt;
&amp;gt;   the pre-packed prx data (which varies based on which Posting subclass&lt;br/&gt;
&amp;gt;   created the RawPosting object). I haven&apos;t got things 100% stable yet, but&lt;br/&gt;
&amp;gt;   preliminary results seem to indicate that this technique, which is a riff&lt;br/&gt;
&amp;gt;   on your persistent arrays, improves indexing speed by about 15%.&lt;/p&gt;

&lt;p&gt;Fabulous!!&lt;/p&gt;

&lt;p&gt;I think it&apos;s the custom memory management I&apos;m doing with slices into&lt;br/&gt;
shared byte[] arrays for the postings that made the persistent hash&lt;br/&gt;
approach work well, this time around (when I had previously tried this&lt;br/&gt;
it was slower).&lt;/p&gt;</comment>
                    <comment id="12492668" author="mikemccand" created="Mon, 30 Apr 2007 14:49:00 +0100"  >&lt;p&gt;Results with the above patch:&lt;/p&gt;

&lt;p&gt;RAM = 32 MB&lt;br/&gt;
NUM THREADS = 1&lt;br/&gt;
MERGE FACTOR = 10&lt;/p&gt;


&lt;p&gt;  2000000 DOCS @ ~550 bytes plain text&lt;/p&gt;


&lt;p&gt;    No term vectors nor stored fields&lt;/p&gt;

&lt;p&gt;      AUTOCOMMIT = true (commit whenever RAM is full)&lt;/p&gt;

&lt;p&gt;        old&lt;br/&gt;
          2000000 docs in 782.8 secs&lt;br/&gt;
          index size = 436M&lt;/p&gt;

&lt;p&gt;        new&lt;br/&gt;
          2000000 docs in 93.4 secs&lt;br/&gt;
          index size = 430M&lt;/p&gt;

&lt;p&gt;        Total Docs/sec:             old  2554.8; new 21421.1 [  738.5% faster]&lt;br/&gt;
        Docs/MB @ flush:            old   128.0; new  4058.0 [ 3069.6% more]&lt;br/&gt;
        Avg RAM used (MB) @ flush:  old   140.2; new    38.0 [   72.9% less]&lt;/p&gt;


&lt;p&gt;      AUTOCOMMIT = false (commit only once at the end)&lt;/p&gt;

&lt;p&gt;        old&lt;br/&gt;
          2000000 docs in 780.2 secs&lt;br/&gt;
          index size = 436M&lt;/p&gt;

&lt;p&gt;        new&lt;br/&gt;
          2000000 docs in 90.6 secs&lt;br/&gt;
          index size = 427M&lt;/p&gt;

&lt;p&gt;        Total Docs/sec:             old  2563.3; new 22086.8 [  761.7% faster]&lt;br/&gt;
        Docs/MB @ flush:            old   128.0; new  4118.4 [ 3116.7% more]&lt;br/&gt;
        Avg RAM used (MB) @ flush:  old   144.6; new    36.4 [   74.8% less]&lt;/p&gt;



&lt;p&gt;    With term vectors (positions + offsets) and 2 small stored fields&lt;/p&gt;

&lt;p&gt;      AUTOCOMMIT = true (commit whenever RAM is full)&lt;/p&gt;

&lt;p&gt;        old&lt;br/&gt;
          2000000 docs in 1227.6 secs&lt;br/&gt;
          index size = 2.1G&lt;/p&gt;

&lt;p&gt;        new&lt;br/&gt;
          2000000 docs in 559.8 secs&lt;br/&gt;
          index size = 2.1G&lt;/p&gt;

&lt;p&gt;        Total Docs/sec:             old  1629.2; new  3572.5 [  119.3% faster]&lt;br/&gt;
        Docs/MB @ flush:            old    93.1; new  4058.0 [ 4259.1% more]&lt;br/&gt;
        Avg RAM used (MB) @ flush:  old   193.4; new    38.5 [   80.1% less]&lt;/p&gt;


&lt;p&gt;      AUTOCOMMIT = false (commit only once at the end)&lt;/p&gt;

&lt;p&gt;        old&lt;br/&gt;
          2000000 docs in 1229.2 secs&lt;br/&gt;
          index size = 2.1G&lt;/p&gt;

&lt;p&gt;        new&lt;br/&gt;
          2000000 docs in 241.0 secs&lt;br/&gt;
          index size = 2.1G&lt;/p&gt;

&lt;p&gt;        Total Docs/sec:             old  1627.0; new  8300.0 [  410.1% faster]&lt;br/&gt;
        Docs/MB @ flush:            old    93.1; new  4118.4 [ 4323.9% more]&lt;br/&gt;
        Avg RAM used (MB) @ flush:  old   150.5; new    38.4 [   74.5% less]&lt;/p&gt;



&lt;p&gt;  200000 DOCS @ ~5,500 bytes plain text&lt;/p&gt;


&lt;p&gt;    No term vectors nor stored fields&lt;/p&gt;

&lt;p&gt;      AUTOCOMMIT = true (commit whenever RAM is full)&lt;/p&gt;

&lt;p&gt;        old&lt;br/&gt;
          200000 docs in 352.2 secs&lt;br/&gt;
          index size = 406M&lt;/p&gt;

&lt;p&gt;        new&lt;br/&gt;
          200000 docs in 86.4 secs&lt;br/&gt;
          index size = 403M&lt;/p&gt;

&lt;p&gt;        Total Docs/sec:             old   567.9; new  2313.7 [  307.4% faster]&lt;br/&gt;
        Docs/MB @ flush:            old    83.5; new   420.0 [  402.7% more]&lt;br/&gt;
        Avg RAM used (MB) @ flush:  old    76.8; new    38.1 [   50.4% less]&lt;/p&gt;


&lt;p&gt;      AUTOCOMMIT = false (commit only once at the end)&lt;/p&gt;

&lt;p&gt;        old&lt;br/&gt;
          200000 docs in 399.2 secs&lt;br/&gt;
          index size = 406M&lt;/p&gt;

&lt;p&gt;        new&lt;br/&gt;
          200000 docs in 89.6 secs&lt;br/&gt;
          index size = 400M&lt;/p&gt;

&lt;p&gt;        Total Docs/sec:             old   501.0; new  2231.0 [  345.3% faster]&lt;br/&gt;
        Docs/MB @ flush:            old    83.5; new   422.6 [  405.8% more]&lt;br/&gt;
        Avg RAM used (MB) @ flush:  old    76.7; new    36.2 [   52.7% less]&lt;/p&gt;



&lt;p&gt;    With term vectors (positions + offsets) and 2 small stored fields&lt;/p&gt;

&lt;p&gt;      AUTOCOMMIT = true (commit whenever RAM is full)&lt;/p&gt;

&lt;p&gt;        old&lt;br/&gt;
          200000 docs in 594.2 secs&lt;br/&gt;
          index size = 1.7G&lt;/p&gt;

&lt;p&gt;        new&lt;br/&gt;
          200000 docs in 229.0 secs&lt;br/&gt;
          index size = 1.7G&lt;/p&gt;

&lt;p&gt;        Total Docs/sec:             old   336.6; new   873.3 [  159.5% faster]&lt;br/&gt;
        Docs/MB @ flush:            old    47.9; new   420.0 [  776.9% more]&lt;br/&gt;
        Avg RAM used (MB) @ flush:  old   157.8; new    38.0 [   75.9% less]&lt;/p&gt;


&lt;p&gt;      AUTOCOMMIT = false (commit only once at the end)&lt;/p&gt;

&lt;p&gt;        old&lt;br/&gt;
          200000 docs in 605.1 secs&lt;br/&gt;
          index size = 1.7G&lt;/p&gt;

&lt;p&gt;        new&lt;br/&gt;
          200000 docs in 181.3 secs&lt;br/&gt;
          index size = 1.7G&lt;/p&gt;

&lt;p&gt;        Total Docs/sec:             old   330.5; new  1103.1 [  233.7% faster]&lt;br/&gt;
        Docs/MB @ flush:            old    47.9; new   422.6 [  782.2% more]&lt;br/&gt;
        Avg RAM used (MB) @ flush:  old   132.0; new    37.1 [   71.9% less]&lt;/p&gt;



&lt;p&gt;  20000 DOCS @ ~55,000 bytes plain text&lt;/p&gt;


&lt;p&gt;    No term vectors nor stored fields&lt;/p&gt;

&lt;p&gt;      AUTOCOMMIT = true (commit whenever RAM is full)&lt;/p&gt;

&lt;p&gt;        old&lt;br/&gt;
          20000 docs in 180.8 secs&lt;br/&gt;
          index size = 350M&lt;/p&gt;

&lt;p&gt;        new&lt;br/&gt;
          20000 docs in 79.1 secs&lt;br/&gt;
          index size = 349M&lt;/p&gt;

&lt;p&gt;        Total Docs/sec:             old   110.6; new   252.8 [  128.5% faster]&lt;br/&gt;
        Docs/MB @ flush:            old    25.0; new    46.8 [   87.0% more]&lt;br/&gt;
        Avg RAM used (MB) @ flush:  old   112.2; new    44.3 [   60.5% less]&lt;/p&gt;


&lt;p&gt;      AUTOCOMMIT = false (commit only once at the end)&lt;/p&gt;

&lt;p&gt;        old&lt;br/&gt;
          20000 docs in 180.1 secs&lt;br/&gt;
          index size = 350M&lt;/p&gt;

&lt;p&gt;        new&lt;br/&gt;
          20000 docs in 75.9 secs&lt;br/&gt;
          index size = 347M&lt;/p&gt;

&lt;p&gt;        Total Docs/sec:             old   111.0; new   263.5 [  137.3% faster]&lt;br/&gt;
        Docs/MB @ flush:            old    25.0; new    47.5 [   89.7% more]&lt;br/&gt;
        Avg RAM used (MB) @ flush:  old   111.1; new    42.5 [   61.7% less]&lt;/p&gt;



&lt;p&gt;    With term vectors (positions + offsets) and 2 small stored fields&lt;/p&gt;

&lt;p&gt;      AUTOCOMMIT = true (commit whenever RAM is full)&lt;/p&gt;

&lt;p&gt;        old&lt;br/&gt;
          20000 docs in 323.1 secs&lt;br/&gt;
          index size = 1.4G&lt;/p&gt;

&lt;p&gt;        new&lt;br/&gt;
          20000 docs in 183.9 secs&lt;br/&gt;
          index size = 1.4G&lt;/p&gt;

&lt;p&gt;        Total Docs/sec:             old    61.9; new   108.7 [   75.7% faster]&lt;br/&gt;
        Docs/MB @ flush:            old    10.4; new    46.8 [  348.3% more]&lt;br/&gt;
        Avg RAM used (MB) @ flush:  old    74.2; new    44.9 [   39.5% less]&lt;/p&gt;


&lt;p&gt;      AUTOCOMMIT = false (commit only once at the end)&lt;/p&gt;

&lt;p&gt;        old&lt;br/&gt;
          20000 docs in 323.5 secs&lt;br/&gt;
          index size = 1.4G&lt;/p&gt;

&lt;p&gt;        new&lt;br/&gt;
          20000 docs in 135.6 secs&lt;br/&gt;
          index size = 1.4G&lt;/p&gt;

&lt;p&gt;        Total Docs/sec:             old    61.8; new   147.5 [  138.5% faster]&lt;br/&gt;
        Docs/MB @ flush:            old    10.4; new    47.5 [  354.8% more]&lt;br/&gt;
        Avg RAM used (MB) @ flush:  old    74.3; new    42.9 [   42.2% less]&lt;/p&gt;
</comment>
                    <comment id="12492674" author="yseeley@gmail.com" created="Mon, 30 Apr 2007 15:10:35 +0100"  >&lt;p&gt;How does this work with pending deletes?&lt;br/&gt;
I assume that if autocommit is false, then you need to wait until the end when you get a real lucene segment to delete the pending terms?&lt;/p&gt;

&lt;p&gt;Also, how has the merge policy (or index invariants) of lucene segments changed?&lt;br/&gt;
If autocommit is off, then you wait until the end to create a big lucene segment.  This new segment may be much larger than segments to it&apos;s &quot;left&quot;.  I suppose the idea of merging rightmost segments should just be dropped in favor of merging the smallest adjacent segments?  Sorry if this has already been covered... as I said, I&apos;m trying to follow along at a high level.&lt;/p&gt;</comment>
                    <comment id="12492748" author="mikemccand" created="Mon, 30 Apr 2007 19:54:27 +0100"  >&lt;p&gt;&amp;gt; How does this work with pending deletes?&lt;br/&gt;
&amp;gt; I assume that if autocommit is false, then you need to wait until the end when you get a real lucene segment to delete the pending terms?&lt;/p&gt;

&lt;p&gt;Yes, all of this sits &quot;below&quot; the pending deletes layer since this&lt;br/&gt;
change writes a single segment either when RAM is full&lt;br/&gt;
(autoCommit=true) or when writer is closed (autoCommit=false).  Then&lt;br/&gt;
the deletes get applied like normal (I haven&apos;t changed that part).&lt;/p&gt;

&lt;p&gt;&amp;gt; Also, how has the merge policy (or index invariants) of lucene segments changed?&lt;br/&gt;
&amp;gt; If autocommit is off, then you wait until the end to create a big lucene segment.  This new segment may be much larger than segments to it&apos;s &quot;left&quot;.  I suppose the idea of merging rightmost segments should just be dropped in favor of merging the smallest adjacent segments?  Sorry if this has already been covered... as I said, I&apos;m trying to follow along at a high level.&lt;/p&gt;

&lt;p&gt;Has not been covered, and as usual these are excellent questions&lt;br/&gt;
Yonik!&lt;/p&gt;

&lt;p&gt;I haven&apos;t yet changed anything about merge policy, but you&apos;re right&lt;br/&gt;
the current invariants won&apos;t hold anymore.  In fact they already don&apos;t&lt;br/&gt;
hold if you &quot;flush by RAM&quot; now (APIs are exposed in 2.1 to let you do&lt;br/&gt;
this).  So we need to do something.&lt;/p&gt;

&lt;p&gt;I like your idea to relax merge policy (&amp;amp; invariants) to allow&lt;br/&gt;
&quot;merging of any adjacent segments&quot; (not just rightmost ones) and then&lt;br/&gt;
make the policy merge the smallest ones / most similarly sized ones,&lt;br/&gt;
measuring size by net # bytes in the segment.  This would preserve the&lt;br/&gt;
&quot;docID monotonicity invariance&quot;.&lt;/p&gt;

&lt;p&gt;If we take that approach then it would automatically resolve&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-845&quot; title=&quot;If you &amp;quot;flush by RAM usage&amp;quot; then IndexWriter may over-merge&quot;&gt;&lt;del&gt;LUCENE-845&lt;/del&gt;&lt;/a&gt; as well (which would otherwise block this issue).&lt;/p&gt;</comment>
                    <comment id="12497528" author="mikemccand" created="Mon, 21 May 2007 19:14:40 +0100"  >&lt;p&gt;Attached latest patch.&lt;/p&gt;

&lt;p&gt;I&apos;m now working towards simplify &amp;amp; cleaning up the code &amp;amp; design:&lt;br/&gt;
eliminated dead code leftover from the previous iterations, use&lt;br/&gt;
existing RAMFile instead of my own new class, refactored&lt;br/&gt;
duplicate/confusing code, added comments, etc. It&apos;s getting closer to&lt;br/&gt;
a committable state but still has a ways to go.&lt;/p&gt;

&lt;p&gt;I also renamed the class from MultiDocumentWriter to DocumentsWriter.&lt;/p&gt;

&lt;p&gt;To summarize the current design:&lt;/p&gt;

&lt;p&gt;  1. Write stored fields &amp;amp; term vectors to files in the Directory&lt;br/&gt;
     immediately (don&apos;t buffer these in RAM).&lt;/p&gt;

&lt;p&gt;  2. Write freq &amp;amp; prox postings to RAM directly as a byte stream&lt;br/&gt;
     instead of first pass as int[] and then second pass as a byte&lt;br/&gt;
     stream.  This single-pass instead of double-pass is a big&lt;br/&gt;
     savings.  I use slices into shared byte[] arrays to efficiently&lt;br/&gt;
     allocate bytes to the postings the need them.&lt;/p&gt;

&lt;p&gt;  3. Build Postings hash that holds the Postings for many documents at&lt;br/&gt;
     once instead of a single doc, keyed by unique term.  Not tearing&lt;br/&gt;
     down &amp;amp; rebuilding the Postings hash w/ every doc saves alot of&lt;br/&gt;
     time.  Also when term vectors are off this saves quicksort for&lt;br/&gt;
     every doc and this gives very good performance gain.&lt;/p&gt;

&lt;p&gt;     When the Postings hash is full (used up the allowed RAM usage) I&lt;br/&gt;
     then create a real Lucene segment when autoCommit=true, else a&lt;br/&gt;
     &quot;partial segment&quot;.&lt;/p&gt;

&lt;p&gt;  4. Use my own &quot;partial segment&quot; format that differs from Lucene&apos;s&lt;br/&gt;
     normal segments in that it is optimized for merging (and unusable&lt;br/&gt;
     for searching).  This format, and the merger I created to work&lt;br/&gt;
     with this format, performs merging mostly by copying blocks of&lt;br/&gt;
     bytes instead of reinterpreting every vInt in each Postings list.&lt;br/&gt;
     These partial segments are are only created when IndexWriter has&lt;br/&gt;
     autoCommit=false, and then on commit they are merged into the&lt;br/&gt;
     real Lucene segment format.&lt;/p&gt;

&lt;p&gt;  5. Reuse the Posting, PostingVector, char[] and byte[] objects that&lt;br/&gt;
     are used by the Postings hash.&lt;/p&gt;

&lt;p&gt;I plan to keep simplifying the design &amp;amp; implementation.  Specifically,&lt;br/&gt;
I&apos;m going to test removing #4 above entirely (using my own &quot;partial&lt;br/&gt;
segment&quot; format that&apos;s optimized for merging not searching).&lt;/p&gt;

&lt;p&gt;While doing this may give back some of the performance gains, that&lt;br/&gt;
code is the source of much added complexity in the patch, and, it&lt;br/&gt;
duplicates the current SegmentMerger code.  It was more necessary&lt;br/&gt;
before (when we would merge thousands of single-doc segments in&lt;br/&gt;
memory) but now that each segment contains many docs I think we are no&lt;br/&gt;
longer gaining as much performance from it.&lt;/p&gt;

&lt;p&gt;I plan instead to write all segments in the &quot;real&quot; Lucene segment&lt;br/&gt;
format and use the current SegmentMerger, possibly w/ some small&lt;br/&gt;
changes, to do the merges even when autoCommit=false.  Since we have&lt;br/&gt;
another issue (&lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-856&quot; title=&quot;Optimize segment merging&quot;&gt;&lt;del&gt;LUCENE-856&lt;/del&gt;&lt;/a&gt;) to optimize segment merging I can carry&lt;br/&gt;
over any optimizations that we may want to keep into that issue.  If&lt;br/&gt;
this doesn&apos;t lose much performance it will make the approach here even&lt;br/&gt;
simpler.&lt;/p&gt;</comment>
                    <comment id="12502790" author="mikemccand" created="Fri, 8 Jun 2007 14:31:03 +0100"  >&lt;p&gt;Latest working patch attached.&lt;/p&gt;

&lt;p&gt;I&apos;ve cutover to using Lucene&apos;s normal segment merging for all merging&lt;br/&gt;
(ie, I no longer use a different merge-efficient format for segments&lt;br/&gt;
when autoCommit=false); this has substantially simplified the code.&lt;/p&gt;

&lt;p&gt;All unit tests pass except disk-full test and certain contrib tests&lt;br/&gt;
(gdata-server, lucli, similarity, wordnet) that I think I&apos;m not&lt;br/&gt;
causing.&lt;/p&gt;

&lt;p&gt;Other changes:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;Consolidated flushing of a new segment back into IndexWriter&lt;br/&gt;
    (previously DocumentsWriter would do its own flushing when&lt;br/&gt;
    autoCommit=false).&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    I would also like to consolidate merging entirely into&lt;br/&gt;
    IndexWriter; right now DocumentsWriter does its own merging of the&lt;br/&gt;
    flushed segments when autoCommit=false (this is because those&lt;br/&gt;
    segments are &quot;partial&quot; meaning they do not have their own stored&lt;br/&gt;
    fields or term vectors).  I&apos;m trying to find a clean way to do&lt;br/&gt;
    this...&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;Thread concurrency now works: each thread writes into a separate&lt;br/&gt;
    Postings hash (up until a limit (currently 5) at which point the&lt;br/&gt;
    threads share the Postings hashes) and then when flushing the&lt;br/&gt;
    segment I merge the docIDs together. I flush when the total RAM&lt;br/&gt;
    used across threads is over the limit.  I ran a test comparing&lt;br/&gt;
    thread concurrency on current trunk vs this patch, which I&apos;ll post&lt;br/&gt;
    next.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Reduced bytes used per-unique-term to be lower than current&lt;br/&gt;
    Lucene.  This means the worst-case document (many terms, all of&lt;br/&gt;
    which are unique) should use less RAM overall than Lucene trunk&lt;br/&gt;
    does.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Added some new unit test cases; added missing &quot;writer.close()&quot; to&lt;br/&gt;
    one of the contrib tests.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Cleanup, comments, etc.  I think the code is getting more&lt;br/&gt;
    &quot;approachable&quot; now.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="12502793" author="mikemccand" created="Fri, 8 Jun 2007 14:33:30 +0100"  >&lt;p&gt;I ran a benchmark using more than 1 thread to do indexing, in order to&lt;br/&gt;
test &amp;amp; compare concurrency of trunk and the patch.  The test is the&lt;br/&gt;
same as above, and runs on a 4 core Mac Pro (OS X) box with 4 drive&lt;br/&gt;
RAID 0 IO system.&lt;/p&gt;

&lt;p&gt;Here are the raw results:&lt;/p&gt;

&lt;p&gt;DOCS = ~5,500 bytes plain text&lt;br/&gt;
RAM = 32 MB&lt;br/&gt;
MERGE FACTOR = 10&lt;br/&gt;
With term vectors (positions + offsets) and 2 small stored fields&lt;br/&gt;
AUTOCOMMIT = false (commit only once at the end)&lt;/p&gt;

&lt;p&gt;NUM THREADS = 1&lt;/p&gt;

&lt;p&gt;        new&lt;br/&gt;
          200000 docs in 172.3 secs&lt;br/&gt;
          index size = 1.7G&lt;/p&gt;

&lt;p&gt;        old&lt;br/&gt;
          200000 docs in 539.5 secs&lt;br/&gt;
          index size = 1.7G&lt;/p&gt;

&lt;p&gt;        Total Docs/sec:             old   370.7; new  1161.0 [  213.2% faster]&lt;br/&gt;
        Docs/MB @ flush:            old    47.9; new   334.6 [  598.7% more]&lt;br/&gt;
        Avg RAM used (MB) @ flush:  old   131.9; new    33.1 [   74.9% less]&lt;/p&gt;


&lt;p&gt;NUM THREADS = 2&lt;/p&gt;

&lt;p&gt;        new&lt;br/&gt;
          200001 docs in 130.8 secs&lt;br/&gt;
          index size = 1.7G&lt;/p&gt;

&lt;p&gt;        old&lt;br/&gt;
          200001 docs in 452.8 secs&lt;br/&gt;
          index size = 1.7G&lt;/p&gt;

&lt;p&gt;        Total Docs/sec:             old   441.7; new  1529.3 [  246.2% faster]&lt;br/&gt;
        Docs/MB @ flush:            old    47.9; new   301.5 [  529.7% more]&lt;br/&gt;
        Avg RAM used (MB) @ flush:  old   226.1; new    35.2 [   84.4% less]&lt;/p&gt;


&lt;p&gt;NUM THREADS = 3&lt;/p&gt;

&lt;p&gt;        new&lt;br/&gt;
          200002 docs in 105.4 secs&lt;br/&gt;
          index size = 1.7G&lt;/p&gt;

&lt;p&gt;        old&lt;br/&gt;
          200002 docs in 428.4 secs&lt;br/&gt;
          index size = 1.7G&lt;/p&gt;

&lt;p&gt;        Total Docs/sec:             old   466.8; new  1897.9 [  306.6% faster]&lt;br/&gt;
        Docs/MB @ flush:            old    47.9; new   277.8 [  480.2% more]&lt;br/&gt;
        Avg RAM used (MB) @ flush:  old   289.8; new    37.0 [   87.2% less]&lt;/p&gt;


&lt;p&gt;NUM THREADS = 4&lt;/p&gt;

&lt;p&gt;        new&lt;br/&gt;
          200003 docs in 104.8 secs&lt;br/&gt;
          index size = 1.7G&lt;/p&gt;

&lt;p&gt;        old&lt;br/&gt;
          200003 docs in 440.4 secs&lt;br/&gt;
          index size = 1.7G&lt;/p&gt;

&lt;p&gt;        Total Docs/sec:             old   454.1; new  1908.5 [  320.3% faster]&lt;br/&gt;
        Docs/MB @ flush:            old    47.9; new   259.9 [  442.9% more]&lt;br/&gt;
        Avg RAM used (MB) @ flush:  old   293.7; new    37.1 [   87.3% less]&lt;/p&gt;


&lt;p&gt;NUM THREADS = 5&lt;/p&gt;

&lt;p&gt;        new&lt;br/&gt;
          200004 docs in 99.5 secs&lt;br/&gt;
          index size = 1.7G&lt;/p&gt;

&lt;p&gt;        old&lt;br/&gt;
          200004 docs in 425.0 secs&lt;br/&gt;
          index size = 1.7G&lt;/p&gt;

&lt;p&gt;        Total Docs/sec:             old   470.6; new  2010.5 [  327.2% faster]&lt;br/&gt;
        Docs/MB @ flush:            old    47.9; new   245.3 [  412.6% more]&lt;br/&gt;
        Avg RAM used (MB) @ flush:  old   390.9; new    38.3 [   90.2% less]&lt;/p&gt;


&lt;p&gt;NUM THREADS = 6&lt;/p&gt;

&lt;p&gt;        new&lt;br/&gt;
          200005 docs in 106.3 secs&lt;br/&gt;
          index size = 1.7G&lt;/p&gt;

&lt;p&gt;        old&lt;br/&gt;
          200005 docs in 427.1 secs&lt;br/&gt;
          index size = 1.7G&lt;/p&gt;

&lt;p&gt;        Total Docs/sec:             old   468.2; new  1882.3 [  302.0% faster]&lt;br/&gt;
        Docs/MB @ flush:            old    47.8; new   248.5 [  419.3% more]&lt;br/&gt;
        Avg RAM used (MB) @ flush:  old   340.9; new    38.7 [   88.6% less]&lt;/p&gt;


&lt;p&gt;NUM THREADS = 7&lt;/p&gt;

&lt;p&gt;        new&lt;br/&gt;
          200006 docs in 106.1 secs&lt;br/&gt;
          index size = 1.7G&lt;/p&gt;

&lt;p&gt;        old&lt;br/&gt;
          200006 docs in 435.2 secs&lt;br/&gt;
          index size = 1.7G&lt;/p&gt;

&lt;p&gt;        Total Docs/sec:             old   459.6; new  1885.3 [  310.2% faster]&lt;br/&gt;
        Docs/MB @ flush:            old    47.8; new   248.7 [  420.0% more]&lt;br/&gt;
        Avg RAM used (MB) @ flush:  old   408.6; new    39.1 [   90.4% less]&lt;/p&gt;


&lt;p&gt;NUM THREADS = 8&lt;/p&gt;

&lt;p&gt;        new&lt;br/&gt;
          200007 docs in 109.0 secs&lt;br/&gt;
          index size = 1.7G&lt;/p&gt;

&lt;p&gt;        old&lt;br/&gt;
          200007 docs in 469.2 secs&lt;br/&gt;
          index size = 1.7G&lt;/p&gt;

&lt;p&gt;        Total Docs/sec:             old   426.3; new  1835.2 [  330.5% faster]&lt;br/&gt;
        Docs/MB @ flush:            old    47.8; new   251.3 [  425.5% more]&lt;br/&gt;
        Avg RAM used (MB) @ flush:  old   448.9; new    39.0 [   91.3% less]&lt;/p&gt;



&lt;p&gt;Some quick comments:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;Both trunk &amp;amp; the patch show speedups if you use more than 1 thread&lt;br/&gt;
    to do indexing.  This is expected since the machine has concurrency. &lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;The biggest speedup is from 1-&amp;gt;2 threads but still good gains from&lt;br/&gt;
    2-&amp;gt;5 threads.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Best seems to be 5 threads.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;The patch allows better concurrency: relatively speaking it speeds&lt;br/&gt;
    up faster than the trunk (the % faster increases as we add&lt;br/&gt;
    threads) as you increase # threads.  I think this makes sense&lt;br/&gt;
    because we flush less often with the patch, and, flushing is time&lt;br/&gt;
    consuming and single threaded.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="12505346" author="mikemccand" created="Fri, 15 Jun 2007 20:00:53 +0100"  >&lt;p&gt;Attached latest patch.&lt;/p&gt;

&lt;p&gt;I think this patch is ready to commit.  I will let it sit for a while&lt;br/&gt;
so people can review it.&lt;/p&gt;

&lt;p&gt;We still need to do &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-845&quot; title=&quot;If you &amp;quot;flush by RAM usage&amp;quot; then IndexWriter may over-merge&quot;&gt;&lt;del&gt;LUCENE-845&lt;/del&gt;&lt;/a&gt; before it can be committed as is.&lt;/p&gt;

&lt;p&gt;However one option instead would be to commit this patch, but leave&lt;br/&gt;
IndexWriter flushing by doc count by default and then later switch it&lt;br/&gt;
to flush by net RAM usage once &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-845&quot; title=&quot;If you &amp;quot;flush by RAM usage&amp;quot; then IndexWriter may over-merge&quot;&gt;&lt;del&gt;LUCENE-845&lt;/del&gt;&lt;/a&gt; is done.  I like this option&lt;br/&gt;
best.&lt;/p&gt;

&lt;p&gt;All tests pass (I&apos;ve re-enabled the disk full tests and fixed error&lt;br/&gt;
handling so they now pass) on Windows XP, Debian Linux and OS X.&lt;/p&gt;

&lt;p&gt;Summary of the changes in this rev:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;Finished cleaning up &amp;amp; commenting code&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Exception handling: if there is a disk full or any other exception&lt;br/&gt;
    while adding a document or flushing then the index is rolled back&lt;br/&gt;
    to the last commit point.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Added more unit tests&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Removed my profiling tool from the patch (not intended to be&lt;br/&gt;
    committed)&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Fixed a thread safety issue where if you flush by doc count you&lt;br/&gt;
    would sometimes get more than the doc count at flush than you&lt;br/&gt;
    requested.  I moved the thread synchronization for determining&lt;br/&gt;
    flush time down into DocumentsWriter.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Also fixed thread safety of calling flush with one thread while&lt;br/&gt;
    other threads are still adding documents.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;The biggest change is: absorbed all merging logic back into&lt;br/&gt;
    IndexWriter.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    Previously in DocumentsWriter I was tracking my own&lt;br/&gt;
    flushed/partial segments and merging them on my own (but using&lt;br/&gt;
    SegmentMerger).  This makes DocumentsWriter much simpler: now its&lt;br/&gt;
    sole purpose is to gather added docs and write a new segment.&lt;/p&gt;

&lt;p&gt;    This turns out to be a big win:&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Code is much simpler (no duplication of &quot;merging&quot;&lt;br/&gt;
        policy/logic)&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;21-25% additional performance gain for autoCommit=false case&lt;br/&gt;
        when stored fields &amp;amp; vectors are used&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;IndexWriter.close() no longer takes an unexpected long time to&lt;br/&gt;
        close in autoCommit=false case&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    However I had to make a change to the index format to do this.&lt;br/&gt;
    The basic idea is to allow multiple segments to share access to&lt;br/&gt;
    the &quot;doc store&quot; (stored fields, vectors) index files.&lt;/p&gt;

&lt;p&gt;    The change is quite simple: FieldsReader/VectorsReader are now&lt;br/&gt;
    told the doc offset that they should start from when seeking in&lt;br/&gt;
    the index stream (this info is stored in SegmentInfo).  When&lt;br/&gt;
    merging segments we don&apos;t merge the &quot;doc store&quot; files when all&lt;br/&gt;
    segments are sharing the same ones (big performance gain), else,&lt;br/&gt;
    we make a private copy of the &quot;doc store&quot; files (ie as segments&lt;br/&gt;
    normally are on the trunk today).&lt;/p&gt;

&lt;p&gt;    The change is fully backwards compatible (I added a test case to&lt;br/&gt;
    the backwards compatibility unit test to be sure) and the change&lt;br/&gt;
    is only used when autoCommit=false.&lt;/p&gt;

&lt;p&gt;    When autoCommit=false, the writer will append stored fields /&lt;br/&gt;
    vectors to a single set of files even though it is flushing normal&lt;br/&gt;
    segments whenever RAM is full.  These normal segments all refer to&lt;br/&gt;
    the single shared set of &quot;doc store&quot; files.  Then when segments&lt;br/&gt;
    are merged, the newly merged segment has its own &quot;private&quot; doc&lt;br/&gt;
    stores again.  So the sharing only occurs for the &quot;level 0&quot;&lt;br/&gt;
    segments.&lt;/p&gt;

&lt;p&gt;    I still need to update fileformats doc with this change.&lt;/p&gt;</comment>
                    <comment id="12505373" author="yseeley@gmail.com" created="Fri, 15 Jun 2007 22:26:33 +0100"  >&lt;p&gt;&amp;gt; When merging segments we don&apos;t merge the &quot;doc store&quot; files when all segments are sharing the same ones (big performance gain), &lt;/p&gt;

&lt;p&gt;Is this only in the case where the segments have no deleted docs?&lt;/p&gt;</comment>
                    <comment id="12505418" author="mikemccand" created="Sat, 16 Jun 2007 02:00:45 +0100"  >&lt;p&gt;&amp;gt; &amp;gt; When merging segments we don&apos;t merge the &quot;doc store&quot; files when all segments are sharing the same ones (big performance gain),&lt;br/&gt;
&amp;gt; &lt;br/&gt;
&amp;gt; Is this only in the case where the segments have no deleted docs? &lt;/p&gt;

&lt;p&gt;Right.  Also the segments must be contiguous which the current merge&lt;br/&gt;
policy ensures but future merge policies may not.&lt;/p&gt;</comment>
                    <comment id="12505822" author="mikemccand" created="Mon, 18 Jun 2007 14:56:56 +0100"  >&lt;p&gt;OK, I attached a new version (take9) of the patch that reverts back to&lt;br/&gt;
the default of &quot;flush after every 10 documents added&quot; in IndexWriter.&lt;br/&gt;
This removes the dependency on &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-845&quot; title=&quot;If you &amp;quot;flush by RAM usage&amp;quot; then IndexWriter may over-merge&quot;&gt;&lt;del&gt;LUCENE-845&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;However, I still think we should later (once &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-845&quot; title=&quot;If you &amp;quot;flush by RAM usage&amp;quot; then IndexWriter may over-merge&quot;&gt;&lt;del&gt;LUCENE-845&lt;/del&gt;&lt;/a&gt; is done)&lt;br/&gt;
default IndexWriter to flush by RAM usage since this will generally&lt;br/&gt;
give the best &quot;out of the box&quot; performance.  I will open a separate&lt;br/&gt;
issue to change the default after this issue is resolved.&lt;/p&gt;</comment>
                    <comment id="12506576" author="steven_parkes" created="Wed, 20 Jun 2007 16:44:39 +0100"  >&lt;p&gt;I&apos;ve started looking at this, what it would take to merge with the merge policy stuff (&lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-847&quot; title=&quot;Factor merge policy out of IndexWriter&quot;&gt;&lt;del&gt;LUCENE-847&lt;/del&gt;&lt;/a&gt;). Noticed that there are a couple of test failures?&lt;/p&gt;</comment>
                    <comment id="12506580" author="mikemccand" created="Wed, 20 Jun 2007 16:58:57 +0100"  >&lt;p&gt;Oh, were the test failures only in the TestBackwardsCompatibility?&lt;/p&gt;

&lt;p&gt;Because I changed the index file format, I added 2 more ZIP files to&lt;br/&gt;
that unit test, but, &quot;svn diff&quot; doesn&apos;t pick up the new zip files.  So&lt;br/&gt;
I&apos;m attaching them.  Can you pull off these zip files into your&lt;br/&gt;
src/test/org/apache/lucene/index and test again?  Thanks.&lt;/p&gt;
</comment>
                    <comment id="12506609" author="steven_parkes" created="Wed, 20 Jun 2007 18:37:19 +0100"  >&lt;p&gt;Yeah, that was it.&lt;/p&gt;

&lt;p&gt;I&apos;ll be delving more into the code as I try to figure out how it will dove tail with the merge policy factoring.&lt;/p&gt;</comment>
                    <comment id="12506718" author="mikemccand" created="Thu, 21 Jun 2007 00:25:49 +0100"  >&lt;p&gt;&amp;gt; Yeah, that was it.&lt;/p&gt;

&lt;p&gt;Phew!&lt;/p&gt;

&lt;p&gt;&amp;gt; I&apos;ll be delving more into the code as I try to figure out how it will&lt;br/&gt;
&amp;gt; dove tail with the merge policy factoring.&lt;/p&gt;

&lt;p&gt;OK, thanks.  I am very eager to get some other eyeballs looking for&lt;br/&gt;
issues with this patch!&lt;/p&gt;

&lt;p&gt;I &lt;b&gt;think&lt;/b&gt; this patch and the merge policy refactoring should be fairly&lt;br/&gt;
separate.&lt;/p&gt;

&lt;p&gt;With this patch, &quot;flushing&quot; RAM -&amp;gt; Lucene segment is no longer a&lt;br/&gt;
&quot;mergeSegments&quot; call which I think simplifies IndexWriter.  Previously&lt;br/&gt;
mergeSegments had lots of extra logic to tell if it was merging RAM&lt;br/&gt;
segments (= a flush) vs merging &quot;real&quot; segments but now it&apos;s simpler&lt;br/&gt;
because mergeSegments really only merges segments.&lt;/p&gt;</comment>
                    <comment id="12506752" author="michaelbusch" created="Thu, 21 Jun 2007 04:05:36 +0100"  >&lt;p&gt;Hi Mike,&lt;/p&gt;

&lt;p&gt;my first comment on this patch is: Impressive!&lt;/p&gt;

&lt;p&gt;It&apos;s also quite overwhelming at the beginning, but I&apos;m trying to dig into it. I&apos;ll probably have more questions, here&apos;s the first one:&lt;/p&gt;

&lt;p&gt;Does DocumentsWriter also solve the problem DocumentWriter had before &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-880&quot; title=&quot;DocumentWriter closes TokenStreams too early&quot;&gt;&lt;del&gt;LUCENE-880&lt;/del&gt;&lt;/a&gt;? I believe the answer is yes. Even though you close the TokenStreams in the finally clause of invertField() like DocumentWriter did before 880 this is safe, because addPosition() serializes the term strings and payload bytes into the posting hash table right away. Is that right?&lt;/p&gt;</comment>
                    <comment id="12506778" author="michaelbusch" created="Thu, 21 Jun 2007 07:51:13 +0100"  >&lt;p&gt;Mike,&lt;/p&gt;

&lt;p&gt;the benchmarks you run focus on measuring the pure indexing performance. I think it would be interesting to know how big the speedup is in real-life scenarios, i. e. with StandardAnalyzer and maybe even HTML parsing? For sure the speedup will be less, but it should still be a significant improvement. Did you run those kinds of benchmarks already?&lt;/p&gt;</comment>
                    <comment id="12506811" author="mikemccand" created="Thu, 21 Jun 2007 10:35:44 +0100"  >&lt;p&gt;&amp;gt; Does DocumentsWriter also solve the problem DocumentWriter had&lt;br/&gt;
&amp;gt; before &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-880&quot; title=&quot;DocumentWriter closes TokenStreams too early&quot;&gt;&lt;del&gt;LUCENE-880&lt;/del&gt;&lt;/a&gt;? I believe the answer is yes. Even though you&lt;br/&gt;
&amp;gt; close the TokenStreams in the finally clause of invertField() like&lt;br/&gt;
&amp;gt; DocumentWriter did before 880 this is safe, because addPosition()&lt;br/&gt;
&amp;gt; serializes the term strings and payload bytes into the posting hash&lt;br/&gt;
&amp;gt; table right away. Is that right?&lt;/p&gt;

&lt;p&gt;That&apos;s right.  When I merged in the fix for &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-880&quot; title=&quot;DocumentWriter closes TokenStreams too early&quot;&gt;&lt;del&gt;LUCENE-880&lt;/del&gt;&lt;/a&gt;, I realized&lt;br/&gt;
with this patch it&apos;s fine to close the token stream immediately after&lt;br/&gt;
processing all of its tokens because everything about the token stream&lt;br/&gt;
has been &quot;absorbed&quot; into postings hash.&lt;/p&gt;

&lt;p&gt;&amp;gt; the benchmarks you run focus on measuring the pure indexing&lt;br/&gt;
&amp;gt; performance. I think it would be interesting to know how big the&lt;br/&gt;
&amp;gt; speedup is in real-life scenarios, i. e. with StandardAnalyzer and&lt;br/&gt;
&amp;gt; maybe even HTML parsing? For sure the speedup will be less, but it&lt;br/&gt;
&amp;gt; should still be a significant improvement. Did you run those kinds&lt;br/&gt;
&amp;gt; of benchmarks already?&lt;/p&gt;

&lt;p&gt;Good question ... I haven&apos;t measured the performance cost of using&lt;br/&gt;
StandardAnalyzer or HTML parsing but I will test &amp;amp; post back.&lt;/p&gt;</comment>
                    <comment id="12506907" author="mikemccand" created="Thu, 21 Jun 2007 15:00:19 +0100"  >&lt;p&gt;OK I ran tests comparing analyzer performance.&lt;/p&gt;

&lt;p&gt;It&apos;s the same test framework as above, using the ~5,500 byte Europarl&lt;br/&gt;
docs with autoCommit=true, 32 MB RAM buffer, no stored fields nor&lt;br/&gt;
vectors, and CFS=false, indexing 200,000 documents.&lt;/p&gt;

&lt;p&gt;The SimpleSpaceAnalyzer is my own whitespace analyzer that minimizes&lt;br/&gt;
GC cost by not allocating a Term or String for every token in every&lt;br/&gt;
document.&lt;/p&gt;

&lt;p&gt;Each run is best time of 2 runs:&lt;/p&gt;

&lt;p&gt;  ANALYZER            PATCH (sec) TRUNK (sec)  SPEEDUP&lt;br/&gt;
  SimpleSpaceAnalyzer  79.0       326.5        4.1 X&lt;br/&gt;
  StandardAnalyzer    449.0       674.1        1.5 X&lt;br/&gt;
  WhitespaceAnalyzer  104.0       338.9        3.3 X&lt;br/&gt;
  SimpleAnalyzer      104.7       328.0        3.1 X&lt;/p&gt;

&lt;p&gt;StandardAnalyzer is definiteely rather time consuming!&lt;/p&gt;</comment>
                    <comment id="12506961" author="michaelbusch" created="Thu, 21 Jun 2007 18:08:52 +0100"  >&lt;p&gt;&amp;gt; OK I ran tests comparing analyzer performance.&lt;/p&gt;

&lt;p&gt;Thanks for the numbers Mike. Yes the gain is less with StandardAnalyzer&lt;br/&gt;
but 1.5X faster is still very good!&lt;/p&gt;


&lt;p&gt;I have some question about the extensibility of your code. For flexible&lt;br/&gt;
indexing we want to be able in the future to implement different posting&lt;br/&gt;
formats and we might even want to allow our users to implement own &lt;br/&gt;
posting formats.&lt;/p&gt;

&lt;p&gt;When I implemented multi-level skipping I tried to keep this in mind. &lt;br/&gt;
Therefore I put most of the functionality in the two abstract classes&lt;br/&gt;
MultiLevelSkipListReader/Writer. Subclasses implement the actual format&lt;br/&gt;
of the skip data. I think with this design it should be quite easy to&lt;br/&gt;
implement different formats in the future while limiting the code&lt;br/&gt;
complexity.&lt;/p&gt;

&lt;p&gt;With the old DocumentWriter I think this is quite simple to do too by&lt;br/&gt;
adding a class like PostingListWriter, where subclasses define the actual &lt;br/&gt;
format (because DocumentWriter is so simple).&lt;/p&gt;

&lt;p&gt;Do you think your code is easily extensible in this regard? I&apos;m &lt;br/&gt;
wondering because of all the optimizations you&apos;re doing like e. g.&lt;br/&gt;
sharing byte arrays. But I&apos;m certainly not familiar enough with your code &lt;br/&gt;
yet, so I&apos;m only guessing here.&lt;/p&gt;</comment>
                    <comment id="12506974" author="mikemccand" created="Thu, 21 Jun 2007 18:59:00 +0100"  >&lt;p&gt;&amp;gt; Do you think your code is easily extensible in this regard? I&apos;m &lt;br/&gt;
&amp;gt; wondering because of all the optimizations you&apos;re doing like e. g.&lt;br/&gt;
&amp;gt; sharing byte arrays. But I&apos;m certainly not familiar enough with your code &lt;br/&gt;
&amp;gt; yet, so I&apos;m only guessing here.&lt;/p&gt;

&lt;p&gt;Good question!&lt;/p&gt;

&lt;p&gt;DocumentsWriter is definitely more complex than DocumentWriter, but it&lt;br/&gt;
doesn&apos;t prevent extensibility and I think will work very well when we&lt;br/&gt;
do flexible indexing.&lt;/p&gt;

&lt;p&gt;The patch now has dedicated methods for writing into the freq/prox/etc&lt;br/&gt;
streams (&apos;writeFreqByte&apos;, &apos;writeFreqVInt&apos;, &apos;writeProxByte&apos;,&lt;br/&gt;
&apos;writeProxVInt&apos;, etc.), but, this could easily be changed to instead&lt;br/&gt;
use true IndexOutput streams.  This would then hide all details of&lt;br/&gt;
shared byte arrays from whoever is doing the writing.&lt;/p&gt;

&lt;p&gt;The way I roughly see flexible indexing working in the future is&lt;br/&gt;
DocumentsWriter will be responsible for keeping track of unique terms&lt;br/&gt;
seen (in its hash table), holding the Posting instance (which could be&lt;br/&gt;
subclassed in the future) for each term, flushing a real segment when&lt;br/&gt;
full, handling shared byte arrays, etc.  Ie all the &quot;infrastructure&quot;.&lt;/p&gt;

&lt;p&gt;But then the specific logic of what bytes are written into which&lt;br/&gt;
streams (freq/prox/vectors/others) will be handled by a separate class&lt;br/&gt;
or classes that we can plug/unplug according to some &quot;schema&quot;.&lt;br/&gt;
DocumentsWriter would call on these classes and provide the&lt;br/&gt;
IndexOutput&apos;s for all streams for the Posting, per position, and these&lt;br/&gt;
classes write their own format into the IndexOutputs.&lt;/p&gt;

&lt;p&gt;I think a separation like that would work well: we could have good&lt;br/&gt;
performance and also extensibility.  Devil is in the details of&lt;br/&gt;
course...&lt;/p&gt;

&lt;p&gt;I obviously haven&apos;t factored DocumentsWriter in this way (it has its&lt;br/&gt;
own addPosition that writes the current Lucene index format) but I&lt;br/&gt;
think this is very doable in the future.&lt;/p&gt;</comment>
                    <comment id="12507567" author="doronc" created="Sat, 23 Jun 2007 07:59:02 +0100"  >&lt;p&gt;Mike, I am considering testing the performance of this patch on a somewhat different use case, real one I think. After indexing 25M docs of TREC .gov2 (~500GB of docs) I pushed the index terms to create a spell correction index, by using the contrib spell checker. Docs here are &lt;b&gt;very&lt;/b&gt; short - For each index term a document is created, containing some N-GRAMS. On the specific machine I used there are 2 CPUs but the SpellChecker indexing does not take advantage of that. Anyhow, 126,684,685 words==documents were indexed. &lt;br/&gt;
For the docs adding step I had:&lt;br/&gt;
    mergeFactor = 100,000&lt;br/&gt;
    maxBufferedDocs = 10,000&lt;br/&gt;
So no merging took place.&lt;br/&gt;
This step took 21 hours, and created 12,685 segments, total size 15 - 20 GB. &lt;br/&gt;
Then I optimized the index with&lt;br/&gt;
    mergeFactor = 400&lt;br/&gt;
(Larger values were hard on the open files limits.)&lt;/p&gt;

&lt;p&gt;I thought it would be interesting to see how the new code performs in this scenario, what do you think?&lt;/p&gt;

&lt;p&gt;If you too find this comparison interesting, I have two more questions:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;what settings do you recommend?&lt;/li&gt;
	&lt;li&gt;is there any chance for speed-up in optimize()?  I didn&apos;t read your&lt;br/&gt;
    new code yet, but at least from some comments here it seems that &lt;br/&gt;
    on disk merging was not changed... is this (still) so? I would skip the &lt;br/&gt;
    optimize part if this is not of interest for the comparison. (In fact I am &lt;br/&gt;
    still waiting for my optimize() to complete, but if it is not of interest I &lt;br/&gt;
    will just interrupt it...)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Thanks,&lt;br/&gt;
Doron&lt;/p&gt;</comment>
                    <comment id="12507587" author="mikemccand" created="Sat, 23 Jun 2007 10:43:11 +0100"  >
&lt;p&gt;&amp;gt; I thought it would be interesting to see how the new code performs in this scenario, what do you think?&lt;/p&gt;

&lt;p&gt;Yes I&apos;d be very interested to see the results of this.  It&apos;s a&lt;br/&gt;
somewhat &quot;unusual&quot; indexing situation (such tiny docs) but it&apos;s a real&lt;br/&gt;
world test case.  Thanks!&lt;/p&gt;

&lt;p&gt;&amp;gt;  - what settings do you recommend?&lt;/p&gt;

&lt;p&gt;I think these are likely the important ones in this case:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;Flush by RAM instead of doc count&lt;br/&gt;
    (writer.setRAMBufferSizeMB(...)).&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Give it as much RAM as you can.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Use maybe 3 indexing threads (if you can).&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Turn off compound file.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;If you have stored fields/vectors (seems not in this case) use&lt;br/&gt;
    autoCommit=false.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Use a trivial analyzer that doesn&apos;t create new String/new Token&lt;br/&gt;
    (re-use the same Token, and use the char[] based term text&lt;br/&gt;
    storage instead of the String one).&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Re-use Document/Field instances.  The DocumentsWriter is fine with&lt;br/&gt;
    this and it saves substantial time from GC especially because your&lt;br/&gt;
    docs are so tiny (per-doc overhead is otherwise a killer).  In&lt;br/&gt;
    IndexLineFiles I made a StringReader that lets me reset its String&lt;br/&gt;
    value; this way I didn&apos;t have to change the Field instances stored&lt;br/&gt;
    in the Document.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&amp;gt;  - is there any chance for speed-up in optimize()?  I didn&apos;t read&lt;br/&gt;
&amp;gt;    your new code yet, but at least from some comments here it seems&lt;br/&gt;
&amp;gt;    that on disk merging was not changed... is this (still) so? I would&lt;/p&gt;

&lt;p&gt;Correct: my patch doesn&apos;t touch merging and optimizing.  All it does&lt;br/&gt;
now is gather many docs in RAM and then flush a new segment when it&apos;s&lt;br/&gt;
time.  I&apos;ve opened a separate issue (&lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-856&quot; title=&quot;Optimize segment merging&quot;&gt;&lt;del&gt;LUCENE-856&lt;/del&gt;&lt;/a&gt;) for optimizations&lt;br/&gt;
in segment merging.&lt;/p&gt;</comment>
                    <comment id="12507708" author="doronc" created="Sun, 24 Jun 2007 20:56:05 +0100"  >&lt;p&gt;Just to clarify your comment on reusing field and doc instances - to my understanding reusing a field instance is ok &lt;b&gt;only&lt;/b&gt; after the containing doc was added to the index.&lt;/p&gt;

&lt;p&gt;For a &quot;fair&quot; comparison I ended up not following most of your recommendations, including the reuse field/docs one and the non-compound one (apologies&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;), but I might use them later. &lt;/p&gt;

&lt;p&gt;For the first 100,000,000 docs (==speller words) the speed-up is quite amazing:&lt;br/&gt;
    Orig:    Speller: added 100000000 words in 10912 seconds = 3 hours 1 minutes 52 seconds&lt;br/&gt;
    New:   Speller: added 100000000 words in 58490 seconds = 16 hours 14 minutes 50 seconds&lt;br/&gt;
This is 5.3 times faster !!!&lt;/p&gt;

&lt;p&gt;This btw was with maxBufDocs=100,000 (I forgot to set the MEM param). &lt;br/&gt;
I stopped the run now, I don&apos;t expect to learn anything new by letting it continue.&lt;/p&gt;

&lt;p&gt;When trying with  MEM=512MB, it at first seemed faster, but then there were now and then local slow-downs, and eventually it became a bit slower than the previous run. I know these are not merges, so they are either flushes (RAM directed), or GC activity. I will perhaps run with GC debug flags and perhaps add a print at flush so to tell the culprit for these local slow-downs.&lt;/p&gt;

&lt;p&gt;Other than that, I will perhaps try to index .GOV2 (25 Million HTML docs) with this patch. The way I indexed it before it took about 4 days - running in 4 threads, and creating 36 indexes. This is even more a real life scenario, it involves HTML parsing, standard analysis, and merging (to some extent). Since there are 4 threads each one will get, say, 250MB. Again, for a &quot;fair&quot; comparison, I will remain with compound.&lt;/p&gt;</comment>
                    <comment id="12507716" author="mikemccand" created="Sun, 24 Jun 2007 21:56:36 +0100"  >
&lt;p&gt;&amp;gt; Just to clarify your comment on reusing field and doc instances - to my&lt;br/&gt;
&amp;gt; understanding reusing a field instance is ok &lt;b&gt;only&lt;/b&gt; after the containing&lt;br/&gt;
&amp;gt; doc was added to the index.&lt;/p&gt;

&lt;p&gt;Right, if your documents are very &quot;regular&quot; you should get a sizable&lt;br/&gt;
speedup (especially for tiny docs), with or without this patch, if you&lt;br/&gt;
make a single Document and add &lt;b&gt;separate&lt;/b&gt; Field instances to it for&lt;br/&gt;
each field, and then reuse the Document and Field instances for all&lt;br/&gt;
the docs you want to add.&lt;/p&gt;

&lt;p&gt;It&apos;s not easy to reuse Field instances now (there&apos;s no&lt;br/&gt;
setStringValue()).  I made a ReusableStringReader to do this but you&lt;br/&gt;
could also make your own class that implements Fieldable.&lt;/p&gt;

&lt;p&gt;&amp;gt; For a &quot;fair&quot; comparison I ended up not following most of your&lt;br/&gt;
&amp;gt; recommendations, including the reuse field/docs one and the non-compound&lt;br/&gt;
&amp;gt; one (apologies&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;), but I might use them later.&lt;/p&gt;

&lt;p&gt;OK, when you say &quot;fair&quot; I think you mean because you already had a&lt;br/&gt;
previous run that used compound file, you had to use compound file in&lt;br/&gt;
the run with the &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-843&quot; title=&quot;improve how IndexWriter uses RAM to buffer added documents&quot;&gt;&lt;del&gt;LUCENE-843&lt;/del&gt;&lt;/a&gt; patch (etc)?  The recommendations above&lt;br/&gt;
should speed up Lucene with or without my patch.&lt;/p&gt;

&lt;p&gt;&amp;gt; For the first 100,000,000 docs (==speller words) the speed-up is quite&lt;br/&gt;
&amp;gt; amazing:&lt;br/&gt;
&amp;gt;     Orig:    Speller: added 100000000 words in 10912 seconds = 3 hours 1&lt;br/&gt;
&amp;gt;     minutes 52 seconds&lt;br/&gt;
&amp;gt;     New:   Speller: added 100000000 words in 58490 seconds = 16 hours 14&lt;br/&gt;
&amp;gt;     minutes 50 seconds&lt;br/&gt;
&amp;gt; This is 5.3 times faster !!!&lt;/p&gt;

&lt;p&gt;Wow!  I think the speedup might be even more if both of your runs followed&lt;br/&gt;
the suggestions above.&lt;/p&gt;

&lt;p&gt;&amp;gt; This btw was with maxBufDocs=100,000 (I forgot to set the MEM param).&lt;br/&gt;
&amp;gt; I stopped the run now, I don&apos;t expect to learn anything new by letting it&lt;br/&gt;
&amp;gt; continue.&lt;br/&gt;
&amp;gt;&lt;br/&gt;
&amp;gt; When trying with  MEM=512MB, it at first seemed faster, but then there&lt;br/&gt;
&amp;gt; were now and then local slow-downs, and eventually it became a bit slower&lt;br/&gt;
&amp;gt; than the previous run. I know these are not merges, so they are either&lt;br/&gt;
&amp;gt; flushes (RAM directed), or GC activity. I will perhaps run with GC debug&lt;br/&gt;
&amp;gt; flags and perhaps add a print at flush so to tell the culprit for these&lt;br/&gt;
&amp;gt; local slow-downs.&lt;/p&gt;

&lt;p&gt;Hurm, odd.  I haven&apos;t pushed RAM buffer up to 512 MB so it could be GC&lt;br/&gt;
cost somehow makes things worse ... curious.&lt;/p&gt;

&lt;p&gt;&amp;gt; Other than that, I will perhaps try to index .GOV2 (25 Million HTML docs)&lt;br/&gt;
&amp;gt; with this patch. The way I indexed it before it took about 4 days -&lt;br/&gt;
&amp;gt; running in 4 threads, and creating 36 indexes. This is even more a real&lt;br/&gt;
&amp;gt; life scenario, it involves HTML parsing, standard analysis, and merging&lt;br/&gt;
&amp;gt; (to some extent). Since there are 4 threads each one will get, say,&lt;br/&gt;
&amp;gt; 250MB. Again, for a &quot;fair&quot; comparison, I will remain with compound.&lt;/p&gt;

&lt;p&gt;OK, because you&apos;re doing StandardAnalyzer and HTML parsing and&lt;br/&gt;
presumably loading one-doc-per-file, most of your time is spent&lt;br/&gt;
outside of Lucene indexing so I&apos;d expect less that 50% speedup in&lt;br/&gt;
this case.&lt;/p&gt;</comment>
                    <comment id="12510662" author="mikemccand" created="Fri, 6 Jul 2007 12:52:03 +0100"  >&lt;p&gt;Re-opening this issue: I saw one failure of the contrib/benchmark&lt;br/&gt;
TestPerfTasksLogic.testParallelDocMaker() tests due to an intermittant&lt;br/&gt;
thread-safety issue.  It&apos;s hard to get the failure to happen (it&apos;s&lt;br/&gt;
happened only once in ~20 runs of contrib/benchmark) but I see where&lt;br/&gt;
the issue is.  Will commit a fix shortly.&lt;/p&gt;</comment>
                    <comment id="12512264" author="steven_parkes" created="Thu, 12 Jul 2007 22:27:58 +0100"  >&lt;p&gt;Did we lose the triggered merge stuff from 887, i.e.,, should it be&lt;/p&gt;

&lt;p&gt;        if (triggerMerge) &lt;/p&gt;
{
          /* new merge policy
          if (0 == docWriter.getMaxBufferedDocs())
            maybeMergeSegments(mergeFactor * numDocs / 2);
          else
            maybeMergeSegments(docWriter.getMaxBufferedDocs());
          */
          maybeMergeSegments(docWriter.getMaxBufferedDocs());
        }
</comment>
                    <comment id="12512275" author="mikemccand" created="Thu, 12 Jul 2007 22:59:52 +0100"  >&lt;p&gt;Woops ... you are right; thanks for catching it!  I will add a unit&lt;br/&gt;
test &amp;amp; fix it.  I will also make the flush(boolean triggerMerge,&lt;br/&gt;
boolean flushDocStores) protected, not public, and move the javadoc&lt;br/&gt;
back to the public flush().&lt;/p&gt;</comment>
                </comments>
                <issuelinks>
                        <issuelinktype id="10032">
                <name>Blocker</name>
                                                <inwardlinks description="is blocked by">
                            <issuelink>
            <issuekey id="12365610">LUCENE-845</issuekey>
        </issuelink>
                    </inwardlinks>
                            </issuelinktype>
                        <issuelinktype id="10030">
                <name>Reference</name>
                                                <inwardlinks description="is related to">
                            <issuelink>
            <issuekey id="12376448">SOLR-342</issuekey>
        </issuelink>
                    </inwardlinks>
                            </issuelinktype>
                    </issuelinks>
                <attachments>
                    <attachment id="12360213" name="index.presharedstores.cfs.zip" size="1913" author="mikemccand" created="Wed, 20 Jun 2007 16:58:57 +0100" />
                    <attachment id="12360214" name="index.presharedstores.nocfs.zip" size="5225" author="mikemccand" created="Wed, 20 Jun 2007 16:58:57 +0100" />
                    <attachment id="12353973" name="LUCENE-843.patch" size="144490" author="mikemccand" created="Thu, 22 Mar 2007 17:06:45 +0000" />
                    <attachment id="12354163" name="LUCENE-843.take2.patch" size="151745" author="mikemccand" created="Sun, 25 Mar 2007 15:30:36 +0100" />
                    <attachment id="12354431" name="LUCENE-843.take3.patch" size="159259" author="mikemccand" created="Wed, 28 Mar 2007 13:49:18 +0100" />
                    <attachment id="12354752" name="LUCENE-843.take4.patch" size="192165" author="mikemccand" created="Mon, 2 Apr 2007 15:43:21 +0100" />
                    <attachment id="12356500" name="LUCENE-843.take5.patch" size="244479" author="mikemccand" created="Mon, 30 Apr 2007 11:39:19 +0100" />
                    <attachment id="12357792" name="LUCENE-843.take6.patch" size="215257" author="mikemccand" created="Mon, 21 May 2007 19:14:40 +0100" />
                    <attachment id="12359276" name="LUCENE-843.take7.patch" size="193733" author="mikemccand" created="Fri, 8 Jun 2007 14:31:01 +0100" />
                    <attachment id="12359906" name="LUCENE-843.take8.patch" size="208297" author="mikemccand" created="Fri, 15 Jun 2007 20:00:52 +0100" />
                    <attachment id="12360022" name="LUCENE-843.take9.patch" size="208495" author="mikemccand" created="Mon, 18 Jun 2007 14:56:56 +0100" />
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>11.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Tue, 3 Apr 2007 14:23:33 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>12898</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    <customfieldvalue key="10120"><![CDATA[Patch Available]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>26886</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                    <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                <customfieldname>Time in Status</customfieldname>
                <customfieldvalues>
                    
                </customfieldvalues>
            </customfield>
                            </customfields>
    </item>
</channel>
</rss>