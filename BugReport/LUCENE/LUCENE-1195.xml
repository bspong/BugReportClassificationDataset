<!-- 
RSS generated by JIRA (5.2.8#851-sha1:3262fdc28b4bc8b23784e13eadc26a22399f5d88) at Tue Jul 16 13:25:40 UTC 2013

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/LUCENE-1195/LUCENE-1195.xml?field=key&field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>5.2.8</version>
        <build-number>851</build-number>
        <build-date>26-02-2013</build-date>
    </build-info>

<item>
            <title>[LUCENE-1195] Performance improvement for TermInfosReader</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-1195</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Currently we have a bottleneck for multi-term queries: the dictionary lookup is being done&lt;br/&gt;
twice for each term. The first time in Similarity.idf(), where searcher.docFreq() is called.&lt;br/&gt;
The second time when the posting list is opened (TermDocs or TermPositions).&lt;/p&gt;

&lt;p&gt;The dictionary lookup is not cheap, that&apos;s why a significant performance improvement is&lt;br/&gt;
possible here if we avoid the second lookup. An easy way to do this is to add a small LRU &lt;br/&gt;
cache to TermInfosReader. &lt;/p&gt;

&lt;p&gt;I ran some performance experiments with an LRU cache size of 20, and an mid-size index of&lt;br/&gt;
500,000 documents from wikipedia. Here are some test results:&lt;/p&gt;

&lt;p&gt;50,000 AND queries with 3 terms each:&lt;br/&gt;
old:                  152 secs&lt;br/&gt;
new (with LRU cache): 112 secs (26% faster)&lt;/p&gt;

&lt;p&gt;50,000 OR queries with 3 terms each:&lt;br/&gt;
old:                  175 secs&lt;br/&gt;
new (with LRU cache): 133 secs (24% faster)&lt;/p&gt;

&lt;p&gt;For bigger indexes this patch will probably have less impact, for smaller once more.&lt;/p&gt;

&lt;p&gt;I will attach a patch soon.&lt;/p&gt;</description>
                <environment></environment>
            <key id="12389627">LUCENE-1195</key>
            <summary>Performance improvement for TermInfosReader</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png">Closed</status>
                    <resolution id="1">Fixed</resolution>
                                <assignee username="michaelbusch">Michael Busch</assignee>
                                <reporter username="michaelbusch">Michael Busch</reporter>
                        <labels>
                    </labels>
                <created>Tue, 26 Feb 2008 22:59:32 +0000</created>
                <updated>Sat, 11 Oct 2008 13:49:36 +0100</updated>
                    <resolved>Fri, 23 May 2008 18:22:36 +0100</resolved>
                                            <fixVersion>2.4</fixVersion>
                                <component>core/index</component>
                        <due></due>
                    <votes>0</votes>
                        <watches>3</watches>
                                                    <comments>
                    <comment id="12572747" author="yseeley@gmail.com" created="Wed, 27 Feb 2008 00:55:11 +0000"  >&lt;p&gt;Nice results!&lt;br/&gt;
I assume this is on a non-optimized index?&lt;br/&gt;
One of the unexpected things I&apos;ve seen in Solr is that enumerating over all the terms of a non-optimized index is a very significant component of the total time (including iterating over termdocs when necessary).  This was with terms with a relatively low df though.&lt;/p&gt;</comment>
                    <comment id="12572750" author="michaelbusch" created="Wed, 27 Feb 2008 01:03:18 +0000"  >&lt;p&gt;Test details:&lt;br/&gt;
The index has 500,000 docs and 3191625 unique terms. To construct the queries &lt;br/&gt;
I used terms with 1000&amp;lt;df&amp;lt;3000, the index has 3880 of them. I combined the &lt;br/&gt;
terms randomly. Each query has at least one hit. The AND queries have 25 hits &lt;br/&gt;
on average, the OR queries 5641. &lt;/p&gt;

&lt;p&gt;The LRU cache was pretty small with a size of just 20.&lt;/p&gt;

&lt;p&gt;The index is unoptimized with 11 segments.&lt;/p&gt;

&lt;p&gt;The searcher was warmed for the tests, thus benefiting from FS caching, which&lt;br/&gt;
should be a common scenario for indexes of such a medium size.&lt;/p&gt;</comment>
                    <comment id="12572753" author="yseeley@gmail.com" created="Wed, 27 Feb 2008 01:13:34 +0000"  >&lt;p&gt;Thinking about a common use in Solr: doing a query and faceting that query by a field... would blow out the cache (due to iterating over all the terms in a single field) if it&apos;s a global cache?  Is there a good way to prevent that from happening (perhaps just change lucene&apos;s existing single -entry thread local cache to a multi-entry thread local cache?)&lt;/p&gt;</comment>
                    <comment id="12572768" author="blueye118" created="Wed, 27 Feb 2008 03:28:39 +0000"  >&lt;p&gt;When faceting(iterating all terms), will each term be looked up twice in dictionary?&lt;/p&gt;</comment>
                    <comment id="12572913" author="yseeley@gmail.com" created="Wed, 27 Feb 2008 14:31:01 +0000"  >&lt;blockquote&gt;&lt;p&gt;When faceting(iterating all terms), will each term be looked up twice in dictionary?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;No, but consider the case of one thread iterating over all the terms in a field (due to faceting, a range query, prefix query, etc)  That would tend to destroy the cache for other threads doing simple queries unless a way could be found to bypass the cache during enumeration somehow, or make each cache thread specific.&lt;/p&gt;


</comment>
                    <comment id="12573039" author="michaelbusch" created="Wed, 27 Feb 2008 19:31:30 +0000"  >&lt;p&gt;Here is the simple patch. The cache is only used in TermInfosReader.get(Term).&lt;/p&gt;

&lt;p&gt;So if for example a RangeQuery gets a TermEnum from the IndexReader, then&lt;br/&gt;
enumerating the terms using the TermEnum will not replace the terms in the&lt;br/&gt;
cache.&lt;/p&gt;

&lt;p&gt;The LRUCache itself is not synchronized. It might happen that multiple &lt;br/&gt;
threads lookup the same term at the same time, then we might get an cache &lt;br/&gt;
miss. But I think such a situation should be very rare, and it&apos;s therefore&lt;br/&gt;
better to avoid the synchronization overhead?&lt;/p&gt;

&lt;p&gt;I set the default cache size to 1024. A cache entry is a (Term, TermInfo)&lt;br/&gt;
tuple. TermInfo needs 24 bytes, I think a Term approx. 20-30 bytes? So&lt;br/&gt;
the cache would need about 1024 * ~50 bytes = 50Kb plus a bit overhead&lt;br/&gt;
from the LinkedHashMap. This is the memory requirement per index segment,&lt;br/&gt;
so a non-optimized index with 20 segments would need about 1MB more memory&lt;br/&gt;
with this cache. I think this is acceptable? Otherwise we can also decrease&lt;br/&gt;
the cache size.&lt;/p&gt;

&lt;p&gt;All core &amp;amp; contrib tests pass.&lt;/p&gt;</comment>
                    <comment id="12573045" author="yseeley@gmail.com" created="Wed, 27 Feb 2008 19:42:25 +0000"  >&lt;blockquote&gt;&lt;p&gt;The LRUCache itself is not synchronized. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Unfortunately, it needs to be... no getting around it.&lt;/p&gt;</comment>
                    <comment id="12573049" author="yseeley@gmail.com" created="Wed, 27 Feb 2008 19:46:54 +0000"  >&lt;blockquote&gt;
&lt;p&gt;Here is the simple patch. The cache is only used in TermInfosReader.get(Term).&lt;/p&gt;

&lt;p&gt;So if for example a RangeQuery gets a TermEnum from the IndexReader, then&lt;br/&gt;
enumerating the terms using the TermEnum will not replace the terms in the&lt;br/&gt;
cache.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;But for each term, TermDocs.seek() will be called, and that will do a TermInfosReader.get(Term), replacing items in the cache.&lt;br/&gt;
For SegmentReader, seek(TermEnum) actually doesn&apos;t call TermInfosReader.get(Term), but any kind of multi-reader does.&lt;/p&gt;</comment>
                    <comment id="12573065" author="michaelbusch" created="Wed, 27 Feb 2008 21:03:24 +0000"  >&lt;blockquote&gt;
&lt;p&gt;Unfortunately, it needs to be... no getting around it.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;You&apos;re right, and I&apos;m stupid &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; &lt;br/&gt;
Actually what I meant was that the get() and put() methods don&apos;t need to&lt;br/&gt;
be synchronized if the underlying data structure, i. e.the LinkedHashMap,&lt;br/&gt;
that I&apos;m using is thread-safe, otherwise it might return inconsistent&lt;br/&gt;
data. &lt;br/&gt;
But the LinkedHashMap is not, unless I decorate it with &lt;br/&gt;
Collections.synchronizedMap(). Do you know what&apos;s faster? Using the&lt;br/&gt;
synchronized map or making get() and put() synchronized? Probably&lt;br/&gt;
there&apos;s not really a difference, because the decorator that &lt;br/&gt;
Collections.synchronizedMap() returns just does essentially the same?&lt;/p&gt;</comment>
                    <comment id="12573073" author="yseeley@gmail.com" created="Wed, 27 Feb 2008 21:26:53 +0000"  >&lt;p&gt;There&apos;s higher level synchronization too (ensuring that two different threads don&apos;t generate the same cache entry at the same time), and I agree that should not be done in this case.&lt;/p&gt;

&lt;p&gt;Just use Collections.synchronizedMap(), it will be the same speed, more readable, and can be easily replaced later anyway.&lt;/p&gt;</comment>
                    <comment id="12598587" author="michaelbusch" created="Wed, 21 May 2008 09:37:33 +0100"  >&lt;p&gt;Changes in the patch:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;the used cache is thread-safe now&lt;/li&gt;
	&lt;li&gt;added a ThreadLocal to TermInfosReader, so that each thread has its own cache of size 1024 now&lt;/li&gt;
	&lt;li&gt;SegmentTermEnum.scanTo() returns now the number of invocations of next(). TermInfosReader only&lt;br/&gt;
  puts TermInfo objects into the cache if scanTo() has called next() more than once. Thus, if e. g.&lt;br/&gt;
  a WildcardQuery or RangeQuery iterates over terms in order, only the first term will be put into&lt;br/&gt;
  the cache. This is an addition to the ThreadLocal that prevents one thread from wiping out its&lt;br/&gt;
  own cache with such a query. &lt;/li&gt;
	&lt;li&gt;added a new package org/apache/lucene/util/cache that has a SimpleMapCache (taken from &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-831&quot; title=&quot;Complete overhaul of FieldCache API/Implementation&quot;&gt;LUCENE-831&lt;/a&gt;)&lt;br/&gt;
  and the SimpleLRUCache that was part of the previous patch here. I decided to put the caches in&lt;br/&gt;
  a separate package, because we can reuse them for different things like &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-831&quot; title=&quot;Complete overhaul of FieldCache API/Implementation&quot;&gt;LUCENE-831&lt;/a&gt; or e. g. after&lt;br/&gt;
  deprecating Hits as LRU cache for recently loaded stored documents.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I reran the same performance experiments and it turns out that the speedup is still the same and&lt;br/&gt;
the overhead of the ThreadLocal is in the noise. So I think this should be a good approach now?&lt;/p&gt;

&lt;p&gt;I also ran similar performance tests on a bigger index with about 4.3 million documents. The &lt;br/&gt;
speedup with 50k AND queries was, as expected, not as significant anymore. However, the speedup&lt;br/&gt;
was still about 7%. I haven&apos;t run the OR queries on the bigger index yet, but most likely the&lt;br/&gt;
speedup will not be very significant anymore.&lt;/p&gt;

&lt;p&gt;All unit tests pass.&lt;/p&gt;</comment>
                    <comment id="12599265" author="michaelbusch" created="Fri, 23 May 2008 03:18:49 +0100"  >&lt;p&gt;In the previous patch was a silly thread-safety problem that I fixed now. &lt;br/&gt;
Some threads in the TestIndexReaderReopen test occasionally hit &lt;br/&gt;
errors (I fixed the testcase to fail now whenever an error is hit).&lt;/p&gt;

&lt;p&gt;I made some other changes to the TermInfosReader. I&apos;m not using&lt;br/&gt;
two ThreadLocals anymore for the SegmentTermEnum and Cache,&lt;br/&gt;
but added a small inner class called ThreadResources which holds&lt;br/&gt;
references to those two objects. I also minimized the amount of&lt;br/&gt;
ThreadLocal.get() calls by passing around the enumerator.&lt;/p&gt;

&lt;p&gt;Furthermore I got rid of the private scanEnum() method and inlined&lt;br/&gt;
it into the get() method to fix the above mentioned thread-safety &lt;br/&gt;
problem. And I also realized that the cache itself does not have to&lt;br/&gt;
be thread-safe, because we put it into a ThreadLocal.&lt;/p&gt;

&lt;p&gt;I reran the same performance test that I ran for the first patch and&lt;br/&gt;
this version seems to be even faster: 107secs vs. 112secs with &lt;br/&gt;
the first patch (~30% improvement compared to trunk, 152secs).&lt;/p&gt;

&lt;p&gt;All tests pass, including the improved&lt;br/&gt;
TestIndexReaderReopen.testThreadSafety(), which I ran multiple&lt;br/&gt;
times.&lt;/p&gt;

&lt;p&gt;OK I think this patch is ready now, I&apos;m planning to commit it in a&lt;br/&gt;
day or so.&lt;/p&gt;</comment>
                    <comment id="12599438" author="michaelbusch" created="Fri, 23 May 2008 18:22:36 +0100"  >&lt;p&gt;Committed.&lt;/p&gt;</comment>
                    <comment id="12599606" author="yseeley@gmail.com" created="Sat, 24 May 2008 16:17:38 +0100"  >&lt;blockquote&gt;&lt;p&gt;SegmentTermEnum.scanTo() returns now the number of invocations of next(). TermInfosReader only&lt;br/&gt;
puts TermInfo objects into the cache if scanTo() has called next() more than once. Thus, if e. g.&lt;br/&gt;
a WildcardQuery or RangeQuery iterates over terms in order, only the first term will be put into&lt;br/&gt;
the cache. This is an addition to the ThreadLocal that prevents one thread from wiping out its&lt;br/&gt;
own cache with such a query.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Hmmm, clever, and pretty much free.&lt;/p&gt;

&lt;p&gt;It doesn&apos;t seem like it would eliminate something like a RangeQuery adding to the cache, but does reduce the amount of pollution.  Seems like about 1/64th of the terms would be added to the cache?  (every 128th term and the term following that... due to &quot;numScans &amp;gt; 1&quot; check).&lt;/p&gt;

&lt;p&gt;Still, it would take a range query covering 64K terms to completely wipe the cache, and as long as that range query is slow relative to the term lookups, I suppose it doesn&apos;t matter much if the cache gets wiped anyway.  A single additional hash lookup per term probably shouldn&apos;t slow the execution of something like a range query that much either.&lt;/p&gt;
</comment>
                    <comment id="12630076" author="rengels@ix.netcom.com" created="Thu, 11 Sep 2008 04:12:10 +0100"  >&lt;p&gt;A &quot;safe&quot; ThreadLocal that can be used for more deterministic memory usage.&lt;/p&gt;

&lt;p&gt;Probably a bit slower than the JDK ThreadLocal, due to the synchronization.&lt;/p&gt;

&lt;p&gt;Offers a &quot;purge()&quot; method to force the cleanup of stale entries.  Probably most useful in code like this:&lt;/p&gt;

&lt;p&gt;	SomeLargeObject slo; // maybe a RAMDirectory?&lt;br/&gt;
	try &lt;/p&gt;
{
		slo = new SomeLargeObject(); // or other creation mechanism;
	}
&lt;p&gt; catch (OutOfMemoryException e) &lt;/p&gt;
{
		SafeThreadLocal.purge();
		// now try again
		slo = new SomeLargeObject(); // or other creation mechanism;
	}

</comment>
                    <comment id="12630091" author="rengels@ix.netcom.com" created="Thu, 11 Sep 2008 06:16:28 +0100"  >&lt;p&gt;Also, SafeThreadLocal can be trivially changed to reduce the synchronization times, by using a synchronized map - then only the access is sync&apos;d.&lt;/p&gt;

&lt;p&gt;Since a ThreadLocal in Lucene is primarily read (after initial creation), a 1.5 lock designed for read often, write rarely would be best.&lt;/p&gt;
</comment>
                </comments>
                    <attachments>
                    <attachment id="12382617" name="lucene-1195.patch" size="19200" author="michaelbusch" created="Fri, 23 May 2008 03:18:49 +0100" />
                    <attachment id="12382442" name="lucene-1195.patch" size="15678" author="michaelbusch" created="Wed, 21 May 2008 09:37:32 +0100" />
                    <attachment id="12376663" name="lucene-1195.patch" size="4284" author="michaelbusch" created="Wed, 27 Feb 2008 19:31:30 +0000" />
                    <attachment id="12389898" name="SafeThreadLocal.java" size="1360" author="rengels@ix.netcom.com" created="Thu, 11 Sep 2008 04:12:10 +0100" />
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>4.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Wed, 27 Feb 2008 00:55:11 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>12550</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>26534</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                    <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                <customfieldname>Time in Status</customfieldname>
                <customfieldvalues>
                    
                </customfieldvalues>
            </customfield>
                            </customfields>
    </item>
</channel>
</rss>