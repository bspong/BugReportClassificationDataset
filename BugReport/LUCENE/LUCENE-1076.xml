<!-- 
RSS generated by JIRA (5.2.8#851-sha1:3262fdc28b4bc8b23784e13eadc26a22399f5d88) at Tue Jul 16 13:31:06 UTC 2013

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/LUCENE-1076/LUCENE-1076.xml?field=key&field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>5.2.8</version>
        <build-number>851</build-number>
        <build-date>26-02-2013</build-date>
    </build-info>

<item>
            <title>[LUCENE-1076] Allow MergePolicy to select non-contiguous merges</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-1076</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;I started work on this but with &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-1044&quot; title=&quot;Behavior on hard power shutdown&quot;&gt;&lt;del&gt;LUCENE-1044&lt;/del&gt;&lt;/a&gt; I won&apos;t make much progress&lt;br/&gt;
on it for a while, so I want to checkpoint my current state/patch.&lt;/p&gt;

&lt;p&gt;For backwards compatibility we must leave the default MergePolicy as&lt;br/&gt;
selecting contiguous merges.  This is necessary because some&lt;br/&gt;
applications rely on &quot;temporal monotonicity&quot; of doc IDs, which means&lt;br/&gt;
even though merges can re-number documents, the renumbering will&lt;br/&gt;
always reflect the order in which the documents were added to the&lt;br/&gt;
index.&lt;/p&gt;

&lt;p&gt;Still, for those apps that do not rely on this, we should offer a&lt;br/&gt;
MergePolicy that is free to select the best merges regardless of&lt;br/&gt;
whether they are continuguous.  This requires fixing IndexWriter to&lt;br/&gt;
accept such a merge, and, fixing LogMergePolicy to optionally allow&lt;br/&gt;
it the freedom to do so.&lt;/p&gt;</description>
                <environment></environment>
            <key id="12383870">LUCENE-1076</key>
            <summary>Allow MergePolicy to select non-contiguous merges</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png">Closed</status>
                    <resolution id="1">Fixed</resolution>
                                <assignee username="mikemccand">Michael McCandless</assignee>
                                <reporter username="mikemccand">Michael McCandless</reporter>
                        <labels>
                    </labels>
                <created>Tue, 4 Dec 2007 13:07:04 +0000</created>
                <updated>Fri, 3 Jun 2011 17:37:19 +0100</updated>
                    <resolved>Sat, 14 May 2011 13:45:19 +0100</resolved>
                            <version>2.3</version>
                                <fixVersion>3.2</fixVersion>
                <fixVersion>4.0-ALPHA</fixVersion>
                                <component>core/index</component>
                        <due></due>
                    <votes>0</votes>
                        <watches>4</watches>
                                                    <comments>
                    <comment id="12548250" author="mikemccand" created="Tue, 4 Dec 2007 13:07:41 +0000"  >&lt;p&gt;Attached my current patch.  It compiles, but, quite a few tests fail!&lt;/p&gt;</comment>
                    <comment id="12733770" author="shaie" created="Tue, 21 Jul 2009 19:21:59 +0100"  >&lt;p&gt;So Mike - just to clarify. If I have 3 segments: A (0-52), B (53-124) and C (125-145), and you decide to merge A and C, what will be the new doc IDs of all segments? will they start from 53? or will you shift all the documents so that the segments will be B (0-71) and A+C (72-145)?&lt;/p&gt;</comment>
                    <comment id="12733771" author="mikemccand" created="Tue, 21 Jul 2009 19:30:03 +0100"  >&lt;p&gt;Well... one option might be &quot;the newly merged segment always replaces the leftmost segment&quot;.  Another option could be to leave it undefined, ie IW makes no commitment as to where it will place the newly merged segment so you should not rely on it.  Presumably apps that rely on Lucene&apos;s internal doc ID to &quot;mean something&quot; would not use a merge policy that selects non-contiguous segments.&lt;/p&gt;

&lt;p&gt;Unfortunately, with the current index format, there&apos;s a big cost to allowing non-contiguous segments to be merged: it means the doc stores will always be merged.  Whereas, today, if you build up a large new index, no merging is done for the doc stores.&lt;/p&gt;

&lt;p&gt;If we someday allowed a single segment to reference multiple original doc stores (logically concatenating &lt;span class=&quot;error&quot;&gt;&amp;#91;possibly many&amp;#93;&lt;/span&gt; slices out of them), which would presumably be a perf hit when retrieving the stored doc or term vectors, then this cost would go away.&lt;/p&gt;</comment>
                    <comment id="12733778" author="shaie" created="Tue, 21 Jul 2009 19:46:28 +0100"  >&lt;p&gt;Well ... what I was thinking of is that even if the app does not care about internal doc IDs, the Lucene code may very well care. If we don&apos;t shift doc IDs back, it means maxDoc will continue to grow, and at some point (extreme case though), maxDoc will equal 1M, while there will be just 50K docs in the index.&lt;/p&gt;

&lt;p&gt;AFAIU, maxDoc is used today to determine array length in FieldCache, I&apos;ve seen it used in IndexSearcher to sort the sub readers (at least in the past) etc. So perhaps alongside maxDoc we&apos;ll need to keep a curNumDocs member to track the actual number of documents?&lt;/p&gt;

&lt;p&gt;But I have a feeling this will also get complicated.&lt;/p&gt;</comment>
                    <comment id="12733780" author="mikemccand" created="Tue, 21 Jul 2009 19:51:13 +0100"  >&lt;p&gt;maxDoc() is computed by simply summing docCount of all segments in the index; it shouldn&apos;t ever grow.&lt;/p&gt;</comment>
                    <comment id="12733782" author="shaie" created="Tue, 21 Jul 2009 19:55:12 +0100"  >&lt;p&gt;But how is a new doc ID allocated? Not by calling maxDoc()? So if maxDoc()  = 50K, but there is a document w/ ID 1M (and possibly another one w/ 50K), won&apos;t that be a problem?&lt;/p&gt;</comment>
                    <comment id="12733783" author="shaie" created="Tue, 21 Jul 2009 19:58:23 +0100"  >&lt;p&gt;Besides Mike, there&apos;s something I don&apos;t understand from a previous comment you&apos;ve made: You commented that today if I build a large index, the doc stores are not merged, while if we&apos;ll move to merging non contiguous segments, they will. I&apos;m afraid I&apos;m not familiar with this area of Lucene well &amp;#8211; if I merge two consecutive segments, how come I don&apos;t merge their doc stores?&lt;/p&gt;</comment>
                    <comment id="12733819" author="mikemccand" created="Tue, 21 Jul 2009 21:47:29 +0100"  >&lt;blockquote&gt;&lt;p&gt;But how is a new doc ID allocated?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Doc IDs are logically assigned by summing docCount of all segments before me, as my base, and then adding to the &quot;index&quot; of the doc within my segment.  Ie, the base of a given segment is not stored anywhere, so we are always free to shuffle up the order of segments and nothing in Lucene should care (but, the app might).&lt;/p&gt;</comment>
                    <comment id="12733822" author="mikemccand" created="Tue, 21 Jul 2009 21:57:13 +0100"  >&lt;blockquote&gt;&lt;p&gt;if I merge two consecutive segments, how come I don&apos;t merge their doc stores&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Multiple segments are able to share a single set of doc-store (=&lt;br/&gt;
stored fields &amp;amp; term vectors) files, today.  This only happens when&lt;br/&gt;
multiple segments are written in a single IndexWriter session with&lt;br/&gt;
autoCommit=false.&lt;/p&gt;

&lt;p&gt;EG if I open a writer, index all of wikipedia w/ autoCommit false, and&lt;br/&gt;
close it, you&apos;ll see a single large set of doc store files (eg _0.fdt,&lt;br/&gt;
_0.fdx, _0.tvf, _0.tvd, _0.tvx).&lt;/p&gt;

&lt;p&gt;Whenever RAM is full (with postings &amp;amp; norms data), a new segment is&lt;br/&gt;
flushed, but the doc store files are kept open &amp;amp; shared with further&lt;br/&gt;
flushed segments.&lt;/p&gt;

&lt;p&gt;A single segment then refers to the shared doc stores, but records its&lt;br/&gt;
&quot;offset&quot; within them.&lt;/p&gt;

&lt;p&gt;So, when we merge contiguous segments, because the resulting docs are&lt;br/&gt;
also contiguous in the doc stores, we are able to store a single doc&lt;br/&gt;
store offset in the merged segment, referencing the orignial doc&lt;br/&gt;
store, and it works fine.&lt;/p&gt;

&lt;p&gt;But if we merge non-contiguous segments, we must then pull out &amp;amp; merge&lt;br/&gt;
the &quot;slices&quot; from the doc stores into a new [private to the new&lt;br/&gt;
segment] set of doc store files.&lt;/p&gt;

&lt;p&gt;For apps that store term vectors w/ positions &amp;amp; offsets, and have many&lt;br/&gt;
stored fields, and have heterogenous field name -&amp;gt; number assignments&lt;br/&gt;
(see &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-1737&quot; title=&quot;Always use bulk-copy when merging stored fields and term vectors&quot;&gt;&lt;del&gt;LUCENE-1737&lt;/del&gt;&lt;/a&gt; to fix that), the merging of doc stores can easily&lt;br/&gt;
dominate the merge cost.&lt;/p&gt;</comment>
                    <comment id="12733831" author="tsmith" created="Tue, 21 Jul 2009 22:16:42 +0100"  >&lt;p&gt;i suppose you could do a preliminary round of merging that would merge together segments that share doc store/termvector data&lt;/p&gt;

&lt;p&gt;once this preliminary round of merging is done, you would then no longer have the need to slice the doc stores up, just merge them together (contiguous or non-contiguous wouldn&apos;t matter anymore, however if a &quot;segmented session&quot; still exists higher up, this would prevent you from selecting these segments, or newer segments)&lt;/p&gt;

&lt;p&gt;it might even be desirable to have a &quot;commit()&quot; optionally perform this merging prior to the commit finishing as this will result in each commit producing one segment, regardless of the number of flushes that were done under the hood&lt;/p&gt;</comment>
                    <comment id="12733844" author="jasonrutherglen" created="Tue, 21 Jul 2009 22:51:36 +0100"  >&lt;blockquote&gt;&lt;p&gt;if I merge two consecutive segments, how come I don&apos;t&lt;br/&gt;
merge their doc stores?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;You may want to take a look at&lt;br/&gt;
SegmentInfo.docStoreOffset/docStoreSegment which is the pointer&lt;br/&gt;
to the docstore file data for that SI. &lt;/p&gt;</comment>
                    <comment id="12734146" author="shaie" created="Wed, 22 Jul 2009 16:08:19 +0100"  >&lt;p&gt;Thanks for the education everyone.&lt;/p&gt;

&lt;p&gt;Mike - it feels to me, even though I can&apos;t pin point it at the moment (FieldCache maybe?), that if maxDoc won&apos;t reflect the number of documents in the index we&apos;ll run into troubles. Therefore I suggest you consider introducing another numDocs() method which returns the actual number of documents there are in the index.&lt;/p&gt;</comment>
                    <comment id="12734169" author="mikemccand" created="Wed, 22 Jul 2009 16:55:34 +0100"  >&lt;p&gt;maxDoc() does reflect the number of docs in the index.  It&apos;s simply the sum of docCount for all segments.  Shuffling the order of the segments, or allowing non-contiguous segments to be merged, won&apos;t change how maxDoc() is computed.&lt;/p&gt;

&lt;p&gt;New docIDs are allocating by incrementing an integer (starting with 0) for the buffered docs.  When a segment gets flushed, we reset that to 0.  Ie, docIDs are stored within one segment; they have no &quot;context&quot; from prior segments.&lt;/p&gt;</comment>
                    <comment id="12734174" author="shaie" created="Wed, 22 Jul 2009 17:01:18 +0100"  >&lt;p&gt;Oh. Thanks for correcting me. In that case, I take what I said back.&lt;/p&gt;

&lt;p&gt;I think this together w/ &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-1750&quot; title=&quot;Create a MergePolicy that limits the maximum size of it&amp;#39;s segments&quot;&gt;&lt;del&gt;LUCENE-1750&lt;/del&gt;&lt;/a&gt; can really help speed up segment merges in certain scenarios. Will wait for you to come back to it &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                    <comment id="12734190" author="mikemccand" created="Wed, 22 Jul 2009 18:01:23 +0100"  >&lt;blockquote&gt;&lt;p&gt;Will wait for you to come back to it&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Feel free to take it, too &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;I think &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-1737&quot; title=&quot;Always use bulk-copy when merging stored fields and term vectors&quot;&gt;&lt;del&gt;LUCENE-1737&lt;/del&gt;&lt;/a&gt; is also very important for speeding up merging, especially because it&apos;s so &quot;unexpected&quot; that just by adding different fields to your docs, or the same fields in different orders, can so severely impact merge performance.&lt;/p&gt;</comment>
                    <comment id="12734191" author="mikemccand" created="Wed, 22 Jul 2009 18:02:12 +0100"  >&lt;p&gt;Unassigning myself.&lt;/p&gt;</comment>
                    <comment id="12734194" author="shaie" created="Wed, 22 Jul 2009 18:06:54 +0100"  >&lt;blockquote&gt;&lt;p&gt;Feel free to take it, too&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I don&apos;t mind to take a stab at it. But this doesn&apos;t mean you can unassign yourself. I&apos;ll need someone to commit it &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;.&lt;/p&gt;</comment>
                    <comment id="12735905" author="shaie" created="Tue, 28 Jul 2009 04:46:18 +0100"  >&lt;p&gt;Can someone please help me understand what&apos;s going on here? After I applied the patch to trunk, TestIndexWriter.testOptimizeMaxNumSegments2() fails. The failure happens only if CMS is used, and doesn&apos;t when SMS is used. I dug deeper into the test and what happens is that the test asks to optimize(maxNumSegments) and expects that either: (1) if the number of segments was &amp;lt; maxNumSegments than the resulting number of segments is exactly as it was before and (2) otherwise it should be exactly maxNumSegments.&lt;/p&gt;

&lt;p&gt;First, the javadocs of optimize(maxNumSegments) say that it will result in &amp;lt;= maxNumSegments, but I understand the LogMergePolicy ensures that if you ask for maxNumSegments, that&apos;s the number of segments you&apos;ll get.&lt;/p&gt;

&lt;p&gt;While trying to debug what&apos;s wrong w/ the change so far, I managed to reduce the test to this code:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; void test1() &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; Exception {
    MockRAMDirectory dir = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; MockRAMDirectory();

    &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; Document doc = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; Document();
    doc.add(&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; Field(&lt;span class=&quot;code-quote&quot;&gt;&quot;content&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;aaa&quot;&lt;/span&gt;, Field.Store.YES, Field.Index.ANALYZED));

    IndexWriter writer  = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; IndexWriter(dir, &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; WhitespaceAnalyzer(), &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;, IndexWriter.MaxFieldLength.LIMITED);
&lt;span class=&quot;code-comment&quot;&gt;//    writer.setMergeScheduler(&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; SerialMergeScheduler());
&lt;/span&gt;    LogDocMergePolicy ldmp = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; LogDocMergePolicy();
    ldmp.setMinMergeDocs(1);
    writer.setMergePolicy(ldmp);
    writer.setMergeFactor(3);
    writer.setMaxBufferedDocs(2);

    MergeScheduler ms = writer.getMergeScheduler();
&lt;span class=&quot;code-comment&quot;&gt;//  writer.setInfoStream(&lt;span class=&quot;code-object&quot;&gt;System&lt;/span&gt;.out);
&lt;/span&gt;    
    &lt;span class=&quot;code-comment&quot;&gt;// Add enough documents to create several segments (uncomitted) and kick off
&lt;/span&gt;    &lt;span class=&quot;code-comment&quot;&gt;// some threads.
&lt;/span&gt;    &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; (&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; i = 0; i &amp;lt; 20; i++) {
      writer.addDocument(doc);
    }
    writer.commit();
    
    &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (ms &lt;span class=&quot;code-keyword&quot;&gt;instanceof&lt;/span&gt; ConcurrentMergeScheduler) {
      &lt;span class=&quot;code-comment&quot;&gt;// Wait &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; all merges to complete
&lt;/span&gt;      ((ConcurrentMergeScheduler) writer.getMergeScheduler()).sync();
    }
    
    SegmentInfos sis = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; SegmentInfos();
    sis.read(dir);
    
    &lt;span class=&quot;code-object&quot;&gt;System&lt;/span&gt;.out.println(&lt;span class=&quot;code-quote&quot;&gt;&quot;numSegments after add + commit ==&amp;gt; &quot;&lt;/span&gt; + sis.size());
    
    &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; segCount = sis.size();
    
    &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; maxNumSegments = 3;
    writer.optimize(maxNumSegments);
    writer.commit();
    
    &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (ms &lt;span class=&quot;code-keyword&quot;&gt;instanceof&lt;/span&gt; ConcurrentMergeScheduler) {
      &lt;span class=&quot;code-comment&quot;&gt;// Wait &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; all merges to complete
&lt;/span&gt;      ((ConcurrentMergeScheduler) writer.getMergeScheduler()).sync();
    }
    
    sis = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; SegmentInfos();
    sis.read(dir);
    &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; optSegCount = sis.size();
    
    &lt;span class=&quot;code-object&quot;&gt;System&lt;/span&gt;.out.println(&lt;span class=&quot;code-quote&quot;&gt;&quot;numSegments after optimize (&quot;&lt;/span&gt; + maxNumSegments + &lt;span class=&quot;code-quote&quot;&gt;&quot;) + commit ==&amp;gt; &quot;&lt;/span&gt; + sis.size());
    
    &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (segCount &amp;lt; maxNumSegments)
      Assert.assertEquals(segCount, optSegCount);
    &lt;span class=&quot;code-keyword&quot;&gt;else&lt;/span&gt;
      Assert.assertEquals(maxNumSegments, optSegCount);
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This fails almost every time that I run it, so if you try it - make sure to run it a couple of times. I then switched to trunk, but it fails almost consistently on trunk also !?!?&lt;/p&gt;

&lt;p&gt;Can someone please have a look and tell me what&apos;s wrong (is it the test, or did I hit a true bug in the code?)?&lt;/p&gt;</comment>
                    <comment id="12735968" author="shaie" created="Tue, 28 Jul 2009 08:40:38 +0100"  >&lt;p&gt;Hmm ... I think I found the problem. I added commit() after cms.sync() and it never failed again. So I checked the output of infoStream in two cases (failure and success) and found this: in the success case, the pending merges occurred before the last addDocument calls happened (actually the last 2), therefore commit() committed those pending merges output, and sync() afterwards did nothing.&lt;/p&gt;

&lt;p&gt;In the failure case, the last pending merge happened &lt;b&gt;after&lt;/b&gt; commit() was called, either as (or not) part of the sync() call, but it was never committed.&lt;/p&gt;

&lt;p&gt;So it looks to me that I should add this test case as testOptimizeMaxNumSegments3() (even though it has nothing to do w/ optimize()), just to cover this aspect and also document CMS.sync() to mention that a commit() after it is required if the outcome of the merges should be reflected in the index (i.e., committed).&lt;/p&gt;

&lt;p&gt;Did I get it right?&lt;/p&gt;</comment>
                    <comment id="12735971" author="shaie" created="Tue, 28 Jul 2009 08:45:59 +0100"  >&lt;p&gt;BTW, the second sync() call comes after optimize(), which is redundant as far as I understand, since optimize() or optimize(int) will wait for all merges to complete, which CMS merges.&lt;/p&gt;

&lt;p&gt;I wonder then if it won&apos;t be useful to have a commit(doWait=true), which won&apos;t require calling sync() or waitForMerges()?&lt;/p&gt;</comment>
                    <comment id="12736141" author="mikemccand" created="Tue, 28 Jul 2009 18:01:44 +0100"  >&lt;blockquote&gt;&lt;p&gt;I added commit() after cms.sync() and it never failed again. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think you are right!  Since we try to read the dir (sis.read), if we don&apos;t commit then the changes won&apos;t be present since IndexWriter is opened w/ autoCommit false.  Another simple check would be to call getReader().getSequentialSubReaders() and check how many segments there are, instead of having to go through the Directory to check it.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;BTW, the second sync() call comes after optimize(), which is redundant as far as I understand&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I agree.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I wonder then if it won&apos;t be useful to have a commit(doWait=true), which won&apos;t require calling sync() or waitForMerges()?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think we can leave this separate (ie you should call waitForMerges() if you need to), because commit normally has nothing to do w/ merging, since merging doesn&apos;t change any docs in the index.  Commit only ensure that changes to the index are pushed to stable storage.  Whereas eg optimize is all about doing merges so it makes sense for it to have a doWait?&lt;/p&gt;</comment>
                    <comment id="12736206" author="shaie" created="Tue, 28 Jul 2009 19:32:49 +0100"  >&lt;p&gt;Ok I agree that commit should not wait for merges. It does seem not related to segment merging.&lt;/p&gt;</comment>
                    <comment id="12985900" author="mikemccand" created="Mon, 24 Jan 2011 18:55:56 +0000"  >&lt;p&gt;I&apos;ll take a crack at this.  It&apos;s compelling, now that we always bulk merge doc stores...&lt;/p&gt;</comment>
                    <comment id="12988308" author="mikemccand" created="Fri, 28 Jan 2011 23:26:56 +0000"  >&lt;p&gt;Patch.  I think it&apos;s ready to commit...&lt;/p&gt;

&lt;p&gt;To stress test ooo merging, I made a fun new MockRandomMergePolicy&lt;br/&gt;
(swapped in half the time by LTC.newIndexWriterConfig) which randomly&lt;br/&gt;
decides when to do a merge, then randomly picks how many segments to&lt;br/&gt;
merge, and then randomly picks which ones.  Also, I modified&lt;br/&gt;
LogMergePolicy to add a boolean get/setRequireContiguousMerge,&lt;br/&gt;
defaulting to false.&lt;/p&gt;

&lt;p&gt;Many tests rely on in-order docIDs during merging, so I had to wire&lt;br/&gt;
them to use in-order LogMP.&lt;/p&gt;

&lt;p&gt;I also reworked how buffered deletes are managed, so that each&lt;br/&gt;
&quot;packet&quot; of buffered deletes, as well as each flushed segment, is now&lt;br/&gt;
assigned an incrementing gen.  This way, when it&apos;s time to apply&lt;br/&gt;
deletes, the algorithm is easy: only delete packets with gen &amp;gt;= this&lt;br/&gt;
segment should coalesce and apply.&lt;/p&gt;

&lt;p&gt;Separately, eventually, I&apos;d like to switch to a better default MP,&lt;br/&gt;
something like BSMP where immense merges are done w/ small mergeFactor&lt;br/&gt;
(eg, 2), and tiny merges are done w/ large mergeFactor.&lt;/p&gt;</comment>
                    <comment id="12988356" author="jasonrutherglen" created="Sat, 29 Jan 2011 01:45:40 +0000"  >&lt;blockquote&gt;&lt;p&gt;I also reworked how buffered deletes are managed, so that each&lt;br/&gt;
&quot;packet&quot; of buffered deletes, as well as each flushed segment, is now&lt;br/&gt;
assigned an incrementing gen.  This way, when it&apos;s time to apply&lt;br/&gt;
deletes, the algorithm is easy: only delete packets with gen &amp;gt;= this&lt;br/&gt;
segment should coalesce and apply.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I saw this and thought it was interesting.  Why is the gen needed?&lt;/p&gt;</comment>
                    <comment id="12988458" author="mikemccand" created="Sat, 29 Jan 2011 13:36:06 +0000"  >&lt;blockquote&gt;&lt;p&gt;I saw this and thought it was interesting. Why is the gen needed?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;So, at first I added it because the pushing of merged delete packets&lt;br/&gt;
got too hairy, eg when merges interleave you&apos;d have to handle deletes&lt;br/&gt;
being pushed onto each other&apos;s internal merged segments.&lt;/p&gt;

&lt;p&gt;Also, we really needed a transactional data structure here, because&lt;br/&gt;
before DW could push more deletes into an existing packet (ie the&lt;br/&gt;
packet was not write once), which made tracking problematic if the&lt;br/&gt;
merge wanted to record that the first batch of deletes had been&lt;br/&gt;
applied but not any subsequent pushes.&lt;/p&gt;

&lt;p&gt;But, after making the change, I realized that today (trunk, 3.1) we&lt;br/&gt;
are badly inefficient!  We apply deletes to segments being merged, but&lt;br/&gt;
then we place the merged segment back in the same position.  This is&lt;br/&gt;
inefficient because later when this segment gets merged, we wastefully&lt;br/&gt;
re-apply the same deletes (plus, new ones, which do need to be&lt;br/&gt;
applied).  This is a total waste.&lt;/p&gt;

&lt;p&gt;So, by decoupling tracking of where you are in the deletes packet&lt;br/&gt;
stream, from the physical location of your segment in the index, we&lt;br/&gt;
fix this waste.  Also, it&apos;s quite a bit simpler now &amp;#8211; we no longer&lt;br/&gt;
have to merge deletes on completing a merge.&lt;/p&gt;</comment>
                    <comment id="12988465" author="mikemccand" created="Sat, 29 Jan 2011 15:18:56 +0000"  >&lt;p&gt;Another benefit of the transaction log for deletes is, because they are write-once (ie, after a set of buffered deletes is pushed, they are never changed), we can switch to a more efficient data structure than TreeMap on push.&lt;/p&gt;

&lt;p&gt;Eg, we can pull the del Terms (sorted) and store them in an array.&lt;/p&gt;</comment>
                    <comment id="12988499" author="mikemccand" created="Sat, 29 Jan 2011 19:49:49 +0000"  >&lt;p&gt;Committed to trunk; I&apos;ll let this age for a while before back porting.&lt;/p&gt;

&lt;p&gt;On the backport, I&apos;ll leave the default contiguous merges.&lt;/p&gt;</comment>
                    <comment id="12990127" author="mikemccand" created="Thu, 3 Feb 2011 15:25:27 +0000"  >&lt;p&gt;I think we can do this for 3.1, but I&apos;ll leave the default as always doing contiguous merges.&lt;/p&gt;</comment>
                    <comment id="12990541" author="mikemccand" created="Fri, 4 Feb 2011 12:06:07 +0000"  >&lt;p&gt;I changed my mind! Pushing this to 3.2 now.&lt;/p&gt;</comment>
                    <comment id="13014728" author="mikemccand" created="Fri, 1 Apr 2011 17:41:02 +0100"  >&lt;p&gt;Phew, this patch almost fell below the event horizon of my TODO list...&lt;/p&gt;

&lt;p&gt;I&apos;m attaching new modernized one.  I also mod&apos;d the policy to not select two max-sized merges at once.  I think it&apos;s ready to commit...&lt;/p&gt;</comment>
                    <comment id="13028176" author="shaie" created="Tue, 3 May 2011 13:09:00 +0100"  >&lt;p&gt;Patch against 3x. This is not ready to commit yet, as many tests fail on exceptions like this:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;    [junit] java.lang.IndexOutOfBoundsException
    [junit]     at java.util.AbstractList.subList(AbstractList.java:763)
    [junit]     at java.util.Vector.subList(Vector.java:975)
    [junit]     at org.apache.lucene.index.IndexWriter.commitMerge(IndexWriter.java:3550)
    [junit]     at org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:4057)
    [junit]     at org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:3631)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Mike says there was  an earlier commit (handled how deletes are flushed) that is a dependency of that, and that I can continue only he back-ports that.&lt;/p&gt;

&lt;p&gt;In the meantime, I&apos;ve fixed tests that assumed LogMP (for setting compound and mergeFactor) by adding LTC.setUseCompoundFile and LTC.setMergeFactor as utility methods.&lt;/p&gt;

&lt;p&gt;Will continue after Mike back-ports the dependencies.&lt;/p&gt;</comment>
                    <comment id="13029650" author="mikemccand" created="Fri, 6 May 2011 00:23:50 +0100"  >&lt;p&gt;Thanks Shai!&lt;/p&gt;</comment>
                    <comment id="13031293" author="mikemccand" created="Tue, 10 May 2011 19:34:18 +0100"  >&lt;p&gt;I think we should change 3.2&apos;s default MP to TieredMP?&lt;/p&gt;

&lt;p&gt;This means docIDs may be reordered, since Tiered MP can merge out-of-order segments.&lt;/p&gt;</comment>
                    <comment id="13032134" author="rcmuir" created="Wed, 11 May 2011 23:26:52 +0100"  >&lt;p&gt;+1 Mike&lt;/p&gt;</comment>
                    <comment id="13032274" author="simonw" created="Thu, 12 May 2011 07:41:43 +0100"  >&lt;blockquote&gt;&lt;p&gt;This means docIDs may be reordered, since Tiered MP can merge out-of-order segments.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I think this is a very hard break and it should depend on the Version you pass to IWC. Stuff like that is really a good usecase for Version. I had customers in the past that heavily depend on the lucene doc ID while it is not recommended but with this change their app will suddenly not work anymore. so we should make sure that they can upgrade seamlessly!&lt;/p&gt;</comment>
                    <comment id="13032282" author="thetaphi" created="Thu, 12 May 2011 08:09:39 +0100"  >&lt;blockquote&gt;

&lt;blockquote&gt;&lt;p&gt;This means docIDs may be reordered, since Tiered MP can merge out-of-order segments.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I think this is a very hard break and it should depend on the Version you pass to IWC. Stuff like that is really a good usecase for Version. I had customers in the past that heavily depend on the lucene doc ID while it is not recommended but with this change their app will suddenly not work anymore. so we should make sure that they can upgrade seamlessly!&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think we should also warn people that have this problem to use IndexUpgrader (&lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-3082&quot; title=&quot;Add tool to upgrade all segments of an index to last recent supported index format without optimizing&quot;&gt;&lt;del&gt;LUCENE-3082&lt;/del&gt;&lt;/a&gt;), because it has the same problem. Segments are reordered (segments that were upgraded before a call to MP&apos;s optimize come first, then the upgraded ones). Maybe we should add this to JavaDocs in 3.x.&lt;/p&gt;

&lt;p&gt;I&apos;ll reopen &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-3082&quot; title=&quot;Add tool to upgrade all segments of an index to last recent supported index format without optimizing&quot;&gt;&lt;del&gt;LUCENE-3082&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;</comment>
                    <comment id="13032344" author="mikemccand" created="Thu, 12 May 2011 11:32:38 +0100"  >&lt;blockquote&gt;&lt;p&gt;I think this is a very hard break and it should depend on the Version you pass to IWC.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;+1&lt;/p&gt;

&lt;p&gt;I&apos;ll make it TieredMP if version &amp;gt;= 3.2, else LogByteSizeMP.&lt;/p&gt;</comment>
                    <comment id="13032497" author="mikemccand" created="Thu, 12 May 2011 17:37:44 +0100"  >&lt;p&gt;Patch against 3.x, changing default if matchVersion is &amp;gt;= LUCENE_32 passed to IWC.&lt;/p&gt;</comment>
                    <comment id="13043519" author="rcmuir" created="Fri, 3 Jun 2011 17:37:19 +0100"  >&lt;p&gt;Bulk closing for 3.2&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12478049" name="LUCENE-1076-3x.patch" size="54517" author="shaie" created="Tue, 3 May 2011 13:09:00 +0100" />
                    <attachment id="12478989" name="LUCENE-1076.patch" size="10675" author="mikemccand" created="Thu, 12 May 2011 17:37:44 +0100" />
                    <attachment id="12475238" name="LUCENE-1076.patch" size="114325" author="mikemccand" created="Fri, 1 Apr 2011 17:41:02 +0100" />
                    <attachment id="12469713" name="LUCENE-1076.patch" size="134759" author="mikemccand" created="Fri, 28 Jan 2011 23:26:56 +0000" />
                    <attachment id="12370939" name="LUCENE-1076.patch" size="8477" author="mikemccand" created="Tue, 4 Dec 2007 13:07:41 +0000" />
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>5.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Tue, 21 Jul 2009 18:21:59 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>12669</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>26653</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                    <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                <customfieldname>Time in Status</customfieldname>
                <customfieldvalues>
                    
                </customfieldvalues>
            </customfield>
                            </customfields>
    </item>
</channel>
</rss>