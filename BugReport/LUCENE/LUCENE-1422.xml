<!-- 
RSS generated by JIRA (5.2.8#851-sha1:3262fdc28b4bc8b23784e13eadc26a22399f5d88) at Tue Jul 16 13:05:39 UTC 2013

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/LUCENE-1422/LUCENE-1422.xml?field=key&field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>5.2.8</version>
        <build-number>851</build-number>
        <build-date>26-02-2013</build-date>
    </build-info>

<item>
            <title>[LUCENE-1422] New TokenStream API</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-1422</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;This is a very early version of the new TokenStream API that &lt;br/&gt;
we started to discuss here:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.gossamer-threads.com/lists/lucene/java-dev/66227&quot; class=&quot;external-link&quot;&gt;http://www.gossamer-threads.com/lists/lucene/java-dev/66227&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This implementation is a bit different from what I initially&lt;br/&gt;
proposed in the thread above. I introduced a new class called&lt;br/&gt;
AttributedToken, which contains the same termBuffer logic &lt;br/&gt;
from Token. In addition it has a lazily-initialized map of&lt;br/&gt;
Class&amp;lt;? extends Attribute&amp;gt; -&amp;gt; Attribute. Attribute is also a&lt;br/&gt;
new class in a new package, plus several implementations like&lt;br/&gt;
PositionIncrementAttribute, PayloadAttribute, etc.&lt;/p&gt;

&lt;p&gt;Similar to my initial proposal is the prototypeToken() method&lt;br/&gt;
which the consumer (e. g. DocumentsWriter) needs to call.&lt;br/&gt;
The token is created by the tokenizer at the end of the chain&lt;br/&gt;
and pushed through all filters to the end consumer. The &lt;br/&gt;
tokenizer and also all filters can add Attributes to the &lt;br/&gt;
token and can keep references to the actual types of the&lt;br/&gt;
attributes that they need to read of modify. This way, when&lt;br/&gt;
boolean nextToken() is called, no casting is necessary.&lt;/p&gt;

&lt;p&gt;I added a class called TestNewTokenStreamAPI which is not &lt;br/&gt;
really a test case yet, but has a static demo() method, which&lt;br/&gt;
demonstrates how to use the new API.&lt;/p&gt;

&lt;p&gt;The reason to not merge Token and TokenStream into one class &lt;br/&gt;
is that we might have caching (or tee/sink) filters in the &lt;br/&gt;
chain that might want to store cloned copies of the tokens&lt;br/&gt;
in a cache. I added a new class NewCachingTokenStream that&lt;br/&gt;
shows how such a class could work. I also implemented a deep&lt;br/&gt;
clone method in AttributedToken and a &lt;br/&gt;
copyFrom(AttributedToken) method, which is needed for the &lt;br/&gt;
caching. Both methods have to iterate over the list of &lt;br/&gt;
attributes. The Attribute subclasses itself also have a&lt;br/&gt;
copyFrom(Attribute) method, which unfortunately has to down-&lt;br/&gt;
cast to the actual type. I first thought that might be very&lt;br/&gt;
inefficient, but it&apos;s not so bad. Well, if you add all&lt;br/&gt;
Attributes to the AttributedToken that our old Token class&lt;br/&gt;
had (like offsets, payload, posIncr), then the performance&lt;br/&gt;
of the caching is somewhat slower (~40%). However, if you &lt;br/&gt;
add less attributes, because not all might be needed, then&lt;br/&gt;
the performance is even slightly faster than with the old API.&lt;br/&gt;
Also the new API is flexible enough so that someone could&lt;br/&gt;
implement a custom caching filter that knows all attributes&lt;br/&gt;
the token can have, then the caching should be just as &lt;br/&gt;
fast as with the old API.&lt;/p&gt;


&lt;p&gt;This patch is not nearly ready, there are lot&apos;s of things &lt;br/&gt;
missing:&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;unit tests&lt;/li&gt;
	&lt;li&gt;change DocumentsWriter to use new API&lt;br/&gt;
  (in backwards-compatible fashion)&lt;/li&gt;
	&lt;li&gt;patch is currently java 1.5; need to change before&lt;br/&gt;
  commiting to 2.9&lt;/li&gt;
	&lt;li&gt;all TokenStreams and -Filters should be changed to use&lt;br/&gt;
  new API&lt;/li&gt;
	&lt;li&gt;javadocs incorrect or missing&lt;/li&gt;
	&lt;li&gt;hashcode and equals methods missing in Attributes and&lt;br/&gt;
  AttributedToken&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I wanted to submit it already for brave people to give me &lt;br/&gt;
early feedback before I spend more time working on this.&lt;/p&gt;</description>
                <environment></environment>
            <key id="12406373">LUCENE-1422</key>
            <summary>New TokenStream API</summary>
                <type id="2" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/newfeature.png">New Feature</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png">Closed</status>
                    <resolution id="1">Fixed</resolution>
                                <assignee username="michaelbusch">Michael Busch</assignee>
                                <reporter username="michaelbusch">Michael Busch</reporter>
                        <labels>
                    </labels>
                <created>Tue, 14 Oct 2008 09:48:53 +0100</created>
                <updated>Fri, 25 Sep 2009 17:23:09 +0100</updated>
                    <resolved>Thu, 11 Dec 2008 18:48:09 +0000</resolved>
                                            <fixVersion>2.9</fixVersion>
                                <component>modules/analysis</component>
                        <due></due>
                    <votes>0</votes>
                        <watches>0</watches>
                                                    <comments>
                    <comment id="12639495" author="cutting" created="Tue, 14 Oct 2008 18:16:09 +0100"  >&lt;p&gt;prototypeToken() and nextToken() seem not the most descriptive names here.  Perhaps getToken() and incrementToken() would be better?  Also, would it be better to make the prototypeToken/getToken implementations idempotent?  This might then look like, e.g.:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; AttributedToken getToken() &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; IOException {
  &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (token == &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;) {
    token = input.getToken();
    positionIncrement = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; PositionIncrementAttribute();
    token.addAttribute(positionIncrement);
  }
  &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; token;
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;I&apos;m also not fond of the name AttributedToken, but don&apos;t yet have a better suggestion...&lt;/p&gt;
</comment>
                    <comment id="12639811" author="michaelbusch" created="Wed, 15 Oct 2008 12:15:31 +0100"  >&lt;p&gt;Thanks for your suggestions, Doug. It makes perfect sense to make getToken() idempotent.&lt;br/&gt;
Also, addAttribute() should be idempotent, because a Token can have only one instance &lt;br/&gt;
of the an attribute.&lt;/p&gt;

&lt;p&gt;I changed prototypeToken() to getToken(), nextToken() to incrementToken and actually &lt;br/&gt;
added the attribute logic to Token itself. That has some advantages, but also the &lt;br/&gt;
following disadvantage. If people want to use the new API before 3.0, i. e. before the&lt;br/&gt;
deprecated members of Token have been removed, and they want to use something like the&lt;br/&gt;
CachingTokenFilter or Tee/Sink-TokenFilter, then caching is more expensive. The reson &lt;br/&gt;
is that Token itself has members for positionIncrement, offsets, etc. but you then also &lt;br/&gt;
have to add the appropriate attributes to Token to use the new API. But I think this&lt;br/&gt;
drawback would be acceptable?&lt;/p&gt;

&lt;p&gt;I also changed the way to add attributes to a Token:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;&lt;span class=&quot;code-keyword&quot;&gt;protected&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; void addTokenAttributes() {
  posIncrAtt = reusableToken.addAttribute(PositionIncrementAttribute.class);
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The addTokenAttributes() method belongs to TokenStream and is called from getToken()&lt;br/&gt;
only when a new Token instance was created, i. e. in its first call.&lt;/p&gt;

&lt;p&gt;Note the signature of Token.addAttribute:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &amp;lt;T &lt;span class=&quot;code-keyword&quot;&gt;extends&lt;/span&gt; Attribute&amp;gt; T addAttribute(&lt;span class=&quot;code-object&quot;&gt;Class&lt;/span&gt;&amp;lt;T&amp;gt; attClass);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now you don&apos;t pass in an actual instance of *Attribute, but its class. The method will&lt;br/&gt;
then create a new instance via reflection. This approach makes the addAttribute() &lt;br/&gt;
method itself idempotent.&lt;/p&gt;

&lt;p&gt;I changed all core tokenizers and filters to have an implementation of the new API.&lt;/p&gt;

&lt;p&gt;For backwards-compatibility I added a (deprecated) static setUseNewAPI() method to &lt;br/&gt;
TokenStream. I also changed the DocumentsWriter to use the new API in case &lt;br/&gt;
useNewAPI==true;&lt;/p&gt;

&lt;p&gt;I still have to do several things, including javadocs, testcases, hashcode(), etc.&lt;/p&gt;</comment>
                    <comment id="12641273" author="michaelbusch" created="Tue, 21 Oct 2008 06:19:01 +0100"  >&lt;p&gt;I added several things in this new patch:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;hashCode() and equals() now incorporate the attributes&lt;/li&gt;
	&lt;li&gt;patch compiles against Java 1.4&lt;/li&gt;
	&lt;li&gt;all core tests pass with and without the new API turned&lt;br/&gt;
   on (via TokenStream.setUseNewAPI(true))&lt;/li&gt;
	&lt;li&gt;Added setToken() method to InvertedDocConsumerPerField&lt;br/&gt;
   and TermsHashConsumerPerField and updated the &lt;br/&gt;
   implementing classes. I have actually a question here,&lt;br/&gt;
   because I don&apos;t know these classes very well yet. Would&lt;br/&gt;
   it be better to add the Token to the DocInverter.FieldInvertState?&lt;br/&gt;
   I think I also have to review &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-1426&quot; title=&quot;Next steps towards flexible indexing&quot;&gt;&lt;del&gt;LUCENE-1426&lt;/del&gt;&lt;/a&gt; to see if these&lt;br/&gt;
   changes are not in conflict ( I think 1426 should be committed&lt;br/&gt;
   first?)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Outstanding:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;dedicated junits for new APIs, even though the existing tests&lt;br/&gt;
   already cover a lot when setUseNewAPI(true)&lt;/li&gt;
	&lt;li&gt;javadocs&lt;/li&gt;
	&lt;li&gt;contrib streams and filters&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="12641309" author="mikemccand" created="Tue, 21 Oct 2008 09:48:18 +0100"  >&lt;p&gt;Michael I think you left out oal.analysis.tokenattributes.* from your patch?  (I&apos;m trying to compile things).&lt;/p&gt;</comment>
                    <comment id="12641322" author="mikemccand" created="Tue, 21 Oct 2008 10:47:27 +0100"  >&lt;blockquote&gt;&lt;p&gt;Would it be better to add the Token to the DocInverter.FieldInvertState?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think it would be slightly better, since it&apos;d save a the recursive method calls to setToken mirroring what the &quot;add(Token)&quot; is about to do anyway, but the problem is the token can be different for each Field being added.  EG, I could have a doc with 10 instances of field &quot;foo&quot;, whereby each instance is doing a different analysis and so the reused token may change.  But the InvertedDocConsumer API doesn&apos;t have a start() method for each instance of a field, and so there&apos;s no clean place for each consumer to lookup the attribute(s) it needs to use?  Maybe you could add such a start method, and then switch to putting the Token into FieldInvertState?&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;I think I also have to review &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-1426&quot; title=&quot;Next steps towards flexible indexing&quot;&gt;&lt;del&gt;LUCENE-1426&lt;/del&gt;&lt;/a&gt; to see if these&lt;br/&gt;
changes are not in conflict ( I think 1426 should be committed&lt;br/&gt;
first?)&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;OK I will commit &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-1426&quot; title=&quot;Next steps towards flexible indexing&quot;&gt;&lt;del&gt;LUCENE-1426&lt;/del&gt;&lt;/a&gt; first.  I think mostly there won&apos;t be conflicts, except in FreqProxTermsWriterPerThread which should be minor.  &lt;/p&gt;</comment>
                    <comment id="12641329" author="michaelbusch" created="Tue, 21 Oct 2008 10:59:14 +0100"  >&lt;p&gt;Oups, sorry, this should work now.&lt;/p&gt;

&lt;p&gt;Another question: Is it correct that the DocInverterPerField&lt;br/&gt;
only takes into account the endOffset(), not the startOffset()?&lt;/p&gt;</comment>
                    <comment id="12641334" author="mikemccand" created="Tue, 21 Oct 2008 11:20:12 +0100"  >&lt;blockquote&gt;
&lt;p&gt;Another question: Is it correct that the DocInverterPerField&lt;br/&gt;
only takes into account the endOffset(), not the startOffset()?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think it&apos;s correct, if rather confusing: DocInverterPerField tracks endOffset only for shifting the offsets of Tokens when a document has more than one Fieldable instance per textual field.  (Other consumers, like FreqProxTermsWriter do need to pay attention to both the start &amp;amp; end offset).&lt;/p&gt;</comment>
                    <comment id="12641530" author="michaelbusch" created="Tue, 21 Oct 2008 20:08:16 +0100"  >&lt;blockquote&gt;
&lt;p&gt;I think it would be slightly better, since it&apos;d save a the recursive method calls to setToken mirroring what the &quot;add(Token)&quot; is about to do anyway, but the problem is the token can be different for each Field being added. EG, I could have a doc with 10 instances of field &quot;foo&quot;, whereby each instance is doing a different analysis and so the reused token may change. But the InvertedDocConsumer API doesn&apos;t have a start() method for each instance of a field, and so there&apos;s no clean place for each consumer to lookup the attribute(s) it needs to use? Maybe you could add such a start method, and then switch to putting the Token into FieldInvertState?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yeah I agree, that&apos;s better. I should probably add a start(Fieldable) method? I currently don&apos;t need to access the Fieldable instance in the start method, but it can&apos;t hurt to pass it along?&lt;/p&gt;</comment>
                    <comment id="12641549" author="mikemccand" created="Tue, 21 Oct 2008 20:29:58 +0100"  >&lt;blockquote&gt;&lt;p&gt;I should probably add a start(Fieldable) method? I currently don&apos;t need to access the Fieldable instance in the start method, but it can&apos;t hurt to pass it along?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think that&apos;s the right approach!&lt;/p&gt;</comment>
                    <comment id="12641567" author="michaelbusch" created="Tue, 21 Oct 2008 21:15:39 +0100"  >&lt;blockquote&gt;
&lt;p&gt;I think that&apos;s the right approach!&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;OK done. &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; Works great...&lt;/p&gt;

&lt;p&gt;Another question. Currently the members in Token, like positionIncrement are deprecated with a comment that they will be made private. We should be able to do so in the next release, i. e. 2.9, right? It&apos;s not a public or protected API, so there shouldn&apos;t be the need to wait for 3.0 based on our backwards-compatibility policy. Or did I miss a discussion behind this, maybe because Token is a class that many people extend in the o.a.l.a package and we agreed to make an exception here?&lt;/p&gt;

&lt;p&gt;The reason why I&apos;m asking is Grant&apos;s suggestion of putting all default Attributes into Token and having methods like getPositionIncrement() return the value from the corresponding PositionIncrementAttribute. It won&apos;t be possible to do that if I can&apos;t remove those mentioned members, unless I subclass Token or with a performance hit, if I do something like this for example:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; getPositionIncrement() {
  &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (&lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.positionIncrementAttribute != &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;) {
    &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.positionIncrementAttribute.getPositionIncrement();
  &lt;span class=&quot;code-keyword&quot;&gt;else&lt;/span&gt; {
    &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.positionIncrement;
  }
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Seems inefficient and messy.&lt;/p&gt;

&lt;p&gt;So my question is if it&apos;s ok if I remove the deprecated members with this patch in the 2.9 release? Thoughts?&lt;/p&gt;</comment>
                    <comment id="12641580" author="dmsmith" created="Tue, 21 Oct 2008 21:42:25 +0100"  >&lt;blockquote&gt;&lt;p&gt;Another question. Currently the members in Token, like positionIncrement are deprecated with a comment that they will be made private.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;These members were being directly accessed in various contribs. To me that makes them part of the published API, whether intentional or not. All of the code in Lucene core and contrib now use accessors.&lt;/p&gt;

&lt;p&gt;I marked them as deprecated so that people would have time to clean up their code. My assumption was that even though it is package protected that someone might create their own &quot;contrib&quot; with their Token subclass in o.a.l.analysis. I did&apos;t think the backward compatibility policy discussed package protected and I wasn&apos;t eager to open that discussion.&lt;/p&gt;

&lt;p&gt;Strictly speaking, it does break backward compatibility.&lt;/p&gt;

&lt;p&gt;But, it should have never had package protected members. For precisely this reason.&lt;/p&gt;</comment>
                    <comment id="12641583" author="michaelbusch" created="Tue, 21 Oct 2008 21:49:07 +0100"  >&lt;blockquote&gt;
&lt;p&gt;Strictly speaking, it does break backward compatibility.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes I agree. But my take here is that the package-private methods are expert methods for which we don&apos;t have to guarantee backwards-compatibility the same way we do for public and protected APIs (i. e. only break compatibility in a version change X.Y-&amp;gt;(X+1).0). Of course we have to update all contribs.&lt;/p&gt;</comment>
                    <comment id="12641598" author="gsingers" created="Tue, 21 Oct 2008 22:14:09 +0100"  >&lt;blockquote&gt;
&lt;p&gt;Yes I agree. But my take here is that the package-private methods are expert methods for which we don&apos;t have to guarantee backwards-compatibility the same way we do for public and protected APIs (i. e. only break compatibility in a version change X.Y-&amp;gt;(X+1).0). Of course we have to update all contribs.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I don&apos;t know about that.  I don&apos;t think it has been decided one way or the other.  At one extreme, the policy says: &quot;That&apos;s to say, any code developed against X.0 should continue to run without alteration against all X.N releases. &quot;  Since, it is perfectly legal to access package methods by declaring oneself to be part of that package, and, since we don&apos;t sign our jars to prevent such a move, I can see that others may feel free to use them when they deem it beneficial.  On the other hand, convention/good programming style suggests it&apos;s not good to write programs that rely on package level APIs in libraries, so caveat emptor.&lt;/p&gt;

&lt;p&gt;I do note that we broke Luke once before when we changed something that was package scoped.  Maybe a shame on us, maybe a shame on Luke,  I don&apos;t know.&lt;/p&gt;

&lt;p&gt;That being said, I doubt a lot of people are doing this, so...  &lt;br/&gt;
I&apos;d suggest a vote on it on dev if breaking it is decided on, like we did w/ Fieldable and that we explicitly state what is happening in CHANGES, etc.  &lt;/p&gt;

&lt;p&gt;Said vote could either be specific to this problem, or it could attempt to address the question of whether package scoped things are subject to the back-compat. policy.&lt;/p&gt;</comment>
                    <comment id="12641600" author="michaelbusch" created="Tue, 21 Oct 2008 22:18:42 +0100"  >&lt;blockquote&gt;
&lt;p&gt;Said vote could either be specific to this problem, or it could attempt to address the question of whether package scoped things are subject to the back-compat. policy.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Agreed. I&apos;d prefer the latter. I will call a vote on java-dev later today...&lt;/p&gt;</comment>
                    <comment id="12643394" author="michaelbusch" created="Wed, 29 Oct 2008 01:15:16 +0000"  >&lt;p&gt;Because mulitple people mentioned it would be better to merge&lt;br/&gt;
TokenStream and Token into one class, I thought more about it&lt;br/&gt;
and now I think I prefer that approach too. I implemented it&lt;br/&gt;
and added a class TokenStreamState to capture a state of a stream&lt;br/&gt;
which can be used for buffering of Tokens. The performance of the&lt;br/&gt;
CachingTokenFilter is lower than before if all attributes that the&lt;br/&gt;
Token had are used, but slightly better if fewer are used (which&lt;br/&gt;
was previously not possible). I also had some ideas of making &lt;br/&gt;
buffering of Tokens perform better, but this patch is now &lt;br/&gt;
already pretty long, so I decided to add better buffering with&lt;br/&gt;
a separate issue at a later point. Here is a sumamry of my &lt;br/&gt;
changes:&lt;/p&gt;

&lt;p&gt;Changes in the analysis package:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Added the tokenattributes subpackage, known from the previous&lt;br/&gt;
  patches&lt;/li&gt;
	&lt;li&gt;Added the abstract class AttributeSource that owns the&lt;br/&gt;
  attribute map and appropriate methods to get and add the&lt;br/&gt;
  attributes. TokenStream now extends AttributeSource.&lt;/li&gt;
	&lt;li&gt;Deprecated the Token class.&lt;/li&gt;
	&lt;li&gt;Added TokenStream#start(), TokenStream#initialize() and&lt;br/&gt;
  TokenStream#incrementToken(). start() must be called by a &lt;br/&gt;
  consumer before incrementToken() is called the first time.&lt;br/&gt;
  start() calls initialize(), which can be used by TokenStreams&lt;br/&gt;
  and TokenFilters to add or get attributes. I separated &lt;br/&gt;
  start() and initialize() to enforce in TokenFilters that&lt;br/&gt;
  input.start() is called. I think it would be a pitfall for&lt;br/&gt;
  bugs (happened to me before I added initialize().&lt;/li&gt;
	&lt;li&gt;Added another subclass of AttributeSource called&lt;br/&gt;
  TokenStreamState which can be used to capture a current state&lt;br/&gt;
  of a TokenStream, e. g. for buffering purposes. I changed the&lt;br/&gt;
  CachingTokenFilter and Sink/Tee-TokenFilter to make use of &lt;br/&gt;
  this new class.&lt;/li&gt;
	&lt;li&gt;Changed all core TokenStreams and TokenFilters to implement&lt;br/&gt;
  the new methods and deprecated the next(Token) methods, but&lt;br/&gt;
  left them for backwards compatibility.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Changes in the indexer package:&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Changed DocInverterPerField.processFields to use the new API&lt;br/&gt;
  if TokenStream.useNewAPI() is set to true. I also added an &lt;br/&gt;
  inner class to DocInverterPerThread called &lt;br/&gt;
  BackwardsCompatibilityStream that allows me to set a Token&lt;br/&gt;
  and all Attributes just return the values from the token.&lt;br/&gt;
  That allows me to change all consuemrs in the indexer &lt;br/&gt;
  package to not use Token anymore at all, but only &lt;br/&gt;
  TokenStream, without a performance hit.&lt;/li&gt;
	&lt;li&gt;Added start(Fieldable) method to InvertedDocConsumerPerField&lt;br/&gt;
  and TermsHashConsumerPerField that is called to notify the &lt;br/&gt;
  consumers that one field &lt;b&gt;instance&lt;/b&gt; is now going to be &lt;br/&gt;
  processed, so that they can get the attribute references&lt;br/&gt;
  from DocInverter.FieldInvertState.attributeSource. Also &lt;br/&gt;
  changed the signature of the add() method of the above &lt;br/&gt;
  mentioned classes to not take a Token anymore.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Changes in queryparser package:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Changed QueryParser so that it uses a CachingTokenFilter&lt;br/&gt;
  instead of a List to buffer tokens. &lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Testcases:&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Added class TokenStreamTestUtils to the analysis test&lt;br/&gt;
  package which contains two inner helper classes:&lt;br/&gt;
  BackwardsCompatibleStream and BackwardsCompatibleFilter.&lt;br/&gt;
  Both overwrite TokenStream#next(Token) and call &lt;br/&gt;
  incrementToken() and then copy all attribute values to the&lt;br/&gt;
  Token to be returned to the caller of next(Token). That &lt;br/&gt;
  simplifies it to make existing tests run in old and new API&lt;br/&gt;
  mode.&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;All test cases pass with useNewAPI=true and false. I think &lt;br/&gt;
this patch is mostly done now (I just have to update &lt;br/&gt;
analysis/package.html and cleanup imports), unless we&apos;re not&lt;br/&gt;
happy with the APIs. &lt;br/&gt;
Please give me some feedback about this approach.&lt;/p&gt;</comment>
                    <comment id="12643397" author="gsingers" created="Wed, 29 Oct 2008 02:30:50 +0000"  >&lt;blockquote&gt;
&lt;p&gt;Because mulitple people mentioned it would be better to merge&lt;br/&gt;
TokenStream and Token into one class,&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;They did?  I only see Doug&apos;s comment, is there another one?  Would be good to see the reasoning.  Not that I&apos;m against what you&apos;ve done, just curious.&lt;/p&gt;


&lt;p&gt;Some questions (I&apos;ve only glanced at the patch, so I still need to apply the patch):&lt;/p&gt;

&lt;p&gt;Should useNewAPI be static?  Seems like I could want to use the new one in some cases, but not in others?????&lt;/p&gt;

&lt;p&gt;Why the static &quot;constructors&quot; on TokenStreamState?  Why not just have constructors for that?&lt;/p&gt;</comment>
                    <comment id="12643399" author="michaelbusch" created="Wed, 29 Oct 2008 02:56:01 +0000"  >&lt;blockquote&gt;
&lt;p&gt;They did? I only see Doug&apos;s comment, is there another one?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Mike did in &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-1426&quot; title=&quot;Next steps towards flexible indexing&quot;&gt;&lt;del&gt;LUCENE-1426&lt;/del&gt;&lt;/a&gt;. I also don&apos;t see a reason anymore to have Token as a separate class. I think the TokenStreamState for the buffering usecases is more elegant...&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Should useNewAPI be static? Seems like I could want to use the new one in some cases, but not in others?????&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;wow, a lot of question marks &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;br/&gt;
Good point. I made it static so that I could set it to true or false in LuceneTestCase.java to run all tests in both modes, but you&apos;re right, it should be per TokenStream instance. So maybe a static method for the default setting, and a non-static that can be used to override it?&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Why the static &quot;constructors&quot; on TokenStreamState? Why not just have constructors for that?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;There is not really a reason. I thought TokenStreamState.capture(TokenStream) would look more intuitive. (the word capture indicates what happens)&lt;/p&gt;</comment>
                    <comment id="12643468" author="gsingers" created="Wed, 29 Oct 2008 11:23:49 +0000"  >&lt;blockquote&gt;
&lt;p&gt;Good point. I made it static so that I could set it to true or false in LuceneTestCase.java to run all tests in both modes, but you&apos;re right, it should be per TokenStream instance. So maybe a static method for the default setting, and a non-static that can be used to override it?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I don&apos;t think you need the static, just fold it into a constructor and have one that is a default, too.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;There is not really a reason. I thought TokenStreamState.capture(TokenStream) would look more intuitive. (the word capture indicates what happens)&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Constructors are about as intuitive as you can get for constructing a new object &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/wink.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Mike did in &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-1426&quot; title=&quot;Next steps towards flexible indexing&quot;&gt;&lt;del&gt;LUCENE-1426&lt;/del&gt;&lt;/a&gt;. I also don&apos;t see a reason anymore to have Token as a separate class. I think the TokenStreamState for the buffering usecases is more elegant...&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Thanks.  I see it now.  I think one thing that might help is to rename Attribute to be TokenAttribute.  For me, I&apos;m just trying to think how to explain this stuff to new users and it doesn&apos;t feel as cut-and-dried as saying &quot;A TokenStream produces Tokens.  Tokens have attributes like offset, position, etc.&quot;.  Now we are saying &quot;A TokenStream produces Attributes.  Attributes describe a token with offset, position, etc.&quot;.  Not a big deal, though.  I like where this is heading.&lt;/p&gt;

&lt;p&gt;I&apos;m want to check it out for Solr, too, to see if there are any effects.&lt;/p&gt;

&lt;p&gt;Have you done any performance testing?  I would think it would be faster.  Out of curiosity,   do you think this would allow us to implement something like described in the paper here: &lt;a href=&quot;http://arnoldit.com/wordpress/2008/09/06/text-processing-why-servers-choke/&quot; class=&quot;external-link&quot;&gt;http://arnoldit.com/wordpress/2008/09/06/text-processing-why-servers-choke/&lt;/a&gt;&lt;/p&gt;</comment>
                    <comment id="12643472" author="mikemccand" created="Wed, 29 Oct 2008 11:50:15 +0000"  >
&lt;p&gt;I like this new &quot;idempotent&quot; approach!&lt;/p&gt;

&lt;p&gt;I reviewed the patch.  It looks good!:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;It seems like AttributeSource is fairly generic &amp;#8211; mabye it should&lt;br/&gt;
    go into util?  It seems like we could eventually use the same&lt;br/&gt;
    approach for extensibility to index writing/reading APIs?&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Performance &amp;#8211; have you compared before/after simple tokenization&lt;br/&gt;
    speeds?  Eg, tokenize all docs in Wikipedia w/ different&lt;br/&gt;
    analyzers.  contrib/benchmark makes this easy now.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;I assume we would statically default TokenStream.useNewAPI to&lt;br/&gt;
    true?&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;You are using a Token (perThreadlocalToken) and a&lt;br/&gt;
    BackwardsCompatibilityStream when indexing a NOT_ANALYZED Field,&lt;br/&gt;
    in DocInverterPerField &amp;#8211; can you fix that code to use&lt;br/&gt;
    not-deprecated new stuff?  EG maybe make a SingleTokenTokenStream&lt;br/&gt;
    or something that&apos;s re-used for this purpose?&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Once we can set useNewAPI per TokenStream instance, what happens&lt;br/&gt;
    if someone builds up an analyzer chain that mixes old &amp;amp; new&lt;br/&gt;
    TokenStream/Filters?  Are all permutations OK?&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Are DocInverter.BackwardsCompatibilityStream and&lt;br/&gt;
    TokenStreamTestUtils.BackwardsCompatibleStream basically the same&lt;br/&gt;
    thing?  If so, can we merge them?  It seems like others may needs&lt;br/&gt;
    this (when mixing old/new analyzer chains &amp;#8211; the question above).&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Can you add back-compat-future-freedom warnings to these new APIs?&lt;br/&gt;
    (Ie &quot;these are new &amp;amp; subject to change&quot;)&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Do we really need the separate subclass TokenStreamState?  Should&lt;br/&gt;
    we absorb its methods into AttributeSource?&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;In DocInverterPerField you check if the attrSource has posIncr &amp;amp;&lt;br/&gt;
    offset attrs, instead of just looking them up and getting the&lt;br/&gt;
    default instance if it wasn&apos;t already there.  This then requires&lt;br/&gt;
    you to default posIncr &amp;amp; offsets in two places &amp;#8211;&lt;br/&gt;
    DocInverterPerField &amp;amp; each of the attr classes.  Wouldn&apos;t it be&lt;br/&gt;
    cleaner to always look them up, and let the default instance of&lt;br/&gt;
    each be the one place that holds the default?&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Some files are missing copyright header.  Also, the comments at the&lt;br/&gt;
    top of TermAttribute.java &amp;amp; TypeAttribute.java should be fixed&lt;br/&gt;
    (they are about pos incr now).  Also there is a &quot;must should&quot;&lt;br/&gt;
    in the javadocs in TokenStrea.java.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;On the new TokenStream.start method: is a TokenStream allowed to&lt;br/&gt;
    not allow more than 1 start invocation?  Meaning, it cannot repeat&lt;br/&gt;
    itself.  Just like we are grappling with on DocIdset.iterator()&lt;br/&gt;
    it&apos;d be good to address this semantics up front.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Many unit tests had to be changed with this patch.  Was this&lt;br/&gt;
    entirely due to avoiding the newly deprecated APIs, or, are there&lt;br/&gt;
    any tests that fail if they were not changed?  (This is a simple&lt;br/&gt;
    check to run, actually &amp;#8211; only apply your core changes, then run&lt;br/&gt;
    all tests).  If it&apos;s the former (which it should be), maybe we&lt;br/&gt;
    should leave a few tests using the deprecated APIs to ensure we&lt;br/&gt;
    don&apos;t break them before 3.0?&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="12644881" author="michaelbusch" created="Tue, 4 Nov 2008 02:36:10 +0000"  >&lt;p&gt;Thanks for the thorough review, Mike! I made most of the changes you suggested. However, I&apos;ve some comments to some of your points:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;I assume we would statically default TokenStream.useNewAPI to&lt;br/&gt;
true?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I don&apos;t think it should default to true. How it currently works (and this answers also some of your other questions) is that one can&apos;t mix old and new streams and filters in the same chain. If someone enables the new API, then &lt;b&gt;all&lt;/b&gt; streams and filters in one chain have to implement the new API. The reason is that you can&apos;t &quot;simulate&quot; the old API by calling the new methods in an efficient way. We would have to copy all values from the Token that next(Token) returns into the appropriate Attributes.&lt;br/&gt;
This would slow down the ingestion performance and I think would affect backwards-compatibility: We guarantee that you can update from 2.4 to 2.9 without getting compile and runtime errors, but I think the performance should also not decrease significantly? That&apos;s the main reason why I left two implementations in all core streams and filters, for next(Token) and incrementToken().&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Are DocInverter.BackwardsCompatibilityStream and&lt;br/&gt;
TokenStreamTestUtils.BackwardsCompatibleStream basically the same&lt;br/&gt;
thing? If so, can we merge them? It seems like others may needs&lt;br/&gt;
this (when mixing old/new analyzer chains - the question above).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Sorry for the silly naming of these classes. They are not the same. The one in DocInverter is basically a wrapper with Attributes around an old Token. This is used so that almost all consumers in the indexer package can already use the new API in an efficient way.&lt;br/&gt;
The one in the test package however is used, so that TokenStream and -Filter implementations in our testcases don&apos;t have to have implementations for both the old and new API. If the old next(Token) is called, then it calls incrementToken() and copies over the values from the Attributes to the Token. Since performance is not critical in testcases, I decided to take this approach to reduce code duplication in the tests.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Many unit tests had to be changed with this patch. Was this entirely due to avoiding the newly deprecated APIs, or, are there any tests that fail if they were not changed? (This is a simple check to run, actually - only apply your core changes, then run all tests). If it&apos;s the former (which it should be), maybe we should leave a few tests using the deprecated APIs to ensure we don&apos;t break them before 3.0?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;That&apos;s actually the reason why I want to keep the static setUseNewAPI() method. When I test this patch I run all tests twice, in both modes. That way I can be sure to not break backwards compatibility.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;On the new TokenStream.start method: is a TokenStream allowed to&lt;br/&gt;
not allow more than 1 start invocation? Meaning, it cannot repeat&lt;br/&gt;
itself. Just like we are grappling with on DocIdset.iterator()&lt;br/&gt;
it&apos;d be good to address this semantics up front.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Not sure I follow you here. Could you elaborate?&lt;/p&gt;</comment>
                    <comment id="12645060" author="mikemccand" created="Tue, 4 Nov 2008 20:05:08 +0000"  >&lt;blockquote&gt;&lt;p&gt;...one can&apos;t mix old and new streams and filters in the same chain&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Ahh, I see.  So on upgrading if someone has a chain involving an&lt;br/&gt;
external TokenFilter that does not implement the new API, then the&lt;br/&gt;
whole chain should be downgraded to the old API until that filter is&lt;br/&gt;
upgraded.&lt;/p&gt;

&lt;p&gt;OK I agree we should default it to false, then in 3.0 hardwire it to&lt;br/&gt;
true (and remove all the deprecated methods).&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;We guarantee that you can update from 2.4 to 2.9 without getting compile and runtime errors, but I think the performance should also not decrease significantly?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I agree: I think this should be (is?) part of the back compat promise.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;They are not the same.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;OK this makes sense!&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;When I test this patch I run all tests twice, in both modes. That way I can be sure to not break backwards compatibility.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;But: what if you revert all changes to all tests, and leave useNewAPI&lt;br/&gt;
false?  Nothing should fail, right?  (This is the &quot;true&quot; back compat&lt;br/&gt;
test, I think).&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Not sure I follow you here. Could you elaborate?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;In &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-1427&quot; title=&quot;QueryWrapperFilter should not do scoring&quot;&gt;&lt;del&gt;LUCENE-1427&lt;/del&gt;&lt;/a&gt;, we were discussing whether a DocIdSet should &quot;declare&quot;&lt;br/&gt;
whether it can be iterated over more than once, or not, so that things&lt;br/&gt;
that know they need to consume it more than once would know whether to&lt;br/&gt;
use an intermediate cache.  I&apos;m not sure we&apos;d need it, here, but since&lt;br/&gt;
we are changing the API now I thought we should think about it.&lt;br/&gt;
But... we can (and should) do this as a separate issue, if and when it&lt;br/&gt;
proves out to a real use case, so let&apos;s take it off the table for now.&lt;/p&gt;</comment>
                    <comment id="12645565" author="cutting" created="Thu, 6 Nov 2008 20:24:14 +0000"  >&lt;p&gt;&amp;gt; But: what if you revert all changes to all tests [ ... ]&lt;br/&gt;
&amp;gt; This is the &quot;true&quot; back compat test, I think&lt;/p&gt;

&lt;p&gt;Perhaps we could formalize this.  We could specify a subversion URL in build.xml that points to a release tag, the tag of the oldest release we need to be compatible with.  We add a test that retrieves the tests from this tag and compiles and runs them.&lt;/p&gt;</comment>
                    <comment id="12645573" author="mikemccand" created="Thu, 6 Nov 2008 20:37:13 +0000"  >&lt;blockquote&gt;&lt;p&gt;Perhaps we could formalize this. We could specify a subversion URL in build.xml that points to a release tag, the tag of the oldest release we need to be compatible with. We add a test that retrieves the tests from this tag and compiles and runs them.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;+1&lt;/p&gt;

&lt;p&gt;Hopefully someone who understands ant/build.xml very well (not me!) will do this &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;Once we have that working, we could then make it part of the nightly build.&lt;/p&gt;</comment>
                    <comment id="12645836" author="michaelbusch" created="Fri, 7 Nov 2008 17:48:01 +0000"  >&lt;blockquote&gt;
&lt;p&gt;Perhaps we could formalize this. We could specify a subversion URL in build.xml that points to a release tag, the tag of the oldest release we need to be compatible with. We add a test that retrieves the tests from this tag and compiles and runs them.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I like this approach too. The only concern I have is about the package-private APIs. As we recently discussed they can always change, so if any test cases use package-private APIs that changed in trunk since the last release, then the old testcases would not even compile.&lt;/p&gt;

&lt;p&gt;However, I worked on a patch for this and will open a new issue soon.&lt;/p&gt;</comment>
                    <comment id="12645986" author="gsingers" created="Sat, 8 Nov 2008 14:31:23 +0000"  >&lt;p&gt;Good idea, Doug, on the back-compat testing.  Something we should definitely add.&lt;/p&gt;

&lt;p&gt;Michael and I did some performance tests at ApacheCon on this, and it seems like it is as fast as the old way, but we should probably formalize this by adding to the Tokenize algorithm in the benchmarker so it is easier for people to verify it.&lt;/p&gt;

&lt;p&gt;I still don&apos;t see the point to the &lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;static&lt;/span&gt; TokenStreamState capture(TokenStream from) &lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;  Just use the constructor.  Wrapping a constructor in a static doesn&apos;t gain you anything except an extra function call.&lt;/p&gt;</comment>
                    <comment id="12646146" author="michaelbusch" created="Mon, 10 Nov 2008 02:04:58 +0000"  >&lt;blockquote&gt;
&lt;ul&gt;
	&lt;li&gt;In DocInverterPerField you check if the attrSource has posIncr &amp;amp;&lt;br/&gt;
      offset attrs, instead of just looking them up and getting the&lt;br/&gt;
      default instance if it wasn&apos;t already there. This then requires&lt;br/&gt;
      you to default posIncr &amp;amp; offsets in two places -&lt;br/&gt;
      DocInverterPerField &amp;amp; each of the attr classes. Wouldn&apos;t it be&lt;br/&gt;
      cleaner to always look them up, and let the default instance of&lt;br/&gt;
      each be the one place that holds the default?&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;So here&apos;s one thing about the new API that is not entirely clear to me yet. Let&apos;s say the consumer, in this case DocInverterPerField, wants to use the offset. You are right, it&apos;s not desirable to define default values in two places (the consumer and the attribute). Now the other alternative is to call addAttribute(OffsetAttribute.class) in the consumer. If no offset attribute is present, this will just add a default instance and the consumer can use its default value. So far so good. But what if there is e. g. a TokenFilter in the chain that checks input.hasAttribute(OffsetAttribute.class) and does something it would not do if that check returned false? It seems &quot;something&quot; is not clearly defined here yet. I think the &quot;something&quot; is the difference between a producer (TokenStream/Filter) adding an attribute vs. a consumer. Thoughts?&lt;/p&gt;

&lt;p&gt;Hmm or maybe in this case the consumer is not behaving correctly. If there is no offset attribute, then the consumer should not have to write any data structures that use an offset, right? That&apos;s why you currently have the start() method in TermVectorTermsWriterPerField that checks if offsets need to be written or not.&lt;br/&gt;
I think the issue here is that one consumer, the DocInverterPerField, deals with one value, the offset, that multiple subconsumers might want to use. That&apos;s why we don&apos;t want to throw an exception in DocInverterPerField if OffsetAttribute is not present, because it can&apos;t know if this is really a misconfiguration or not. Only the actual subconsumer (in this case TermVectorTermsWriterPerField) is able to decide that. For example if someone writes a new custom consumer that can only work if OffsetAttribute is present, then we would probably want that consumer to throw an exception if it&apos;s not present. But if the DocInverterPerField adds the OffsetAttribute to be able to use it&apos;s default value, we would not get the exception. &lt;/p&gt;

&lt;p&gt;But maybe all this is fine and we should just say we strictly separate the Attributes from the configuration, which means that the knowledge about the presence or absence of an Attribute may not be used by anybody (filter or consumer) to determine anything about the configuration. This is how the TermVectors currently work. The configuration (whether or not to store positions and/or offsets) is stored in the Field, the data is carried by the OffsetAttribute. If we decide to define the API this way, do we even need AttributeSource.hasAttribute() and getAttribute()? Or wouldn&apos;t addAttribute() be sufficient then?&lt;/p&gt;</comment>
                    <comment id="12646773" author="michaelbusch" created="Wed, 12 Nov 2008 01:35:24 +0000"  >&lt;blockquote&gt;
&lt;p&gt;It seems &quot;something&quot; is not clearly defined here yet.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;OK, I added this to the javadocs of TokenStream to make things clear:&lt;/p&gt;

&lt;p&gt;  &lt;b&gt;The workflow of the new TokenStream API is as follows:&lt;/b&gt;&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;Instantiation of TokenStream/TokenFilters&lt;/li&gt;
	&lt;li&gt;The consumer calls TokenStream#start() which triggers calls to #initialize() of the stream and all filters in the chain.&lt;/li&gt;
	&lt;li&gt;Filters can use #initialize() to get or add attributes and store local references to the attribute.&lt;/li&gt;
	&lt;li&gt;After #start() returns the consumer retrieves attributes from the stream and stores local references to all attributes it wants to access&lt;/li&gt;
	&lt;li&gt;The consumer calls #incrementToken() until it returns false and consumes the attributes after each call.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;  To make sure that filters and consumers know which attributes are available&lt;br/&gt;
  the attributes must be added in the #initialize() call. Filters and &lt;br/&gt;
  consumers are not required to check for availability of attributes in #incrementToken().&lt;/p&gt;
</comment>
                    <comment id="12646786" author="michaelbusch" created="Wed, 12 Nov 2008 03:21:05 +0000"  >&lt;p&gt;Changes in this patch:&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;updated patch to current trunk&lt;/li&gt;
	&lt;li&gt;moved AttributeSource and Attribute to o.a.l.util&lt;/li&gt;
	&lt;li&gt;removed TokenStreamState and moved captureState() and restoreState() into AttributeSource&lt;/li&gt;
	&lt;li&gt;added non-static setUseAPI() method; added warning that mixing of old and new streams/filters is not supported&lt;/li&gt;
	&lt;li&gt;DocInverterPerField now uses only non-deprecated classes for NOT_ANALYZED fields&lt;/li&gt;
	&lt;li&gt;Added warnings saying that the new APIs might change in the future&lt;/li&gt;
	&lt;li&gt;DocInverterPerField now adds PositionIncrementAttribute and OffsetAttribute in case they are not present in the stream and thus uses the default values defined in the attributes&lt;/li&gt;
	&lt;li&gt;added missing license headers and fixed javadocs&lt;/li&gt;
	&lt;li&gt;Removed TokenStreamTestUtils and references to Token from &lt;b&gt;all&lt;/b&gt; unit tests (except the now also deprecated TestToken); LuceneTestCase enables the new API in setup() for all tests, so the current tests only test the new API; backwards-compatibility is tested with &quot;ant test-tag&quot; (see &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-1440&quot; title=&quot;Add ability to run backwards-compatibility tests automatically&quot;&gt;&lt;del&gt;LUCENE-1440&lt;/del&gt;&lt;/a&gt;)&lt;/li&gt;
	&lt;li&gt;fixed some small bugs that I encountered after changing the unit test&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;All current and 2.4 tests pass.&lt;/p&gt;</comment>
                    <comment id="12647337" author="mikemccand" created="Thu, 13 Nov 2008 17:18:54 +0000"  >&lt;p&gt;Looks good!&lt;/p&gt;

&lt;p&gt;I&apos;m seeing this failure (in test I just committed this AM).  I think&lt;br/&gt;
it&apos;s OK, because the new API is enabled for all tests and I&apos;m using&lt;br/&gt;
the old API with that analyzer?&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;    [junit] Testcase: testExclusiveLowerNull(org.apache.lucene.search.TestRangeQuery):	Caused an ERROR
    [junit] This token does not have the attribute &apos;class org.apache.lucene.analysis.tokenattributes.TermAttribute&apos;.
    [junit] java.lang.IllegalArgumentException: This token does not have the attribute &apos;class org.apache.lucene.analysis.tokenattributes.TermAttribute&apos;.
    [junit] 	at org.apache.lucene.util.AttributeSource.getAttribute(AttributeSource.java:124)
    [junit] 	at org.apache.lucene.index.TermsHashPerField.start(TermsHashPerField.java:252)
    [junit] 	at org.apache.lucene.index.DocInverterPerField.processFields(DocInverterPerField.java:144)
    [junit] 	at org.apache.lucene.index.DocFieldConsumersPerField.processFields(DocFieldConsumersPerField.java:36)
    [junit] 	at org.apache.lucene.index.DocFieldProcessorPerThread.processDocument(DocFieldProcessorPerThread.java:234)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:760)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.addDocument(DocumentsWriter.java:738)
    [junit] 	at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:2039)
    [junit] 	at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:2013)
    [junit] 	at org.apache.lucene.search.TestRangeQuery.insertDoc(TestRangeQuery.java:304)
    [junit] 	at org.apache.lucene.search.TestRangeQuery.initializeIndex(TestRangeQuery.java:287)
    [junit] 	at org.apache.lucene.search.TestRangeQuery.testExclusiveLowerNull(TestRangeQuery.java:315)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Some other random questions:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;I&apos;m a little confused by &apos;start&apos; and &apos;initialize&apos; in TokenStream.&lt;br/&gt;
    DocInverterPerField (consumer of this API) calls&lt;br/&gt;
    analyzer.reusableTokenStream to get a token stream.  Then it calls&lt;br/&gt;
    stream.reset().  Then it calls stream.start() which then calls&lt;br/&gt;
    stream.initialize().  Can we consolidate these (there are 4 places&lt;br/&gt;
    that we call to &quot;start&quot; the stream now)?&lt;br/&gt;
.&lt;br/&gt;
    EG why can&apos;t analyzer.reusableTokenStream() do the init internally&lt;br/&gt;
    in the new API?  (Also, in StopFilter, initialize() sets termAtt &amp;amp;&lt;br/&gt;
    posIncrAtt, but I would think this only needs to happen once when&lt;br/&gt;
    that TokenFilter is created?  BackCompatTokenStream add the attrs&lt;br/&gt;
    in its ctor, which seems better.).&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;BackCompatTokenStream is calling attributes.put directly but all&lt;br/&gt;
    others use super&apos;s addAttribute.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Why is BackCompatTokenStream overriding so many methods?  EG&lt;br/&gt;
    has/get/addAttribute &amp;#8211; won&apos;t super do the same thing?&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Maybe add reasons to some of the asserts, eg StopFilter has&lt;br/&gt;
    &quot;assert termAtt != null&quot;, so maybe append to that &quot;: initialize()&lt;br/&gt;
    wasn&apos;t called&quot;.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="12647346" author="michaelbusch" created="Thu, 13 Nov 2008 17:47:48 +0000"  >&lt;blockquote&gt;
&lt;p&gt;Looks good! &lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Thanks for reviewing ... again!&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;I&apos;m a little confused by &apos;start&apos; and &apos;initialize&apos; in TokenStream.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Good question. The only reason why I added two separate methods here was to enforce that TokenFilter#start() always calls input.start() first. I think otherwise this will be a pitfall for people who want to implement their own filters and override start(). (happened to me when I modified the tests a couple of times)&lt;br/&gt;
On the other hand I agree that Three methods (start(), initialize(), reset()) are confusing. I guess we can deprecate reset() in the future and have start() rewind the stream if supported by the stream. (I think you mentioned a boolean method could return true if rewind is supported?)&lt;br/&gt;
So I&apos;d be ok with collapsing start/initialize into one method if you prefer that. I&apos;d just have to add an explicit warning to the javadocs in TokenFilter.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;I&apos;m seeing this failure (in test I just committed this AM). I think&lt;br/&gt;
it&apos;s OK, because the new API is enabled for all tests and I&apos;m using&lt;br/&gt;
the old API with that analyzer?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Yeah, I&apos;ve to modify the new test to use the new API.&lt;/p&gt;

&lt;blockquote&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;BackCompatTokenStream is calling attributes.put directly but all&lt;br/&gt;
others use super&apos;s addAttribute.&lt;/li&gt;
	&lt;li&gt;Why is BackCompatTokenStream overriding so many methods? EG&lt;br/&gt;
has/get/addAttribute - won&apos;t super do the same thing?&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;I had a different implementation of BCTS, so this is left-over. Of course you&apos;re right, I can simplify that class.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Maybe add reasons to some of the asserts, eg StopFilter has&lt;br/&gt;
&quot;assert termAtt != null&quot;, so maybe append to that &quot;: initialize()&lt;br/&gt;
wasn&apos;t called&quot;.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Yeah good idea, will do.&lt;/p&gt;</comment>
                    <comment id="12647557" author="mikemccand" created="Fri, 14 Nov 2008 09:59:01 +0000"  >&lt;blockquote&gt;&lt;p&gt;So I&apos;d be ok with collapsing start/initialize into one method if you prefer that.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Or, could we just continue to use reset() for this purpose?  I think it&apos;s ok via Javadocs to state clearly that if you override reset you have to be sure to call super.reset().&lt;/p&gt;

&lt;p&gt;And then tokenizers like CharTokenizer should not addAttribute on every reset/initialize, right?  They should just do it once in their ctor?&lt;/p&gt;</comment>
                    <comment id="12647737" author="michaelbusch" created="Fri, 14 Nov 2008 22:06:07 +0000"  >&lt;blockquote&gt;
&lt;p&gt;Or, could we just continue to use reset() for this purpose? I think it&apos;s ok via Javadocs to state clearly that if you override reset you have to be sure to call super.reset().&lt;/p&gt;

&lt;p&gt;And then tokenizers like CharTokenizer should not addAttribute on every reset/initialize, right? They should just do it once in their ctor?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;OK I like that! Let&apos;s do it that way, it&apos;s simpler and less confusing.&lt;/p&gt;

&lt;p&gt;I&apos;m making the change now, then hopefully the patch is ready for committing...&lt;/p&gt;</comment>
                    <comment id="12648047" author="michaelbusch" created="Sun, 16 Nov 2008 23:20:56 +0000"  >&lt;p&gt;I removed start() and initialize().&lt;br/&gt;
I also fixed some javadocs and added a pretty long section to package.html in the analysis package, describing the new APIs.&lt;/p&gt;

&lt;p&gt;All trunk and 2.4 tests pass.&lt;/p&gt;

&lt;p&gt;I guess I&apos;ll wait until &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-1448&quot; title=&quot;add getFinalOffset() to TokenStream&quot;&gt;&lt;del&gt;LUCENE-1448&lt;/del&gt;&lt;/a&gt; is committed (and update this patch again... &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/sad.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; ). I&apos;ll try to review 1448 later today (need to leave for a few hours now...)&lt;/p&gt;</comment>
                    <comment id="12648291" author="michaelbusch" created="Mon, 17 Nov 2008 19:55:00 +0000"  >&lt;p&gt;OK we decided to commit this before &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-1448&quot; title=&quot;add getFinalOffset() to TokenStream&quot;&gt;&lt;del&gt;LUCENE-1448&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I&apos;ll wait another day and then commit this if nobody objects.&lt;/p&gt;</comment>
                    <comment id="12648818" author="michaelbusch" created="Tue, 18 Nov 2008 23:49:04 +0000"  >&lt;p&gt;Committed revision 718798.&lt;/p&gt;

&lt;p&gt;Thanks everyone who helped here with reviews and comments!&lt;/p&gt;</comment>
                    <comment id="12650093" author="hossman" created="Mon, 24 Nov 2008 01:34:44 +0000"  >&lt;p&gt;FWIW: In previous versions of Lucene, a class like this would have been legal...&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; class DummyStream &lt;span class=&quot;code-keyword&quot;&gt;extends&lt;/span&gt; TokenFilter {
  &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; count = 5;
  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; DummyStream() {  &lt;span class=&quot;code-keyword&quot;&gt;super&lt;/span&gt;(&lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;);  }
  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; Token next() {
    &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; (0 &amp;lt; count--) ? &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; Token(&lt;span class=&quot;code-quote&quot;&gt;&quot;YES&quot;&lt;/span&gt;, 0, 0) : &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;;
  }
  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; void reset() &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; IOException {count = 5}
  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; void close() &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; IOException {}
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;...there wouldn&apos;t have been much reason to write a subclass like this (better to subclass TokenStream directly for this type of usecase), but it would have worked.  with the new AttributeSource class, TokenFilter actually cares about the TokenStream constructor arg during construction, and a constructor like this will cause a NullPointerException.&lt;/p&gt;

&lt;p&gt;It&apos;s an esoteric enough possibility that we probably don&apos;t need to worry about it, but i&apos;m documenting it for posterity.  (I only noticed because Solr had a test case where it was constructing a stock TokenFilter using a null &quot;input&quot; arg just to then test the object in other ways)&lt;/p&gt;

&lt;p&gt;if anyone does run into a problem with something like this, your best solution is probably to subclass TokenStream directly, it has a no arg constructor.&lt;/p&gt;

&lt;p&gt;for searching...&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;java.lang.NullPointerException
	at org.apache.lucene.util.AttributeSource.&amp;lt;init&amp;gt;(AttributeSource.java:69)
	at org.apache.lucene.analysis.TokenStream.&amp;lt;init&amp;gt;(TokenStream.java:91)
	at org.apache.lucene.analysis.TokenFilter.&amp;lt;init&amp;gt;(TokenFilter.java:42)
	...
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                    <comment id="12655636" author="gsingers" created="Thu, 11 Dec 2008 13:40:20 +0000"  >&lt;blockquote&gt;
&lt;p&gt; Outstanding:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;contrib streams and filters&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;What happened to fixing the contribs?  Seems incomplete without it.&lt;/p&gt;</comment>
                    <comment id="12655735" author="michaelbusch" created="Thu, 11 Dec 2008 18:48:09 +0000"  >&lt;p&gt;See &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-1460&quot; title=&quot;Change all contrib TokenStreams/Filters to use the new TokenStream API&quot;&gt;&lt;del&gt;LUCENE-1460&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;</comment>
                </comments>
                <issuelinks>
                        <issuelinktype id="10030">
                <name>Reference</name>
                                <outwardlinks description="relates to">
                            <issuelink>
            <issuekey id="12408785">LUCENE-1460</issuekey>
        </issuelink>
                    </outwardlinks>
                                            </issuelinktype>
                    </issuelinks>
                <attachments>
                    <attachment id="12392076" name="lucene-1422.patch" size="33673" author="michaelbusch" created="Tue, 14 Oct 2008 09:50:25 +0100" />
                    <attachment id="12392178" name="lucene-1422.take2.patch" size="45957" author="michaelbusch" created="Wed, 15 Oct 2008 12:15:31 +0100" />
                    <attachment id="12392562" name="lucene-1422.take3.patch" size="80520" author="michaelbusch" created="Tue, 21 Oct 2008 10:59:14 +0100" />
                    <attachment id="12392549" name="lucene-1422.take3.patch" size="63108" author="michaelbusch" created="Tue, 21 Oct 2008 06:19:01 +0100" />
                    <attachment id="12392966" name="lucene-1422-take4.patch" size="158827" author="michaelbusch" created="Wed, 29 Oct 2008 01:15:16 +0000" />
                    <attachment id="12393760" name="lucene-1422-take5.patch" size="211513" author="michaelbusch" created="Wed, 12 Nov 2008 03:21:05 +0000" />
                    <attachment id="12394026" name="lucene-1422-take6.patch" size="230303" author="michaelbusch" created="Sun, 16 Nov 2008 23:20:55 +0000" />
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>7.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Tue, 14 Oct 2008 17:16:09 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>12329</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10120"><![CDATA[Patch Available]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>26306</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                    <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                <customfieldname>Time in Status</customfieldname>
                <customfieldvalues>
                    
                </customfieldvalues>
            </customfield>
                            </customfields>
    </item>
</channel>
</rss>