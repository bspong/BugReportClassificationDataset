<!-- 
RSS generated by JIRA (5.2.8#851-sha1:3262fdc28b4bc8b23784e13eadc26a22399f5d88) at Tue Jul 16 13:29:48 UTC 2013

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/LUCENE-675/LUCENE-675.xml?field=key&field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>5.2.8</version>
        <build-number>851</build-number>
        <build-date>26-02-2013</build-date>
    </build-info>

<item>
            <title>[LUCENE-675] Lucene benchmark: objective performance test for Lucene</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-675</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;We need an objective way to measure the performance of Lucene, both indexing and querying, on a known corpus. This issue is intended to collect comments and patches implementing a suite of such benchmarking tests.&lt;/p&gt;

&lt;p&gt;Regarding the corpus: one of the widely used and freely available corpora is the original Reuters collection, available from &lt;a href=&quot;http://www-2.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.tar.gz&quot; class=&quot;external-link&quot;&gt;http://www-2.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.tar.gz&lt;/a&gt; or &lt;a href=&quot;http://people.csail.mit.edu/u/j/jrennie/public_html/20Newsgroups/20news-18828.tar.gz&quot; class=&quot;external-link&quot;&gt;http://people.csail.mit.edu/u/j/jrennie/public_html/20Newsgroups/20news-18828.tar.gz&lt;/a&gt;. I propose to use this corpus as a base for benchmarks. The benchmarking suite could automatically retrieve it from known locations, and cache it locally.&lt;/p&gt;</description>
                <environment></environment>
            <key id="12350386">LUCENE-675</key>
            <summary>Lucene benchmark: objective performance test for Lucene</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png">Closed</status>
                    <resolution id="1">Fixed</resolution>
                                <assignee username="gsingers">Grant Ingersoll</assignee>
                                <reporter username="ab">Andrzej Bialecki </reporter>
                        <labels>
                    </labels>
                <created>Thu, 21 Sep 2006 06:16:21 +0100</created>
                <updated>Sat, 21 Aug 2010 18:15:46 +0100</updated>
                    <resolved>Sat, 13 Jan 2007 04:16:33 +0000</resolved>
                                                                    <due></due>
                    <votes>3</votes>
                        <watches>0</watches>
                                                    <comments>
                    <comment id="12436431" author="ab" created="Thu, 21 Sep 2006 06:18:24 +0100"  >&lt;p&gt;This is just a starting point for discussion - it&apos;s a pretty old file I found lying around, so it may not even compile with modern Lucene. Requires commons-compress.&lt;/p&gt;</comment>
                    <comment id="12436437" author="psmith@apache.org" created="Thu, 21 Sep 2006 06:43:49 +0100"  >&lt;p&gt;If you&apos;re looking for freely available text in bulk, what about:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.gutenberg.org/wiki/Main_Page&quot; class=&quot;external-link&quot;&gt;http://www.gutenberg.org/wiki/Main_Page&lt;/a&gt;&lt;/p&gt;</comment>
                    <comment id="12436442" author="ab" created="Thu, 21 Sep 2006 07:31:20 +0100"  >&lt;p&gt;Yes, that could be a good additional source. However, IMHO the primary corpus should be widely known and standardized, hence my proposal of the Reuters.&lt;/p&gt;

&lt;p&gt;(I mistakenly copy&amp;amp;paste-d the urls in the comment above - of course the corpus they&apos;re pointing at is the &quot;20 Newsgroups&quot;, not the Reuters one. Correct url for the Reuters corpus is  &lt;a href=&quot;http://www.daviddlewis.com/resources/testcollections/reuters21578/&quot; class=&quot;external-link&quot;&gt;http://www.daviddlewis.com/resources/testcollections/reuters21578/&lt;/a&gt; ).&lt;/p&gt;</comment>
                    <comment id="12436443" author="psmith@apache.org" created="Thu, 21 Sep 2006 07:34:09 +0100"  >&lt;p&gt;From a strict performance point of view, a standard set of important, but don&apos;t forget other languages.&lt;/p&gt;

&lt;p&gt;From a tokenization point of view (seperate to this issues), perhaps the Gutenberg project would be useful to test correctness of the analysis phase.&lt;/p&gt;</comment>
                    <comment id="12436502" author="karl.wettin" created="Thu, 21 Sep 2006 12:07:36 +0100"  >&lt;p&gt;It is also interesting to know how much time is consumed to assemble an instance of Document from the storage. According to my own tests this is the major reason to why InstantiatedIndex is so much faster than a FS/RAMDirectory. I also presume it to be the bottleneck of any RDBMS-, RMI- or any other &quot;proxy&quot;-based storage. &lt;/p&gt;</comment>
                    <comment id="12436516" author="gsingers" created="Thu, 21 Sep 2006 13:23:42 +0100"  >&lt;p&gt;Since this has dependencies, do you think we should put it under contrib?  I would be for a Performance directory and we could then organize it from there.  Perhaps into packages for quantitative and qualitative performance.  &lt;/p&gt;</comment>
                    <comment id="12436518" author="ab" created="Thu, 21 Sep 2006 13:37:11 +0100"  >&lt;p&gt;The dependency on commons-compress could be avoided - I used this just to be able to unpack tar.gz files, we can use Ant for that. If you meant the dependency on the corpus - can&apos;t Ant download this too as a dependency?&lt;/p&gt;

&lt;p&gt;Re: Project Gutenberg - good point, this is a good source for multi-lingual documents. The &quot;Europarl&quot; collection is another, although a bit more hefty, so that could be suitable for running large-scale benchmarks, and texts from Project Gutenberg for running small-scale tests.&lt;/p&gt;</comment>
                    <comment id="12436519" author="gsingers" created="Thu, 21 Sep 2006 13:49:06 +0100"  >&lt;p&gt;Yeah, ANT can do this, I think.  Take a look at the DB contrib package, it downloads.  I think I can setup the necessary stuff in contrib, if people think that is a good idea.  First contribution will be this file and then we can go from there.  I think Otis has run some perf. stuff too, but I am not sure if it can be contributed.  I think someone else has really studied query perf. so it would be cool if that was added too.&lt;/p&gt;</comment>
                    <comment id="12436587" author="otis" created="Thu, 21 Sep 2006 18:31:37 +0100"  >&lt;p&gt;I still haven&apos;t gotten my employer to sign and fax the CCLA, so I&apos;m stuck and can&apos;t contribute my search benchmark.&lt;/p&gt;

&lt;p&gt;I have a suggestion for a name for this - Lube, for Lucene Benchmark - contrib/lube.&lt;/p&gt;</comment>
                    <comment id="12436858" author="mikemccand" created="Fri, 22 Sep 2006 14:47:34 +0100"  >&lt;p&gt;I think this is an incredibly important initiative: with every&lt;br/&gt;
non-trivial change to Lucene (eg lock-less commits) we must verify&lt;br/&gt;
performance did not get worse.  But, as things stand now, it&apos;s an&lt;br/&gt;
ad-hoc thing that each developer needs to do.&lt;/p&gt;

&lt;p&gt;So (as a consumer of this), I would love to have a ready-to-use&lt;br/&gt;
standard test that I could run to check if I&apos;ve slowed things down&lt;br/&gt;
with lock-less commits.&lt;/p&gt;

&lt;p&gt;In the mean time I&apos;ve been using Europarl for my testing.&lt;/p&gt;

&lt;p&gt;Also important to realize is there are many dimensions to test.  With&lt;br/&gt;
lock-less I&apos;m focusing entirely on &quot;wall clock time to open readers&lt;br/&gt;
and writers&quot; in different use cases like pure indexing, pure&lt;br/&gt;
searching, highly interactive mixed indexing/searching, etc.  And this&lt;br/&gt;
is actually hard to test cleanly because in certain cases (highly&lt;br/&gt;
interactive case, or many readers case), the current Lucene hits many&lt;br/&gt;
&quot;commit lock&quot; retries and/or timeouts (whereas lock-less doesn&apos;t).  So&lt;br/&gt;
what&apos;s a &quot;fair&quot; comparison in this case?&lt;/p&gt;

&lt;p&gt;In addition to standardizing on the corpus I think we ideallly need&lt;br/&gt;
standardized hardware / OS / software configuration as well, so the&lt;br/&gt;
numbers are easily comparable across time.  Even the test process&lt;br/&gt;
itself is important, eg details like &quot;you should reboot the box before&lt;br/&gt;
each run&quot; and &quot;discard results from first run then take average of&lt;br/&gt;
next 3 runs as your result&quot;, are important.  It would be wonderful if&lt;br/&gt;
we could get this into a nightly automated regression test so we could&lt;br/&gt;
track over time how the performance has changed (and, for example,&lt;br/&gt;
quickly detect accidental regressions).  We should probably open this&lt;br/&gt;
as a separate issue which depends first on this issue being complete.&lt;/p&gt;</comment>
                    <comment id="12436934" author="klaasm" created="Fri, 22 Sep 2006 18:31:13 +0100"  >&lt;p&gt;A few notes on benchmarks:&lt;/p&gt;

&lt;p&gt;First, it is important to realize that no benchmark will ever fully-capture all aspects of lucene performance, particularly since so many real-world data distributions are so varied.  That said, they are useful tools, especially if they are componentized to measure various aspects of lucene performance (the narrower the goal of the benchmark it, the better a benchmark can be created).&lt;/p&gt;

&lt;p&gt;It is rather unrealistic to expect to standardize hardware / os ... better to compare before/after numbers on a single configuration, rather than comparing the numbers among configurations.  The test process &lt;em&gt;is&lt;/em&gt; important, but anything crucial should be built into the test (like the number of iterations; taking the average, etc).  Concerning the specifics of this: Requiring reboots is onerous and not an important criterion (at least for unix systems-&lt;del&gt;I&apos;m not sufficiently familiar with windows to comment).  Better to stipulate a relatively quiscient machine.  Or perhaps not&lt;/del&gt;-it might be useful to see how the machine load affects lucene performance.  Also, the arithmetic mean is a terrible way of combining results due to its emphasis on outliers.  Better is the average over minimum times of small sets of runs.  &lt;/p&gt;

&lt;p&gt;Of course, any scheme has its problems.  In general, the most important thing when using benchmarks is being aware of the limitations of the benchmark and methodology used.&lt;/p&gt;</comment>
                    <comment id="12436949" author="gsingers" created="Fri, 22 Sep 2006 19:14:08 +0100"  >&lt;p&gt;My comments are marked by GSI&lt;br/&gt;
-----------&lt;/p&gt;

&lt;p&gt;In the mean time I&apos;ve been using Europarl for my testing.&lt;/p&gt;

&lt;p&gt;GSI: perhaps you can contribute once this is setup&lt;/p&gt;

&lt;p&gt;Also important to realize is there are many dimensions to test. With&lt;br/&gt;
lock-less I&apos;m focusing entirely on &quot;wall clock time to open readers&lt;br/&gt;
and writers&quot; in different use cases like pure indexing, pure&lt;br/&gt;
searching, highly interactive mixed indexing/searching, etc. And this&lt;br/&gt;
is actually hard to test cleanly because in certain cases (highly&lt;br/&gt;
interactive case, or many readers case), the current Lucene hits many&lt;br/&gt;
&quot;commit lock&quot; retries and/or timeouts (whereas lock-less doesn&apos;t). So&lt;br/&gt;
what&apos;s a &quot;fair&quot; comparison in this case?&lt;/p&gt;

&lt;p&gt;GSI:  I am planning on taking Andrzej contribution and refactoring it into components that can be reused, as well as creating a &quot;standard&quot; benchmark which will be easy to run through a simple ant task, i.e. ant run-baseline&lt;/p&gt;

&lt;p&gt;GSI: From here, anybody can contribute their own (I will provide interfaces to facilitate this) benchmarks which others can choose to run. &lt;/p&gt;


&lt;p&gt;In addition to standardizing on the corpus I think we ideallly need&lt;br/&gt;
standardized hardware / OS / software configuration as well, so the&lt;br/&gt;
numbers are easily comparable across time. &lt;/p&gt;

&lt;p&gt;GSI: Not really feasible unless you are proposing to buy us machines &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;  I think more important is the ability to do a before and after evaluation (that runs each test several times) as you make changes.  Anybody should be able to do the same.  Run the benchmark, apply the patch and then rerun the benchmark.&lt;/p&gt;</comment>
                    <comment id="12436972" author="dweiss" created="Fri, 22 Sep 2006 19:55:08 +0100"  >&lt;p&gt;First &amp;#8211; I think it&apos;s a good initiative. Grant, when you&apos;re thinking about the infrastructure, it would be pretty neat to have a way of logging performance in a way so that one could draw charts from them. You know, for the visual folks &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;Anyway, my other idea is that benchmarking Lucene can be performed on two levels: one is the user level, where the entire operation counts (such as indexing, searching etc). Another aspect is measurement of atomic parts &lt;em&gt;within&lt;/em&gt; the big operation so that you know how much of the whole thing each subpart takes. I wrote an interesting piece of code once that allows measuring times for named operation (per-thread) in a recursive way. Looks something like this:&lt;/p&gt;

&lt;p&gt;perfLogger.start(&quot;indexing&quot;);&lt;br/&gt;
try {&lt;br/&gt;
  .. code (with recursion etc)  ...&lt;br/&gt;
  perfLogger.start(&quot;subpart&quot;);&lt;br/&gt;
  try { &lt;/p&gt;

&lt;p&gt;  } finally &lt;/p&gt;
{
     perfLogger.stop();
  }
&lt;p&gt;} finally {&lt;br/&gt;
  perfLogger.stop();&lt;br/&gt;
}&lt;/p&gt;

&lt;p&gt;in the output you get something like this:&lt;/p&gt;

&lt;p&gt;indexing: 5 seconds;&lt;br/&gt;
   -&amp;gt;subpart: : 2 seconds;&lt;br/&gt;
   -&amp;gt; ...&lt;/p&gt;

&lt;p&gt;Of course everything comes at a price and the above logging costs some CPU cycles (my implementation stored a nesting stack in ThreadLocals).&lt;/p&gt;

&lt;p&gt;One can always put that code in &apos;if&apos; clauses attached to final variables and enable logging only for benchmarking targets (the compiler will get rid of logging statements then).&lt;/p&gt;

&lt;p&gt;If folks are interested I can dig out that performance logger and maybe adopt it to what Grant comes up with.&lt;/p&gt;</comment>
                    <comment id="12436979" author="mikemccand" created="Fri, 22 Sep 2006 20:17:08 +0100"  >&lt;p&gt;I agree: a simple ant-accessible benchmark to enable &quot;before and&lt;br/&gt;
after&quot; runs is an awesome step forward.  And that a standardized HW/SW&lt;br/&gt;
testing environment is not really realistic now.&lt;/p&gt;

&lt;p&gt;&amp;gt; GSI: perhaps you can contribute once this is setup &lt;/p&gt;

&lt;p&gt;I will try!&lt;/p&gt;</comment>
                    <comment id="12436980" author="doronc" created="Fri, 22 Sep 2006 20:19:06 +0100"  >&lt;p&gt;Few things that would be nice to have in this performance package/framework - &lt;/p&gt;

&lt;p&gt;() indexing only overall time.&lt;br/&gt;
() indexing only time changes as the index grows (might be the case that indexing performance starts to misbehave from a certain size or so).&lt;br/&gt;
() search single user while indexing&lt;br/&gt;
() search only single user&lt;br/&gt;
() search only concurrent users&lt;br/&gt;
() short queries&lt;br/&gt;
() long queries&lt;br/&gt;
() wild card queries&lt;br/&gt;
() range queries&lt;br/&gt;
() queries with rare words&lt;br/&gt;
() queries with common words&lt;br/&gt;
() tokenization/analysis only (above indexing measurements include tokenization, but it would be important to be able to &quot;prove&quot; to oneself that tokenization/analysis time is not hurt by  a recent change).&lt;/p&gt;

&lt;p&gt;() parametric control over:&lt;br/&gt;
() () location of test input data.&lt;br/&gt;
() () location of output index.&lt;br/&gt;
() () location of output log/results.&lt;br/&gt;
() ()  total collection size (total number of bytes/characters read from collection)&lt;br/&gt;
() () document (average) size (bytes/chars) - test can break input data and recompose it into documents of desired size.&lt;br/&gt;
() () &quot;implicit iteration size&quot; - merge-factor, max-buffered-docs&lt;br/&gt;
() () &quot;explicit iteration size&quot; - how often the perf test calls&lt;br/&gt;
() () long queries text&lt;br/&gt;
() () short queries text&lt;br/&gt;
() () which parts of the test framework capabilities to run&lt;br/&gt;
() () number of users / threads.&lt;br/&gt;
() () queries pace - how many queries are fired in, say, a minute.&lt;/p&gt;

&lt;p&gt;Additional points:&lt;br/&gt;
() Would help if all test run parameters are maintained in a properties (or xml config) file, so one can easily modify the test input/output without having to recompile the code.&lt;br/&gt;
() Output to allow easy creation of graphs or so - perhaps best would be to have an result object, so others can easily extend with additional output formats.&lt;br/&gt;
() index size as part of output.&lt;br/&gt;
() number of index files as part of output &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/help_16.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;br/&gt;
() indexing input module that can loop over the input collection. This allows to test indexing of a collection larger than the actual input collection being used. &lt;/p&gt;
</comment>
                    <comment id="12440990" author="gsingers" created="Mon, 9 Oct 2006 23:09:28 +0100"  >&lt;p&gt;OK, I have a preliminary implementation based on adapting Andrzej&apos;s approach.  The interesting thing about this approach, is it is easy to adapt to be more or less exhaustive (i.e. how many of the parameters does one wish to have the system alter as it runs)  Thus, you can have it change the merge factors, max buffered docs, number of documents indexed, number of different queries run, etc.  The tradeoff, of course, is the length of time it takes to run these.&lt;/p&gt;

&lt;p&gt;So my question to those interested, is what is a good baseline running time for testing in a standard way?  My initial thought is to have something that takes between 15-30 minutes to run, but I am not sure on this.  Another approach would be to have three &quot;baselines&quot;:  1. quick validation (5 minutes to run...) 2. standard (15-45) 3. exhaustive (1-10 hours).  &lt;/p&gt;

&lt;p&gt;I know several others have built benchmarking suites for their internal use, what has been your strategy? &lt;/p&gt;

&lt;p&gt;Thoughts, ideas, insights?&lt;/p&gt;

&lt;p&gt;Thanks,&lt;br/&gt;
Grant&lt;/p&gt;</comment>
                    <comment id="12440994" author="creamyg" created="Mon, 9 Oct 2006 23:28:07 +0100"  >&lt;p&gt;The indexing benchmarking apps I wrote take command line arguments for how many docs and how many reps.  My standard test is to do 1000 docs and 6 reps.  Within a couple seconds the first rep is done and the app is printing out results.  For rapid development, having something that speedy is really handy.&lt;/p&gt;</comment>
                    <comment id="12441281" author="cutting" created="Tue, 10 Oct 2006 21:54:20 +0100"  >&lt;p&gt;As Marvin points out, quick micro-benchmarks are great to have.  But other effects only show up when things get very large.  So I think we need at least two baselines: micro and macro.&lt;/p&gt;</comment>
                    <comment id="12444156" author="creamyg" created="Tue, 24 Oct 2006 01:33:43 +0100"  >&lt;p&gt;Grant had asked me if he could reuse some code from the indexer benchmarks I wrote. Here are the relevant files, contributed with the expectation they will be cannibalized, not included verbatim. &lt;/p&gt;</comment>
                    <comment id="12444157" author="creamyg" created="Tue, 24 Oct 2006 01:35:33 +0100"  >&lt;p&gt;One more file...&lt;/p&gt;</comment>
                    <comment id="12447346" author="gsingers" created="Mon, 6 Nov 2006 03:23:14 +0000"  >&lt;p&gt;OK, here is a first crack at a standard benchmark contribution based on Andrzej original contribution and some updates/changes by me.  I wasn&apos;t nearly as ambitious  as some of the comments attached here, but I think most of them are good things to strive for and will greatly benefit Lucene.&lt;/p&gt;

&lt;p&gt;I checked in the basic contrib directory structure, plus some library dependencies, as I wasn&apos;t sure how svn diff handles those.  I am posting this in patch format to solicit comments first instead of just committing and accepting patches.  My thoughts are I&apos;ll take a round of comments and make updates as warranted and then make an initial commit.  &lt;/p&gt;

&lt;p&gt;I am particularly interested in the interface/Driver specification and whether people think this approach is useful or not.  My thoughts behind it were it might be nice to have a standard way of creating/running benchmarks that could be driven by XML configuration files (some examples are in the conf directory).  I am not 100% sold on this and am open to compelling arguments why we should just have each benchmark have it&apos;s own main() method.&lt;/p&gt;

&lt;p&gt;As for the actual Benchmarker, I have created a &quot;standard&quot; version, which runs off the Reuters collection that is downloaded automatically by the ANT task.  There are two ANT targets for the two benchmarks: run-micro-standard and run-standard.  The micro version takes a few minutes to run on my machine (it indexes 2000 docs), the other one takes a lot longer.&lt;/p&gt;

&lt;p&gt;There are several support classes in the stats and util packages.  The stats package supports building and maintaining information about benchmarks.  The utils package contains one class for extracting information out of the Reuters documents for indexing.&lt;/p&gt;

&lt;p&gt;The ReutersQueries class contains a set of Queries I created by looking at some of the docs in the collection and are a myriad of term, phrase, span, wildcard and other types of queries.  They aren&apos;t exhaustive by any means.&lt;/p&gt;

&lt;p&gt;It should be stressed that these benchmarks are best used in gathering before and after numbers.  Furthermore, these aren&apos;t the be all end all of benchmarking for Lucene.  I hope the interface nature will encourage others to submit benchmarks for specific areas of Lucene not covered by this version.&lt;/p&gt;

&lt;p&gt;Thanks to all who contributed their code/thoughts.  Patch to follow&lt;/p&gt;</comment>
                    <comment id="12447347" author="gsingers" created="Mon, 6 Nov 2006 03:25:14 +0000"  >&lt;p&gt;Initial Benchmark code based on Andrzej original contribution plus some changes by me to use the Reuters &quot;standard&quot; collection maintained at &lt;a href=&quot;http://www.daviddlewis.com/resources/testcollections/reuters21578/reuters21578.tar.gz&quot; class=&quot;external-link&quot;&gt;http://www.daviddlewis.com/resources/testcollections/reuters21578/reuters21578.tar.gz&lt;/a&gt;&lt;/p&gt;
</comment>
                    <comment id="12447348" author="gsingers" created="Mon, 6 Nov 2006 03:26:49 +0000"  >&lt;p&gt;To run, checkout contrib/benchmark and then apply the benchmark.patch in the contrib/benchmark directory.&lt;/p&gt;</comment>
                    <comment id="12447744" author="doronc" created="Tue, 7 Nov 2006 10:50:05 +0000"  >&lt;p&gt;I tried it and it is working nice! - &lt;br/&gt;
1st run downloaded the documents from the Web before starting to index. &lt;br/&gt;
2nd run started right off - as input docs are already in place - great. &lt;/p&gt;

&lt;p&gt;Seems the only output is what is printed to stdout, right? &lt;/p&gt;

&lt;p&gt;I got something like this: &lt;br/&gt;
----------------------------&lt;br/&gt;
     &lt;span class=&quot;error&quot;&gt;&amp;#91;echo&amp;#93;&lt;/span&gt; Working Directory: work&lt;br/&gt;
     &lt;span class=&quot;error&quot;&gt;&amp;#91;java&amp;#93;&lt;/span&gt; Testing 4 different permutations.&lt;br/&gt;
     &lt;span class=&quot;error&quot;&gt;&amp;#91;java&amp;#93;&lt;/span&gt; #-- ID: td-00_10_10, Sun Nov 05 22:40:49 PST 2006, heap=1065484288 &amp;#8211;&lt;br/&gt;
     &lt;span class=&quot;error&quot;&gt;&amp;#91;java&amp;#93;&lt;/span&gt; # source=work\reuters-out, directory=org.apache.lucene.store.FSDirectory@D:\devoss\lucene\java\trunk\contrib\benchmark\work\index&lt;br/&gt;
     &lt;span class=&quot;error&quot;&gt;&amp;#91;java&amp;#93;&lt;/span&gt; # maxBufferedDocs=10, mergeFactor=10, compound=true, optimize=true&lt;br/&gt;
     &lt;span class=&quot;error&quot;&gt;&amp;#91;java&amp;#93;&lt;/span&gt; # Query data: R-reopen, W-warmup, T-retrieve, N-no&lt;br/&gt;
     &lt;span class=&quot;error&quot;&gt;&amp;#91;java&amp;#93;&lt;/span&gt; # qd-0110 R W NT &lt;span class=&quot;error&quot;&gt;&amp;#91;body:salomon&amp;#93;&lt;/span&gt;&lt;br/&gt;
     &lt;span class=&quot;error&quot;&gt;&amp;#91;java&amp;#93;&lt;/span&gt; # qd-0111 R W T &lt;span class=&quot;error&quot;&gt;&amp;#91;body:salomon&amp;#93;&lt;/span&gt;&lt;br/&gt;
     &lt;span class=&quot;error&quot;&gt;&amp;#91;java&amp;#93;&lt;/span&gt; # qd-0100 R NW NT &lt;span class=&quot;error&quot;&gt;&amp;#91;body:salomon&amp;#93;&lt;/span&gt;&lt;br/&gt;
...&lt;br/&gt;
     &lt;span class=&quot;error&quot;&gt;&amp;#91;java&amp;#93;&lt;/span&gt; # qd-14011 NR W T &lt;span class=&quot;error&quot;&gt;&amp;#91;body:fo*&amp;#93;&lt;/span&gt;&lt;br/&gt;
     &lt;span class=&quot;error&quot;&gt;&amp;#91;java&amp;#93;&lt;/span&gt; # qd-14000 NR NW NT &lt;span class=&quot;error&quot;&gt;&amp;#91;body:fo*&amp;#93;&lt;/span&gt;&lt;br/&gt;
     &lt;span class=&quot;error&quot;&gt;&amp;#91;java&amp;#93;&lt;/span&gt; # qd-14001 NR NW T &lt;span class=&quot;error&quot;&gt;&amp;#91;body:fo*&amp;#93;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;     &lt;span class=&quot;error&quot;&gt;&amp;#91;java&amp;#93;&lt;/span&gt; Start Time: Sun Nov 05 22:41:38 PST 2006&lt;br/&gt;
     &lt;span class=&quot;error&quot;&gt;&amp;#91;java&amp;#93;&lt;/span&gt;  - processed 500, run id=0&lt;br/&gt;
     &lt;span class=&quot;error&quot;&gt;&amp;#91;java&amp;#93;&lt;/span&gt;  - processed 1000, run id=0&lt;br/&gt;
     &lt;span class=&quot;error&quot;&gt;&amp;#91;java&amp;#93;&lt;/span&gt;  - processed 1500, run id=0&lt;br/&gt;
     &lt;span class=&quot;error&quot;&gt;&amp;#91;java&amp;#93;&lt;/span&gt;  - processed 2000, run id=0&lt;br/&gt;
     &lt;span class=&quot;error&quot;&gt;&amp;#91;java&amp;#93;&lt;/span&gt; End Time: Sun Nov 05 22:41:48 PST 2006&lt;br/&gt;
     &lt;span class=&quot;error&quot;&gt;&amp;#91;java&amp;#93;&lt;/span&gt; warm = Warm Index Reader&lt;br/&gt;
     &lt;span class=&quot;error&quot;&gt;&amp;#91;java&amp;#93;&lt;/span&gt; srch = Search Index&lt;br/&gt;
     &lt;span class=&quot;error&quot;&gt;&amp;#91;java&amp;#93;&lt;/span&gt; trav = Traverse Hits list, optionally retrieving document&lt;/p&gt;

&lt;p&gt;     &lt;span class=&quot;error&quot;&gt;&amp;#91;java&amp;#93;&lt;/span&gt; # testData id	operation	runCnt	recCnt	rec/s	avgFreeMem	avgTotalMem&lt;br/&gt;
     &lt;span class=&quot;error&quot;&gt;&amp;#91;java&amp;#93;&lt;/span&gt; td-00_100_100	addDocument	1	2000	472.0321	4493681	22611558&lt;br/&gt;
     &lt;span class=&quot;error&quot;&gt;&amp;#91;java&amp;#93;&lt;/span&gt; td-00_100_100	optimize	1	1	2.857143	4229488	22716416&lt;br/&gt;
     &lt;span class=&quot;error&quot;&gt;&amp;#91;java&amp;#93;&lt;/span&gt; td-00_100_100	qd-0110-warm	1	2000	40000.0	4250992	22716416&lt;br/&gt;
     &lt;span class=&quot;error&quot;&gt;&amp;#91;java&amp;#93;&lt;/span&gt; td-00_100_100	qd-0110-srch	1	1	Infinity	4221288	22716416&lt;br/&gt;
...&lt;br/&gt;
     &lt;span class=&quot;error&quot;&gt;&amp;#91;java&amp;#93;&lt;/span&gt; td-00_100_100	qd-4110-srch	1	1	Infinity	3993624	22716416&lt;br/&gt;
     &lt;span class=&quot;error&quot;&gt;&amp;#91;java&amp;#93;&lt;/span&gt; td-00_100_100	qd-4110-trav	1	0	NaN	3993624	22716416&lt;br/&gt;
     &lt;span class=&quot;error&quot;&gt;&amp;#91;java&amp;#93;&lt;/span&gt; td-00_100_100	qd-4111-warm	1	2000	50000.0	3853192	22716416&lt;br/&gt;
...&lt;br/&gt;
BUILD SUCCESSFUL&lt;br/&gt;
Total time: 1 minute 0 seconds&lt;br/&gt;
----------------------------&lt;/p&gt;

&lt;p&gt;I think the &quot;infinity&quot; and &quot;NAN&quot; are caused by op time too short for divide-by-sec.&lt;br/&gt;
This can be avoided by modifying getRate() in TimeData:&lt;br/&gt;
  public double getRate() &lt;/p&gt;
{
    double rps = (double) count * 1000.0 / (double) (elapsed&amp;gt;0 ? elapsed : 1);
    return rps;
  }

&lt;p&gt;I like much the logic of loading test data from the Web, and the scaleUp and maximumDocumentsToIndex params are handy. &lt;/p&gt;

&lt;p&gt;It seems that all the test logic and some of its data (queries) are java coded. I initially thought of a setting where we define tasks/jobs that are parameterized, like:&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;createIndex(params)&lt;/li&gt;
	&lt;li&gt;writeToIndex(params):&lt;/li&gt;
	&lt;li&gt;addDocs()&lt;/li&gt;
	&lt;li&gt;optimize()&lt;/li&gt;
	&lt;li&gt;readFromIndex(params):&lt;/li&gt;
	&lt;li&gt;searchIndex()&lt;/li&gt;
	&lt;li&gt;fetchData()&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;..and compose a test by an XML that says which of these simple jobs to run, with what params, in which order, serial/parallel, how long/often etc. &lt;br/&gt;
Then creating a different test is as easy as creating a different XML that configures that test. &lt;/p&gt;

&lt;p&gt;On the other hand, chances are, I know, that most useful cases would be those already defined here - standard and micro-standard, so can ask &quot;why bothering changing to define these building blocks&quot;. I am not sure here, but thought I&apos;ll bring it up. &lt;/p&gt;

&lt;p&gt;About Using the driver - seems nice and clean to me. I don&apos;t know the Digester but it seems to read the config from the XML correctly.&lt;/p&gt;

&lt;p&gt;Other comments:&lt;br/&gt;
1. I think there is a redundant call to params.showRunData(params.getId()) in runBenchmark(File,Options);&lt;br/&gt;
2. Seems that rec/sec would be a bit more accurately computed by aggregating elapsed times (instead of rate) in showRunData()&lt;br/&gt;
3. If TimeData not found (only memData) I think additional 0.0 should be printed&lt;br/&gt;
4. columns allignments with tabs and floats is imperfect.&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;br/&gt;
5. It would be nice I think to also get a summary of the results by &quot;task&quot; - e.g. srch, optimize, something like:&lt;br/&gt;
     &lt;span class=&quot;error&quot;&gt;&amp;#91;java&amp;#93;&lt;/span&gt; # testData id     operation           runCnt     recCnt          rec/s       avgFreeMem      avgTotalMem&lt;br/&gt;
     &lt;span class=&quot;error&quot;&gt;&amp;#91;java&amp;#93;&lt;/span&gt;                   warm                    60       2000       42,628.8        8,235,758       23,048,192&lt;br/&gt;
     &lt;span class=&quot;error&quot;&gt;&amp;#91;java&amp;#93;&lt;/span&gt;                   srch                   120          1          571.4        8,300,613       23,048,192&lt;br/&gt;
     &lt;span class=&quot;error&quot;&gt;&amp;#91;java&amp;#93;&lt;/span&gt;                   optimize                 1          1            2.9        9,375,732       23,048,192&lt;br/&gt;
     &lt;span class=&quot;error&quot;&gt;&amp;#91;java&amp;#93;&lt;/span&gt;                   trav                   120        107       30,517.8        8,326,046       23,048,192&lt;br/&gt;
     &lt;span class=&quot;error&quot;&gt;&amp;#91;java&amp;#93;&lt;/span&gt;                   addDocument              1       2000          441.8        7,310,929       22,206,872&lt;/p&gt;

&lt;p&gt;Attached timedata.zip has modifies TimeData.java and TestData.java for &lt;span class=&quot;error&quot;&gt;&amp;#91;1 to 5&amp;#93;&lt;/span&gt; above, and for the NAN/inifinite. &lt;/p&gt;</comment>
                    <comment id="12447781" author="gsingers" created="Tue, 7 Nov 2006 13:04:19 +0000"  >&lt;p&gt;1st run downloaded the documents from the Web before starting to index. &lt;br/&gt;
2nd run started right off - as input docs are already in place - great. &lt;/p&gt;

&lt;p&gt;Seems the only output is what is printed to stdout, right? &lt;/p&gt;


&lt;p&gt;GSI: The Benchmarker interface does return the TimeData, so other implementations, etc. could use the results programmatically.&lt;/p&gt;



&lt;p&gt;I like much the logic of loading test data from the Web, and the scaleUp and maximumDocumentsToIndex params are handy. &lt;/p&gt;

&lt;p&gt;It seems that all the test logic and some of its data (queries) are java coded. I initially thought of a setting where we define tasks/jobs that are parameterized, like:&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;createIndex(params)&lt;/li&gt;
	&lt;li&gt;writeToIndex(params):&lt;/li&gt;
	&lt;li&gt;addDocs()&lt;/li&gt;
	&lt;li&gt;optimize()&lt;/li&gt;
	&lt;li&gt;readFromIndex(params):&lt;/li&gt;
	&lt;li&gt;searchIndex()&lt;/li&gt;
	&lt;li&gt;fetchData()&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;GSI: I definitely agree that we want a more flexible one to meet people&apos;s benchmarking needs.  I wanted at least one test that is &quot;standard&quot; in that you can&apos;t change the parameters and test cases, so that we can all be on the same page on a run.  Then, when people are having discussions on performance they can say &quot;I ran the standard benchmark before and after and here are the results&quot; and we all know what they are talking about.  I think all the components are there for a parameterized version, all it takes is someone to extend the Standard one or implement there own that reads in a config file.  I will try to put in a fully parameterized version soon.  &lt;/p&gt;


&lt;p&gt;GSI: Thanks for the fixes, I will incorporate into my version and post another patch soon.&lt;/p&gt;</comment>
                    <comment id="12449117" author="doronc" created="Sun, 12 Nov 2006 10:01:34 +0000"  >&lt;p&gt;I looked at extending the benchmark with:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;different test &quot;scenarios&quot;, i.e. other sequences of operations.&lt;/li&gt;
	&lt;li&gt;multithreaded tests, e.g. several queries in parallel.&lt;/li&gt;
	&lt;li&gt;rate of events, e.g. &quot;2 queries arriving per second&quot;, or &quot;one query per second in parallel with 20 new documents in a minute&quot;.&lt;/li&gt;
	&lt;li&gt;different data sources (input documents, queries).&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;For this I made lots of changes to the benchmark code, using parts of it and rewriting other parts. &lt;br/&gt;
I would like to submit this code in a few days - it is running already but some functionality is missing.&lt;/p&gt;

&lt;p&gt;I would like to describe how it works to hopefully get early feedback. &lt;/p&gt;

&lt;p&gt;There are several &quot;basic tasks&quot; defined - all extending an (abstract) class PerfTask:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;AddDocTask&lt;/li&gt;
	&lt;li&gt;OptimizeTask&lt;/li&gt;
	&lt;li&gt;CreateIndexTask&lt;br/&gt;
etc. &lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;To further extend the benchmark &apos;framework&apos;, new tasks can be added. Each task must implement the abstract method: doLogic(). For instance, in AddDocTask this method (doLogic) would call indexWriter.addDocument().&lt;br/&gt;
There are also setup() and tearDown() methods for performing work that should not be timed for that task. &lt;/p&gt;

&lt;p&gt;A special TaskSequence task contains other tasks. It is either parallel or sequential, which tells if it executes its child tasks serially or in parallel. &lt;br/&gt;
TaskSequence also supports &quot;rate&quot;: the pace in which its child tasks are &quot;fired&quot; can be controlled.&lt;/p&gt;

&lt;p&gt;With these tasks, it is possible to describe a performance test &apos;algorithm&apos; in a simple syntax.&lt;br/&gt;
(&apos;algorithm&apos; may be too big a word for this...?)&lt;/p&gt;

&lt;p&gt;A test invocation takes two parameters: &lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;test.properties - file with various config properties.&lt;/li&gt;
	&lt;li&gt;test.alg               - file with the algorithm.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;By convention, for each task class  &quot;OpNameTask&quot;,  the command  &quot;OpName&quot;  is valid in test.alg.&lt;/p&gt;

&lt;p&gt;Adding a single document is done by:&lt;br/&gt;
    AddDoc&lt;/p&gt;

&lt;p&gt;Adding 3 documents:&lt;br/&gt;
   AddDoc&lt;br/&gt;
   AddDoc&lt;br/&gt;
   AddDoc&lt;/p&gt;

&lt;p&gt;Or, alternatively:&lt;br/&gt;
   &lt;/p&gt;
{ AddDoc }
&lt;p&gt; : 3&lt;/p&gt;

&lt;p&gt;So, &apos;&lt;/p&gt;
{&apos; and &apos;}
&lt;p&gt;&apos; indicate a serial sequence of (child) tasks. &lt;/p&gt;

&lt;p&gt;To fire 100 queries in a row:&lt;br/&gt;
  &lt;/p&gt;
{ Search } : 100&lt;br/&gt;
&lt;br/&gt;
To fire 100 queries in parallel:&lt;br/&gt;
  [ Search ] : 100&lt;br/&gt;
&lt;br/&gt;
So, &apos;&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39; and &amp;#39;&amp;#93;&lt;/span&gt;&apos; indicate a parallel group of tasks. &lt;br/&gt;
&lt;br/&gt;
To fire 100 queries in a row, 2 queries per second (120 per minute):&lt;br/&gt;
  { Search }
&lt;p&gt; : 100 : 120&lt;/p&gt;

&lt;p&gt;Similar, but in parallel:&lt;br/&gt;
  [ Search ] : 100 : 120&lt;/p&gt;

&lt;p&gt;A sequence task can be named for identifying it in reports:&lt;br/&gt;
  &lt;/p&gt;
{ &quot;QueriesA&quot; Search }
&lt;p&gt; : 100 : 120&lt;/p&gt;

&lt;p&gt;And there are tasks that create reports. &lt;/p&gt;

&lt;p&gt;There are more tasks, and more to tell on the alg syntax, but this post is already long..&lt;/p&gt;

&lt;p&gt;I find this quite powerful for perf testing.&lt;br/&gt;
What do you (and you) think?&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Doron&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="12449118" author="doronc" created="Sun, 12 Nov 2006 10:12:27 +0000"  >&lt;p&gt;I am attaching a sample tiny.* - the .alg and .properties files I currently use - I think they may help to understand how this works.&lt;/p&gt;</comment>
                    <comment id="12449409" author="gsingers" created="Mon, 13 Nov 2006 16:09:50 +0000"  >&lt;p&gt;OK, how about I commit my changes, then you can add a patch that shows your ideas?&lt;/p&gt;</comment>
                    <comment id="12449419" author="doronc" created="Mon, 13 Nov 2006 17:04:38 +0000"  >&lt;p&gt;Sounds good.&lt;/p&gt;

&lt;p&gt;In this case I will add my stuff under a new package: org.apache.lucene.benchmark2. (this package would have no dependencies in org.apache.lucene.benchmark.). I will also add tarkets in buid.xml, and add .alg, and .alg files under conf.&lt;br/&gt;
Makes sense?&lt;/p&gt;

&lt;p&gt;Do you already know when you are going to commit it?&lt;/p&gt;</comment>
                    <comment id="12449450" author="gsingers" created="Mon, 13 Nov 2006 19:06:30 +0000"  >&lt;p&gt;I&apos;m not a big fan of tacking a number on to the end of Java names, as it doesn&apos;t let you know much about what&apos;s in the file or package.  How about ConfigurableBenchmarker or PropertyBasedBenchmarker or something along those lines, since what you are proposing is a property based one.  I think it can just go in the benchmark package or you could make a sub package under there that is more descriptive.&lt;/p&gt;

&lt;p&gt;I will try to commit tonight or tomorrow morning.&lt;/p&gt;</comment>
                    <comment id="12449779" author="doronc" created="Tue, 14 Nov 2006 20:43:24 +0000"  >&lt;p&gt;Good point on names with numbers - I&apos;m renaming the package to taskBenchmark, as I think of it as &quot;task sequence&quot; based, more than as propetries based. &lt;/p&gt;</comment>
                    <comment id="12449947" author="doronc" created="Wed, 15 Nov 2006 09:12:16 +0000"  >&lt;p&gt;Would be nice to get some feedback on what I already have at this point for the &quot;task based benchmark framework for Lucene&quot;.  &lt;/p&gt;

&lt;p&gt;So I am packing it as a zip file. I would probably resubmit as a patch when Grant commits the current benchmark code.&lt;br/&gt;
See attached taskBenchmark.zip.&lt;/p&gt;

&lt;p&gt;To try out taskBenchmark, unzip under contrib/benchmark, on top of Grant&apos;s benchmark.patch.&lt;br/&gt;
This would do 3 changes:&lt;/p&gt;

&lt;p&gt;1. replace build.xml - only change there is adding two targets: run-task-standard and run-task-micro-standard.&lt;/p&gt;

&lt;p&gt;2. add 4 new files under conf:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;task-standard.properties&lt;/li&gt;
	&lt;li&gt;task-standard.alg&lt;/li&gt;
	&lt;li&gt;task-micro-standard.properties&lt;/li&gt;
	&lt;li&gt;task-micro-standard.alg&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;3. add a src package &apos;taskBenchmark&apos; side by side with current &apos;benchmark&apos; package.&lt;/p&gt;

&lt;p&gt;To try it out, go to contrib/benchmark and try &apos;ant run-task-standard&apos; or &apos;ant run-task-micro-standard&apos;. &lt;/p&gt;

&lt;p&gt;See inside the .alg files for how a test is specified.&lt;/p&gt;

&lt;p&gt;The algorithm syntax and the entire package is documented in the package javadoc for taskBenchmark (package.html). &lt;/p&gt;

&lt;p&gt;Regards,&lt;br/&gt;
Doron&lt;/p&gt;</comment>
                    <comment id="12449949" author="doronc" created="Wed, 15 Nov 2006 09:15:26 +0000"  >&lt;p&gt;Attached taskBenchmark.zip as described earlier.&lt;/p&gt;</comment>
                    <comment id="12450041" author="gsingers" created="Wed, 15 Nov 2006 14:30:44 +0000"  >&lt;p&gt;Committed the benchmark patch plus Doron&apos;s update to TestData and TimeData&lt;/p&gt;</comment>
                    <comment id="12450513" author="doronc" created="Thu, 16 Nov 2006 20:17:16 +0000"  >&lt;p&gt;I am attaching benchmark.byTask.patch - to be applied in the contrib/benchmark directory. &lt;/p&gt;

&lt;p&gt;Root package of byTask classes was modified to org.apache.lucene.benchmark.byTask, in the lines of Grant&apos;s suggestion - seems better cause it keeps all benchmark classes under &lt;br/&gt;
lucene.benchmark.&lt;/p&gt;

&lt;p&gt;I added one a sample .alg under conf and added some documentation. &lt;/p&gt;

&lt;p&gt;Entry point - documentation wise - is the package doc for org.apache.lucene.benchmark.byTask.&lt;/p&gt;

&lt;p&gt;Thanks for any comments on this!&lt;/p&gt;

&lt;p&gt;PS. Before submitting the patch file, I tried to apply it myself on a clean version of the code, just to make sure that it works. But I got errors like this &amp;#8211; Could not retrieve revision 0 of &quot;...\byTask\..&quot; &amp;#8211; for every file under a new folder. So I am not sure if it is just my (Windows) svn patch applying utility, or is it really impossible to apply a patch that creates files in (yet) nonexistent directories.  I searched Lucene mailing lists and SVN mailing lists and went again through the SVN book again but nowhere could I find what is the expected behavior for applying a patch containing new directories. In fact, &quot;svn diff&quot; would not even show you files that are new (again, this is the Windows svn 1.4.2 version). (I used Tortoise SVN to create the patch). This is rather annoying and I might be misunderstanding something basic about SVN, but I thought it&apos;d be better to share this experience here - might save some time for others trying to apply this patch or other patches...&lt;/p&gt;</comment>
                    <comment id="12462117" author="gsingers" created="Thu, 4 Jan 2007 02:39:17 +0000"  >&lt;p&gt;Doron,&lt;/p&gt;

&lt;p&gt;When I apply your patch, I am getting strange errors.  It seems to go through cleanly, but then the new files (for instance, byTask.stats.Report.java) has the whole file occurring twice in each file, thus causing duplicate class exceptions.  This happens for all the files in the byTask package.  The changes in the other files apply cleanly.&lt;/p&gt;

&lt;p&gt;I applied the patch as: patch -p0 -i &amp;lt;patch file&amp;gt; as I always do on a clean version.&lt;/p&gt;

&lt;p&gt;I suspect that your last comment may be at the root of the issue. Can you try applying this again to a clean version and see if you still have issues or whether it is something I am missing?  Can you regenerate this patch, perhaps using a command line tool?  Looking at the patch file, I am not sure what the issue is.  &lt;/p&gt;

&lt;p&gt;Otherwise, based on the documentation, this sounds really interesting and useful.  Based on some of your other patches, I assume you are using this to do benchmarking, no?&lt;/p&gt;

&lt;p&gt;Thanks,&lt;br/&gt;
Grant&lt;/p&gt;</comment>
                    <comment id="12462287" author="doronc" created="Thu, 4 Jan 2007 19:03:42 +0000"  >&lt;p&gt;Grant, thanks for trying this out - I will update the patch shortly. &lt;br/&gt;
I am using this for benchmarking - quite easy to add new stuff - and in fact I added some stuff lately but did not update here because wasn&apos;t sure if others are interested. &lt;br/&gt;
I will verify what I have with svn head and pack it here as an updated patch.&lt;br/&gt;
Regards,&lt;br/&gt;
Doron&lt;/p&gt;</comment>
                    <comment id="12462402" author="doronc" created="Fri, 5 Jan 2007 04:41:49 +0000"  >&lt;p&gt;This update of the byTask package includes:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;allowing to tailor a perf test &quot;programmically&quot; (without an .alg file).&lt;/li&gt;
	&lt;li&gt;maintaining both the &quot;algorithm&quot; and the run-properties in a single .alg file - this is easier to maintain in my opinion.&lt;/li&gt;
	&lt;li&gt;some code cleanup.&lt;/li&gt;
	&lt;li&gt;build.xml has a single &quot;task related&quot; target now: run-task. an ant property is used to invoke other .alg files.&lt;/li&gt;
	&lt;li&gt;documentation updated (package docs under byTask).&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;To apply the patch from the trunk dir:   patch -p0 -i &amp;lt;byTask.2.patch.txt&amp;gt;&lt;br/&gt;
To test it, cd to contrib/benchmark and type:  ant run-task&lt;/p&gt;

&lt;p&gt;Grant, I noticed that the patch file contains EOL characters - Unix/DOS thing I guess.&lt;br/&gt;
But &apos;patch&apos; works cleanly for me either with these characters or without them, so I am leaving these characters there.&lt;br/&gt;
I hope this patch applies cleanly for you.&lt;/p&gt;</comment>
                    <comment id="12463792" author="gsingers" created="Thu, 11 Jan 2007 03:21:38 +0000"  >&lt;p&gt;Hey Doron, &lt;/p&gt;

&lt;p&gt;Your patch uses JDK 1.5.  I am assuming it is safe to use Class.getName in place of Class.getSimpleName, right?  I think once I do that plus change the String.contains calls to String.indexOf it should all be fine, right?  I have it compiling and running, so that is a good sign.  I will look to commit soon.&lt;/p&gt;

&lt;p&gt;-Grant&lt;/p&gt;</comment>
                    <comment id="12463830" author="doronc" created="Thu, 11 Jan 2007 07:47:42 +0000"  >&lt;p&gt;Oops... I had the impression that compiling with compliance level 1.4 is sufficient to prevent this, but guess I need to read again what that compliance level setting guarantees exactly. &lt;/p&gt;

&lt;p&gt;Anyhow there are a 3 things that require 1.5:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Boolean.parseBoolean() --&amp;gt; Boolean.valueOf().booleanValue()&lt;/li&gt;
	&lt;li&gt;String.contains() --&amp;gt; indexOf()&lt;/li&gt;
	&lt;li&gt;Class.getSimpleName() --&amp;gt; ?&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Modifying Class.getSimpleName() to Class.getName() would not be very nice - queries prints and task names prints would be quite ugly. To fix that I added a method simpleName(Class) to byTask.util.Format. I am attaching an updated patch - byTask.jre1.4.patch.txt - that includes this method and removes the Java 1.5  dependency.&lt;/p&gt;

&lt;p&gt;Thanks for catching this!&lt;br/&gt;
Doron&lt;/p&gt;</comment>
                    <comment id="12464410" author="gsingers" created="Sat, 13 Jan 2007 04:15:18 +0000"  >&lt;p&gt;Doron, &lt;/p&gt;

&lt;p&gt;I have committed your additions.  This truly is great stuff.  Thank you so much for contributing.  The documentation (code and package level) is well done, the output is very readable.  The alg language is a bit cryptic and takes a little deciphering, but you do document it quite nicely.   I like the extendability factor and I think it will make it easier for people to contribute benchmarking capabilities.&lt;/p&gt;

&lt;p&gt;I would love to see someone mod the reporting mechanism in the future to allow for printing info to something other than System.out, as I know people have expressed interest in being able to slurp the output into Excel or similar number crunching tools.   This could also lead to the possibility of running some of the algorithms nightly and then integrating with JUnitPerf or some other performance unit testing approach.&lt;/p&gt;

&lt;p&gt;We may want to consider deprecating the other benchmarking stuff, although, I suppose it can&apos;t hurt to have multiple opinions in this area.&lt;/p&gt;

&lt;p&gt;At any rate, this is very much appreciated.  I would encourage everyone who is interested in benchmarking to take a look and provide feedback.  I&apos;m going to mark this bug as finished for now as I think we have a good baseline for benchmarking at this point.&lt;/p&gt;

&lt;p&gt;Thanks again,&lt;br/&gt;
Grant&lt;/p&gt;

</comment>
                    <comment id="12464411" author="gsingers" created="Sat, 13 Jan 2007 04:16:33 +0000"  >&lt;p&gt;Have committed a baseline benchmarking suite thanks to Doron and Andrzej.   Bugs can now be opened specific to the code in the contrib area.&lt;/p&gt;</comment>
                    <comment id="12466241" author="gsingers" created="Sat, 20 Jan 2007 13:20:43 +0000"  >&lt;p&gt;This has been committed and is available for use.  New issues can be opened on specific problems.&lt;/p&gt;</comment>
                    <comment id="12901057" author="creamyg" created="Sat, 21 Aug 2010 18:15:46 +0100"  >&lt;p&gt;During the course of a recent IP audit, I determined that two out of three&lt;br/&gt;
files I contributed to &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-675&quot; title=&quot;Lucene benchmark: objective performance test for Lucene&quot;&gt;&lt;del&gt;LUCENE-675&lt;/del&gt;&lt;/a&gt; back in 2006 were in fact based on an&lt;br/&gt;
original written by Murray Walker: LuceneIndexer.java and&lt;br/&gt;
BenchmarkingIndexer.pm.   (The third file, &quot;extract_reuters.plx&quot;, was my own&lt;br/&gt;
work as advertised.)&lt;/p&gt;

&lt;p&gt;Murray has graciously expressed a willingness to license his work to Apache,&lt;br/&gt;
but since the files in question were not used, the consensus opinion is that&lt;br/&gt;
it would be best to delete them.  For further reference, see the&lt;br/&gt;
legal-discuss@a.o archives: &amp;lt;&lt;a href=&quot;http://markmail.org/message/4esu3owjxft5n2f7&quot; class=&quot;external-link&quot;&gt;http://markmail.org/message/4esu3owjxft5n2f7&lt;/a&gt;&amp;gt;.&lt;/p&gt;

&lt;p&gt;I feel very fortunate that the problematic contributions were not integrated&lt;br/&gt;
into Lucene and that it was the work of an eminently reasonable solo author&lt;br/&gt;
whose work was inadvertently contributed without permission.  I apologize to&lt;br/&gt;
Murray and to the Lucene community for my errors.&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12345153" name="benchmark.byTask.patch" size="341655" author="doronc" created="Thu, 16 Nov 2006 20:17:16 +0000" />
                    <attachment id="12344361" name="benchmark.patch" size="72440" author="gsingers" created="Mon, 6 Nov 2006 03:25:14 +0000" />
                    <attachment id="12348320" name="byTask.2.patch.txt" size="223388" author="doronc" created="Fri, 5 Jan 2007 04:42:36 +0000" />
                    <attachment id="12348701" name="byTask.jre1.4.patch.txt" size="224022" author="doronc" created="Thu, 11 Jan 2007 07:48:23 +0000" />
                    <attachment id="12343479" name="extract_reuters.plx" size="3630" author="creamyg" created="Tue, 24 Oct 2006 01:33:43 +0100" />
                    <attachment id="12341249" name="LuceneBenchmark.java" size="32048" author="ab" created="Thu, 21 Sep 2006 06:18:24 +0100" />
                    <attachment id="12345014" name="taskBenchmark.zip" size="66584" author="doronc" created="Wed, 15 Nov 2006 09:15:26 +0000" />
                    <attachment id="12344460" name="timedata.zip" size="6935" author="doronc" created="Tue, 7 Nov 2006 10:50:05 +0000" />
                    <attachment id="12344833" name="tiny.alg" size="2584" author="doronc" created="Sun, 12 Nov 2006 10:12:27 +0000" />
                    <attachment id="12344834" name="tiny.properties" size="994" author="doronc" created="Sun, 12 Nov 2006 10:12:27 +0000" />
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>10.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Thu, 21 Sep 2006 05:43:49 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>13077</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>27055</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                    <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                <customfieldname>Time in Status</customfieldname>
                <customfieldvalues>
                    
                </customfieldvalues>
            </customfield>
                            </customfields>
    </item>
</channel>
</rss>