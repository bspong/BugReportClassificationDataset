<!-- 
RSS generated by JIRA (5.2.8#851-sha1:3262fdc28b4bc8b23784e13eadc26a22399f5d88) at Tue Jul 16 13:08:02 UTC 2013

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/LUCENE-2075/LUCENE-2075.xml?field=key&field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>5.2.8</version>
        <build-number>851</build-number>
        <build-date>26-02-2013</build-date>
    </build-info>

<item>
            <title>[LUCENE-2075] Share the Term -&gt; TermInfo cache across threads</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-2075</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Right now each thread creates its own (thread private) SimpleLRUCache,&lt;br/&gt;
holding up to 1024 terms.&lt;/p&gt;

&lt;p&gt;This is rather wasteful, since if there are a high number of threads&lt;br/&gt;
that come through Lucene, you&apos;re multiplying the RAM usage.  You&apos;re&lt;br/&gt;
also cutting way back on likelihood of a cache hit (except the known&lt;br/&gt;
multiple times we lookup a term within-query, which uses one thread).&lt;br/&gt;
In NRT search we open new SegmentReaders (on tiny segments) often&lt;br/&gt;
which each thread must then spend CPU/RAM creating &amp;amp; populating.&lt;/p&gt;

&lt;p&gt;Now that we are on 1.5 we can use java.util.concurrent.*, eg&lt;br/&gt;
ConcurrentHashMap.  One simple approach could be a double-barrel LRU&lt;br/&gt;
cache, using 2 maps (primary, secondary).  You check the cache by&lt;br/&gt;
first checking primary; if that&apos;s a miss, you check secondary and if&lt;br/&gt;
you get a hit you promote it to primary.  Once primary is full you&lt;br/&gt;
clear secondary and swap them.&lt;/p&gt;

&lt;p&gt;Or... any other suggested approach?&lt;/p&gt;</description>
                <environment></environment>
            <key id="12440860">LUCENE-2075</key>
            <summary>Share the Term -&gt; TermInfo cache across threads</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png">Closed</status>
                    <resolution id="1">Fixed</resolution>
                                <assignee username="mikemccand">Michael McCandless</assignee>
                                <reporter username="mikemccand">Michael McCandless</reporter>
                        <labels>
                    </labels>
                <created>Mon, 16 Nov 2009 21:32:44 +0000</created>
                <updated>Fri, 10 May 2013 11:43:03 +0100</updated>
                    <resolved>Fri, 27 Nov 2009 15:33:18 +0000</resolved>
                                            <fixVersion>4.0-ALPHA</fixVersion>
                                <component>core/index</component>
                        <due></due>
                    <votes>1</votes>
                        <watches>1</watches>
                                                    <comments>
                    <comment id="12778645" author="jasonrutherglen" created="Mon, 16 Nov 2009 23:27:00 +0000"  >&lt;p&gt;Solr used CHM as an LRU, however it turned out to be somewhat&lt;br/&gt;
less than truly LRU? I&apos;d expect Google Collections to offer a&lt;br/&gt;
concurrent linked hash map however no dice?&lt;br/&gt;
&lt;a href=&quot;http://code.google.com/p/google-collections/&quot; class=&quot;external-link&quot;&gt;http://code.google.com/p/google-collections/&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;Maybe there&apos;s a way to build a concurrent LRU using their CHM?&lt;/p&gt;</comment>
                    <comment id="12778675" author="earwin" created="Tue, 17 Nov 2009 01:16:34 +0000"  >&lt;p&gt;There&apos;s no such thing in Google Collections. However, look at this - &lt;a href=&quot;http://code.google.com/p/concurrentlinkedhashmap/&quot; class=&quot;external-link&quot;&gt;http://code.google.com/p/concurrentlinkedhashmap/&lt;/a&gt;&lt;/p&gt;</comment>
                    <comment id="12779055" author="mikemccand" created="Tue, 17 Nov 2009 19:14:34 +0000"  >&lt;p&gt;Since Solr already has already created a concurrent LRU, I think we simply reuse that?  Is there any reason not  to?&lt;/p&gt;

&lt;p&gt;I don&apos;t think we need absolutely truly LRU for the terminfo cache.&lt;/p&gt;</comment>
                    <comment id="12779071" author="markrmiller@gmail.com" created="Tue, 17 Nov 2009 19:27:50 +0000"  >&lt;p&gt;We should prob compare with google&apos;s (its apache 2 licensed, so why not)&lt;/p&gt;

&lt;p&gt;Solr has two synchronized lru caches - LRUCache, which is basically just a synchronized LinkedHashMap, and FastLRUCache which I believe tries to minimize the cost of gets - however, unless you have a high hit ratio, it was tested as slower than LRUCache.&lt;/p&gt;</comment>
                    <comment id="12779247" author="mikemccand" created="Wed, 18 Nov 2009 01:35:49 +0000"  >&lt;blockquote&gt;&lt;p&gt;We should prob compare with google&apos;s (its apache 2 licensed, so why not)&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Well, that&apos;s just hosted on code.google.com (ie it&apos;s not &quot;Google&apos;s&quot;), and reading its description it sounds sort of experimental (though they do state that they created a &quot;Production Version&quot;).  It made me a bit nervous... however, it does sound people use it in &quot;production&quot;.&lt;/p&gt;

&lt;p&gt;I think FastLRUCache is probably best for Lucene, because it scales up well w/ high number of threads?  My guess is it&apos;s slower cost for low hit rates is negligible to Lucene, but I&apos;ll run some perf tests.&lt;/p&gt;

&lt;p&gt;It looks like ConcurrentLRUCache (used by FastLRUCache, but the latter does other solr-specific things) is the right low-level one to use for Lucene?&lt;/p&gt;</comment>
                    <comment id="12779253" author="markrmiller@gmail.com" created="Wed, 18 Nov 2009 01:51:35 +0000"  >&lt;blockquote&gt;&lt;p&gt;Well, that&apos;s just hosted on code.google.com (ie it&apos;s not &quot;Google&apos;s&quot;), &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Ah - got that vibe, but it didn&apos;t really hit me.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;though they do state that they created a &quot;Production Version&quot;&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Right - thats what I was thinking we might try. Though the whole, trying this from scratch to learn is a bit scary too &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/wink.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; But hey, I&apos;m not recommending, just perhaps trying it.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I think FastLRUCache is probably best for Lucene, because it scales up well w/ high number of threads?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Indeed - though if we expect a low hit ratio, we might still compare it with regular old synchronized LinkedHashMap to be sure. In certain cases, puts become quite expensive I think.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;It looks like ConcurrentLRUCache (used by FastLRUCache, but the latter does other solr-specific things) is the right low-level one to use for Lucene?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Right.&lt;/p&gt;</comment>
                    <comment id="12779255" author="markrmiller@gmail.com" created="Wed, 18 Nov 2009 01:56:36 +0000"  >&lt;p&gt;When/If yonik finally pops up here, he will have some good info to add I think.&lt;/p&gt;</comment>
                    <comment id="12779354" author="thetaphi" created="Wed, 18 Nov 2009 07:40:19 +0000"  >&lt;p&gt;Should this ConcurrentLRUCache not better be fitted into the o.a.l.util.cache package?&lt;/p&gt;

&lt;p&gt;About the Solr implementation: The generification has a &quot;small&quot; problem: get(), contains(), remove() and other by-key-querying methods should use Object as type for the key, not the generic K, because it is not bad to test with contains any java type (it would just return false). The sun generic howto explains that, also this one: &lt;a href=&quot;http://smallwig.blogspot.com/2007/12/why-does-setcontains-take-object-not-e.html&quot; class=&quot;external-link&quot;&gt;http://smallwig.blogspot.com/2007/12/why-does-setcontains-take-object-not-e.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Very funny video about that: &lt;a href=&quot;http://www.youtube.com/watch?v=wDN_EYUvUq0&quot; class=&quot;external-link&quot;&gt;http://www.youtube.com/watch?v=wDN_EYUvUq0&lt;/a&gt; (explaination starts at 4:35)&lt;/p&gt;</comment>
                    <comment id="12779437" author="mikemccand" created="Wed, 18 Nov 2009 13:15:42 +0000"  >&lt;p&gt;I&apos;ll work out a simple perf test to compare the options...&lt;/p&gt;</comment>
                    <comment id="12779512" author="earwin" created="Wed, 18 Nov 2009 16:18:02 +0000"  >&lt;p&gt;&amp;gt; Well, that&apos;s just hosted on code.google.com (ie it&apos;s not &quot;Google&apos;s&quot;), and reading its description it sounds sort of experimental (though they do state that they created a &quot;Production Version&quot;). It made me a bit nervous... however, it does sound people use it in &quot;production&quot;.&lt;/p&gt;

&lt;p&gt;I run it in production for several months (starting from &apos;experimental&apos; version) as a cache for Filters. No visible problems.&lt;/p&gt;</comment>
                    <comment id="12779514" author="yseeley@gmail.com" created="Wed, 18 Nov 2009 16:29:02 +0000"  >&lt;p&gt;The Solr one could be simplified a lot for Lucene... no need to keep some of the statistics and things like &quot;isLive&quot;.&lt;/p&gt;

&lt;p&gt;Testing via something like the double barrel approach will be tricky.  The behavior of ConcurrentLRUCache (i.e. the cost of puts) depends on the access pattern - in the best cases, a single linear scan would be all that&apos;s needed.  In the worst case, a subset of the  map needs to go into a priority queue.  It&apos;s all in markAndSweep... that&apos;s my monster - let me know if the comments don&apos;t make sense.&lt;/p&gt;

&lt;p&gt;How many entries must be removed to be considered a success also obviously affects whether a single linear scan is enough.  If that&apos;s often the case, some other optimizations can be done such as not collecting the entries for further passes:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
          &lt;span class=&quot;code-comment&quot;&gt;// This entry *could* be in the bottom group.
&lt;/span&gt;          &lt;span class=&quot;code-comment&quot;&gt;// Collect these entries to avoid another full pass... &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; is wasted
&lt;/span&gt;          &lt;span class=&quot;code-comment&quot;&gt;// effort &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; enough entries are normally removed in &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; first pass.
&lt;/span&gt;          &lt;span class=&quot;code-comment&quot;&gt;// An alternate impl could make a full second pass.&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                    <comment id="12779551" author="yseeley@gmail.com" created="Wed, 18 Nov 2009 17:53:44 +0000"  >&lt;p&gt;Here&apos;s a simplified version of Solr&apos;s ConcurrentLRUCache.&lt;/p&gt;</comment>
                    <comment id="12779560" author="thetaphi" created="Wed, 18 Nov 2009 18:02:56 +0000"  >&lt;p&gt;Looks good! Can this cache subclass the abstract (Map)Cache; it is in the correct package but does not subclass Cache?&lt;/p&gt;</comment>
                    <comment id="12779586" author="yseeley@gmail.com" created="Wed, 18 Nov 2009 18:37:52 +0000"  >&lt;p&gt;Here&apos;s a new version extending Cache&amp;lt;K,V&amp;gt;&lt;/p&gt;</comment>
                    <comment id="12779622" author="thetaphi" created="Wed, 18 Nov 2009 19:26:46 +0000"  >&lt;p&gt;As PriorityQueue is generified since Lucene 3.0, I added missing generics. The class now compiles without unchecked warnings. I also removed lots of casts and parameterized the missing parts. Also added K type for inner map.&lt;/p&gt;

&lt;p&gt;Nice work, even if I do not understand it completely &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                    <comment id="12779631" author="thetaphi" created="Wed, 18 Nov 2009 19:40:07 +0000"  >&lt;p&gt;Sorry a small problem with cast. Will upload new patch, soon.&lt;/p&gt;</comment>
                    <comment id="12779632" author="yseeley@gmail.com" created="Wed, 18 Nov 2009 19:44:05 +0000"  >&lt;p&gt;New patch attached - while refreshing my memory on the exact algorithm, I noticed a bug &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;br/&gt;
Things won&apos;t work well after 2B accesses since Integer.MAX_VALUE is used instead of Long.MAX_VALUE.&lt;br/&gt;
Need to go fix Solr now too &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                    <comment id="12779638" author="michaelbusch" created="Wed, 18 Nov 2009 20:05:12 +0000"  >&lt;blockquote&gt;
&lt;p&gt;Things won&apos;t work well after 2B accesses since Integer.MAX_VALUE is used&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;From ReentrantLock javadocs:&lt;br/&gt;
&quot;This lock supports a maximum of 2147483648 recursive locks by the same thread.&quot;&lt;/p&gt;

&lt;p&gt;I think you only use the lock for markAndSweep and everything else uses atomics, but ConcurrentHashMap uses ReentrantLocks internally for each segment. So overall, things wil probably run longer than 2B ops, but not sure how long.&lt;/p&gt;</comment>
                    <comment id="12779639" author="thetaphi" created="Wed, 18 Nov 2009 20:06:06 +0000"  >&lt;p&gt;Hi Yonik, thaks, that you used my class, but I found one type erasure problem in the PQueue, because thee Heap is erasured to Object[] by javac. The getValues() tries to cast this array -&amp;gt; ClassCastException. This is described here: &lt;a href=&quot;http://safalra.com/programming/java/wrong-type-erasure/&quot; class=&quot;external-link&quot;&gt;http://safalra.com/programming/java/wrong-type-erasure/&lt;/a&gt;&lt;br/&gt;
The same happens in myInsertWithOverflow().&lt;/p&gt;

&lt;p&gt;Will fix.&lt;/p&gt;</comment>
                    <comment id="12779640" author="yseeley@gmail.com" created="Wed, 18 Nov 2009 20:11:54 +0000"  >&lt;blockquote&gt;&lt;p&gt;&quot;This lock supports a maximum of 2147483648 recursive locks by the same thread.&quot;&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I read this as a maximum of recursive locks (which this class won&apos;t do at all)... not the total number of times one can successfully lock/unlock the lock.&lt;/p&gt;

&lt;p&gt;This cache impl should be able to support 1B operations per second for almost 300 years (i.e. the time it would take to overflow a long).&lt;/p&gt;</comment>
                    <comment id="12779641" author="psmith@apache.org" created="Wed, 18 Nov 2009 20:16:56 +0000"  >&lt;blockquote&gt;&lt;p&gt;This cache impl should be able to support 1B operations per second for almost 300 years (i.e. the time it would take to overflow a long).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Hopefully Sun has released Java 7 by then. &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                    <comment id="12779646" author="thetaphi" created="Wed, 18 Nov 2009 20:25:06 +0000"  >&lt;p&gt;Patch that fixes the bug in javac with typed arrays (because of that it does not allow generic array creation - the problem is that heap is a generic array in PQ, but implemented as Object[]).&lt;/p&gt;

&lt;p&gt;I fixed the PQueue by returning a List&amp;lt;CacheEntry&amp;lt;K,V&amp;gt;&amp;gt; values() and also made the private maxSize in the PriorityQueue protected. So it does not need to implement an own insertWithOverflow. As this class moves to Lucene Core, we should not make such bad hacks. &lt;/p&gt;

&lt;p&gt;We need a good testcase for the whole cache class. It was hard to me to find a good test that hits the PQueue at all (its only used in special cases). Hard stuff &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/sad.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                    <comment id="12779885" author="thetaphi" created="Thu, 19 Nov 2009 08:51:50 +0000"  >&lt;p&gt;Updated patch, adds missing @Overrides, we added in 3.0 and also makes the private PQ implement Iterable, the markAndSweep code is now synactical sugar &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                    <comment id="12780571" author="mikemccand" created="Fri, 20 Nov 2009 14:39:23 +0000"  >&lt;p&gt;First cut at a benchmark.  First, download&lt;br/&gt;
&lt;a href=&quot;http://concurrentlinkedhashmap.googlecode.com/files/clhm-production.jar&quot; class=&quot;external-link&quot;&gt;http://concurrentlinkedhashmap.googlecode.com/files/clhm-production.jar&lt;/a&gt;&lt;br/&gt;
and put into your lib subdir, then run &quot;ant -lib&lt;br/&gt;
lib/clhm-production.jar compile-core&quot;, then run it something like&lt;br/&gt;
this:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
java -server -Xmx1g -Xms1g -cp build/classes/java:lib/clhm-production.jar org.apache.lucene.util.cache.LRUBench 4 5.0 0.0 1024 1024
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The args are:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;numThreads&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;runSec&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;sharePct &amp;#8211; what %tg of the terms should be shared b/w the threads&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;cacheSize&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;termCountPerThread &amp;#8211; how many terms each thread will cycle through&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;The benchmark first sets up arrays of strings, per thread, based&lt;br/&gt;
termsCountPerThread &amp;amp; sharePct.  Then each thread steps through the&lt;br/&gt;
array, and for each entry, tries to get the string, and if it&apos;s not&lt;br/&gt;
present, puts it.  It records the hit &amp;amp; miss count, and prints summary&lt;br/&gt;
stats in the end, doing 3 rounds.&lt;/p&gt;

&lt;p&gt;To mimic Lucene, each entry is tested twice in a row, ie, the 2nd time&lt;br/&gt;
we test the entry, it should be a hit.  Ie we expect a hit rate of 50%&lt;br/&gt;
if sharePct is 0.&lt;/p&gt;

&lt;p&gt;Here&apos;s my output from the above command line, using Java 1.6.0_14 (64&lt;br/&gt;
bit) on OpenSolaris:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
numThreads=4 runSec=5.0 sharePct=0.0 cacheSize=1024 termCountPerThread=1024

LRU cache size is 1024; each thread steps through 1024 strings; 0 of which are common

round 0
  sync(LinkedHashMap): Mops/sec=2.472 hitRate=50.734
  DoubleBarreLRU: Mops/sec=20.502 hitRate=50
  ConcurrentLRU: Mops/sec=17.936 hitRate=84.409
  ConcurrentLinkedHashMap: Mops/sec=1.248 hitRate=50.033

round 1
  sync(LinkedHashMap): Mops/sec=2.766 hitRate=50.031
  DoubleBarreLRU: Mops/sec=17.66 hitRate=50
  ConcurrentLRU: Mops/sec=17.82 hitRate=83.726
  ConcurrentLinkedHashMap: Mops/sec=1.266 hitRate=50.331

round 2
  sync(LinkedHashMap): Mops/sec=2.714 hitRate=50.168
  DoubleBarreLRU: Mops/sec=17.912 hitRate=50
  ConcurrentLRU: Mops/sec=17.866 hitRate=84.156
  ConcurrentLinkedHashMap: Mops/sec=1.26 hitRate=50.254
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;NOTE: I&apos;m not sure about the correctness of DoubleBarrelLRU &amp;#8211; I just&lt;br/&gt;
quickly wrote it.&lt;/p&gt;

&lt;p&gt;Also, the results for ConcurrentLRUCache are invalid (its hit rate is&lt;br/&gt;
way too high) &amp;#8211; I think this is because its eviction process can take&lt;br/&gt;
a longish amount of time, which temporarily allows the map to hold way&lt;br/&gt;
too many entries, and means it&apos;s using up alot more transient RAM than&lt;br/&gt;
it should.&lt;/p&gt;

&lt;p&gt;In theory DoubleBarrelLRU should be vulnerable to the same issue, but&lt;br/&gt;
in practice it seems to affect it much less (I guess because&lt;br/&gt;
CHM.clear() must be very fast).&lt;/p&gt;

&lt;p&gt;I&apos;m not sure how to fix the benchmark to workaround that... maybe we&lt;br/&gt;
bring back the cleaning thread (from Solr&apos;s version), and give it a&lt;br/&gt;
high priority?&lt;/p&gt;

&lt;p&gt;Another idea: I wonder whether a simple cache-line like cache would be&lt;br/&gt;
sufficient.  Ie, we hash to a fixed slot and we evict whatever is&lt;br/&gt;
there.&lt;/p&gt;</comment>
                    <comment id="12780795" author="yseeley@gmail.com" created="Fri, 20 Nov 2009 22:31:20 +0000"  >&lt;blockquote&gt;&lt;p&gt;Also, the results for ConcurrentLRUCache are invalid (its hit rate is&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;way too high) - I think this is because its eviction process can take&lt;br/&gt;
a longish amount of time, which temporarily allows the map to hold way&lt;br/&gt;
too many entries, and means it&apos;s using up alot more transient RAM than&lt;br/&gt;
it should.&lt;/p&gt;

&lt;p&gt;Yep - there&apos;s no hard limit.  It&apos;s not an issue in practice in Solr since doing the work to generate a new entry to put in the cache is much more expensive than cache cleaning (i.e. generation will never swamp cleaning).  Seems like a realistic benchmark would do some amount of work on a cache miss?  Or perhaps putting it in lucene and doing real benchmarks?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Another idea: I wonder whether a simple cache-line like cache would be sufficient. Ie, we hash to a fixed slot and we evict whatever is&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;there.&lt;/p&gt;

&lt;p&gt;We need to balance the overhead of the cache with the hit ratio and the cost of a miss. for the String intern cache, the cost of a miss is very low, hence lowering overhead but giving up hit ratio is the right trade-off.  For this term cache, the cost of a miss seems relatively high, and warrants increasing overhead to increase the hit ratio.&lt;/p&gt;</comment>
                    <comment id="12780819" author="yseeley@gmail.com" created="Fri, 20 Nov 2009 23:03:43 +0000"  >&lt;p&gt;Aside: a singe numeric range query will be doing many term seeks (one at the start of each enumeration).  It doesn&apos;t look like these will currently utilize the cache - can someone refresh my memory on why this is?  We should keep the logic that prevents the cache while iterating over terms with a term enumerator, but it seems like using the cache for the initial seek would be nice.&lt;/p&gt;</comment>
                    <comment id="12780824" author="thetaphi" created="Fri, 20 Nov 2009 23:11:33 +0000"  >&lt;p&gt;The initial seek should really be optimized, this also affects the new AutomatonTermEnum for the future of RegEx queries, WildCardQueries and maybe FuzzyQueries with DFAs. With the automaton enum, depending of the DFA, there can be lot&apos;s of seeks (&lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-1606&quot; title=&quot;Automaton Query/Filter (scalable regex)&quot;&gt;&lt;del&gt;LUCENE-1606&lt;/del&gt;&lt;/a&gt;).&lt;/p&gt;</comment>
                    <comment id="12780944" author="mikemccand" created="Sat, 21 Nov 2009 10:47:18 +0000"  >
&lt;blockquote&gt;&lt;p&gt;a singe numeric range query will be doing many term seeks (one at the start of each enumeration). It doesn&apos;t look like these will currently utilize the cache - can someone refresh my memory on why this is?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;You&apos;re right &amp;#8211; here&apos;s the code/comment:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
  /** Returns an enumeration of terms starting at or after the named term. */
  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; SegmentTermEnum terms(Term term) &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; IOException {
    &lt;span class=&quot;code-comment&quot;&gt;// don&apos;t use the cache in &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; call because we want to reposition the
&lt;/span&gt;    &lt;span class=&quot;code-comment&quot;&gt;// enumeration
&lt;/span&gt;    get(term, &lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;);
    &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; (SegmentTermEnum)getThreadResources().termEnum.clone();
  }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I think this is because &quot;useCache&quot; (the 2nd arg to get) is overloaded&lt;br/&gt;
&amp;#8211; if you look at get(), if useCache is true and you have a cache hit,&lt;br/&gt;
it doesn&apos;t do it&apos;s &quot;normal&quot; side-effect of repositioning the&lt;br/&gt;
thread-private TermEnum.  So you&apos;d get incorrect results.&lt;/p&gt;

&lt;p&gt;If get had a 2nd arg &quot;repositionTermEnum&quot;, to decouple caching from&lt;br/&gt;
repositioning, then we could make use of the cache for NRQ (&amp;amp; soon&lt;br/&gt;
AutomatonTermEnum as well), though, this isn&apos;t so simple because the&lt;br/&gt;
cache entry (just a TermInfo) doesn&apos;t store the term&apos;s ord.  And we&lt;br/&gt;
don&apos;t want to add ord to TermInfo since, eg, this sucks up alot of&lt;br/&gt;
extra RAM storing the terms index.  Probably we should make a new&lt;br/&gt;
class that&apos;s used for caching, and not reuse TermInfo.&lt;/p&gt;

&lt;p&gt;This was also done before NumericRangeQuery, ie, all MTQs before NRQ&lt;br/&gt;
did a single seek.&lt;/p&gt;

&lt;p&gt;BTW the flex branch fixes this &amp;#8211; TermsEnum.seek always checks the&lt;br/&gt;
cache.&lt;/p&gt;</comment>
                    <comment id="12780946" author="mikemccand" created="Sat, 21 Nov 2009 11:03:39 +0000"  >&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;Also, the results for ConcurrentLRUCache are invalid (its hit rate is way too high) - I think this is because its eviction process can take a longish amount of time, which temporarily allows the map to hold way too many entries, and means it&apos;s using up alot more transient RAM than it should.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yep - there&apos;s no hard limit. It&apos;s not an issue in practice in Solr since doing the work to generate a new entry to put in the cache is much more expensive than cache cleaning (i.e. generation will never swamp cleaning).  Seems like a realistic benchmark would do some amount of work on a cache miss? Or perhaps putting it in lucene and doing real benchmarks?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I agree the test is synthetic, so the blowup we&apos;re seeing is a worse&lt;br/&gt;
case sitatuion, but are you really sure this can never be hit in&lt;br/&gt;
practice?&lt;/p&gt;

&lt;p&gt;EG as CPUs gain more and more cores... it becomes more and more&lt;br/&gt;
possible with time that the 1 thread that&apos;s trying to do the cleaning&lt;br/&gt;
will be swamped by the great many threads generating.  Then if the CPU&lt;br/&gt;
is over-saturated (too many threads running), that 1 thread doing the&lt;br/&gt;
cleaning only gets slices of CPU time vs all the other threads that&lt;br/&gt;
may be generating...&lt;/p&gt;

&lt;p&gt;It makes me nervous using a collection that, in the &quot;perfect storm&quot;,&lt;br/&gt;
suddenly consumes way too much RAM.  It&apos;s a leaky abstraction.&lt;/p&gt;

&lt;p&gt;That said, I agree the test is obviously very synthetic.  It&apos;s not&lt;br/&gt;
like a real Lucene installation will be pushing 2M QPS through Lucene&lt;br/&gt;
any time soon...&lt;/p&gt;

&lt;p&gt;But still I&apos;m more comfortable w/ the simplicity of the double-barrel&lt;br/&gt;
approach.  In my tests its performance is in the same ballpark as&lt;br/&gt;
ConcurrentLRUCache; it&apos;s much simpler; and the .clear() calls appear&lt;br/&gt;
in practice to very quickly free up the entries.&lt;/p&gt;

&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;Another idea: I wonder whether a simple cache-line like cache would be sufficient. Ie, we hash to a fixed slot and we evict whatever is there.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;We need to balance the overhead of the cache with the hit ratio and the cost of a miss. for the String intern cache, the cost of a miss is very low, hence lowering overhead but giving up hit ratio is the right trade-off. For this term cache, the cost of a miss seems relatively high, and warrants increasing overhead to increase the hit ratio.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;OK I agree.&lt;/p&gt;

&lt;p&gt;Yet another option... would be to create some sort of &quot;thread-private&lt;br/&gt;
Query scope&quot;, ie, a store that&apos;s created &amp;amp; cleared per-Query where&lt;br/&gt;
Lucene can store things.  When a Term&apos;s info is retrieved, it&apos;d be&lt;br/&gt;
stored here, and then that &quot;query-private&quot; cache is consulted whenever&lt;br/&gt;
that Term is looked up again within that query.  This would be the&lt;br/&gt;
&quot;perfect cache&quot; in that a single query would never see its terms&lt;br/&gt;
evicted due to other queries burning through the cache...&lt;/p&gt;

&lt;p&gt;Though, net/net I suspect the overhead of creating/pulling from this&lt;br/&gt;
new cache would just be an overall search slowdown in practice.&lt;/p&gt;</comment>
                    <comment id="12780977" author="yseeley@gmail.com" created="Sat, 21 Nov 2009 14:10:15 +0000"  >&lt;blockquote&gt;&lt;p&gt;I agree the test is synthetic, so the blowup we&apos;re seeing is a worse case sitatuion, but are you really sure this can never be hit in practice?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I&apos;m personally comfortable that Solr isn&apos;t going to hit this for it&apos;s uses of the cache... it&apos;s simply the relative cost of generating a cache entry vs doing some cleaning.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;But still I&apos;m more comfortable w/ the simplicity of the double-barrel approach. In my tests its performance is in the same ballpark as ConcurrentLRUCache;&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;But it wouldn&apos;t be the same performance in Lucene - a cache like LinkedHashMap would achieve a higher hit rate in real world scenarios.&lt;/p&gt;</comment>
                    <comment id="12781040" author="thetaphi" created="Sat, 21 Nov 2009 20:08:35 +0000"  >&lt;blockquote&gt;&lt;p&gt;BTW the flex branch fixes this - TermsEnum.seek always checks the cache.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Can we fix this for trunk, too? But I think &lt;b&gt;this&lt;/b&gt; issue talks about trunk.&lt;/p&gt;</comment>
                    <comment id="12781045" author="rcmuir" created="Sat, 21 Nov 2009 20:28:35 +0000"  >&lt;p&gt;Hi, I applied automaton patch and its benchmark (&lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-1606&quot; title=&quot;Automaton Query/Filter (scalable regex)&quot;&gt;&lt;del&gt;LUCENE-1606&lt;/del&gt;&lt;/a&gt;) against the flex branch, and kept with the old TermEnum api.&lt;/p&gt;

&lt;p&gt;I tested two scenarios, an old index created with 3.0 (trunk) and a new index created with flex branch.&lt;br/&gt;
in both cases, its slower than trunk, but I assume this is due to flex branch not being optimized yet?... (last i saw it used new String() placeholder for utf conversion)&lt;/p&gt;

&lt;p&gt;but i think it is fair to compare the flex branch with itself, with old idx versus new idx. I can only assume with a new idx it is using the caching.&lt;br/&gt;
these numbers are stable on HEAD and do not deviate much.&lt;br/&gt;
feel free to look at the benchmark code over there and suggest improvements if you think there is an issue with it.&lt;/p&gt;

&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Pattern&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Iter&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;AvgHits&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;AvgMS (old idx)&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;AvgMS (new idx)&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;N?N?N?N&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;10&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1000.0&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;86.6&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;70.2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;?NNNNNN&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;10&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;10.0&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;3.0&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;??NNNNN&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;10&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;100.0&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;12.5&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;7.2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;???NNNN&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;10&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1000.0&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;86.9&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;34.8&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;????NNN&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;10&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;10000.0&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;721.2&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;530.5&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;NN??NNN&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;10&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;100.0&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;8.3&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;4.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;NN?N*&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;10&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;10000.0&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;149.1&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;143.2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;?NN*&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;10&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;100000.0&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1061.4&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;836.7&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;*N&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;10&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1000000.0&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;16329.7&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;11480.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;NNNNN??&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;10&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;100.0&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2.7&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;2.2&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

</comment>
                    <comment id="12781110" author="mikemccand" created="Sun, 22 Nov 2009 10:35:22 +0000"  >&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;BTW the flex branch fixes this - TermsEnum.seek always checks the cache.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Can we fix this for trunk, too? But I think this issue talks about trunk.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Right, this issue is about trunk (not flex API).  I think we could fix this (see my suggestions above), basically decoupling &quot;useCache&quot; from &quot;seekTheEnum&quot;... but we have to fix the terminfo cache to also store the term&apos;s ord.  I&apos;ll try out this approach...&lt;/p&gt;</comment>
                    <comment id="12781112" author="mikemccand" created="Sun, 22 Nov 2009 10:38:13 +0000"  >
&lt;blockquote&gt;&lt;p&gt;in both cases, its slower than trunk, but I assume this is due to flex branch not being optimized yet?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The automaton benchmark looks great &amp;#8211; I&apos;ll dig into why the flex branch&lt;br/&gt;
is slower in both of these cases.&lt;/p&gt;

&lt;p&gt;The first case tests old API on top of an old index, which I&apos;m&lt;br/&gt;
surprised to see not matching trunk&apos;s performance.  The flex changes&lt;br/&gt;
are supposed to &quot;optimize&quot; that case by directly using the old (trunk)&lt;br/&gt;
code.&lt;/p&gt;

&lt;p&gt;The second test tests old API emulated over a flex index, which I&apos;m&lt;br/&gt;
also surprised to see is not faster than trunk &amp;#8211; there must be&lt;br/&gt;
something silly going on in the API emulation.&lt;/p&gt;

&lt;p&gt;I&apos;ll dig...&lt;/p&gt;

&lt;p&gt;When I tested MTQs (TermRangeQuery, WildcardQuery), using flex API on&lt;br/&gt;
flex index, they were reasonably faster, so I&apos;ll also try to get&lt;br/&gt;
automaton&apos;s FilteredTermEnum cutover to the flex API, and test that.&lt;/p&gt;</comment>
                    <comment id="12781370" author="mikemccand" created="Mon, 23 Nov 2009 11:44:07 +0000"  >&lt;p&gt;Attached patch; all tests pass:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;Switches the terms dict cache away from per-thread cache to shared&lt;br/&gt;
    (DoubleBarrelLRU) cache&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Still uses the cache when seeking the term enum&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;However, I&apos;m baffled: I re-ran the BenchWildcard test and saw no&lt;br/&gt;
measurable improvement in ????NNN query (yet, I confirmed it&apos;s now&lt;br/&gt;
storing into and then hitting on the cache), but I did see a gain in&lt;br/&gt;
the *N query (from ~4300 msec before to ~3500 msec) which I can&apos;t&lt;br/&gt;
explain because that query doens&apos;t use the cache at all (just the&lt;br/&gt;
linear scan).  I&apos;m confused....&lt;/p&gt;

&lt;p&gt;Robert maybe you can try this patch plus automaton patch and see if&lt;br/&gt;
you see this same odd behavior?&lt;/p&gt;</comment>
                    <comment id="12781372" author="mikemccand" created="Mon, 23 Nov 2009 11:48:39 +0000"  >&lt;p&gt;I ended up subclassing TermInfo (had to remove its &quot;final&quot; modifier) to make a TermInfoAndOrd class that adds &quot;int termOrd&quot;, and then fixed TermInfosReader.get to create that class and put/get into cache.&lt;/p&gt;

&lt;p&gt;Now, in get we always consult the cache, and store into it for the non-sequential case, but now the 2nd boolean arg is &quot;mustSeekEnum&quot;.  So if we have a cache hit, but must seek the enum, we fall through and do the existing scan/seek logic, but avoid the binary search through the terms index since we can use the ord from the cache hit.&lt;/p&gt;</comment>
                    <comment id="12781383" author="rcmuir" created="Mon, 23 Nov 2009 12:31:34 +0000"  >&lt;blockquote&gt;&lt;p&gt;Robert maybe you can try this patch plus automaton patch and see if you see this same odd behavior?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;confirmed, though on my machine, it is 4 second avg *N versus 6 second avg *N &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;I havent looked at the code, but fyi, even the smart mode is always &quot;in term order&quot; traversal, its just skipping over terms.&lt;br/&gt;
I think numeric range might do &quot;out of order&quot;? Uwe can confirm.&lt;br/&gt;
I don&apos;t know if this matters, either.&lt;/p&gt;</comment>
                    <comment id="12781384" author="thetaphi" created="Mon, 23 Nov 2009 12:33:07 +0000"  >&lt;p&gt;Have you tried aut with NRQ, too? If not I will run a comparison with a integer index before/after this patch and measure query times. For lower precSteps it should get faster, as seeking the TermEnum is optimized.&lt;/p&gt;

&lt;p&gt;To your patch: Looks good, I would only add @Overrides to the DoubleBarrelCache. What do we do with Yonik/mine&apos;s cache?&lt;/p&gt;</comment>
                    <comment id="12781385" author="thetaphi" created="Mon, 23 Nov 2009 12:41:50 +0000"  >&lt;blockquote&gt;&lt;p&gt;think numeric range might do &quot;out of order&quot;? Uwe can confirm.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;No its also in order. It starts with highest precision (first lower end, then upper end sub-range), which has a shift value of 0. This is smaller that lower prec terms with a bigger shift value that come later. And of course each sub-range is in ascending order because the terms are &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                    <comment id="12781386" author="rcmuir" created="Mon, 23 Nov 2009 12:42:48 +0000"  >&lt;p&gt;Uwe, thanks. so both the enums behave in a very similar way.&lt;/p&gt;</comment>
                    <comment id="12781391" author="mikemccand" created="Mon, 23 Nov 2009 13:06:38 +0000"  >&lt;blockquote&gt;&lt;p&gt;To your patch: Looks good, I would only add @Overrides to the DoubleBarrelCache.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Ahh right will do &amp;#8211; not quite in Java 5 mode yet &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/wink.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;What do we do with Yonik/mine&apos;s cache?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Solr&apos;s ConcurrentLRUCache makes me somewhat nervous, that it can blow up under high (admittedly, rather synthetic, by today&apos;s standards) load.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Have you tried aut with NRQ, too?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I haven&apos;t; that&apos;d be great if you could &amp;amp; report back.&lt;/p&gt;

&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;Robert maybe you can try this patch plus automaton patch and see if you see this same odd behavior?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;confirmed, though on my machine, it is 4 second avg *N versus 6 second avg *N &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Weird &amp;#8211; I can&apos;t explain why this full scan is faster but the skipping scan is not.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I havent looked at the code, but fyi, even the smart mode is always &quot;in term order&quot; traversal, its just skipping over terms.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Right &amp;#8211; but that skipping variant is now pulling from cache.  Let&apos;s see what NRQ results look like... though it does quite a bit less seeking than eg the ????NNN query.&lt;/p&gt;</comment>
                    <comment id="12781395" author="thetaphi" created="Mon, 23 Nov 2009 13:21:53 +0000"  >&lt;p&gt;I updated the patch to add overrides. I also had to add one SupressWarnings, because the get() method does an unchecked cast (because it modifies the map, which is not in the contract of get(), but it&apos;s safe, because it only adds the key to the second map, if the first map already contains it, and therefore the key has correct type).&lt;/p&gt;

&lt;p&gt;I will start now my tests with NRQ.&lt;/p&gt;</comment>
                    <comment id="12781402" author="mikemccand" created="Mon, 23 Nov 2009 13:47:26 +0000"  >&lt;p&gt;Thanks Uwe!&lt;/p&gt;

&lt;p&gt;I attached another one: made DBLRU final, tweaked javadocs, fixed spelling in the saturation comment, add you guys to the CHANGES entry.&lt;/p&gt;</comment>
                    <comment id="12781407" author="yseeley@gmail.com" created="Mon, 23 Nov 2009 14:05:28 +0000"  >&lt;p&gt;What about replacing the expensive division with a comparison with zero?&lt;/p&gt;

&lt;p&gt;if (putCount.decrementAndGet()==0) &lt;/p&gt;
{
  secondary.clear();
  swapCount.getAndIncrement();
  putCount.set(maxSize);
}

&lt;p&gt;Also, I don&apos;t think there&apos;s a need for swapCount?  A simple &quot;volatile boolean swapped&quot; should do?&lt;/p&gt;</comment>
                    <comment id="12781410" author="thetaphi" created="Mon, 23 Nov 2009 14:10:47 +0000"  >&lt;p&gt;I tested with an 5 mio doc index containing trie ints, but it seems that trie does not really profit from the seeking cache. With the default precStep of 4 no difference (max. 16 seeks per query), and with precStep of 1 (max. 64 seeks per query) it was even a little bit slower on average (???). The test compares also with FieldCacheRangeFilter which is always faster (because no deletes, optimized index), also the field cache loading time did not really change (linear scan in term enum).&lt;/p&gt;

&lt;p&gt;PrecisionStep: 4&lt;br/&gt;
trunk:&lt;br/&gt;
loading field cache time: 6367.667678 ms&lt;br/&gt;
avg number of terms: 68.1&lt;br/&gt;
TRIE:       best time=6.323709 ms; worst time=414.367469 ms; avg=201.18463369999998 ms; sum=32004735&lt;br/&gt;
FIELDCACHE: best time=64.770523 ms; worst time=265.487652 ms; avg=155.5479675 ms; sum=32004735&lt;/p&gt;

&lt;p&gt;patch:&lt;br/&gt;
loading field cache time: 6295.055377 ms&lt;br/&gt;
avg number of terms: 68.1&lt;br/&gt;
TRIE:       best time=5.288102 ms; worst time=415.290771 ms; avg=195.72079685 ms; sum=32004735&lt;br/&gt;
FIELDCACHE: best time=65.511957 ms; worst time=202.482438 ms; avg=138.69083925 ms; sum=32004735&lt;/p&gt;

&lt;p&gt;&amp;#8212;&lt;/p&gt;

&lt;p&gt;PrecisionStep: 1&lt;br/&gt;
trunk:&lt;br/&gt;
loading field cache time: 6416.105399 ms&lt;br/&gt;
avg number of terms: 19.85&lt;br/&gt;
TRIE:       best time=6.51228 ms; worst time=410.624255 ms; avg=192.33796475 ms; sum=32002505&lt;br/&gt;
FIELDCACHE: best time=65.349088 ms; worst time=211.308979 ms; avg=143.71657580000002 ms; sum=32002505&lt;/p&gt;

&lt;p&gt;patch:&lt;br/&gt;
loading field cache time: 6809.792026 ms&lt;br/&gt;
avg number of terms: 19.85&lt;br/&gt;
TRIE:       best time=6.814832 ms; worst time=436.396525 ms; avg=205.6526038 ms; sum=32002505&lt;br/&gt;
FIELDCACHE: best time=64.939539 ms; worst time=277.474371 ms; avg=142.58939345 ms; sum=32002505&lt;/p&gt;</comment>
                    <comment id="12781452" author="mikemccand" created="Mon, 23 Nov 2009 16:27:18 +0000"  >&lt;blockquote&gt;
&lt;p&gt;What about replacing the expensive division with a comparison with zero?&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (putCount.decrementAndGet()==0) { secondary.clear(); swapCount.getAndIncrement(); putCount.set(maxSize); }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/blockquote&gt;

&lt;p&gt;Good idea!  I&apos;ll do that.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Also, I don&apos;t think there&apos;s a need for swapCount? A simple &quot;volatile boolean swapped&quot; should do?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Excellent &amp;#8211; I&apos;ll do that too.&lt;/p&gt;</comment>
                    <comment id="12781454" author="mikemccand" created="Mon, 23 Nov 2009 16:30:17 +0000"  >&lt;p&gt;Uwe, why do you see so much variance in your times?  EG best for trie is 5-6 msec, but avg is ~190-205 msec.&lt;/p&gt;</comment>
                    <comment id="12781460" author="thetaphi" created="Mon, 23 Nov 2009 16:34:07 +0000"  >&lt;p&gt;Because of random ranges on the whole range. If you only request a very short range, it is faster (less seeks because maybe only highest precision affected for very short ranges) vs a full range query which may seek the maximum count. It is reproduceable, because the random seed was identical.&lt;/p&gt;</comment>
                    <comment id="12781505" author="mikemccand" created="Mon, 23 Nov 2009 17:59:54 +0000"  >&lt;blockquote&gt;&lt;p&gt;Because of random ranges on the whole range. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Ahh, OK.&lt;/p&gt;

&lt;p&gt;I wonder if your test is getting any cache hits at all &amp;#8211; if you do random ranges, and never repeat queries, then likely your hit rate is quite low?&lt;/p&gt;</comment>
                    <comment id="12781511" author="mikemccand" created="Mon, 23 Nov 2009 18:15:16 +0000"  >&lt;p&gt;New patch, folding in Yonik&apos;s suggestions, adding a unit test (carried&lt;br/&gt;
over from TestSimpleLRUCache &amp;#8211; I&apos;ll &quot;svn mv&quot; when I commit it), and&lt;br/&gt;
deprecating SimpleLRUCache.&lt;/p&gt;</comment>
                    <comment id="12781594" author="thetaphi" created="Mon, 23 Nov 2009 20:12:25 +0000"  >&lt;blockquote&gt;&lt;p&gt;I wonder if your test is getting any cache hits at all - if you do random ranges, and never repeat queries, then likely your hit rate is quite low?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I am quite sure that also Robert&apos;s test is random (as he explained).&lt;/p&gt;

&lt;p&gt;I fixed the test to only test few queries and repeat them quite often. For precStep=4 and long values, I got about 28 seeks per query, but there was no speed improvement. Maybe 28 seeks / query is too less for an effect. The number of terms seen per query was 70, so about 2.5 terms/seek which is typical for precStep=4 with this index value density (5 Mio random number in the range 2^-63..2^63). It is also important, that the random ranges hit many documents (in avg 1/3 of all docs), so most time in my opinion is used in collecting the results. Maybe I should try shorter and limited ranges.&lt;/p&gt;

&lt;p&gt;Robert: How many term enum seeks did your queries produce?&lt;/p&gt;

&lt;p&gt;Currently I am indexing a 100 Mio docs, precStep=1, long values index (64 terms per doc). Let&apos;s see what happens here.&lt;/p&gt;

&lt;p&gt;If you deprecate SimpleLRUCache, you can also deprecate the MapCache abstract super class. But I wouldn&apos;t like to deprecate these classes, as I for myself use them in my own code for e.g. caching queries etc. And even if you deprecate the Map, why remove the tests, they should stay alive until the class is removed?&lt;/p&gt;

&lt;p&gt;Uwe&lt;/p&gt;</comment>
                    <comment id="12781621" author="thetaphi" created="Mon, 23 Nov 2009 21:10:26 +0000"  >&lt;p&gt;I changed my benchmark to better show the seek caching effect. For NRQ the overall improvement has no neglectible effect.&lt;/p&gt;

&lt;p&gt;I chenged the rewrite mode of the NRQ to SCORING_BOOLEAN_QUEY and then just rewrote the queries to BQ and measured time. So no TermDocs/Collecting was in effect:&lt;/p&gt;

&lt;p&gt;trunk: avg number of terms: 68.537; avg seeks=28.838; best time=1.022756 ms; worst time=17.036802 ms; avg=1.8388833272 ms&lt;br/&gt;
patch: avg number of terms: 68.537; avg seeks=28.838; best time=1.066616 ms; worst time=12.80917 ms; avg=1.6932529156 ms&lt;/p&gt;

&lt;p&gt;You see the effect of the caching. The code ran 5000 rewrites with each query repeated 20 times.&lt;/p&gt;</comment>
                    <comment id="12781627" author="mikemccand" created="Mon, 23 Nov 2009 21:17:17 +0000"  >&lt;blockquote&gt;&lt;p&gt;I am quite sure that also Robert&apos;s test is random (as he explained).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;It&apos;s not random &amp;#8211; it&apos;s the specified pattern, parsed to&lt;br/&gt;
WildcardQuery, run 10 times, then take best or avg time.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;I fixed the test to only test few queries and repeat them quite often. For precStep=4 and long values, I got about 28 seeks per query, but there was no speed improvement. Maybe 28 seeks / query is too less for an effect. The number of terms seen per query was 70, so about 2.5 terms/seek which is typical for precStep=4 with this index value density (5 Mio random number in the range 2^-63..2^63). It is also important, that the random ranges hit many documents (in avg 1/3 of all docs), so most time in my opinion is used in collecting the results. Maybe I should try shorter and limited ranges.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;OK... it sounds like the differences may simply be in the noise for NRQ.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;If you deprecate SimpleLRUCache, you can also deprecate the MapCache abstract super class. But I wouldn&apos;t like to deprecate these classes, as I for myself use them in my own code for e.g. caching queries etc.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Hmm... I felt like because nothing in Lucene uses SimpleLRUCache&lt;br/&gt;
anymore, we should deprecate &amp;amp; remove it.  I don&apos;t think we should be&lt;br/&gt;
in the business of creating/exporting (as public classes) such&lt;br/&gt;
collections, unless we continue to use them.&lt;/p&gt;

&lt;p&gt;I even wonder why we don&apos;t put these classes into oal.index, and make&lt;br/&gt;
them package private.  Ie, I think we make them public only to share&lt;br/&gt;
them across packages within lucene, not because we want/expect apps to&lt;br/&gt;
consume them.  The term &quot;public&quot; is heavily overloaded,&lt;br/&gt;
unfortunately.&lt;/p&gt;

&lt;p&gt;Also, I already put a strong note to this effect in&lt;br/&gt;
DoubleBarrelLRUCache, ie we reserve future right to up and remove the&lt;br/&gt;
class.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;And even if you deprecate the Map, why remove the tests, they should stay alive until the class is removed?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Oh good point &amp;#8211; I&apos;ll resurrect &amp;amp; keep it.&lt;/p&gt;</comment>
                    <comment id="12781628" author="mikemccand" created="Mon, 23 Nov 2009 21:18:23 +0000"  >&lt;blockquote&gt;&lt;p&gt;You see the effect of the caching. The code ran 5000 rewrites with each query repeated 20 times.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;OK so it sounds like NRQ in practice won&apos;t see a boost from this (it&apos;s too good already &amp;#8211; does too little seeking &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/wink.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;, though with this &lt;span class=&quot;error&quot;&gt;&amp;#91;contrived&amp;#93;&lt;/span&gt; test you were able to show the new seek cache is having a small positive effect.&lt;/p&gt;</comment>
                    <comment id="12781633" author="yseeley@gmail.com" created="Mon, 23 Nov 2009 21:27:05 +0000"  >&lt;blockquote&gt;&lt;p&gt;For NRQ the overall improvement has no neglectible effect. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Perhaps it&apos;s just the ratio of seeks to TermDocs.next() for the relatively large indexes you were testing against?&lt;/p&gt;</comment>
                    <comment id="12781667" author="thetaphi" created="Mon, 23 Nov 2009 22:24:03 +0000"  >&lt;blockquote&gt;&lt;p&gt;Perhaps it&apos;s just the ratio of seeks to TermDocs.next() for the relatively large indexes you were testing against?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Exactly thats the difference to AutomatonQuery: The AutomatonTermEnum does lot&apos;s of seeking in the TermEnum to not scan all terms. But each term normally has the same (small) docFreq. For NRQ, some terms have a very high docFreq (e.g. up to 100,000 for this large index in lowest precision), so most of the time in the query is enumerating docs.&lt;/p&gt;</comment>
                    <comment id="12781855" author="thetaphi" created="Tue, 24 Nov 2009 10:25:28 +0000"  >&lt;p&gt;Just one question: The cache is initialized with max 1024 entries. Why that number. If we share the cache between multiple threads, maybe we should raise the max size. Or make it configureable?&lt;/p&gt;

&lt;p&gt;The entries in the cache are not very costly, why not use 8192 or 16384, MTQs would be happy with that?&lt;/p&gt;</comment>
                    <comment id="12781863" author="mikemccand" created="Tue, 24 Nov 2009 11:03:23 +0000"  >&lt;p&gt;Well, I just kept 1024 since that&apos;s what we currently do &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/wink.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;OK I just did a rough tally &amp;#8211; I think we&apos;re looking at ~100 bytes (on&lt;br/&gt;
32 bit JRE) per entry, including CHMs HashEntry, array in CHM,&lt;br/&gt;
TermInfoAndOrd, Term &amp;amp; its String text.&lt;/p&gt;

&lt;p&gt;Not to mention DBLRU has 2X multiplier at peak, so 200 bytes.&lt;/p&gt;

&lt;p&gt;So at 1024 we&apos;re looking at ~200KB peak used by this cache already,&lt;br/&gt;
per segment which is able to saturate that cache... so for a 20&lt;br/&gt;
segment index you&apos;re at ~4MB additional RAM consumed... so I don&apos;t&lt;br/&gt;
think we should increase this default.&lt;/p&gt;

&lt;p&gt;Also, I don&apos;t think this cache is/should be attempting to achieve a&lt;br/&gt;
high hit rate &lt;b&gt;across&lt;/b&gt; queries, only &lt;b&gt;within&lt;/b&gt; a single query when that&lt;br/&gt;
query resolves the Term more than once.&lt;/p&gt;

&lt;p&gt;I think caches that wrap more CPU, like Solr&apos;s query cache, are where&lt;br/&gt;
the app should aim for high hit rate.&lt;/p&gt;

&lt;p&gt;Maybe we should even decrease the default size here &amp;#8211; what&apos;s&lt;br/&gt;
important is preventing in-fligh queries from evicting one another&apos;s&lt;br/&gt;
cache entries.&lt;/p&gt;

&lt;p&gt;For NRQ, 1024 is apparently already plenty big for that (relatively&lt;br/&gt;
few seeks occur).&lt;/p&gt;

&lt;p&gt;For automaton query, which does lots of seeking, once flex branch&lt;br/&gt;
lands there is no need for the cache (each lookup is done only once,&lt;br/&gt;
because the TermsEnum actualEnum is able to seek).  Before flex lands,&lt;br/&gt;
the cache is important, but only for automaton query I think.&lt;/p&gt;

&lt;p&gt;And honestly I&apos;m still tempted to do away with this cache altogether&lt;br/&gt;
and create a &quot;query scope&quot;, private to each query while it&apos;s running,&lt;br/&gt;
where terms dict (and other places that need to, over time) could&lt;br/&gt;
store stuff.  That&apos;d give a perfect within-query hit rate and wouldn&apos;t&lt;br/&gt;
tie up any long term RAM...&lt;/p&gt;</comment>
                    <comment id="12781869" author="thetaphi" created="Tue, 24 Nov 2009 11:17:41 +0000"  >&lt;blockquote&gt;
&lt;p&gt;And honestly I&apos;m still tempted to do away with this cache altogether&lt;br/&gt;
and create a &quot;query scope&quot;, private to each query while it&apos;s running,&lt;br/&gt;
where terms dict (and other places that need to, over time) could&lt;br/&gt;
store stuff. That&apos;d give a perfect within-query hit rate and wouldn&apos;t&lt;br/&gt;
tie up any long term RAM...&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;With Query Scope you mean a whole query, so not only a MTQ? If you combine multiple AutomatonQueries in a BooleanQuery it could also profit from the cache (as it is currently).&lt;/p&gt;

&lt;p&gt;I think until Flex, we should commit this and use the cache. When Flex is out, we may think of doing this different.&lt;/p&gt;</comment>
                    <comment id="12781876" author="mikemccand" created="Tue, 24 Nov 2009 11:43:02 +0000"  >&lt;blockquote&gt;&lt;p&gt;With Query Scope you mean a whole query, so not only a MTQ? If you combine multiple AutomatonQueries in a BooleanQuery it could also profit from the cache (as it is currently).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Right, I think the top level query would open up the scope... and free it once it&apos;s done running.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I think until Flex, we should commit this and use the cache. When Flex is out, we may think of doing this different.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;OK let&apos;s go with the shared cache for now, and revisit once flex lands.  I&apos;ll open a new issue...&lt;/p&gt;

&lt;p&gt;But should we drop cache to maybe 512?  Typing up 4MB RAM (with cache size 1024) for a &quot;normal&quot; index is kinda alot...&lt;/p&gt;</comment>
                    <comment id="12781878" author="mikemccand" created="Tue, 24 Nov 2009 11:45:27 +0000"  >&lt;p&gt;OK I opened &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2093&quot; title=&quot;Use query-private scope instead of shared Term-&amp;gt;TermInfo cache&quot;&gt;&lt;del&gt;LUCENE-2093&lt;/del&gt;&lt;/a&gt; to track the &quot;query private scope&quot; idea.&lt;/p&gt;</comment>
                    <comment id="12781880" author="thetaphi" created="Tue, 24 Nov 2009 11:46:29 +0000"  >&lt;p&gt;I would keep it as it is, because we already minimized memory requirements, because before the cache was per-thread.&lt;/p&gt;</comment>
                    <comment id="12781882" author="rcmuir" created="Tue, 24 Nov 2009 11:51:30 +0000"  >&lt;p&gt;i am still triyng to figure out the use case.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;With Query Scope you mean a whole query, so not only a MTQ? If you combine multiple AutomatonQueries in a BooleanQuery it could also profit from the cache (as it is currently).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;isn&apos;t there a method i can use to force these to combine into one AutomatonQuery (I can use union, intersection, etc)?&lt;br/&gt;
I haven&apos;t done this, but we shouldnt create a private scoped-cache for something like this?&lt;/p&gt;</comment>
                    <comment id="12781885" author="thetaphi" created="Tue, 24 Nov 2009 11:55:25 +0000"  >&lt;p&gt;...not only AutomatonQueries can be combined, they can also be combined with other queries and then make use of the cache.&lt;/p&gt;</comment>
                    <comment id="12781886" author="rcmuir" created="Tue, 24 Nov 2009 12:00:29 +0000"  >&lt;p&gt;Uwe i just wonder if the cache would in practice get used much. &lt;/p&gt;</comment>
                    <comment id="12781887" author="thetaphi" created="Tue, 24 Nov 2009 12:03:21 +0000"  >&lt;p&gt;For testing we could add two AtomicIntegers to the cache that counts hits and requests to get a hit rate, only temporary to not affect performance.&lt;/p&gt;</comment>
                    <comment id="12781908" author="mikemccand" created="Tue, 24 Nov 2009 13:21:25 +0000"  >&lt;blockquote&gt;&lt;p&gt;I would keep it as it is, because we already minimized memory requirements, because before the cache was per-thread.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;OK let&apos;s leave it at 1024, but with flex (which automaton query no longer needs the cache for), I think we should drop it and/or cutover to query-private scope.  I don&apos;t think sucking up 4 MB of RAM for this rather limited purpose is warranted.  I&apos;ll add a comment on &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2093&quot; title=&quot;Use query-private scope instead of shared Term-&amp;gt;TermInfo cache&quot;&gt;&lt;del&gt;LUCENE-2093&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;</comment>
                    <comment id="12781913" author="mikemccand" created="Tue, 24 Nov 2009 13:28:33 +0000"  >&lt;blockquote&gt;&lt;p&gt;Uwe i just wonder if the cache would in practice get used much.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This cache (mapping Term -&amp;gt; TermInfo) does get used alot for &quot;normal&quot;&lt;br/&gt;
atomic queries we first hit the terms dict to get the docFreq (to&lt;br/&gt;
compute idf), then later hit it again with the exact same term, to&lt;br/&gt;
get the TermDocs enum.&lt;/p&gt;

&lt;p&gt;So, for these queries our hit rate is 50%, but, it&apos;s rather overkill&lt;br/&gt;
to be using a shared cache for this (query-private scope is much&lt;br/&gt;
cleaner).  EG a large automaton query running concurrently with other&lt;br/&gt;
queries could evict entries before they read the term the 2nd time.&lt;/p&gt;

&lt;p&gt;Existing MTQs (except NRQ) which seek once and then scan to completion&lt;br/&gt;
don&apos;t hit the cache (though, I think they do double-load each term,&lt;br/&gt;
which is wasteful; likely this is part of the perf gains for flex).&lt;/p&gt;

&lt;p&gt;NRQ doens&apos;t do enough seeking wrt iterating/collecting the docs for&lt;br/&gt;
the cache to make that much a difference.&lt;/p&gt;

&lt;p&gt;The upcoming automaton query should benefit.... however in testing we&lt;br/&gt;
saw only the full linear scan benefit, which I&apos;m still needing to get&lt;br/&gt;
to the bottom of.&lt;/p&gt;</comment>
                    <comment id="12781916" author="rcmuir" created="Tue, 24 Nov 2009 13:33:44 +0000"  >&lt;p&gt;Thanks mike, thats what I was missing&lt;br/&gt;
hitting the terms dict twice in the common case explains it to me &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                    <comment id="12782035" author="mikemccand" created="Tue, 24 Nov 2009 17:17:33 +0000"  >&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;I am quite sure that also Robert&apos;s test is random (as he explained).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;It&apos;s not random - it&apos;s the specified pattern, parsed to&lt;br/&gt;
WildcardQuery, run 10 times, then take best or avg time.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Woops &amp;#8211; I was wrong here &amp;#8211; Robert&apos;s test is random: on each iteration, it replaces any N&apos;s in the pattern w/ a random number 0-9.&lt;/p&gt;

&lt;p&gt;Still baffled on why the linear scan shows gains w/ the cache... digging.&lt;/p&gt;</comment>
                    <comment id="12782039" author="rcmuir" created="Tue, 24 Nov 2009 17:22:39 +0000"  >&lt;blockquote&gt;&lt;p&gt;Woops - I was wrong here - Robert&apos;s test is random: on each iteration, it replaces any N&apos;s in the pattern w/ a random number 0-9.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yeah, the terms are equally distributed 0000000-9999999 though, just a &quot;fill&quot;&lt;br/&gt;
The wildcard patterns themselves are filled with random numbers.&lt;/p&gt;

&lt;p&gt;This is my basis for the new wildcard test btw, except maybe 1-10k, definitely want over 8192 &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;br/&gt;
unless you have better ideas?&lt;/p&gt;</comment>
                    <comment id="12782046" author="mikemccand" created="Tue, 24 Nov 2009 17:31:49 +0000"  >&lt;blockquote&gt;&lt;p&gt;This is my basis for the new wildcard test btw, except maybe 1-10k, definitely want over 8192&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Sounds great &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                    <comment id="12782069" author="mikemccand" created="Tue, 24 Nov 2009 18:23:28 +0000"  >&lt;p&gt;OK as best I can tell, the reason why linear scan shows so much faster&lt;br/&gt;
with the new cache, is some kind of odd GC problem when you use&lt;br/&gt;
LinkedHashMap.... the DoubleBarrelLRUCache doesn&apos;t tickle GC in this&lt;br/&gt;
way.&lt;/p&gt;

&lt;p&gt;If you turn on -verbose:gc when running the bench, you see this&lt;br/&gt;
healthy GC behavior during warmup (running wildcard &quot;*&quot;):&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
[GC 262656K-&amp;gt;15659K(1006848K), 0.0357409 secs]
[GC 278315K-&amp;gt;15563K(1006848K), 0.0351360 secs]
[GC 278219K-&amp;gt;15595K(1006848K), 0.0150112 secs]
[GC 278251K-&amp;gt;15563K(1006848K), 0.0054310 secs]
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;All minor collections, all fairly fast, all rather effective (~270 MB&lt;br/&gt;
down to ~15 MB).&lt;/p&gt;

&lt;p&gt;But then when the test gets to the the *N query:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
[GC 323520K-&amp;gt;33088K(1022272K), 0.0377057 secs]
[GC 338432K-&amp;gt;78536K(990592K), 0.1830592 secs]
[GC 344776K-&amp;gt;118344K(1006336K), 0.1205320 secs]
[GC 384584K-&amp;gt;158080K(987264K), 0.2340810 secs]
[GC 400640K-&amp;gt;194264K(979136K), 0.2139520 secs]
[GC 436824K-&amp;gt;230488K(989760K), 0.2017131 secs]
[GC 463192K-&amp;gt;266501K(969152K), 0.1932188 secs]
[GC 499205K-&amp;gt;301317K(989632K), 0.1918106 secs]
[GC 530437K-&amp;gt;335541K(990080K), 0.1907594 secs]
[GC 564661K-&amp;gt;369749K(990528K), 0.1905007 secs]
[GC 599445K-&amp;gt;404117K(990208K), 0.1922657 secs]
[GC 633813K-&amp;gt;438477K(991680K), 0.1994350 secs]
[GC 670157K-&amp;gt;474250K(991040K), 0.2073795 secs]
[GC 705930K-&amp;gt;508842K(992832K), 0.2061273 secs]
[GC 742570K-&amp;gt;543770K(991936K), 0.1980306 secs]
[GC 777498K-&amp;gt;578634K(994560K), 0.1975961 secs]
[GC 815818K-&amp;gt;614010K(993664K), 0.2042480 secs]
[GC 851194K-&amp;gt;649434K(996096K), 0.1940145 secs]
[GC 889754K-&amp;gt;686551K(995264K), 0.1991030 secs]
[Full GC 686551K-&amp;gt;18312K(995264K), 0.1838671 secs]
[GC 258632K-&amp;gt;54088K(997120K), 0.0735258 secs]
[GC 296456K-&amp;gt;90280K(996288K), 0.1382187 secs]
[GC 332648K-&amp;gt;126512K(998592K), 0.1427443 secs]
[GC 371888K-&amp;gt;163096K(997824K), 0.1472803 secs]
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The minor collections are not nearly as effective &amp;#8211; way too many&lt;br/&gt;
objects are for some reason being marked as live (even though they are&lt;br/&gt;
not) and promoted to the older generation, thus making the minor&lt;br/&gt;
collection much more costly and also requiring major collection every&lt;br/&gt;
so often.&lt;/p&gt;

&lt;p&gt;Now here&apos;s the really crazy thing: if I move the *N query up to be the&lt;br/&gt;
first query the benchmark runs, GC is healthy:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
[GC 323868K-&amp;gt;17216K(1027840K), 0.0060598 secs]
[GC 322496K-&amp;gt;17128K(1006016K), 0.0062586 secs]
[GC 322408K-&amp;gt;17160K(1027712K), 0.0008879 secs]
[GC 321672K-&amp;gt;17192K(1027776K), 0.0003269 secs]
[GC 321704K-&amp;gt;18669K(1028608K), 0.0012964 secs]
[GC 324205K-&amp;gt;18741K(1027968K), 0.0104134 secs]
[GC 324277K-&amp;gt;18613K(1029632K), 0.0083720 secs]
[GC 326261K-&amp;gt;18677K(1029056K), 0.0003520 secs]
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And the query runs about as fast as w/ the new cache.&lt;/p&gt;

&lt;p&gt;So..... somehow, running the other queries sets object state up to&lt;br/&gt;
confuse GC later.  I&apos;m pretty sure it&apos;s the linking that the&lt;br/&gt;
LinkedHashMap (in SimpleLRUCache) is doing, because if I forcefully&lt;br/&gt;
turn off all caching, GC acts healthy again, and that query runs as&lt;br/&gt;
fast as it does w/ the patch.&lt;/p&gt;

&lt;p&gt;DoubleBarrelLRUCache doens&apos;t tickle GC in this way, so the *N query&lt;br/&gt;
runs fast with it.&lt;/p&gt;

&lt;p&gt;Sheesh!!&lt;/p&gt;</comment>
                    <comment id="12782071" author="jasonrutherglen" created="Tue, 24 Nov 2009 18:24:54 +0000"  >&lt;blockquote&gt;&lt;p&gt; And honestly I&apos;m still tempted to do away with this&lt;br/&gt;
cache altogether and create a &quot;query scope&quot;, private to each&lt;br/&gt;
query while it&apos;s running, where terms dict (and other places&lt;br/&gt;
that need to, over time) could store stuff. That&apos;d give a&lt;br/&gt;
perfect within-query hit rate and wouldn&apos;t tie up any long term&lt;br/&gt;
RAM... &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Sounds better than the caching approach which if I recall&lt;br/&gt;
correctly, Michael B. noted was kind of a hack (i.e. this isn&apos;t&lt;br/&gt;
really a cache, isn&apos;t it just a convenient way of recalling&lt;br/&gt;
immediately previously read data whose scope is really within&lt;br/&gt;
the query itself).&lt;/p&gt;</comment>
                    <comment id="12782085" author="mikemccand" created="Tue, 24 Nov 2009 18:48:21 +0000"  >&lt;blockquote&gt;&lt;p&gt;Sounds better than the caching approach&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Right, the cache is a blunt (but effective) tool for simply sharing&lt;br/&gt;
information w/in a single query.  &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-1195&quot; title=&quot;Performance improvement for TermInfosReader&quot;&gt;&lt;del&gt;LUCENE-1195&lt;/del&gt;&lt;/a&gt; was the original issue.&lt;/p&gt;

&lt;p&gt;I&apos;d rather make the private query scope (&lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2093&quot; title=&quot;Use query-private scope instead of shared Term-&amp;gt;TermInfo cache&quot;&gt;&lt;del&gt;LUCENE-2093&lt;/del&gt;&lt;/a&gt;) and then remove&lt;br/&gt;
this cache, but, that&apos;s a bigger change.&lt;/p&gt;

&lt;p&gt;Also, once flex lands, the MTQ&apos;s will require the cache alot less&lt;br/&gt;
(because the TermsEnum API can produce a docs() directly w/o going&lt;br/&gt;
back to the terms dict); my guess is we can drop the cache size to&lt;br/&gt;
something relatively tiny (32) and get most of the gains.&lt;/p&gt;</comment>
                    <comment id="12782094" author="rcmuir" created="Tue, 24 Nov 2009 19:06:21 +0000"  >&lt;p&gt;Mike, I think you also might be seeing strangeness with that wildcard test due to the fact that most Terms automaton seeks to do not actually exist.&lt;br/&gt;
instead its simply &apos;the next possible subsequence&apos;, according to the DFA, and it relies on sort order of TermEnum to do the rest...&lt;/p&gt;</comment>
                    <comment id="12782098" author="mikemccand" created="Tue, 24 Nov 2009 19:11:10 +0000"  >&lt;p&gt;New patch attached &amp;#8211; restores (deprecated) TestSimpleLRUCache.  I think this one is ready to commit?&lt;/p&gt;</comment>
                    <comment id="12782112" author="thetaphi" created="Tue, 24 Nov 2009 19:29:45 +0000"  >&lt;p&gt;Should we additionally deprecate the SimpleMapCache, as it is an abstract super class no longer used? The LinkedHashMap based cache simply extends this class and initializes it with an LinkedHashMap instance in the ctor.&lt;/p&gt;</comment>
                    <comment id="12782116" author="mikemccand" created="Tue, 24 Nov 2009 19:31:41 +0000"  >&lt;blockquote&gt;
&lt;p&gt;Mike, I think you also might be seeing strangeness with that wildcard test due to the fact that most Terms automaton seeks to do not actually exist.&lt;br/&gt;
instead its simply &apos;the next possible subsequence&apos;, according to the DFA, and it relies on sort order of TermEnum to do the rest...&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Hang on &amp;#8211; the weirdness I was seeing was for the *N query, which does&lt;br/&gt;
full linear scan...  as best I can tell, the weird GC problems with&lt;br/&gt;
LinkedHashMap completely explain that weirdness (and boy was&lt;br/&gt;
it weird!).&lt;/p&gt;

&lt;p&gt;But it sounds like you&apos;re talking about the seek-intensive ????NNN&lt;br/&gt;
query?  In that case it&apos;s only 1 in 10 seek&apos;d terms that don&apos;t exist&lt;br/&gt;
(though it is a rather contrived test).&lt;/p&gt;

&lt;p&gt;I guess if we created a much more sparse index, then re-ran that&lt;br/&gt;
query, we&apos;d see many more seeks to non-existent terms.&lt;/p&gt;

&lt;p&gt;But I think in general seek to non-existent term is harmless, because,&lt;br/&gt;
since it did not exist, it&apos;s not like you (or the app) will turnaround&lt;br/&gt;
and ask for that term&apos;s docFreq, the TermDocs, etc.  Ie, we don&apos;t&lt;br/&gt;
cache that &apos;I could not find term XXX&apos;, but I think we don&apos;t need to.&lt;/p&gt;</comment>
                    <comment id="12782117" author="mikemccand" created="Tue, 24 Nov 2009 19:33:12 +0000"  >&lt;blockquote&gt;&lt;p&gt;hould we additionally deprecate the SimpleMapCache, as it is an abstract super class no longer used?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Woops, sorry, you said that already and I forgot &amp;#8211; I&apos;ll do that.&lt;/p&gt;</comment>
                    <comment id="12782118" author="mikemccand" created="Tue, 24 Nov 2009 19:35:33 +0000"  >&lt;p&gt;Also deprecates SimpleMapCache.&lt;/p&gt;</comment>
                    <comment id="12782121" author="rcmuir" created="Tue, 24 Nov 2009 19:43:31 +0000"  >&lt;blockquote&gt;
&lt;p&gt;But it sounds like you&apos;re talking about the seek-intensive ????NNN&lt;br/&gt;
query? In that case it&apos;s only 1 in 10 seek&apos;d terms that don&apos;t exist&lt;br/&gt;
(though it is a rather contrived test).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I guess I worded this wrong. you are right only 1 in 10 seek&apos;ed terms.&lt;br/&gt;
But it will read a lot of useless terms too. This is because it does not try to seek until there is a mismatch.&lt;/p&gt;

&lt;p&gt;first it will seek to \u0000\u0000\u0000\u0000NNN&lt;br/&gt;
edit: this will return 00000000 which fails, then it will seek to 0000NNN&lt;br/&gt;
this will be a match&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;since this was a match, next it will read sequentially the next term, which will not match, so it must seek again.&lt;br/&gt;
now it must backtrack and will try 0001NNN, match, it will do the sequential thing again.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;perhaps this optimization of &apos;don&apos;t seek unless you encounter a mismatch&apos; is not helping the caching?&lt;br/&gt;
(sorry i cant step thru this thing in my mind easily)&lt;/p&gt;</comment>
                    <comment id="12782137" author="mikemccand" created="Tue, 24 Nov 2009 20:14:56 +0000"  >&lt;blockquote&gt;
&lt;p&gt;I guess I worded this wrong. you are right only 1 in 10 seek&apos;ed terms.&lt;br/&gt;
But it will read a lot of useless terms too. This is because it does not try to seek until there is a mismatch.&lt;/p&gt;

&lt;p&gt;first it will seek to \u0000\u0000\u0000\u0000NNN&lt;br/&gt;
edit: this will return 00000000 which fails, then it will seek to 0000NNN&lt;br/&gt;
this will be a match&lt;/p&gt;

&lt;p&gt;since this was a match, next it will read sequentially the next term, which will not match, so it must seek again.&lt;br/&gt;
now it must backtrack and will try 0001NNN, match, it will do the sequential thing again.&lt;br/&gt;
perhaps this optimization of &apos;don&apos;t seek unless you encounter a mismatch&apos; is not helping the caching?&lt;br/&gt;
(sorry i cant step thru this thing in my mind easily)&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;So it sort of plays ping pong w/ the terms enum API, until it finds an&lt;br/&gt;
intersection.  (This is very similar to how filters are applied&lt;br/&gt;
currently).&lt;/p&gt;

&lt;p&gt;In this case, I agree it should not bother with the &quot;first=true&quot; case&lt;br/&gt;
&amp;#8211; it never matches in this particular test &amp;#8211; it should simply seek&lt;br/&gt;
to the next one.  Inside the term enum API, that seek will fallback to&lt;br/&gt;
a scan, anyway, if it&apos;s &quot;close&quot; (within the same index block).&lt;/p&gt;

&lt;p&gt;So I guess if there&apos;s a non-empty common suffix you should just always seek?&lt;/p&gt;

&lt;p&gt;We should test performance of that.&lt;/p&gt;</comment>
                    <comment id="12782141" author="rcmuir" created="Tue, 24 Nov 2009 20:20:32 +0000"  >&lt;blockquote&gt;&lt;p&gt;So it sort of plays ping pong w/ the terms enum API&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;ping-pong, i like it.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;So I guess if there&apos;s a non-empty common suffix you should just always seek?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;the suffix is just for quick comparison, not used at all in seeking.&lt;/p&gt;

&lt;p&gt;in general, you can usually compute the next spot to go, even on a match.&lt;br/&gt;
if its a regex of &quot;ab&lt;span class=&quot;error&quot;&gt;&amp;#91;cd&amp;#93;&lt;/span&gt;&quot; and the enum is at abc, its pretty stupid to compute abd and seek to it, so I don&apos;t. (as long as there is match, just keep reading).&lt;/p&gt;

&lt;p&gt;otherwise I am seeking the whole time, whenever a term doesn&apos;t match, I calculate the next spot to go to. &lt;/p&gt;

&lt;p&gt;we can work it on that other issue if you want, i don&apos;t mean to clutter this one up... happy to see you improve the *N case here &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;edit: remove the * for simplicity&lt;/p&gt;</comment>
                    <comment id="12782152" author="mikemccand" created="Tue, 24 Nov 2009 20:35:19 +0000"  >&lt;p&gt;OK let&apos;s move this over to &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-1606&quot; title=&quot;Automaton Query/Filter (scalable regex)&quot;&gt;&lt;del&gt;LUCENE-1606&lt;/del&gt;&lt;/a&gt;?  I&apos;ll respond there.&lt;/p&gt;</comment>
                    <comment id="12783121" author="mikemccand" created="Fri, 27 Nov 2009 15:33:18 +0000"  >&lt;p&gt;Thanks all!&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12425286" name="ConcurrentLRUCache.java" size="20749" author="markrmiller@gmail.com" created="Wed, 18 Nov 2009 01:52:27 +0000" />
                    <attachment id="12426002" name="LUCENE-2075.patch" size="21123" author="mikemccand" created="Tue, 24 Nov 2009 19:35:33 +0000" />
                    <attachment id="12425999" name="LUCENE-2075.patch" size="20443" author="mikemccand" created="Tue, 24 Nov 2009 19:11:10 +0000" />
                    <attachment id="12425857" name="LUCENE-2075.patch" size="19592" author="mikemccand" created="Mon, 23 Nov 2009 18:15:16 +0000" />
                    <attachment id="12425838" name="LUCENE-2075.patch" size="11936" author="mikemccand" created="Mon, 23 Nov 2009 13:47:26 +0000" />
                    <attachment id="12425836" name="LUCENE-2075.patch" size="11956" author="thetaphi" created="Mon, 23 Nov 2009 13:21:53 +0000" />
                    <attachment id="12425831" name="LUCENE-2075.patch" size="11780" author="mikemccand" created="Mon, 23 Nov 2009 11:44:07 +0000" />
                    <attachment id="12425608" name="LUCENE-2075.patch" size="25977" author="mikemccand" created="Fri, 20 Nov 2009 14:39:23 +0000" />
                    <attachment id="12425450" name="LUCENE-2075.patch" size="16206" author="thetaphi" created="Thu, 19 Nov 2009 08:51:50 +0000" />
                    <attachment id="12425376" name="LUCENE-2075.patch" size="16073" author="thetaphi" created="Wed, 18 Nov 2009 20:25:06 +0000" />
                    <attachment id="12425374" name="LUCENE-2075.patch" size="15833" author="yseeley@gmail.com" created="Wed, 18 Nov 2009 19:44:05 +0000" />
                    <attachment id="12425364" name="LUCENE-2075.patch" size="15830" author="yseeley@gmail.com" created="Wed, 18 Nov 2009 18:37:52 +0000" />
                    <attachment id="12425354" name="LUCENE-2075.patch" size="15781" author="yseeley@gmail.com" created="Wed, 18 Nov 2009 17:53:44 +0000" />
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>13.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Mon, 16 Nov 2009 23:27:00 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>11703</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>25650</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                    <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                <customfieldname>Time in Status</customfieldname>
                <customfieldvalues>
                    
                </customfieldvalues>
            </customfield>
                            </customfields>
    </item>
</channel>
</rss>