<!-- 
RSS generated by JIRA (5.2.8#851-sha1:3262fdc28b4bc8b23784e13eadc26a22399f5d88) at Tue Jul 16 13:23:11 UTC 2013

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/LUCENE-252/LUCENE-252.xml?field=key&field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>5.2.8</version>
        <build-number>851</build-number>
        <build-date>26-02-2013</build-date>
    </build-info>

<item>
            <title>[LUCENE-252] [PATCH] Problem with Sort logic on tokenized fields</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-252</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;When you set s SortField to a Text field which gets tokenized&lt;br/&gt;
FieldCacheImpl uses the term to do the sort, but then sorting is off &lt;br/&gt;
especially with more then one word in the field. I think it is much &lt;br/&gt;
more logical to sort by field&apos;s string value if the sort field is Tokenized and&lt;br/&gt;
stored. This way you&apos;ll get the CORRECT sort order&lt;/p&gt;</description>
                <environment>&lt;p&gt;Operating System: other&lt;br/&gt;
Platform: All&lt;/p&gt;</environment>
            <key id="12314402">LUCENE-252</key>
            <summary>[PATCH] Problem with Sort logic on tokenized fields</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/bug.png">Bug</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png">Closed</status>
                    <resolution id="1">Fixed</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="amordo@infosciences.com">Aviran Mordo</reporter>
                        <labels>
                    </labels>
                <created>Thu, 29 Jul 2004 21:54:15 +0100</created>
                <updated>Tue, 8 Dec 2009 03:51:44 +0000</updated>
                    <resolved>Tue, 8 Dec 2009 03:51:44 +0000</resolved>
                            <version>1.4</version>
                                                <component>core/search</component>
                        <due></due>
                    <votes>1</votes>
                        <watches>1</watches>
                                                    <comments>
                    <comment id="12321785" author="amordo@infosciences.com" created="Thu, 29 Jul 2004 21:55:50 +0100"  >&lt;p&gt;I wrote a fix that uses the stored values in case the sort field is tokenized&lt;br/&gt;
and stored, or uses the Terms in case the sort field is a Keyword.&lt;/p&gt;

&lt;p&gt;Index: FieldCacheImpl.java&lt;br/&gt;
===================================================================&lt;br/&gt;
RCS file:&lt;br/&gt;
/home/cvspublic/jakarta-lucene/src/java/org/apache/lucene/search/FieldCacheImpl.java,v&lt;br/&gt;
retrieving revision 1.3&lt;br/&gt;
diff -u -r1.3 FieldCacheImpl.java&lt;br/&gt;
&amp;#8212; FieldCacheImpl.java	21 Jul 2004 19:05:46 -0000	1.3&lt;br/&gt;
+++ FieldCacheImpl.java	28 Jul 2004 17:45:41 -0000&lt;br/&gt;
@@ -25,6 +25,8 @@&lt;br/&gt;
 import java.util.Map;&lt;br/&gt;
 import java.util.WeakHashMap;&lt;br/&gt;
 import java.util.HashMap;&lt;br/&gt;
+import org.apache.lucene.document.Field;&lt;br/&gt;
+import java.util.Arrays;&lt;/p&gt;

&lt;p&gt; /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Expert: The default cache implementation, storing all values in memory. @@&lt;br/&gt;
-80,6 +82,29 @@&lt;br/&gt;
     }&lt;br/&gt;
   }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+  class FieldEntry implements Comparable {&lt;br/&gt;
+    String val;&lt;br/&gt;
+    int ind;&lt;br/&gt;
+    FieldEntry(int ind, String val)&lt;br/&gt;
+    &lt;/p&gt;
{
+        this.ind = ind;
+        this.val = val;
+    }
&lt;p&gt;+    public String getVal()&lt;br/&gt;
+    &lt;/p&gt;
{
+        return val;
+    }
&lt;p&gt;+    public int getInd()&lt;br/&gt;
+    &lt;/p&gt;
{
+        return ind;
+    }
&lt;p&gt;+    public int compareTo(Object obj)&lt;br/&gt;
+    &lt;/p&gt;
{
+        return val.compareToIgnoreCase(((FieldEntry)obj).getVal());
+    }
&lt;p&gt;+}&lt;br/&gt;
+&lt;br/&gt;
+&lt;/p&gt;

&lt;p&gt;   /** The internal cache. Maps Entry to array of interpreted term values. **/&lt;br/&gt;
   final Map cache = new WeakHashMap();&lt;br/&gt;
@@ -240,54 +265,92 @@&lt;br/&gt;
     if (ret == null) {&lt;br/&gt;
       final int[] retArray = new int&lt;span class=&quot;error&quot;&gt;&amp;#91;reader.maxDoc()&amp;#93;&lt;/span&gt;;&lt;br/&gt;
       String[] mterms = new String&lt;span class=&quot;error&quot;&gt;&amp;#91;reader.maxDoc()+1&amp;#93;&lt;/span&gt;;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (retArray.length &amp;gt; 0) {&lt;/li&gt;
	&lt;li&gt;TermDocs termDocs = reader.termDocs();&lt;/li&gt;
	&lt;li&gt;TermEnum termEnum = reader.terms (new Term (field, &quot;&quot;));&lt;/li&gt;
	&lt;li&gt;int t = 0;  // current term number&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;// an entry for documents that have no terms in this field&lt;/li&gt;
	&lt;li&gt;// should a document with no terms be at top or bottom?&lt;/li&gt;
	&lt;li&gt;// this puts them at the top - if it is changed, FieldDocSortedHitQueue&lt;/li&gt;
	&lt;li&gt;// needs to change as well.&lt;/li&gt;
	&lt;li&gt;mterms&lt;span class=&quot;error&quot;&gt;&amp;#91;t++&amp;#93;&lt;/span&gt; = null;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;try {&lt;/li&gt;
	&lt;li&gt;if (termEnum.term() == null) 
{
-            throw new RuntimeException (&quot;no terms in field &quot; + field);
-          }&lt;/li&gt;
	&lt;li&gt;do {&lt;/li&gt;
	&lt;li&gt;Term term = termEnum.term();&lt;/li&gt;
	&lt;li&gt;if (term.field() != field) break;&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;// store term text&lt;/li&gt;
	&lt;li&gt;// we expect that there is at most one term per document&lt;/li&gt;
	&lt;li&gt;if (t &amp;gt;= mterms.length) throw new RuntimeException (&quot;there are more&lt;br/&gt;
terms than documents in field \&quot;&quot; + field + &quot;\&quot;&quot;);&lt;/li&gt;
	&lt;li&gt;mterms&lt;span class=&quot;error&quot;&gt;&amp;#91;t&amp;#93;&lt;/span&gt; = term.text();&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;termDocs.seek (termEnum);&lt;/li&gt;
	&lt;li&gt;while (termDocs.next()) 
{
-              retArray[termDocs.doc()] = t;
-            }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;t++;&lt;/li&gt;
	&lt;li&gt;} while (termEnum.next());&lt;/li&gt;
	&lt;li&gt;} finally {&lt;/li&gt;
	&lt;li&gt;termDocs.close();&lt;/li&gt;
	&lt;li&gt;termEnum.close();&lt;br/&gt;
+      Field docField = reader.document(0).getField(field);&lt;br/&gt;
+      if (docField.isStored() &amp;amp;&amp;amp; docField.isTokenized()) {&lt;br/&gt;
+          // Fill entries&lt;br/&gt;
+        FieldEntry[] entries = new FieldEntry&lt;span class=&quot;error&quot;&gt;&amp;#91;reader.maxDoc()&amp;#93;&lt;/span&gt;;&lt;br/&gt;
+        for (int i=0; i&amp;lt;reader.maxDoc(); i++) 
{
+          String fieldValue;
+          if (!reader.isDeleted(i))
+            fieldValue = reader.document(i).get(field);
+          else
+            fieldValue = &quot;&quot;;
+          entries[i] = new FieldEntry (i,fieldValue);
         }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (t == 0) 
{
-          // if there are no terms, make the term array
-          // have a single null entry
-          mterms = new String[1];
-        }
&lt;p&gt; else if (t &amp;lt; mterms.length) {&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;// if there are less terms than documents,&lt;/li&gt;
	&lt;li&gt;// trim off the dead array space&lt;/li&gt;
	&lt;li&gt;String[] terms = new String&lt;span class=&quot;error&quot;&gt;&amp;#91;t&amp;#93;&lt;/span&gt;;&lt;/li&gt;
	&lt;li&gt;System.arraycopy (mterms, 0, terms, 0, t);&lt;/li&gt;
	&lt;li&gt;mterms = terms;&lt;br/&gt;
+        Arrays.sort(entries);&lt;br/&gt;
+        for (int i=0;i&amp;lt;reader.maxDoc();i++)&lt;br/&gt;
+        
{
+          int ind = entries[i].getInd();
+          retArray[ind] = i;
+          mterms[ind]=entries[i].getVal();
         }
&lt;p&gt;       }&lt;br/&gt;
+      else&lt;br/&gt;
+      {&lt;br/&gt;
+          if (retArray.length &amp;gt; 0)&lt;br/&gt;
+          {&lt;br/&gt;
+              TermDocs termDocs = reader.termDocs();&lt;br/&gt;
+              TermEnum termEnum = reader.terms(new Term(field, &quot;&quot;));&lt;br/&gt;
+              int t = 0; // current term number&lt;br/&gt;
+&lt;br/&gt;
+              // an entry for documents that have no terms in this field&lt;br/&gt;
+              // should a document with no terms be at top or bottom?&lt;br/&gt;
+              // this puts them at the top - if it is changed,&lt;br/&gt;
FieldDocSortedHitQueue&lt;br/&gt;
+              // needs to change as well.&lt;br/&gt;
+              mterms&lt;span class=&quot;error&quot;&gt;&amp;#91;t++&amp;#93;&lt;/span&gt; = null;&lt;br/&gt;
+&lt;br/&gt;
+              try&lt;br/&gt;
+              {&lt;br/&gt;
+                  if (termEnum.term() == null)&lt;br/&gt;
+                  &lt;/p&gt;
{
+                      throw new RuntimeException(&quot;no terms in field &quot; + field);
+                  }
&lt;p&gt;+                  do&lt;br/&gt;
+                  &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+                      Term term = termEnum.term();+                      if (term.field() != field)+                          break;++                      // store term text+                      // we expect that there is at most one term per document+                      if (t &amp;gt;= mterms.length)+                          throw new RuntimeException(&amp;quot;there are more terms thandocuments in field &amp;quot;&amp;quot; + field ++                                                     &amp;quot;&amp;quot;&amp;quot;);+                      mterms[t] = term.text();+                      termDocs.seek(termEnum);+                      while (termDocs.next())+                      {
+                          retArray[termDocs.doc()] = t;
+                      }++                      t++;+                  }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;+                  while (termEnum.next());&lt;br/&gt;
+              }&lt;br/&gt;
+              finally&lt;br/&gt;
+              &lt;/p&gt;
{
+                  termDocs.close();
+                  termEnum.close();
+              }
&lt;p&gt;+&lt;br/&gt;
+              if (t == 0)&lt;br/&gt;
+              &lt;/p&gt;
{
+                  // if there are no terms, make the term array
+                  // have a single null entry
+                  mterms = new String[1];
+              }
&lt;p&gt;+              else if (t &amp;lt; mterms.length)&lt;br/&gt;
+              &lt;/p&gt;
{
+                  // if there are less terms than documents,
+                  // trim off the dead array space
+                  String[] terms = new String[t];
+                  System.arraycopy(mterms, 0, terms, 0, t);
+                  mterms = terms;
+              }
&lt;p&gt;+          }&lt;br/&gt;
+      }&lt;br/&gt;
       StringIndex value = new StringIndex (retArray, mterms);&lt;br/&gt;
       store (reader, field, STRING_INDEX, value);&lt;br/&gt;
       return value;&lt;br/&gt;
@@ -309,7 +372,7 @@&lt;br/&gt;
   // inherit javadocs&lt;br/&gt;
   public Object getAuto (IndexReader reader, String field)&lt;br/&gt;
   throws IOException {&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;field = field.intern();&lt;br/&gt;
+  field = field.intern();&lt;br/&gt;
     Object ret = lookup (reader, field, SortField.AUTO);&lt;br/&gt;
     if (ret == null) {&lt;br/&gt;
       TermEnum enumerator = reader.terms (new Term (field, &quot;&quot;));&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="12321786" author="otis@apache.org" created="Fri, 30 Jul 2004 17:26:03 +0100"  >&lt;p&gt;Aviran, could you attach your diff (instead of pasting it in), please?&lt;br/&gt;
Use the &apos;Create a new attachment&apos; link above.  This will keep the patch nice and&lt;br/&gt;
clean, without any wrapped lines and such.  Thanks!&lt;/p&gt;</comment>
                    <comment id="12321787" author="amordo@infosciences.com" created="Fri, 30 Jul 2004 21:03:07 +0100"  >&lt;p&gt;Created an attachment (id=12275)&lt;br/&gt;
A diff patch file that uses the stored values in case the sorted field is tokenized&lt;/p&gt;</comment>
                    <comment id="12478388" author="enis" created="Tue, 6 Mar 2007 12:19:42 +0000"  >&lt;p&gt;In the project Nutch, we have encountered a subtle bug, which I tracked down and found to be related to unintuitive caching in tokenized fields. &lt;/p&gt;

&lt;p&gt;nutch uses several index servers, and the search results from these servers are merged using a dedup field for for deleting dupilcates. The values from this field is cached by FieldCachImpl. The default is the site field, which is indexed and tokenized. However for a Tokenized Field (for example &quot;url&quot; in nutch), FieldCacheImpl returns an array of Terms rather that array of field values, so dedup&apos;ing becomes faulty. &lt;/p&gt;

&lt;p&gt;Current FieldCache implementation does not respect tokenized fields, and as described above caches only terms. I have ported the previous patch and improved it for the 2.0 branch. And i will write a patch for the trunk. &lt;/p&gt;

&lt;p&gt;I am voting for this patch to be committed. &lt;/p&gt;</comment>
                    <comment id="12478399" author="enis" created="Tue, 6 Mar 2007 13:07:24 +0000"  >&lt;p&gt;A bug in FieldCacheImpl_Tokenized_fields_lucene_2.0.patch is fixed, which caused not storing newly built cache. This patch obsoletes FieldCacheImpl_Tokenized_fields_lucene_2.0.patch&lt;/p&gt;</comment>
                    <comment id="12478402" author="enis" created="Tue, 6 Mar 2007 13:12:38 +0000"  >&lt;p&gt;This version patches against the current trunk version 2.2-dev. Please see the above comments about the contents of the patch. &lt;/p&gt;</comment>
                    <comment id="12478856" author="cutting" created="Wed, 7 Mar 2007 19:14:15 +0000"  >&lt;p&gt;I am not convinced that we should encourage folks to use a FieldCache with a tokenized field in this way, expecting the untokenized, stored value.  It is considerably slower to populate the cache this way, greatly reducing its utility.  Instead we should probably improve the documentation on this point, and perhaps even throw an exception for tokenized fields.&lt;/p&gt;</comment>
                    <comment id="12478946" author="hossman" created="Wed, 7 Mar 2007 23:00:15 +0000"  >&lt;p&gt;FieldCacheImpl.getStringIndex already throws an exception if it detects it&apos;s being used on a tokenized field (but i&apos;m not sure if it does a perfect job of detecting that ... it seems to make a naive assumption based on the number of terms) ... i&apos;m guessing that check wasn&apos;t in the code when this bug was initially filed.&lt;/p&gt;

&lt;p&gt;I agree with Doug, silently using the stored field value if/when a field is tokenized is a bad idea .. there is a seperate patch available for doing this that allows people to be much more explicit about their intentions &amp;#8211; see (&lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-769&quot; title=&quot;[PATCH] Performance improvement for some cases of sorted search&quot;&gt;&lt;del&gt;LUCENE-769&lt;/del&gt;&lt;/a&gt;)&lt;/p&gt;</comment>
                    <comment id="12479260" author="enis" created="Thu, 8 Mar 2007 08:25:18 +0000"  >&lt;p&gt;Well, I have spent half a day to find this issue with tokenized field caching, so I absolutely agree on throwing and exception in the getStrings() and getStringIndex() functions of FieldCacheImpl. A snippet would be like : &lt;/p&gt;

&lt;p&gt;Field docField = getField(reader, field);&lt;br/&gt;
      if (docField != null &amp;amp;&amp;amp; docField.isStored() &amp;amp;&amp;amp; docField.isTokenized()) &lt;/p&gt;
{
           throw new RuntimeException(&quot;Caching in Tokenized Fields is not allowed&quot;);
      }

&lt;p&gt;Looking at the timing of cache building tokenized fields are really slow, as Doug mentioned, for a 1.5M real index(from web documents) building the cache on a tokenized field takes 1600 ms on the avarage, but for an untokenized field, it takes 30000 ms on avarage. &lt;/p&gt;

&lt;p&gt;In nutch we have 3 options : 1st is to disallow deleting duplicates on tokenized fields(due to FieldCache), 2nd is to index the tokenized field twice(once tokenized, and once untokenized), 3rd use the above patch and warm the cache initially in the index servers. &lt;/p&gt;

&lt;p&gt;I am in favor of the 3rd option and believe that this patch is necessary and it can be included with an explanatory javadoc. &lt;br/&gt;
another option will be to extend the defalut FieldCacheImpl and allow for tokenized field caching and naming the class similar to &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-769&quot; title=&quot;[PATCH] Performance improvement for some cases of sorted search&quot;&gt;&lt;del&gt;LUCENE-769&lt;/del&gt;&lt;/a&gt;&apos;s such as StoredFieldCacheImpl. If that is ok, i can prepare a patch and send it here. &lt;/p&gt;



</comment>
                    <comment id="12479326" author="yseeley@gmail.com" created="Thu, 8 Mar 2007 14:59:56 +0000"  >&lt;p&gt;I wouldn&apos;t want to see sorting on a tokenized field disallowed (Lucene&apos;s definition of tokenized is that analysis is done on the field value).  There are tokenized fields that simply do things like manipulate a numer into a text-sortable value, lowercase, or do some other sort of normalization that doesn&apos;t produce multiple tokens.&lt;/p&gt;</comment>
                    <comment id="12479354" author="hossman" created="Thu, 8 Mar 2007 16:49:35 +0000"  >&lt;p&gt;definitely in agreement with yonik here, erroring out if &quot;docField.isTokenized()&quot; would prevent some perfectly valid use cases ... my point was that hte current test of &quot;if (t &amp;gt;= mterms.length)&quot; only triggers an error if htere are more total terms in the field then there are documents in the index ... but there can be plenty of situations where a doc has more then one indexed term, but the total number of indexed terms is less hten the number of documents, a better test would be to check and see if we have already recorded a term for this doc.&lt;/p&gt;

&lt;p&gt;I have to say: I&apos;m really not understanding how the current behavior is hindering nutch ... my understanding of the nutch model is that the set of fields is very well known &amp;#8211; why do you need to rely on FieldCache being smart enough to stop you from trying to sort on a tokenized field? (and what does that have to do with deleting duplicates?)&lt;/p&gt;

&lt;p&gt;if nothing else: if nutch needs to prevent using FieldCache based sorting on tokenized fields, why can&apos;t the &quot;if (docField.isTokenized())&quot; logic be done outside of the FieldCacheImpl ... possibly as a way to decide if you want to use the basic sorting or use something like &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-769&quot; title=&quot;[PATCH] Performance improvement for some cases of sorted search&quot;&gt;&lt;del&gt;LUCENE-769&lt;/del&gt;&lt;/a&gt;?&lt;/p&gt;


&lt;p&gt;...perhaps this is something that should be discussed more on java-dev?&lt;/p&gt;
</comment>
                    <comment id="12479552" author="enis" created="Fri, 9 Mar 2007 10:27:18 +0000"  >&lt;p&gt;I also agree with tokenized field caching, which is a use case for nutch. Let me elaborate on the use case. In a nutch deployment, we generate indexes from the web documents, and indeed the set of fields is known a priori. Then the indexes are distributed to several index servers running on hadoop&apos;s RPC calls. Then the query is sent to all of the index servers, the results are collected and merged on the fly. Since the indexes need not be disjoint(since crawling is an adaptive process) the results should be merged, without having a document more then once. So we need a unique key to represent the document. Default nutch codebase uses the site field(url&apos;s hostname), which is untokenized for such a task, and allow only 1 - 2 documents from a site in the search results. For obvious performance reasons, the site field is cached in the index servers with FieldCache.getStrings(). The problem arises when we want to show more than one result from a specific site (for example in a site:apache.org query ), and if we have the same url indexed in more than one index server. We use the tokenized url field in the FieldCache, then deleting duplicates becomes error prone. Since we use FieldCache.getStrings() rather that FieldCache.getStringIndex(), the problem here is not tokenized field sorting, but tokenized field not caching correctly, an example of which is an array like &lt;span class=&quot;error&quot;&gt;&amp;#91;com, edu. www, youtube, &amp;#93;&lt;/span&gt; from the getStrings() method(for each doc, only a token is returned, rather than the whole url). &lt;/p&gt;

&lt;p&gt;Well, if you are still with me, here is my proposal : &lt;/p&gt;

&lt;p&gt;1. in FieldCacheImpl.java in both getStrings and getStringIndex functions add &lt;/p&gt;

&lt;p&gt;Field docField = getField(reader, field);&lt;br/&gt;
      if (docField != null &amp;amp;&amp;amp; docField.isStored() &amp;amp;&amp;amp; docField.isTokenized()) &lt;/p&gt;
{
           throw new RuntimeException(&quot;Caching in Tokenized Fields is not allowed&quot;);
      }
&lt;p&gt; &lt;/p&gt;

&lt;p&gt;2. subclass FieldCacheImpl as StoredFieldCacheImpl and implement stored field caching there, delegating untokenized fields to super class&lt;br/&gt;
3. add the implementation to FieldCache.java :&lt;/p&gt;

&lt;p&gt; public static FieldCache DEFAULT = new FieldCacheImpl();&lt;br/&gt;
 public static FieldCache STORED_CACHE = new StoredCacheImpl();&lt;/p&gt;

&lt;p&gt;this way both lucene internals will not be affected and a stored field caching could be performed. &lt;/p&gt;
</comment>
                    <comment id="12479648" author="hossman" created="Fri, 9 Mar 2007 16:55:17 +0000"  >&lt;p&gt;I&apos;m afraid i&apos;m still not understanding the issue in nutch, it seems like the root of hte problem is..&lt;/p&gt;

&lt;p&gt;&amp;gt; ... We use the tokenized url field in the FieldCache ...&lt;/p&gt;

&lt;p&gt;...if you know this field is tokenized, don&apos;t use it this way.  if you want to use it this way, index it a second time untokenized.&lt;/p&gt;

&lt;p&gt;At a more practical level:&lt;/p&gt;

&lt;p&gt;1) the change you propose to getStrings and getStringIndex is not practical because as we&apos;ve discussed before, a field being tokenized isn&apos;t a garuntee that FieldCache won&apos;t work &amp;#8211; isTokenized just inidcates that an Analyzer was used &amp;#8211; it doesn&apos;t indicate that any real tokenization took place (the analyzer might have just been used to lowercase the field value before indexing, or strip off leading/trailing white space) that doesn&apos;t mean the normal FieldCache can&apos;t be used for sorting.  the converse is also true: !isTokenized doens&apos;t tell you that it&apos;s safe to build the FieldCache &amp;#8211; even if no Analyzer is ever used, multiple Field values can be added for the same field &amp;#8211; and that is hte root cause of hte problem, not tokenization but multiple terms for a given field.&lt;/p&gt;

&lt;p&gt;2) the desired behavior you are requesting in a StoredFieldCacheImpl could be done without making any changes to what so ever to FieldCacheImpl &amp;#8211; since nutch knows exactly which fields it&apos;s indexing multiple tokens for, it can make the choice between using a StoredFieldCacheImple or using a FieldCacheImpl. (but as i&apos;ve said, i really don&apos;t think that&apos;s the right solution)&lt;/p&gt;</comment>
                    <comment id="12480476" author="enis" created="Tue, 13 Mar 2007 16:55:19 +0000"  >&lt;p&gt;I should admit that, considering the case Yonik has mentioned. throwing an exception by checking the Field.isTokenized() is not suitable. However the check if (t &amp;gt;= mterms.length) is only in getStringIndex() and not in getStrings(). I think that a more robust check then the aforementioned should be included in both getStrings and getStringIndex functions. A possibility would be to allocate a boolean array(or BitSet) of the same size with the retArray, and  then use the array to avoid multiple terms per document.  &lt;/p&gt;

&lt;p&gt;&amp;gt; 2) the desired behavior you are requesting in a StoredFieldCacheImpl could be done without making any changes to what so ever to FieldCacheImpl &amp;#8211; since nutch knows exactly which fields it&apos;s indexing multiple tokens for, it can make the choice between using a StoredFieldCacheImple or using a FieldCacheImpl.&lt;/p&gt;

&lt;p&gt;from my previous post  :  In nutch we have 3 options : 1st is to disallow deleting duplicates on tokenized fields(due to FieldCache), 2nd is to index the tokenized field twice(once tokenized, and once untokenized), 3rd use the above patch and warm the cache initially in the index servers.&lt;/p&gt;

&lt;p&gt;Yes indexing a field a second time is an option, but considering my use cases with nutch, why would i want to grow my index by indexing the field twice, instead of tolerating 30 seconds of cache building in a web server, which will serve the indexes for days or even weeks. &lt;/p&gt;

&lt;p&gt;with a class like StoredFieldCacheImpl we can get the desired behaviour w/o modifiying the FieldCacheImpl, and my suggestion in my previous post  without the 1st part does just this. I couldl have sent this to nutch but i think it is a lucene issue. &lt;/p&gt;

&lt;p&gt;Any more suggestions ?&lt;/p&gt;


</comment>
                    <comment id="12787288" author="markrmiller@gmail.com" created="Tue, 8 Dec 2009 03:51:44 +0000"  >&lt;p&gt;This issue is too old - if a new patch/proposal is brought up we can reopen it.&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12312369" name="ASF.LICENSE.NOT.GRANTED--dif.txt" size="6775" author="amordo@infosciences.com" created="Fri, 30 Jul 2004 21:03:07 +0100" />
                    <attachment id="12352745" name="FieldCacheImpl_Tokenized_fields_lucene_2.0.patch" size="8523" author="enis" created="Tue, 6 Mar 2007 12:19:42 +0000" />
                    <attachment id="12352748" name="FieldCacheImpl_Tokenized_fields_lucene_2.0_v1.1.patch" size="8371" author="enis" created="Tue, 6 Mar 2007 13:07:24 +0000" />
                    <attachment id="12352749" name="FieldCacheImpl_Tokenized_fields_lucene_2.2-dev.patch" size="8086" author="enis" created="Tue, 6 Mar 2007 13:12:37 +0000" />
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>4.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_10010" key="com.atlassian.jira.plugin.system.customfieldtypes:importid">
                <customfieldname>Bugzilla Id</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>30382</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Fri, 30 Jul 2004 16:26:03 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>13497</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>27479</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                    <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                <customfieldname>Time in Status</customfieldname>
                <customfieldvalues>
                    
                </customfieldvalues>
            </customfield>
                            </customfields>
    </item>
</channel>
</rss>