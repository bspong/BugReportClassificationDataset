<!-- 
RSS generated by JIRA (5.2.8#851-sha1:3262fdc28b4bc8b23784e13eadc26a22399f5d88) at Tue Jul 16 13:28:42 UTC 2013

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/LUCENE-1990/LUCENE-1990.xml?field=key&field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>5.2.8</version>
        <build-number>851</build-number>
        <build-date>26-02-2013</build-date>
    </build-info>

<item>
            <title>[LUCENE-1990] Add unsigned packed int impls in oal.util</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-1990</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;There are various places in Lucene that could take advantage of an&lt;br/&gt;
efficient packed unsigned int/long impl.  EG the terms dict index in&lt;br/&gt;
the standard codec in &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-1458&quot; title=&quot;Further steps towards flexible indexing&quot;&gt;&lt;del&gt;LUCENE-1458&lt;/del&gt;&lt;/a&gt; could subsantially reduce it&apos;s RAM&lt;br/&gt;
usage.  FieldCache.StringIndex could as well.  And I think &quot;load into&lt;br/&gt;
RAM&quot; codecs like the one in TestExternalCodecs could use this too.&lt;/p&gt;

&lt;p&gt;I&apos;m picturing something very basic like:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-keyword&quot;&gt;interface&lt;/span&gt; PackedUnsignedLongs  {
  &lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt; get(&lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt; index);
  void set(&lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt; index, &lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt; value);
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Plus maybe an iterator for getting and maybe also for setting.  If it&lt;br/&gt;
helps, most of the usages of this inside Lucene will be &quot;write once&quot;&lt;br/&gt;
so eg the set could make that an assumption/requirement.&lt;/p&gt;

&lt;p&gt;And a factory somewhere:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
  PackedUnsignedLongs create(&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; count, &lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt; maxValue);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I think we should simply autogen the code (we can start from the&lt;br/&gt;
autogen code in &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-1410&quot; title=&quot;PFOR implementation&quot;&gt;&lt;del&gt;LUCENE-1410&lt;/del&gt;&lt;/a&gt;), or, if there is an good existing impl&lt;br/&gt;
that has a compatible license that&apos;d be great.&lt;/p&gt;

&lt;p&gt;I don&apos;t have time near-term to do this... so if anyone has the itch,&lt;br/&gt;
please jump!&lt;/p&gt;</description>
                <environment></environment>
            <key id="12438385">LUCENE-1990</key>
            <summary>Add unsigned packed int impls in oal.util</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png">Closed</status>
                    <resolution id="1">Fixed</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="mikemccand">Michael McCandless</reporter>
                        <labels>
                    </labels>
                <created>Sun, 18 Oct 2009 13:53:22 +0100</created>
                <updated>Sun, 27 Nov 2011 12:29:33 +0000</updated>
                    <resolved>Tue, 6 Apr 2010 21:58:39 +0100</resolved>
                            <version>4.0-ALPHA</version>
                                <fixVersion>3.5</fixVersion>
                <fixVersion>4.0-ALPHA</fixVersion>
                                <component>core/index</component>
                        <due></due>
                    <votes>0</votes>
                        <watches>3</watches>
                                                    <comments>
                    <comment id="12775420" author="funtick" created="Tue, 10 Nov 2009 14:00:42 +0000"  >
&lt;p&gt;Specifically for FieldCache, let&apos;s see... suppose Field may have 8 different values, and number of documents is high.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
Value0  0  1  0  0  0  0  0   0  1  0  0  0  0  0 ...  
Value1  1  0  1  0  0  0  0   0  0  0  0  0  0  0 ...  
Value2  0  0  0  1  1  0  0   0  0  0  0  0  0  0 ...  
Value3  0  0  0  0  0  0  0   0  0  0  0  1  0  0 ...  
Value4  0  0  0  0  0  0  1   0  0  0  0  0  0  0 ...  
Value5  0  0  0  0  0  1  0   0  0  0  1  0  1  0 ...  
Value6  0  0  0  0  0  0  0   1  0  1  0  0  0  0 ...  
Value7  0  0  0  0  0  0  0   0  0  0  0  0  0  1 ...  
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;represented as Matrix (or as a Vector); for instance, first row means that Document1 and Document8 have Value0.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;And now, if we go &quot;horizontally&quot; we will end up with 8 arrays of int[]. What if we go &quot;vertically&quot;? Field could be encoded as 3-bit (8 different values).&lt;/p&gt;

&lt;p&gt;CONSTRAINT: specifically for FieldCache, each Column must have the only &quot;1&quot;.&lt;/p&gt;

&lt;p&gt;And we can end with array of 3-bit values storing position in a column! Size of array is IndexReader.maxDoc().&lt;/p&gt;


&lt;p&gt;hope I am reinventing bycycle &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;P.S.&lt;br/&gt;
Of course each solution has pros and cons, I am trying to focus on FieldCache specific use cases.&lt;/p&gt;

&lt;p&gt;1. For a given document ID, find a value for a field&lt;br/&gt;
2. For a given query results, sort it by a field values&lt;br/&gt;
3. For a given query results, count &quot;facet&quot; for each field value&lt;/p&gt;

&lt;p&gt;I don&apos;t think such naive compression is slower than abstract int[] arrays... and we need to change public API of field cache too: if method returns int[] we are not saving any RAM.&lt;/p&gt;

&lt;p&gt;Better is to compare with SOLR use cases and to make API closer to real requirements; SOLR operates with some bitsets instead of arrays...&lt;/p&gt;</comment>
                    <comment id="12776703" author="earwin" created="Wed, 11 Nov 2009 22:46:05 +0000"  >&lt;blockquote&gt;&lt;p&gt;hope I am reinventing bycycle&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I believe some databases do that.&lt;/p&gt;</comment>
                    <comment id="12777054" author="funtick" created="Thu, 12 Nov 2009 17:02:09 +0000"  >&lt;p&gt;Suttiwat sent me a link:&lt;br/&gt;
&lt;a href=&quot;http://blog.juma.me.uk/2008/10/14/32-bit-or-64-bit-jvm-how-about-a-hybrid/&quot; class=&quot;external-link&quot;&gt;http://blog.juma.me.uk/2008/10/14/32-bit-or-64-bit-jvm-how-about-a-hybrid/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This is vendor-specific, and possibly may cause unexpected problems, but we can try in some specific cases:&lt;br/&gt;
&quot;Compressed Oops have been included (but disabled by default) in the performance release JDK6u6p (requires you to fill a survey), so I decided to try it in an internal application and compare it with 64-bit mode and 32-bit mode.&quot;&lt;/p&gt;

&lt;p&gt;-XX:+UseCompressedOops&lt;/p&gt;

&lt;p&gt;There are other vendors around too such as Oracle JRockit which is much faster server-side... &lt;/p&gt;</comment>
                    <comment id="12795228" author="toke" created="Wed, 30 Dec 2009 03:39:12 +0000"  >&lt;p&gt;I have very recently done some experiments with random-access packed positive integer arrays (we could also call them unsigned). The basic approach is to put the bits after each other in an int- or long-array, then extract the value by shifting and masking. Unfortunately I have not been able to determine a single winning strategy for space/speed trade-offs.&lt;/p&gt;

&lt;p&gt;I&apos;ve tried four simple implementations:&lt;/p&gt;

&lt;p&gt;1. Pack the bits right after each other.&lt;br/&gt;
2. Pack the bits right after each other but allow only 1, 2, 4, 8, 16 or 32 as value-bits.&lt;br/&gt;
3. Wrap an int[] or long[] and store the values directly (for comparison with 1 &amp;amp; 2).&lt;br/&gt;
4. Use int[] directly without any wrapping.&lt;/p&gt;

&lt;p&gt;(Code can be found at &lt;a href=&quot;http://summa.svn.sourceforge.net/viewvc/summa/trunk/Common/src/dk/statsbiblioteket/summa/common/util/bits/&quot; class=&quot;external-link&quot;&gt;http://summa.svn.sourceforge.net/viewvc/summa/trunk/Common/src/dk/statsbiblioteket/summa/common/util/bits/&lt;/a&gt; where #1 is BitsArrayPacked, #2 is BitsArrayAligned and #3 is BitsArrayInt)&lt;/p&gt;

&lt;p&gt;The obvious benefit from #2 is that the bits for values are always contained in a single block (int or long), which reduces the amount of operations for set and get considerably. Unfortunately this means that as soon as a value greater than 65535 needs to be stored, the internal representation will require 32 bits/value. This means no space-benefit compared to #3 while the speed penalty remains. A hybrid approach might be considered where the implementation is determined by the number of bits needed to store a value.&lt;/p&gt;

&lt;p&gt;I&apos;ve done some performance tests of the implementations on an aging 1.8GHz single-core laptop (Dell 820). The code can be found at &lt;a href=&quot;http://summa.svn.sourceforge.net/viewvc/summa/trunk/Common/test/dk/statsbiblioteket/summa/common/util/bits/BitsArrayPerformanceTest.java?revision=2038&amp;amp;view=markup&quot; class=&quot;external-link&quot;&gt;http://summa.svn.sourceforge.net/viewvc/summa/trunk/Common/test/dk/statsbiblioteket/summa/common/util/bits/BitsArrayPerformanceTest.java?revision=2038&amp;amp;view=markup&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;In the tables below, the measurements for Packed, Aligned, Int, Constant and int[] are the total time in milliseconds to perform actionCount actions (either read or write). Random numbers (using the same seed) were used for index as well as value and the overhead of constructing the random numbers were subtracted from the measurements. &quot;Constant&quot; is a dummy where no values are stored and the same value is always returned on a get. It is used to measure method-call overhead.&lt;/p&gt;

&lt;p&gt;For arrays of 10M values, 6-33 million values can be read or written per second. If this is within acceptable limits, I&apos;d be happy to try and make a contribution to Lucene based on the code. However, I probably won&apos;t find the time before February or March 2010.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
actionCount arrayLength  actionType    valueMax  Packed(#1) Aligned(#2)     Int(#3)    Constant   &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt;[](#4)
   10000000     1000000       write           1         583         416         984         254         635
   10000000     1000000       write          15         594         499        1172         286         843
   10000000     1000000       write         255         604         478        1057         213         656
   10000000     1000000       write         256         734         765        1062         109         729
   10000000     1000000       write       65535        1036         802        1124         417         734
   10000000     1000000       write       65536        1015        1130        1072         172         781
   10000000     1000000       write     2097151        1020        1187        1052         223         614
   10000000     1000000       write  2147483646        1136        1073         839          73         719
   10000000     1000000        read           1         286         203         786         104           0
   10000000     1000000        read          15         291         182         859          78           0
   10000000     1000000        read         255         672         494         989          67           0
   10000000     1000000        read         256         568         729        1104          93           0
   10000000     1000000        read       65535         833         755        1088          99           0
   10000000     1000000        read       65536         947         974        1062         104           0
   10000000     1000000        read     2097151         999         963        1062          88           0
   10000000     1000000        read  2147483646        1349         869        1260          84           0

   10000000    10000000       write           1         833         568        1458         239        1229
   10000000    10000000       write          15        1427        1255        1432         276        1615
   10000000    10000000       write         255        1599        1578        1448         250        1244
   10000000    10000000       write         256        1656        1520        1317         109        1109
   10000000    10000000       write       65535        1734        1630        1385         245        1307
   10000000    10000000       write       65536        1718        1640        1395         182        1208
   10000000    10000000       write     2097151        1807        1781        1447         250        1291
   10000000    10000000       write  2147483646        1718        1599        1281          73        1099
   10000000    10000000        read           1         562         296        1301         109           0
   10000000    10000000        read          15        1187        1198        1322          83           0
   10000000    10000000        read         255        1421        1432        1526          99           0
   10000000    10000000        read         256        1521        1583        1588         104           0
   10000000    10000000        read       65535        1578        1427        1374          31           0
   10000000    10000000        read       65536        1634        1499        1489         113           0
   10000000    10000000        read     2097151        1625        1374        1468         224           0
   10000000    10000000        read  2147483646        1609        1349        1499          83           0
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                    <comment id="12795747" author="mikemccand" created="Fri, 1 Jan 2010 12:01:23 +0000"  >&lt;p&gt;These are great results &amp;#8211; thanks Toke!&lt;/p&gt;

&lt;p&gt;What&apos;s the difference between the two &quot;sections&quot; of your results?  The bottom section seems to have slower times overall than the top one.&lt;/p&gt;

&lt;p&gt;It&apos;s curious that &quot;aligned&quot; isn&apos;t always a win.&lt;/p&gt;

&lt;p&gt;For the packed case, did you generate specialized code for each of the cases?  I wonder how much / if that&apos;d really help in practice.&lt;/p&gt;

&lt;p&gt;For the standard codec&apos;s terms dict, my guess is we&apos;d just go with packed ints.&lt;/p&gt;</comment>
                    <comment id="12795861" author="toke" created="Sat, 2 Jan 2010 15:03:10 +0000"  >&lt;p&gt;The first section if for 1M values in the structure, the second is for 10M. As the CPU on the test-machine (Intel T2400) has only 2MB of level 2 cache, the increased processing time for the seemingly same amount of work is an effect of more cache-misses.&lt;/p&gt;

&lt;p&gt;Caching also accounts for why the packed version is sometimes better than the aligned. For values representable as 9 or 17 bits, the aligned version needs 16 and 32 bits respectively. In the case with 10M values, the packed version uses 1.1MB and 2.1MB for 9 and 17 bits respectively, while the aligned uses 2MB and 4MB respectively. The simpler logic of the aligned version does not compensate enough for the higher amount of trips around main memory.&lt;/p&gt;

&lt;p&gt;I did not generate any specialized code for the aligned case: No matter the number of bits/value, the amount of shifts, masks and ors is always the same. If the number of bits/value is known beforehand, specialized cases should be used (I made a factory that selects between packed, aligned and direct (#3), depending on the number of bits/value). The reason for not doing so in the first place is that I wanted to let the structure auto-adjust the bits/value when a new value was added. Having different implementations encapsulated in the same class means another level of indirection or conditionals, both of which I wanted to avoid for performance reasons. That being said, I haven&apos;t tested how much of a penalty this would be.&lt;/p&gt;

&lt;p&gt;The standard use case seems to be some sort of update-round, after which no updates are performed. Having a cleanupAndOptimize-call that potentially creates a new and optimized structure, would fit well into this and would avoid the indirection / conditional penalty.&lt;/p&gt;

&lt;p&gt;A whole other matter is long vs. ints. I&apos;ve tried using longs instead of ints as the backing array and the penalty on my 32bit processor was very high (I need to make some tests on this). If it must be possible to set and get longs, it&apos;s hard to avoid using long[] as the internal structure, but if ints are accepted as the only valid values, selecting long[] as backing array for 64 bit machines and int[] for 32 bit, might be the solution.&lt;/p&gt;

&lt;p&gt;All this calls for a factory-approach to hide the fairly complex task of choosing the right implementation.&lt;/p&gt;</comment>
                    <comment id="12796186" author="toke" created="Mon, 4 Jan 2010 14:21:47 +0000"  >&lt;p&gt;I made some small tweaks to improve performance and added long[]-backed versions of Packed (optimal space) and Aligned (no values span underlying blocks), the ran the performance tests on 5 different computers. It seems very clear that level 2 cache (and presumably RAM-speed, but I do not know how to determine that without root-access on a Linux box) plays a bigger role for access speed than mere CPU speed. One 3GHz with 1MB of level 2 cache was about half as fast than a 1.8GHz laptop with 2MB of level 2 cache.&lt;/p&gt;

&lt;p&gt;There is a whole lot of measurements and it is getting hard to digest. I&apos;ve attached logs from the 5 computers, should anyone want to have a look. Some observations are:&lt;/p&gt;

&lt;p&gt;1. The penalty of using long[] instead of int[] on my 32 bit laptop depends on the number of values in the array. For less than a million values, it is severe: The long[]-version if 30-60% slower, depending on whether packed or aligned values are used. Above that, it was 10% slower for Aligned, 25% slower for Packed.&lt;br/&gt;
On the other hand, 64 bit machines dos not seem to care that much whether int[] or long[] is used: There was 10% win for arrays below 1M for one machine, 50% for arrays below 100K for another (8% for 1M, 6% for 10M) for another and a small loss of below 1% for all lenghts above 10K for a third.&lt;/p&gt;

&lt;p&gt;2. There&apos;s a fast drop-off in speed when the array reaches a certain size that is correlated to level 2 cache size. After that, the speed does not decrease much when the array grows. This also affects direct writes to an int[] and has the interesting implication that a packed array out-performs the direct access approach for writes in a number of cases. For reads, there&apos;s no contest: Direct access to int[] is blazingly fast.&lt;/p&gt;

&lt;p&gt;3. The access-speed of the different implementations converges when the number of values in the array rises (think 10M+ values): The slow round-trip to main memory dwarfs the logic used for value-extraction. &lt;/p&gt;

&lt;p&gt;Observation #3 supports Mike McCandless choice of going for the packed approach and #1 suggests using int[] as the internal structure for now. Using int[] as internal structure makes if unfeasible to accept longs as input (or rather: longs with more than 32 significant bits). I don&apos;t know if this is acceptable?&lt;/p&gt;</comment>
                    <comment id="12799701" author="mikemccand" created="Wed, 13 Jan 2010 12:00:22 +0000"  >&lt;blockquote&gt;&lt;p&gt;The first section if for 1M values in the structure, the second is for 10M. As the CPU on the test-machine (Intel T2400) has only 2MB of level 2 cache, the increased processing time for the seemingly same amount of work is an effect of more cache-misses.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Got it.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Caching also accounts for why the packed version is sometimes better than the aligned. For values representable as 9 or 17 bits, the aligned version needs 16 and 32 bits respectively. In the case with 10M values, the packed version uses 1.1MB and 2.1MB for 9 and 17 bits respectively, while the aligned uses 2MB and 4MB respectively. The simpler logic of the aligned version does not compensate enough for the higher amount of trips around main memory.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;OK, though, I think we should somehow exclude these CPU cache effects&lt;br/&gt;
from the test.  In a real production situation, lots of other things&lt;br/&gt;
are competing for that cache, so I think the mostly-cache-miss case is&lt;br/&gt;
most realistic here.&lt;/p&gt;

&lt;p&gt;Actually a good way to do that is to test it in a wider context, eg&lt;br/&gt;
once I can get &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2186&quot; title=&quot;First cut at column-stride fields (index values storage)&quot;&gt;&lt;del&gt;LUCENE-2186&lt;/del&gt;&lt;/a&gt; &quot;integrated&quot;, we can test sorting by int&lt;br/&gt;
field with these different ways of encoding.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;I did not generate any specialized code for the aligned case: No matter the number of bits/value, the amount of shifts, masks and ors is always the same. If the number of bits/value is known beforehand, specialized cases should be used (I made a factory that selects between packed, aligned and direct (#3), depending on the number of bits/value).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;OK I have a start at this under &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2186&quot; title=&quot;First cut at column-stride fields (index values storage)&quot;&gt;&lt;del&gt;LUCENE-2186&lt;/del&gt;&lt;/a&gt;.  I&apos;ll try to clean up &amp;amp;&lt;br/&gt;
post here...&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;The reason for not doing so in the first place is that I wanted to let the structure auto-adjust the bits/value when a new value was added. Having different implementations encapsulated in the same class means another level of indirection or conditionals, both of which I wanted to avoid for performance reasons. That being said, I haven&apos;t tested how much of a penalty this would be.&lt;/p&gt;

&lt;p&gt;The standard use case seems to be some sort of update-round, after which no updates are performed. Having a cleanupAndOptimize-call that potentially creates a new and optimized structure, would fit well into this and would avoid the indirection / conditional penalty.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think the writing API can be a write-once API, where I declare the&lt;br/&gt;
maxValue I will write.  This fits with the indexing chain, where we&lt;br/&gt;
buffer values in RAM and then flush to the segmetn files.&lt;/p&gt;

&lt;p&gt;This way we don&apos;t need the complexity of having to resize the bits&lt;br/&gt;
internally when a new max value is seen.&lt;/p&gt;

&lt;p&gt;So, the factory would take number of values we will write, the max&lt;br/&gt;
value, and I guess an enum for Packed/Aligned/DedicatedArray, and&lt;br/&gt;
return a simple writer that just exposes an &quot;add(long value)&quot; method?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;A whole other matter is long vs. ints. I&apos;ve tried using longs instead of ints as the backing array and the penalty on my 32bit processor was very high (I need to make some tests on this). If it must be possible to set and get longs, it&apos;s hard to avoid using long[] as the internal structure, but if ints are accepted as the only valid values, selecting long[] as backing array for 64 bit machines and int[] for 32 bit, might be the solution.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Hmm... I guess we could still support values requiring more than 32&lt;br/&gt;
bits, encoded into int[], but it&apos;s more hairy as the value could span&lt;br/&gt;
2 or 3 ints.&lt;/p&gt;

&lt;p&gt;Probably we should specialize/default the factory selection based on&lt;br/&gt;
whether the JVM is 32/64 bit?  And, whether the bit size is &amp;lt;= 32.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;All this calls for a factory-approach to hide the fairly complex task of choosing the right implementation.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;I made some small tweaks to improve performance and added long[]-backed versions of Packed (optimal space) and Aligned (no values span underlying blocks), the ran the performance tests on 5 different computers. It seems very clear that level 2 cache (and presumably RAM-speed, but I do not know how to determine that without root-access on a Linux box) plays a bigger role for access speed than mere CPU speed. One 3GHz with 1MB of level 2 cache was about half as fast than a 1.8GHz laptop with 2MB of level 2 cache.&lt;br/&gt;
There is a whole lot of measurements and it is getting hard to digest. I&apos;ve attached logs from the 5 computers, should anyone want to have a look. Some observations are:&lt;/p&gt;

&lt;p&gt;1. The penalty of using long[] instead of int[] on my 32 bit laptop depends on the number of values in the array. For less than a million values, it is severe: The long[]-version if 30-60% slower, depending on whether packed or aligned values are used. Above that, it was 10% slower for Aligned, 25% slower for Packed.&lt;br/&gt;
On the other hand, 64 bit machines dos not seem to care that much whether int[] or long[] is used: There was 10% win for arrays below 1M for one machine, 50% for arrays below 100K for another (8% for 1M, 6% for 10M) for another and a small loss of below 1% for all lenghts above 10K for a third.&lt;/p&gt;

&lt;p&gt;2. There&apos;s a fast drop-off in speed when the array reaches a certain size that is correlated to level 2 cache size. After that, the speed does not decrease much when the array grows. This also affects direct writes to an int[] and has the interesting implication that a packed array out-performs the direct access approach for writes in a number of cases. For reads, there&apos;s no contest: Direct access to int[] is blazingly fast.&lt;/p&gt;

&lt;p&gt;3. The access-speed of the different implementations converges when the number of values in the array rises (think 10M+ values): The slow round-trip to main memory dwarfs the logic used for value-extraction.&lt;/p&gt;

&lt;p&gt;Observation #3 supports Mike McCandless choice of going for the packed approach and #1 suggests using int[] as the internal structure for now. Using int[] as internal structure makes if unfeasible to accept longs as input (or rather: longs with more than 32 significant bits). I don&apos;t know if this is acceptable?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think we should agree on an API that &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2186&quot; title=&quot;First cut at column-stride fields (index values storage)&quot;&gt;&lt;del&gt;LUCENE-2186&lt;/del&gt;&lt;/a&gt; can use to&lt;br/&gt;
write/read packed ints, then get our two patches talking.  I&apos;ll pull&lt;br/&gt;
out the barebones packed ints I&apos;m currently using, and post&lt;br/&gt;
here... then let&apos;s merge/iterate to a common API, so I can cutover to&lt;br/&gt;
the patch from here in &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2186&quot; title=&quot;First cut at column-stride fields (index values storage)&quot;&gt;&lt;del&gt;LUCENE-2186&lt;/del&gt;&lt;/a&gt;?&lt;/p&gt;</comment>
                    <comment id="12799707" author="mikemccand" created="Wed, 13 Jan 2010 12:09:36 +0000"  >&lt;p&gt;How about something like this API, for writing packed ints:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-keyword&quot;&gt;abstract&lt;/span&gt; class Writer {
  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;abstract&lt;/span&gt; void add(&lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt; v) &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; IOException;
  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;abstract&lt;/span&gt; void finish() &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; IOException;
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;then a factory:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-keyword&quot;&gt;enum&lt;/span&gt; Mode {Packed, Aligned, FixedArray};

&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;static&lt;/span&gt; Writer getWriter(IndexOutput out, &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; valueCount, &lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt; maxValue, Mode mode);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;(we can iterate on the names... always the hardest part).&lt;/p&gt;

&lt;p&gt;Packed means full bit packing (most space efficient, but slowest&lt;br/&gt;
decode time), Aligned might waste some bits (eg for nbits=4, that&apos;s&lt;br/&gt;
naturally aligned, but for nbits=7, we&apos;d waste 1 bit per long,&lt;br/&gt;
FixedArray (which&apos;d use byte[], short[], int[], long[]) would&lt;br/&gt;
potentially waste the most bits but have the fastest decode.&lt;/p&gt;

&lt;p&gt;If nbits happens to be 8, 16, 32, 64, the factory should just always&lt;br/&gt;
FixedArray I think?  And of course powers of two will automatically be&lt;br/&gt;
Aligned (with the per-nbits specialized code).&lt;/p&gt;

&lt;p&gt;Wew can also default impls to underlying int[] vs long[] backing store&lt;br/&gt;
depending on 54/32 bit jre, and, nbits.  If jre is 32 bit but nbits is&lt;br/&gt;
&amp;gt; 32 bit I think we just use long[] backing.&lt;/p&gt;

&lt;p&gt;For reading, a similar API:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-keyword&quot;&gt;abstract&lt;/span&gt; class Reader {
  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;abstract&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt; get(index);
}

&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;static&lt;/span&gt; Reader getReader(IndexInput in);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                    <comment id="12799712" author="mikemccand" created="Wed, 13 Jan 2010 12:22:15 +0000"  >&lt;p&gt;Attached patch with my current roughed up approach for packed ints&lt;br/&gt;
(from &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2186&quot; title=&quot;First cut at column-stride fields (index values storage)&quot;&gt;&lt;del&gt;LUCENE-2186&lt;/del&gt;&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Let&apos;s try to standardize the API, then merge the two approaches, then&lt;br/&gt;
I&apos;ll cutover with &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2186&quot; title=&quot;First cut at column-stride fields (index values storage)&quot;&gt;&lt;del&gt;LUCENE-2186&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;It includes gen.py, which autogens dedicated decoders for each of the&lt;br/&gt;
nbits cases, excluding 8, 16, 32, 64 bits, since these are done with&lt;br/&gt;
dedicated array reader impls.&lt;/p&gt;

&lt;p&gt;It uses a single writer (I don&apos;t think we need specialized writers),&lt;br/&gt;
but the writer encodes in the same byte order as&lt;br/&gt;
IndexOutput.writeLong, so that the byte order matches the dedicated&lt;br/&gt;
array reader impls.&lt;/p&gt;

&lt;p&gt;It only encodes into long[] &amp;#8211; we should create cases for int[]&lt;br/&gt;
(selected by the factory depending on 32 vs 64 bit jre).&lt;/p&gt;

&lt;p&gt;We should also explore just reading in a full byte[] and using&lt;br/&gt;
Int/Short/Long buffer to decode.  This API should also allow for a&lt;br/&gt;
future mmap impl as well.&lt;/p&gt;

&lt;p&gt;Probably we should name all of these UnsignedPackedInts, since they&lt;br/&gt;
require values &amp;gt;= 0.  (Hmm, though, the 64 bit case is tricky &amp;#8211; I&lt;br/&gt;
guess we make an exception for that case).&lt;/p&gt;</comment>
                    <comment id="12802542" author="toke" created="Tue, 19 Jan 2010 23:46:20 +0000"  >&lt;p&gt;Introducing yet another level of indirection and making a byte/short/int/long-prvider detached from the implementation of the packed values it tempting. I&apos;m fairly afraid of the overhead of the extra method-calls, but I&apos;ll try it and see what happens.&lt;/p&gt;

&lt;p&gt;I&apos;ve read your (Michael McCandless) code an I can see that the tiny interfaces for Reader and Writer works well for your scenario. However, as the Reader must have (fast) random access, wouldn&apos;t it make sense to make it possible to update values? That way the same code can be used to hold ords for sorting and similar structures.&lt;/p&gt;

&lt;p&gt;Instead of Reader, we could use&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-keyword&quot;&gt;abstract&lt;/span&gt; class Mutator {
  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;abstract&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt; get(&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; index);
  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;abstract&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt; set(&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; index, &lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt; value);
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;...should the index also be a long? No need to be bound by Java&apos;s 31-bit limit on array-length, although I might very well be over-engineering here.&lt;/p&gt;

&lt;p&gt;The whole 32bit vs. 64bit as backing array does present a bit of a problem with persistence. We&apos;ll be in a situation where the index will be optimized for the architecture used for building, not the one used for searching. Leaving the option of a future mmap open means that it is not possible to do a conversion when retrieving the bits, so I have no solution for this (other than doing memory-only).&lt;/p&gt;</comment>
                    <comment id="12802780" author="mikemccand" created="Wed, 20 Jan 2010 10:16:40 +0000"  >&lt;blockquote&gt;&lt;p&gt;Introducing yet another level of indirection and making a byte/short/int/long-prvider detached from the implementation of the packed values it tempting.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;You mean the layer that stores the minValue, so that the full range is&lt;br/&gt;
supported?  I actually think we should absorb that into packed ints,&lt;br/&gt;
so it&apos;s only one method call per lookup, and specialize the &quot;positive&lt;br/&gt;
only&quot; cases to avoid the extra add per lookup.&lt;/p&gt;

&lt;p&gt;With that fix, it&apos;s still a method call per lookup, but I don&apos;t see&lt;br/&gt;
how we can get away from that, unless we allow for exposure of the raw&lt;br/&gt;
array for the no-packing cases (which we could consider...).&lt;/p&gt;

&lt;p&gt;Remember we use packed ints in places where we can accept some loss of&lt;br/&gt;
CPU perf. for improvements in RAM usage (see the comment I just added&lt;br/&gt;
to &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2186&quot; title=&quot;First cut at column-stride fields (index values storage)&quot;&gt;&lt;del&gt;LUCENE-2186&lt;/del&gt;&lt;/a&gt;).&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;However, as the Reader must have (fast) random access, wouldn&apos;t it make sense to make it possible to update values?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yeah, we do eventually want CSF to be updateable, but I don&apos;t think we&lt;br/&gt;
need this for phase 1?  Likewise, I think all we need now for Lucene&lt;br/&gt;
is a &quot;WriteOnceWriter&quot;, not a &quot;RandomAccessWriter&quot;.  Ie, you open a&lt;br/&gt;
writer, you add (sequentially) all values, you close.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;...should the index also be a long?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I would stick with int now (we are doing this for Lucene, whose docIDs&lt;br/&gt;
are still ints...).  Design for today.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;The whole 32bit vs. 64bit as backing array does present a bit of a problem with persistence. We&apos;ll be in a situation where the index will be optimized for the architecture used for building, not the one used for searching. Leaving the option of a future mmap open means that it is not possible to do a conversion when retrieving the bits, so I have no solution for this (other than doing memory-only).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I&apos;m confused &amp;#8211; a future mmap impl shouldn&apos;t put pressure on the file&lt;br/&gt;
format used by packed ints today?  Ie, a future mmap impl can use a&lt;br/&gt;
totally different format than the designed-to-be-slurped-into-RAM&lt;br/&gt;
format for packed ints, today?&lt;/p&gt;

&lt;p&gt;Also, what do you mean by optimized for building not searching?&lt;/p&gt;

&lt;p&gt;Note that on 32 bit machines, if there is actually a gain, we can make&lt;br/&gt;
a backing store with ints yet still allow for storage of nbits&amp;gt;32?  It&lt;br/&gt;
&quot;just&quot; means a value may be split across 2 or 3 values?&lt;/p&gt;</comment>
                    <comment id="12802829" author="paul.elschot@xs4all.nl" created="Wed, 20 Jan 2010 13:29:51 +0000"  >&lt;p&gt;I&apos;ve made a remark at &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-1410&quot; title=&quot;PFOR implementation&quot;&gt;&lt;del&gt;LUCENE-1410&lt;/del&gt;&lt;/a&gt; (a first attempt at a PFOR implementation) about the header structure for encoding this.&lt;br/&gt;
One thing that is not covered here is how to deal with input arrays with intermediate length that are shorter than 32 and longer than 3 or 4. Shorter ones can easily be encoded as vByte.&lt;br/&gt;
Simple9 might be a solution, but it has only 28 data bits and 9 different encoding cases so it appears to be somewhat small.&lt;br/&gt;
There is first attempt at Simple9 at &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2189&quot; title=&quot;Simple9 (de)compression&quot;&gt;&lt;del&gt;LUCENE-2189&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Since the discussion here is on alignment (int/long) I&apos;m wondering how (and whether) to go from the current byte aligned structures to int aligned. Using aligned ints would save the shifting done at IndexInput.getInt() that reads 4 bytes and shifts them into place to create an int from them.&lt;br/&gt;
Simple9 can be int aligned and I&apos;d like to add bigger variations of that, but peferably only ones that add a multiple of 4 bytes.&lt;/p&gt;

&lt;p&gt;So would make sense to add functionality to IndexInput and IndexOutput to allow int aligned access?&lt;br/&gt;
Are java&apos;s data streams and/or nio buffers smart enough to avoid the byte shifting for ints in such cases?&lt;/p&gt;</comment>
                    <comment id="12802854" author="yseeley@gmail.com" created="Wed, 20 Jan 2010 14:53:43 +0000"  >&lt;blockquote&gt;&lt;p&gt;Using aligned ints would save the shifting done at IndexInput.getInt() that reads 4 bytes and shifts them into place to create an int from them.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;How&apos;s that?  Is there some JVM intrinsic?&lt;/p&gt;</comment>
                    <comment id="12802875" author="paul.elschot@xs4all.nl" created="Wed, 20 Jan 2010 15:54:26 +0000"  >&lt;p&gt;For the record: on the flex branch I just saw IntIndexInput and IntIndexOutput.&lt;/p&gt;</comment>
                    <comment id="12802877" author="paul.elschot@xs4all.nl" created="Wed, 20 Jan 2010 16:00:07 +0000"  >&lt;blockquote&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; Using aligned ints would save the shifting done at IndexInput.getInt() that reads 4 bytes and shifts them into place to create an int from them.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;


&lt;p&gt;How&apos;s that? Is there some JVM intrinsic?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;In the aligned case, and with the right byte order, getInt() on a java data stream might be reduced to processor instructions operating on 4 bytes at a time.&lt;/p&gt;

&lt;p&gt;Is that what you mean by JVM intrinsic?&lt;/p&gt;</comment>
                    <comment id="12802885" author="yseeley@gmail.com" created="Wed, 20 Jan 2010 16:25:46 +0000"  >&lt;blockquote&gt;&lt;p&gt;In the aligned case, and with the right byte order, getInt() on a java data stream might be reduced to processor instructions operating on 4 bytes at a time. Is that what you mean by JVM intrinsic?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes.  But are you aware of that having been implemented in any JVMs?&lt;/p&gt;</comment>
                    <comment id="12802888" author="toke" created="Wed, 20 Jan 2010 16:43:27 +0000"  >&lt;blockquote&gt;
&lt;p&gt;Toke:&lt;br/&gt;
Introducing yet another level of indirection and making a byte/short/int/long-prvider detached from the implementation of the packed values it tempting.&lt;/p&gt;&lt;/blockquote&gt;

&lt;blockquote&gt;
&lt;p&gt;You mean the layer that stores the minValue, so that the full range is&lt;br/&gt;
supported? I actually think we should absorb that into packed ints,&lt;br/&gt;
so it&apos;s only one method call per lookup, and specialize the &quot;positive&lt;br/&gt;
only&quot; cases to avoid the extra add per lookup.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;No, I mean the backing array of ints or longs. For memory, the obvious choice is int[] or long[], but designing for flexibility calls for an API, which coincidentally is identical to Reader. So, a 4-bit Aligned could be backed by a DirectInt (which will contain an int[]) or a Persistent (doing mmap or some other persistent-oriented lookup).&lt;/p&gt;

&lt;p&gt;...I should show this in code instead. I&apos;ll try and find the time.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Yeah, we do eventually want CSF to be updateable, but I don&apos;t think we&lt;br/&gt;
need this for phase 1?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Not in the specific scenario, no.&lt;/p&gt;


&lt;blockquote&gt;
&lt;p&gt;Toke:&lt;br/&gt;
The whole 32bit vs. 64bit as backing array does present a bit of a problem with persistence. We&apos;ll be in a situation where the index will be optimized for the architecture used for building, not the one used for searching. Leaving the option of a future mmap open means that it is not possible to do a conversion when retrieving the bits, so I have no solution for this (other than doing memory-only).&lt;/p&gt;&lt;/blockquote&gt;

&lt;blockquote&gt;
&lt;p&gt;I&apos;m confused - a future mmap impl shouldn&apos;t put pressure on the file&lt;br/&gt;
format used by packed ints today? Ie, a future mmap impl can use a&lt;br/&gt;
totally different format than the designed-to-be-slurped-into-RAM&lt;br/&gt;
format for packed ints, today?&lt;/p&gt;

&lt;p&gt;Also, what do you mean by optimized for building not searching?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;When we&apos;re using Aligned, the choice of using int or long for the backing array dictates how the persistent bitstream will be. If the index builder is a 64 bit machine and 7 bits/value is used, the result will be longs, each containing 9 values.&lt;/p&gt;

&lt;p&gt;When the structure is read into memory by the searcher, the backing array will again be long. But if the searcher happens to be a 32 bit machine, the performance will be less than if ints were used for the backing array.&lt;/p&gt;

&lt;p&gt;One way to handle this is to do a conversion, when loading into memory. If the searcher is 64 bit, it will always convert into longs. If the searcher is 32 bit, it will convert into int, if the bits/value is &amp;lt;= 32. The conversion is cheap, so this is no problem in itself.&lt;/p&gt;

&lt;p&gt;However, if we&apos;re planning for the future (or for flexibility, depending on point of view), we would very much like the persistent format to be directly usable, so that we don&apos;t need to load the whole structure into memory. This rules out conversion and sets us back to step 1: The index will be optimized for either 32bit or 64 bit searchers.&lt;/p&gt;

&lt;p&gt;Oh well, we could always just ignore it and say that Aligned is 64 bit based. As it is more memory-efficient than Aligned on 32 bit machines, maybe the slightly smaller number of backing longs will compensate for the overhead of retrieving longs on a 64 bit machine.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Note that on 32 bit machines, if there is actually a gain, we can make&lt;br/&gt;
a backing store with ints yet still allow for storage of nbits&amp;gt;32? It&lt;br/&gt;
&quot;just&quot; means a value may be split across 2 or 3 values?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;My guess is that the number of values vs. the available level 2 cache will play a big role here: For a relatively small number of values, the added logic will be costly. For a larger number of values, the cache-misses will dwarf that.&lt;/p&gt;</comment>
                    <comment id="12802903" author="paul.elschot@xs4all.nl" created="Wed, 20 Jan 2010 17:30:55 +0000"  >&lt;p&gt;Paul: In the aligned case, and with the right byte order, getInt() on a java data stream might be reduced to processor instructions operating on 4 bytes at a time. Is that what you mean by JVM intrinsic?&lt;/p&gt;

&lt;p&gt;Yonik: Yes. But are you aware of that having been implemented in any JVMs?&lt;/p&gt;

&lt;p&gt;I remember reading about an implementation doing that, but I can&apos;t find it back now.&lt;/p&gt;

&lt;p&gt;If one cannot have such ints directly, there is not much point in unpacking via IndexInput.getInt(), it would be better to unpack directly from the bytes.&lt;br/&gt;
Also, in that case, I&apos;d prefer to use single byte increments (above 4 byte increments) for the size of the encoded data for variations on Simple9.&lt;/p&gt;</comment>
                    <comment id="12803628" author="toke" created="Fri, 22 Jan 2010 08:07:51 +0000"  >&lt;p&gt;Looking at bit patterns and persistence, I see 3 different ones: Packed, aligned32 and aligned64. Regardless of whether 32bit or 64bit is used when a packed structure is created, it can be read as both 32bit and 64bit packed. As for the special cases of 8, 16, 32 and 64 bits/value, the bit patterns are identically to both packed and aligned. This leeds me to propose a header designating one of the three structures mentioned.&lt;/p&gt;

&lt;p&gt;The current draft from Michael McCandless states both bitsPerValue and maxValue in the persistent format. It seems a redundant to have both, but I might be missing something here? Either way, the bitsPerValue is ambiguous as it does not translate to memory usage the same way for packed, aligned32 or aligned64. Should I choose, I&apos;d go for maxValue.&lt;/p&gt;

&lt;p&gt;What about a header stating&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
format (&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt; &lt;span class=&quot;code-quote&quot;&gt;&quot;packed&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;aligned32&quot;&lt;/span&gt; or &lt;span class=&quot;code-quote&quot;&gt;&quot;aligned64&quot;&lt;/span&gt;)
valueCount (vInt)
maxValue (vLong)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;?&lt;/p&gt;

&lt;p&gt;I have working code for packed32 and packed64 and am currently fitting it into Michael&apos;s patch. I hope to finish it this weekend.&lt;/p&gt;</comment>
                    <comment id="12803633" author="paul.elschot@xs4all.nl" created="Fri, 22 Jan 2010 08:37:01 +0000"  >&lt;p&gt;How about encoding the header something like this:&lt;/p&gt;

&lt;p&gt;VByte 1 bit.&lt;br/&gt;
Simple9 4 bits. These cases imply valueCount and maxValue.&lt;br/&gt;
For around 4-16 numbers encode the complete header in 5-6 bits, also implying valueCount and maxValue.&lt;br/&gt;
For FrameOfRef, encoding 32 or more numbers, a larger header can be used, 4 bytes for example, maxValue is implied from the number of frame bits. valueCount could be 32, 64, 128 (i.e. 2 bits). Also the number of exceptions could be put there.&lt;/p&gt;

&lt;p&gt;This header &quot;type&quot; can be chosen depending on the given length of the encoded sequence.&lt;/p&gt;</comment>
                    <comment id="12803635" author="mikemccand" created="Fri, 22 Jan 2010 08:39:12 +0000"  >&lt;blockquote&gt;&lt;p&gt;I have working code for packed32 and packed64 and am currently fitting it into Michael&apos;s patch. I hope to finish it this weekend.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Nice!  Sounds like good progress Toke!&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;The current draft from Michael McCandless states both bitsPerValue and maxValue in the persistent format&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I was only storing maxValue as a convenience for the layer above &amp;#8211; we don&apos;t need to do that &amp;#8211; I think storing format (packed, aligned32, aligned64) and bitsPerValue makes sense.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Regardless of whether 32bit or 64bit is used when a packed structure is created, it can be read as both 32bit and 64bit packed.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Right, but with the challenge (if we use 32bit backing array) of properly handling the nbits&amp;gt;32 case (this is perfectly doable... &quot;it&apos;s just software&quot; &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/wink.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; ).&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;As for the special cases of 8, 16, 32 and 64 bits/value, the bit patterns are identically to both packed and aligned.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I had chosen to match IndexOutput/Inputs&apos;s byte order (big-endian) so that the packed format naturally reads back with IndexInput&apos;s readLong/Int/Short (I added a readShort).&lt;/p&gt;

&lt;p&gt;I&apos;m assuming for these special cases that dedicated Reader impls, with byte[], short[], int[], long[] backing array, is faster than eg backing with a long[] and shift/masking per lookup.&lt;/p&gt;

&lt;p&gt;But eg for the nbits=3 case, aligned 32/64 would ensure that no value spans across two underlying entries in the backing array (wasting some bits of storage in exchange).  Whereas the nbits=2 or 4 cases would naturally be aligned anyway...&lt;/p&gt;

&lt;p&gt;One question: the Reader api is now this:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt; get(&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; index);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Which is convenient since obviously long can accommodate all of the underlying possible nbits, but... for small nbits values, this logically entails a cast.  EG say nbits=8, so it&apos;s a direct byte[] backing array.  get() must cast up to long, and caller must operate with long... I&apos;m wondering whether that forced casting is going to hurt performance enough to make us want to have dedicated precision (8, 16, 32, 64) Reader interfaces....&lt;/p&gt;</comment>
                    <comment id="12803638" author="mikemccand" created="Fri, 22 Jan 2010 08:45:21 +0000"  >&lt;blockquote&gt;&lt;p&gt;This header &quot;type&quot; can be chosen depending on the given length of the encoded sequence.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think we should separate the header needed for this issue (single header stored once at the beginning of a potentially massive file), from the per-block header used by int block based formats like SimpleN/PForDelta/etc?&lt;/p&gt;

&lt;p&gt;I think Toke&apos;s proposed header, if we swap in bitsPerValue in place of maxValue, is good for this issue?&lt;/p&gt;</comment>
                    <comment id="12803679" author="toke" created="Fri, 22 Jan 2010 12:27:10 +0000"  >&lt;p&gt;I&apos;ve uploaded a preliminary patch with packed32, packed64, directByte, directShort, directInt and directLong implementations. I&apos;ve used Michael McCandless patch as foundation, but the new patch is generated to be independent from the old one. It uses maxValue instead of bitsPerValue for the header, there&apos;s no test of packed32 and there&apos;s a general need for cleanup. The main missing components are aligned32 and aligned64.&lt;/p&gt;

&lt;p&gt;I&apos;ve done quite a bit of refactoring and (cheater that I am) added setters to all implementations of Reader, although not to the interface. Besides the nitty-gritty details of the implementation, I suspect that the code for selecting which implementation to use is a prime candidate for discussion. It is located in PackedInts and tries to select the best implementation based on preference for packed, aligned and direct paired with preference for 32bit and 64bit.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
  &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;static&lt;/span&gt; IMPLEMENTATION getImplementation(
          &lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt; maxValue, PRIORITY priority, BLOCK_PREFERENCE block) {
    &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; bits = bitsRequired(maxValue);
    &lt;span class=&quot;code-keyword&quot;&gt;switch&lt;/span&gt; (priority) {
      &lt;span class=&quot;code-keyword&quot;&gt;case&lt;/span&gt; direct: {
        bits = getNextFixedSize(bits);
        &lt;span class=&quot;code-keyword&quot;&gt;break&lt;/span&gt;;
      }
      &lt;span class=&quot;code-keyword&quot;&gt;case&lt;/span&gt; aligned: {
        &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (block == BLOCK_PREFERENCE.bit32) {
          &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (bits == 7 || bits &amp;gt;= 11) {
            bits = getNextFixedSize(bits); &lt;span class=&quot;code-comment&quot;&gt;// Align to &lt;span class=&quot;code-object&quot;&gt;byte&lt;/span&gt;, &lt;span class=&quot;code-object&quot;&gt;short&lt;/span&gt;, &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; or &lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt;
&lt;/span&gt;          }
        } &lt;span class=&quot;code-keyword&quot;&gt;else&lt;/span&gt; {
          &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; ((bits &amp;gt;= 13 &amp;amp;&amp;amp; bits &amp;lt;= 15) || (bits &amp;gt;= 22)) {
            bits = getNextFixedSize(bits); &lt;span class=&quot;code-comment&quot;&gt;// Align to &lt;span class=&quot;code-object&quot;&gt;short&lt;/span&gt;, &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; or &lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt;
&lt;/span&gt;          }
        }
      }
    }
    &lt;span class=&quot;code-keyword&quot;&gt;switch&lt;/span&gt; (bits) { &lt;span class=&quot;code-comment&quot;&gt;// The safe choices
&lt;/span&gt;      &lt;span class=&quot;code-keyword&quot;&gt;case&lt;/span&gt; 8: &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; IMPLEMENTATION.directByte;
      &lt;span class=&quot;code-keyword&quot;&gt;case&lt;/span&gt; 16: &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; IMPLEMENTATION.directShort;
      &lt;span class=&quot;code-keyword&quot;&gt;case&lt;/span&gt; 32: &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; IMPLEMENTATION.directInt;
      &lt;span class=&quot;code-keyword&quot;&gt;case&lt;/span&gt; 63:
      &lt;span class=&quot;code-keyword&quot;&gt;case&lt;/span&gt; 64: &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; IMPLEMENTATION.directLong;
    }

    &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (priority == PRIORITY.aligned || bits == 1 || bits == 2 || bits == 4) {
      &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; block == BLOCK_PREFERENCE.bit32 &amp;amp;&amp;amp; bits &amp;lt; 32 ?
              IMPLEMENTATION.aligned32 : IMPLEMENTATION.aligned64;
    }
    &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; block == BLOCK_PREFERENCE.bit32 &amp;amp;&amp;amp; bits &amp;lt; 32 ?
            IMPLEMENTATION.packed32 : IMPLEMENTATION.packed64;

    &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; IMPLEMENTATION.packed64;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I think that an &quot;auto&quot;-value for priority is worth considering: For 9, 17 and 33 bits/value, packed is often faster than aligned due to only using half the memory and thus having lower risk of level 2 cache misses. For high bits/value, such as 30, 31, 62, 63 and 64 (guesstimating here), choosing direct seems to be the best choice for most situations. Users of PackedInts should not be expected to know this.&lt;/p&gt;

&lt;p&gt;I&apos;ll start work on aligned32 and aligned64, but I will leave the rest of the patch alone for now, as I suspect that there&apos;ll be some changes to the current draft.&lt;/p&gt;</comment>
                    <comment id="12803743" author="paul.elschot@xs4all.nl" created="Fri, 22 Jan 2010 16:17:33 +0000"  >&lt;p&gt;The generated code in the patches has quite a few switch statements to decode a single value.&lt;br/&gt;
These switch statements could be avoided by using something like this (adapted from the 1410b patch):&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
/** Decode a value from the compressed array of b bit values by retrieving the corresponding bits.
 * Since numFrameBits is always smaller than the number of bits in an &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt;,
 * at most two ints in the buffer will be used.
 */
&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; decodeCompressedValueBase(&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; compressedPos, &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; numBits) {
  &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; compressedBitPos = numBits * compressedPos;
  &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; intIndex = (compressedBitPos &amp;gt;&amp;gt; 5);
  &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; firstBitPosition = compressedBitPos &amp;amp; 31;
  &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; value = intBuffer.get(intIndex) &amp;gt;&amp;gt;&amp;gt; firstBitPosition;
  &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; ((firstBitPosition + numBits) &amp;gt; 32) { &lt;span class=&quot;code-comment&quot;&gt;// value does not fit in first &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt;
&lt;/span&gt;    intIndex++;
    value |= (intBuffer.get(intIndex) &amp;lt;&amp;lt; (32 - firstBitPosition));
  }
  &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; maxValue = (&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt;) ((1L &amp;lt;&amp;lt; numBits) - 1);
  &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; value &amp;amp; maxValue;
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;As maxValue is essentially a mask, it could also be looked up in an array.&lt;/p&gt;

&lt;p&gt;Could that be faster than these generated switch statements?&lt;/p&gt;</comment>
                    <comment id="12803872" author="toke" created="Fri, 22 Jan 2010 21:11:08 +0000"  >&lt;p&gt;I think Michaels generated code was meant as a temporary solution, until a handcrafted version was available. In packed32 and packed64, the code for decoding a value is &lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt; get(&lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; index) {
    &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt; majorBitPos = index * elementBits;
    &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; elementPos = (&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt;)(majorBitPos &amp;gt;&amp;gt;&amp;gt; BLOCK_BITS); &lt;span class=&quot;code-comment&quot;&gt;// / BLOCK_SIZE
&lt;/span&gt;    &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; bitPos =     (&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt;)(majorBitPos &amp;amp; MOD_MASK); &lt;span class=&quot;code-comment&quot;&gt;// % BLOCK_SIZE);
&lt;/span&gt;
    &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; base = bitPos * FAC_BITPOS;

    &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; ((blocks[elementPos] &amp;lt;&amp;lt; shifts[base]) &amp;gt;&amp;gt;&amp;gt; shifts[base+1]) |
            ((blocks[elementPos+1] &amp;gt;&amp;gt;&amp;gt; shifts[base+2]) &amp;amp; readMasks[bitPos]);
  }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;which looks a lot like your (Paul Elschot)  suggestion. It  avoids all conditionals at the cost of more bit-operations and a dummy element at the end of the backing array. I must admit that my performance testing of the different solutions has been fairly ad hoc (measure, tweak, repeat), so an appropriate test would be in order.&lt;/p&gt;</comment>
                    <comment id="12804062" author="paul.elschot@xs4all.nl" created="Sat, 23 Jan 2010 10:44:44 +0000"  >&lt;p&gt;Nice to see a more mature alternative.&lt;br/&gt;
The trade off between (dummy element/unconditioned extra access) and (conditioned extra access) is of later concern.&lt;br/&gt;
Both the extra access and the condition take cycles, so it&apos;s not clear which one will be faster. It might even depend on the value of elementBits.&lt;/p&gt;</comment>
                    <comment id="12804072" author="paul.elschot@xs4all.nl" created="Sat, 23 Jan 2010 11:48:57 +0000"  >&lt;p&gt;As to whether to use int or long in the interface unsigned packed int, the only numbers that will probably need to be long in the foreseeable future are docids. However this change can be delayed by not allowing an index segment to grow beyond 2^32 or 2^31-1docs, and by only implementing the long docids for multiple index segments.&lt;br/&gt;
So as long as it is ok to assume that an index segment can have MAXINT docs at most, we could use an int interface here.&lt;br/&gt;
Do Nutch and/or Solr already have long docids implemented on multiple index readers/writers or segments?&lt;/p&gt;

&lt;p&gt;The other border is the max size of a document field. If that goes beyond MAXINT, the positions and maybe even the frequencies would need to be changed from int to long. But for now I can&apos;t think of a real use case with a document field that has more than MAXINT positions. That would be like a book with ten million pages of text. Did anyone ever run into this limitation?&lt;/p&gt;</comment>
                    <comment id="12804723" author="mikemccand" created="Mon, 25 Jan 2010 21:21:39 +0000"  >&lt;p&gt;Good progress !&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I think Michaels generated code was meant as a temporary solution, until a handcrafted version was available&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Actually that was intended to be a fast impl... the switch should be&lt;br/&gt;
compiled to a direct lookup (maybe plus a conditional to catch the&lt;br/&gt;
&quot;default&quot; case even though it will never happen...ugh).  But I like&lt;br/&gt;
your impl with no conditional at all.  We should test both.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;As to whether to use int or long in the interface unsigned packed int, the only numbers that will probably need to be long in the foreseeable future are docids.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Also the file offsets into the terms dict, possibly the offsets in RAM&lt;br/&gt;
into the terms dict character data (UTF8 byte[]).  Also, when we do&lt;br/&gt;
column stride fields, we allow storing values &amp;gt; int.  I think we&lt;br/&gt;
should stick with &lt;tt&gt;long get(index)&lt;/tt&gt; for now.&lt;/p&gt;

&lt;p&gt;Other comments:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;Maybe we should move all of this under oal.util.packed?&lt;br/&gt;
    (packedints?  ints?)&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;I think we should remove getMaxValue() from the Reader interface?&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Why create the IMPLEMENTATION enum?  Why not simply return an&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;anonymous&amp;#93;&lt;/span&gt; instance of Writer?&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Why not store bitsPerValue in the header instead of maxValue?  EG&lt;br/&gt;
    maybe my maxValue is 7000, but because I&apos;m using directShort,&lt;br/&gt;
    bitsPerValue is 16.  Also, the maxValue at write time should not&lt;br/&gt;
    have to be known &amp;#8211; eg the factory API should let me ask for a&lt;br/&gt;
    direct short writer without declaring the maxValue I will store.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;I wonder if we should add an optional Object&lt;br/&gt;
    getDirectBackingArray().  The packed/aligned impls would return&lt;br/&gt;
    null, but the direct byte/short/int/long impls would return their&lt;br/&gt;
    array.  This would allow callers to specialize upstream impls to&lt;br/&gt;
    do the direct array lookup without the cast-to-long (like how&lt;br/&gt;
    FieldComparator now has impls for byte,short,int,long).  I suspect&lt;br/&gt;
    for column stride fields, when sorting by an integer field, on a&lt;br/&gt;
    32bit arch, this would be a perf win.  But: let&apos;s wait until we&lt;br/&gt;
    have CSFs, and we can test whether there really is a gain here....&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;I think we shouldn&apos;t put a getWriter on every Reader&lt;br/&gt;
    impl... because it&apos;s a one to many mapping?  Eg the format written&lt;br/&gt;
    by PackedWriter can be read by direct byte/short/int/long,&lt;br/&gt;
    Packed32/64.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;For starters I don&apos;t think we should make reader impls that can&lt;br/&gt;
    read nbits &amp;gt; 31 bits with an int[] backing array.  I think long[]&lt;br/&gt;
    backing array is fine.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;I don&apos;t think we need separate PRIORITY and BLOCK_PREFERENCE?&lt;br/&gt;
    Can&apos;t we have a single enum (STORAGE?) with: packed, aligned32,&lt;br/&gt;
    aligned64?  &quot;Direct&quot; is really just packed with nbits rounded up&lt;br/&gt;
    to 8,16,32,64.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Aligned32/64 is very wasteful for certain nbits... I like the idea&lt;br/&gt;
    of &quot;auto&quot; to avoid risk that caller picks a bad combination.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;I think for starters we should not make any reader impls that do&lt;br/&gt;
    remapping at load time.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="12804743" author="paul.elschot@xs4all.nl" created="Mon, 25 Jan 2010 21:49:36 +0000"  >&lt;blockquote&gt;
&lt;p&gt;As to whether to use int or long in the interface unsigned packed int, the only numbers that will probably need to be long in the foreseeable future are docids.&lt;/p&gt;&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Also the file offsets into the terms dict, possibly the offsets in RAM&lt;br/&gt;
 into the terms dict character data (UTF8 byte[]). Also, when we do&lt;br/&gt;
 column stride fields, we allow storing values &amp;gt; int. I think we&lt;br/&gt;
 should stick with long get(index) for now.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Indeed I missed the offsets. However, a long get(index) will probably get in the way, because there are far fewer offsets&lt;br/&gt;
than normal data, and the 32 bit processors will be around for a long time. So I think we&apos;ll need both long (for the offsets) and int (for the rest) in the end.&lt;/p&gt;</comment>
                    <comment id="12804809" author="toke" created="Tue, 26 Jan 2010 00:17:15 +0000"  >&lt;blockquote&gt;
&lt;p&gt;I think we should remove getMaxValue() from the Reader interface?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes. I only left maxValue in the code because I ran out of time.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Why create the IMPLEMENTATION enum? Why not simply return an&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;anonymous&amp;#93;&lt;/span&gt; instance of Writer?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The IMPLEMENTATION enum is only used internally and is package private. It was introduced to separate decision-making from specific implementations - e.g. the packed writer is the same for packed32 and packed64, although the reader differs. But it could very well be that it confuses more than it helps.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Why not store bitsPerValue in the header instead of maxValue?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;As above - I did not have the time to fix it and wanted to push the patch in order to move the discussion along.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Also, the maxValue at write time should not&lt;br/&gt;
have to be known - eg the factory API should let me ask for a&lt;br/&gt;
direct short writer without declaring the maxValue I will store.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Since Packed and Aligned needs maxValue (or bitsPerValue), this would require two distinct methods in the factory, each returning a subset of the possible implementations. I find that rather confusing.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;I wonder if we should add an optional Object&lt;br/&gt;
getDirectBackingArray(). The packed/aligned impls would return&lt;br/&gt;
null, but the direct byte/short/int/long impls would return their&lt;br/&gt;
array. &lt;span class=&quot;error&quot;&gt;&amp;#91;...&amp;#93;&lt;/span&gt; &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Speaking of API additions, I find that&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; getBitsPerValue();
&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; size();
&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; void set(&lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt; value);
&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; void clear();
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;are trivial to implement for the known implementations. They open up for things like auto-growing to fit higher values by using a delegating wrapper, re-using the structure for counting purposes and sorting in-place.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;I think we shouldn&apos;t put a getWriter on every Reader&lt;br/&gt;
impl... because it&apos;s a one to many mapping? Eg the format written&lt;br/&gt;
by PackedWriter can be read by direct byte/short/int/long,&lt;br/&gt;
Packed32/64.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Quite right.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;For starters I don&apos;t think we should make reader impls that can&lt;br/&gt;
read nbits &amp;gt; 31 bits with an int[] backing array. I think long[]&lt;br/&gt;
backing array is fine.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;The current patch limits nbits to 32 for Packed32. I am confident that an int-backed reader with nbits &amp;gt; 32 will be slower than a long-backed reader on a 32 bit machine.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;I don&apos;t think we need separate PRIORITY and BLOCK_PREFERENCE?&lt;br/&gt;
Can&apos;t we have a single enum (STORAGE?) with: packed, aligned32,&lt;br/&gt;
aligned64? &quot;Direct&quot; is really just packed with nbits rounded up&lt;br/&gt;
to 8,16,32,64.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I agree that it does complicate matters somewhat to have them separated. When calling getReader the BLOCK_PREFERENCE should also be removed, as the block preference will always be the same as that architecture. Removing the &quot;direct&quot; option would require the caller to do some of the logic in some cases: If low processing requirements is a priority, direct is preferably and when the bitsPerValue is calculated, the caller would have to do the if (bitsPerValue &amp;gt; 32) bitsPerValue = 64 and so on.&lt;/p&gt;</comment>
                    <comment id="12829715" author="mikemccand" created="Thu, 4 Feb 2010 19:24:50 +0000"  >&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;Also, the maxValue at write time should not have to be known - eg the factory API should let me ask for a direct short writer without declaring the maxValue I will store.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Since Packed and Aligned needs maxValue (or bitsPerValue), this would require two distinct methods in the factory, each returning a subset of the possible implementations. I find that rather confusing.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Maybe the caller just always uses the bitsRequired method to get the&lt;br/&gt;
required bit width per value?&lt;/p&gt;

&lt;p&gt;Though, when we enable specializing storing of negative values as&lt;br/&gt;
well, that&apos;ll be a hassle...&lt;/p&gt;

&lt;p&gt;OK let&apos;s leave it as you must pass the maxValue for now.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Speaking of API additions, I find that&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; getBitsPerValue();
&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; size();
&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; void set(&lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt; value);
&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; void clear();
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;are trivial to implement for the known implementations. They open up for things like auto-growing to fit higher values by using a delegating wrapper, re-using the structure for counting purposes and sorting in-place.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think the first 2 make sense, but I&apos;d rather not pursue the 2nd two&lt;br/&gt;
at this time.  Ie, I think this API only needs write-once, and then&lt;br/&gt;
read-only.&lt;/p&gt;

&lt;p&gt;If we open up random writing (set/clear), with auto-growing, etc.,&lt;br/&gt;
that does add complexities to the impl.  EG the backing store can no&lt;br/&gt;
longer be final, we&apos;d have to do some locking (or mark the array&lt;br/&gt;
volatile) for thread safety, etc.&lt;/p&gt;

&lt;p&gt;As far as I can tell... Lucene today doesn&apos;t yet need random write to&lt;br/&gt;
the packed ints.  The terms dict index and CSF are the two needs I&lt;br/&gt;
think we have now.  Someday (when CSF supports writing) we will... but&lt;br/&gt;
not yet today?&lt;/p&gt;

&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;I don&apos;t think we need separate PRIORITY and BLOCK_PREFERENCE?  Can&apos;t we have a single enum (STORAGE?) with: packed, aligned32, aligned64? &quot;Direct&quot; is really just packed with nbits rounded up to 8,16,32,64.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I agree that it does complicate matters somewhat to have them separated. When calling getReader the BLOCK_PREFERENCE should also be removed, as the block preference will always be the same as that architecture. Removing the &quot;direct&quot; option would require the caller to do some of the logic in some cases: If low processing requirements is a priority, direct is preferably and when the bitsPerValue is calculated, the caller would have to do the if (bitsPerValue &amp;gt; 32) bitsPerValue = 64 and so on.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;(There&apos;s a bug in the patch in PackedInts.getReader, where it switches&lt;br/&gt;
the block size based on whether JRE is 64 bit: it&apos;s always choosing 64&lt;br/&gt;
bit now).&lt;/p&gt;

&lt;p&gt;The &quot;direct&quot; option only applies during writing (ie, you round up to&lt;br/&gt;
the nearest native type bit width).  At read time it&apos;s just a packed&lt;br/&gt;
8/16/32/64.&lt;/p&gt;

&lt;p&gt;Hmm... maybe we could just add an optional 2nd arg to bitsRequired, a&lt;br/&gt;
boolean eg &quot;roundUpToNative&quot; or something, which if true does that&lt;br/&gt;
rounding for you?  (And then go back to caller computes bit width and&lt;br/&gt;
passes it in?).&lt;/p&gt;</comment>
                    <comment id="12831950" author="mikemccand" created="Wed, 10 Feb 2010 11:23:05 +0000"  >&lt;p&gt;Toke, are you still working on this...?  If not, I can take a crack?  I&apos;d really like to get something online here before we land flex, so the terms dict index isn&apos;t so wasteful of RAM.&lt;/p&gt;</comment>
                    <comment id="12832012" author="toke" created="Wed, 10 Feb 2010 14:49:44 +0000"  >&lt;p&gt;Changing the code to use bitsPerValue instead of maxValue for constructors and persistent format took a bit longer than anticipated. To get things flowing, I&apos;ve attached the code as it is now. I&apos;ve moved the classes to o.a.l.util.packed and performed some clenup too. It still needs aligned32 and aligned64 implementations and more cleanup, which I&apos;ll work on for the next hour today and hopefully some hours tomorrow.&lt;/p&gt;

&lt;p&gt;One current use-case for mutable packed ints would be for StringOrdValComparator (using an auto-grow wrapper), although the gain might be small as the overhead of the Strings is so large. I understand the problem of making all packed ints mutable, but a compromise might be to have a Mutable interface and a new factory-method that returns the same implementations as Mutable instead of Reader? That way it is possible to use the implementations for things such as sorting instead of having to re-implement them. I&apos;ve left the interface for Reader clean as you suggested, but kept the implementations of set in the classes for now, as the code has already been made.&lt;/p&gt;</comment>
                    <comment id="12832820" author="toke" created="Fri, 12 Feb 2010 02:57:21 +0000"  >&lt;p&gt;I&apos;ve read through the comments on &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-1990&quot; title=&quot;Add unsigned packed int impls in oal.util&quot;&gt;&lt;del&gt;LUCENE-1990&lt;/del&gt;&lt;/a&gt; and implemented most of what has been suggested. The attached patch contains implementations for all the variants we&apos;ve talked about, including aligned. There&apos;s a known bug in persistence for aligned64 (and probably also for aligned32) that I haven&apos;t stomped yet. There&apos;s also a clear need for a more elaborate unit-test with regard to persistence.&lt;/p&gt;

&lt;p&gt;Other outstanding issues, as I see them, are whether or not mutable packed arrays should be requestable (as general purpose data structures) and how the factory for creating a writer should work. I have added a getMutable-method to the factory and not touched the return type Reader for the getReader-method. That way read-only users will not be tempted to try and update the received structure. As for the arguments to the factory, Michael McCandless suggested that the preferences should be expressed with (packed | aligned32 | aligned64 | auto). As fas as I can see, this should work. However, I&apos;ve only just reached this conclusion and haven&apos;t had the time to implement it.&lt;/p&gt;

&lt;p&gt;A speed-test has been added and the results from my machine can be seen below. In order for it to be really usable, it should be tried on other machines too.&lt;/p&gt;

&lt;p&gt;I won&apos;t touch the code before sometime next week, but I&apos;ll keep an eye on &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-1990&quot; title=&quot;Add unsigned packed int impls in oal.util&quot;&gt;&lt;del&gt;LUCENE-1990&lt;/del&gt;&lt;/a&gt; comments until then.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
        bitsPerValue          valueCount            getCount    PackedDirectByte   PackedDirectShort            Packed32     PackedAligned32     PackedDirectInt            Packed64     PackedAligned64    PackedDirectLong
                   1                1000            10000000                 167                 141                 258                 242                 172                 264                 242                 183
                   1             1000000            10000000                 224                 232                 266                 233                 246                 262                 238                 338
                   1            10000000            10000000                 359                 469                 280                 278                 508                 278                 272                 551
                   3                1000            10000000                 168                 166                 265                 241                 163                 262                 243                 166
                   3             1000000            10000000                 227                 226                 261                 251                 239                 274                 249                 330
                   3            10000000            10000000                 406                 476                 301                 304                 522                 300                 308                 547
                   4                1000            10000000                 167                 168                 266                 239                 164                 285                 239                 169
                   4             1000000            10000000                 228                 231                 294                 274                 262                 291                 269                 314
                   4            10000000            10000000                 385                 480                 308                 333                 514                 331                 315                 557
                   7                1000            10000000                 172                 174                 278                 248                 162                 271                 238                 177
                   7             1000000            10000000                 224                 236                 289                 281                 272                 278                 277                 345
                   7            10000000            10000000                 405                 473                 389                 447                 516                 399                 402                 553
                   8                1000            10000000                 192                 171                 268                 242                 174                 291                 240                 163
                   8             1000000            10000000                 226                 232                 291                 284                 286                 274                 265                 314
                   8            10000000            10000000                 381                 467                 406                 428                 512                 422                 419                 580

        bitsPerValue          valueCount            getCount   PackedDirectShort            Packed32     PackedAligned32     PackedDirectInt            Packed64     PackedAligned64    PackedDirectLong
                   9                1000            10000000                 166                 274                 241                 170                 261                 237                 163
                   9             1000000            10000000                 229                 299                 273                 250                 284                 275                 327
                   9            10000000            10000000                 483                 443                 477                 519                 438                 455                 568
                  15                1000            10000000                 170                 265                 239                 174                 264                 235                 162
                  15             1000000            10000000                 232                 285                 274                 240                 278                 269                 339
                  15            10000000            10000000                 473                 518                 524                 523                 519                 521                 550
                  16                1000            10000000                 166                 263                 236                 172                 264                 235                 160
                  16             1000000            10000000                 229                 285                 278                 244                 293                 272                 332
                  16            10000000            10000000                 470                 513                 517                 509                 534                 529                 548

        bitsPerValue          valueCount            getCount            Packed32     PackedAligned32     PackedDirectInt            Packed64     PackedAligned64    PackedDirectLong
                  17                1000            10000000                 262                 255                 177                 260                 234                 160
                  17             1000000            10000000                 290                 306                 273                 304                 290                 320
                  17            10000000            10000000                 532                 572                 533                 529                 556                 551
                  28                1000            10000000                 269                 256                 187                 267                 238                 163
                  28             1000000            10000000                 293                 295                 253                 293                 296                 312
                  28            10000000            10000000                 542                 567                 501                 548                 567                 542
                  31                1000            10000000                 260                 235                 177                 266                 232                 158
                  31             1000000            10000000                 292                 294                 244                 296                 297                 328
                  31            10000000            10000000                 552                 563                 516                 562                 568                 548

        bitsPerValue          valueCount            getCount     PackedDirectInt            Packed64     PackedAligned64    PackedDirectLong
                  32                1000            10000000                 172                 263                 241                 166
                  32             1000000            10000000                 241                 291                 297                 320
                  32            10000000            10000000                 519                 556                 573                 546

        bitsPerValue          valueCount            getCount            Packed64     PackedAligned64    PackedDirectLong
                  33                1000            10000000                 264                 239                 159
                  33             1000000            10000000                 293                 374                 319
                  33            10000000            10000000                 559                 595                 552
                  47                1000            10000000                 264                 242                 164
                  47             1000000            10000000                 319                 369                 322
                  47            10000000            10000000                 577                 601                 548
                  49                1000            10000000                 261                 243                 162
                  49             1000000            10000000                 323                 413                 319
                  49            10000000            10000000                 584                 610                 551
                  63                1000            10000000                 269                 235                 161
                  63             1000000            10000000                 396                 369                 313
                  63            10000000            10000000                 592                 596                 559
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;(Java 1.6.0_15-b03, default settings on a Dell Precision M6500: Intel i7 Q 820 @ 1.73GHz, 8 MB level 2 cache,  dual-channel PC 1333 RAM, running Ubuntu Karmic)&lt;/p&gt;</comment>
                    <comment id="12832971" author="mikemccand" created="Fri, 12 Feb 2010 12:10:44 +0000"  >&lt;p&gt;Great progress Toke!&lt;/p&gt;

&lt;p&gt;I guess we should do Mutable since you&apos;re so far along already &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;But, now that we have getMutable, can we make the concrete impls&lt;br/&gt;
package private?  Javadocs for Mutable.set should note that the size&lt;br/&gt;
is fixed once you allocate it.  We have no way to save a&lt;br/&gt;
Mutable... should we add that?  If so, we may want to rename Writer -&amp;gt;&lt;br/&gt;
WriteOnceWriter.  This way consumers can also get a Mutable, do random&lt;br/&gt;
writes, then save, if the &quot;write once&quot; model isn&apos;t a good fit.&lt;/p&gt;

&lt;p&gt;Maybe we should just merge Mutable &amp;amp; Reader, then?  (LongStore?&lt;br/&gt;
LongArray?  PackedLongs?)&lt;/p&gt;

&lt;p&gt;We should state clearly that these are all unsigned ints storage.&lt;/p&gt;

&lt;p&gt;Maybe rename PackedDirectInt to PackedDirect32 (and Short to 16,&lt;br/&gt;
Byte to 8).  Because... while it is using a direct int[] under the hood,&lt;br/&gt;
it&apos;s really using all 32 bits for the full positive int range.  So&lt;br/&gt;
PackedDirect32 can be used even for pos ints that would overflow a&lt;br/&gt;
normal java &quot;int&quot;.  (Though, for long we obviously can&apos;t use that 64th&lt;br/&gt;
bit for positive ints...).&lt;/p&gt;

&lt;p&gt;The @see in the new IndexInput.readShort is wrong (referencing&lt;br/&gt;
writeInt).&lt;/p&gt;

&lt;p&gt;Can you add @lucene.internal to the javadocs?&lt;/p&gt;

&lt;p&gt;Seems like once we stomp the bugs, beef up the tests, and merge&lt;br/&gt;
PRIORITY and BLOCK_PREFERENCE (into maybe STORAGE?) for&lt;br/&gt;
the public API, we are nearly done?  Thanks Toke!&lt;/p&gt;</comment>
                    <comment id="12836267" author="toke" created="Sat, 20 Feb 2010 21:52:20 +0000"  >&lt;p&gt;I am sorry, but personal issues sapped my time and energy this week, so Lucene got bumped down my priority-list. I am going to code4lib next week and I&apos;ll try and get some hacking done in the plane from Denmark to USA, but that depends on whether or not there is a power socket near my seat. If I don&apos;t upload a patch late monday, it will be early march before I&apos;ll get it done&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;But, now that we have getMutable, can we make the concrete impls&lt;br/&gt;
package private? Javadocs for Mutable.set should note that the size&lt;br/&gt;
is fixed once you allocate it.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Agreed on both.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;We have no way to save a Mutable... should we add that?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I dont know enough about persistence in Lucene to make that call. Since the writer is tied to Lucene, it would not work for general purposes, so making a writer for Mutables only seems to make sense if the user uses it to build index-structures?&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Maybe we should just merge Mutable &amp;amp; Reader, then? (LongStore?&lt;br/&gt;
LongArray? PackedLongs?)&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I don&apos;t understand that one? You made a compelling argument for returning immutables to readers earlier (problems with concurrency and having all back ends support writes).&lt;/p&gt;

&lt;p&gt;As for the name... I don&apos;t know. None of the sound right, but I have no other suggestion.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;We should state clearly that these are all unsigned ints storage.&lt;/p&gt;

&lt;p&gt;Maybe rename PackedDirectInt to PackedDirect32 (and Short to 16,&lt;br/&gt;
Byte to 8). Because... while it is using a direct int[] under the hood,&lt;br/&gt;
it&apos;s really using all 32 bits for the full positive int range.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Good point. The rest of your suggestions are also very valid.&lt;/p&gt;</comment>
                    <comment id="12837280" author="toke" created="Tue, 23 Feb 2010 15:33:30 +0000"  >&lt;p&gt;I&apos;ve renamed most of the classes to short form, as the &quot;Packed&quot;-prefix did was not that descriptive and fixed some bugs. Still pending is the mutable writer and a bug in persistence for aligned64. Good news (for Lucene at least) is that an airplane blocking snowdrift means that I have time this week for continued hacking.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;But, now that we have getMutable, can we make the concrete impls&lt;br/&gt;
package private? Javadocs for Mutable.set should note that the size&lt;br/&gt;
is fixed once you allocate it.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The implementations are now package private, but I only put the note about fixed size on the getMutable-method. There&apos;s nothing wrong with creating a custom auto growing Mutable.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;We should state clearly that these are all unsigned ints storage.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Done.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Maybe rename PackedDirectInt to PackedDirect32 (and Short to 16,&lt;br/&gt;
Byte to 8).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Done (Direct8, Direct16, Direct32 and Direct64).&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;The @see in the new IndexInput.readShort is wrong (referencing&lt;br/&gt;
writeInt).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Fixed.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Can you add @lucene.internal to the javadocs?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Should this also be applied to package private classes? Marking those as internal seems redundant.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Seems like once we stomp the bugs, beef up the tests, and merge&lt;br/&gt;
PRIORITY and BLOCK_PREFERENCE (into maybe STORAGE?) for&lt;br/&gt;
the public API, we are nearly done?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I&apos;ve removed BLOCK_PREFERENCE from the API. It&apos;s still used internally, mainly to do controlled testing. Tests are beefed up (and currently fails for aligned, so clearly beefing worked).&lt;/p&gt;</comment>
                    <comment id="12838873" author="toke" created="Fri, 26 Feb 2010 14:03:54 +0000"  >&lt;p&gt;Now we&apos;re getting somewhere. I finally squashed the persistence bug and the tests has been turned up another notch. Everything seems to run as it should. Pending issues, as I see them:&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Review of the code&lt;/li&gt;
	&lt;li&gt;Should we make a MutableWriter?&lt;/li&gt;
	&lt;li&gt;Should we drop support for aligned?&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;The last one is interesting. The code for getting a value from aligned uses devision and a single RAM-request:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt; get(&lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; index) {
    &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; blockPos = index / valuesPerBlock;
    &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; bitPos = (index - (blockPos * valuesPerBlock)) * bitsPerValue;
    &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; (blocks[blockPos] &amp;gt;&amp;gt;&amp;gt; shifts[bitPos]) &amp;amp; readMask;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;where the code for packed uses shift and two RAM-requests:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
    &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt; majorBitPos = index * bitsPerValue;
    &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; elementPos = (&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt;)(majorBitPos &amp;gt;&amp;gt;&amp;gt; BLOCK_BITS); &lt;span class=&quot;code-comment&quot;&gt;// / BLOCK_SIZE
&lt;/span&gt;    &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; bitPos =     (&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt;)(majorBitPos &amp;amp; MOD_MASK); &lt;span class=&quot;code-comment&quot;&gt;// % BLOCK_SIZE);
&lt;/span&gt;
    &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; base = bitPos * FAC_BITPOS;

    &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; ((blocks[elementPos] &amp;lt;&amp;lt; shifts[base]) &amp;gt;&amp;gt;&amp;gt; shifts[base+1]) |
            ((blocks[elementPos+1] &amp;gt;&amp;gt;&amp;gt; shifts[base+2]) &amp;amp; readMasks[bitPos]);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I have done some tests (see the TODO-file in the attached patch) and on 64 bit machines, the difference in access-speed for aligned vs. packed is not that great and not always in favor of aligned. Probably because some space is wasted and the RAM-cache is not so well utilized. If this is also the case for 32 bit machines, I vote for removing aligned and only used packed with the special-case optimizations direct8, direct16, direct32 and direct64. This would also mean that there is only one persistent format.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
java -cp lucene-core-3.1-dev.jar org.apache.lucene.util.packed.PackedIntsPerformance
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Runs throught the performance tests and delivers a simple report, so it should be very easy to test on different platforms. It only measures access speed.&lt;/p&gt;

&lt;p&gt;I consider this patch ready for review and concentrate on other matters until I hear more.&lt;/p&gt;</comment>
                    <comment id="12838908" author="toke" created="Fri, 26 Feb 2010 15:49:46 +0000"  >&lt;p&gt;I couldn&apos;t help making a tiny tweak to the performance test so that it outputs execution time means for the different implementations. I have attached measurements from 5 different 64 bit machines. Looking at the means, I observe the following:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;i7 Q820 and Xeon L5420: Practically no difference between aligned and packed with a small edge to aligned&lt;/li&gt;
	&lt;li&gt;Core 2 and Xeon 5148: Aligned is consistently about 10% slower than packed&lt;/li&gt;
	&lt;li&gt;Xeon MP (old with just 1 MB CPU cache): Aligned ranges from 0-10% slower than packed, depending on bits/value&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;The direct implementations outperforms packed and aligned for all sane cases (using direct8 to hold only 1 bit/value is clearly a bad idea). No surprise there.&lt;/p&gt;

&lt;p&gt;Caveat: The tests were run without any other significantly resource heavy processes disturbing it. This means that there were no fighting for the CPU cache.&lt;/p&gt;

&lt;p&gt;Major caveat: Tests are needed on other processors than 64 bit Intel.&lt;/p&gt;

&lt;p&gt;I would be great if someone could figure out how to make an aligned getter without using division as that is surely the thing that hampers aligned performance.&lt;/p&gt;</comment>
                    <comment id="12839007" author="mikemccand" created="Fri, 26 Feb 2010 20:04:34 +0000"  >&lt;p&gt;Great progress!  I think this is very close.&lt;/p&gt;

&lt;p&gt;Airplane blocking snow drifts!?  Where on earth are you anyway?&lt;/p&gt;

&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;Can you add @lucene.internal to the javadocs?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Should this also be applied to package private classes? Marking those as internal seems redundant.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yeah I agree package private APIs don&apos;t need the @lucene.internal...&lt;/p&gt;

&lt;p&gt;It&apos;s very interesting that align is never a win &amp;#8211; I think in that&lt;br/&gt;
case removing it makes sense?  It&apos;ll be a nice simplification.&lt;/p&gt;

&lt;p&gt;I think we don&apos;t need to make a MutableWriter, at least before&lt;br/&gt;
committing?  Nobody needs it now... (I think?).&lt;/p&gt;

&lt;p&gt;Other small things:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;Can you use @lucene.internal instead of the NOTE that I had put on&lt;br/&gt;
    the classes?&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;We lost &quot;final&quot; in the RamUsageEstimator constants&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Did we ever test performance of the specialized (generated)&lt;br/&gt;
    decoders using switch statements?&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="12839062" author="toke" created="Fri, 26 Feb 2010 22:55:25 +0000"  >&lt;blockquote&gt;
&lt;p&gt;Airplane blocking snow drifts!?  Where on earth are you anyway?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;In Denmark. The guy responsible for clearing the runway did indeed clear the runway. He just forgot that the plane needs to taxi into the runway in the first place. That made us miss our connecting flight.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;It&apos;s very interesting that align is never a win &amp;#8211; I think in that case removing it makes sense?  It&apos;ll be a nice simplification.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Well, practically never wins for the machines I tested on and never wins with my implementation.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Can you use @lucene.internal instead of the NOTE that I had put on the classes?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Done... I think. I&apos;m not very good at this part, so if someone else wants to do some cleanup i JavaDoc and such, they are very welcome by me.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;We lost &quot;final&quot; in the RamUsageEstimator constants&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Strange. Oh well, fixed.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Did we ever test performance of the specialized (generated) decoders using switch statements?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I just did a quick hack in order to measure performance  and I was very surprised that the generated switch-based implementations performs so well. It&apos;s nearly on par with packed most of the time and exceeds it in some cases. I only tested on 3 machines though. The hack is in the &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-1990&quot; title=&quot;Add unsigned packed int impls in oal.util&quot;&gt;&lt;del&gt;LUCENE-1990&lt;/del&gt;&lt;/a&gt;-te20100226c.patch and is called when the performance test is executed.&lt;/p&gt;

&lt;p&gt;Attachment generated_performance-te20100226.txt contains measurements where the py-generated code is tested together with the other implementations.&lt;/p&gt;

&lt;p&gt;Note to self: Switch is not equivalent to a series of if-else, when we&apos;re talking performance and when we switch without omissions in the cases.&lt;/p&gt;</comment>
                    <comment id="12839230" author="mikemccand" created="Sat, 27 Feb 2010 11:13:42 +0000"  >&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;Airplane blocking snow drifts!? Where on earth are you anyway?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;In Denmark. The guy responsible for clearing the runway did indeed clear the runway. He just forgot that the plane needs to taxi into the runway in the first place. That made us miss our connecting flight.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Good grief!&lt;/p&gt;

&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;It&apos;s very interesting that align is never a win - I think in that case removing it makes sense? It&apos;ll be a nice simplification.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Well, practically never wins for the machines I tested on and never wins with my implementation.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think we should remove it...&lt;/p&gt;

&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;Did we ever test performance of the specialized (generated) decoders using switch statements?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I just did a quick hack in order to measure performance and I was very surprised that the generated switch-based implementations performs so well. It&apos;s nearly on par with packed most of the time and exceeds it in some cases. I only tested on 3 machines though. The hack is in the &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-1990&quot; title=&quot;Add unsigned packed int impls in oal.util&quot;&gt;&lt;del&gt;LUCENE-1990&lt;/del&gt;&lt;/a&gt;-te20100226c.patch and is called when the performance test is executed.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Thanks for testing this!  It is interesting.&lt;/p&gt;

&lt;p&gt;I ran the perf test on a CentOS 5.4 machine, java&lt;br/&gt;
1.6.0_17-b04 64 bit server, Intel core 2 duo E8400 (3 ghz) &amp;#8211; attached&lt;br/&gt;
perf-mkm-20100227.txt.  I also show the switch impl close, though&lt;br/&gt;
always a bit behind.&lt;/p&gt;

&lt;p&gt;Seems like we should just stick with the non-gen&apos;d packed impl?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Note to self: Switch is not equivalent to a series of if-else, when we&apos;re talking performance and when we switch without omissions in the cases.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Right, if the switch cases are compact, it should compile into a fast jump&lt;br/&gt;
table (though it may still do an unecessary bounds check).&lt;/p&gt;

&lt;p&gt;I think, once we removed aligned, this is ready to commit?  I think we&lt;br/&gt;
should land this on flex branch?  (It&apos;s using CodecUtil, BytesRef &amp;#8211;&lt;br/&gt;
I&apos;ll merge them when I commit).  Then I can cutover the terms index to&lt;br/&gt;
use packed ints.&lt;/p&gt;</comment>
                    <comment id="12839684" author="toke" created="Mon, 1 Mar 2010 14:11:43 +0000"  >&lt;p&gt;I&apos;ve tested on two 32 bit Windows machines: An Intel T2400 (32 bit only) running XP and an Athlon X2 4850e (64 bit capable) running 32 bit XP. The result can be seen in attachment performance-20100301.txt. Something curious happens with high (32+) bits/value for the T2400 as aligned overtakes packed. However, the overall picture is still that aligned only wins for a few special cases, so now I&apos;ll be happy to remove it from the patch. As a note, generated is also slower than packed on the AMD processor, although not as much as for Intel.&lt;/p&gt;

&lt;p&gt;I have removed all traces of aligned from PackedInts, but kept the classes in the patch, in the case that someone finds a faster way to handle aligned. PackedIntsPerformance still includes both the generated switch-implementation and Aligned32 and Aligned64. It should be possible to apply the patch without Aligned32, Aligned64, AlignedWriter and PackedIntsPerformance.&lt;/p&gt;</comment>
                    <comment id="12839852" author="toke" created="Mon, 1 Mar 2010 20:56:57 +0000"  >&lt;p&gt;Some thoughts on avoiding the generic division by experimenting with reciprocal multiplication: For aligned, the sane number of values/block are &lt;span class=&quot;error&quot;&gt;&amp;#91;3, 5, 6, 7, 8, 9, 10, 16, 21, 32, 64&amp;#93;&lt;/span&gt;. I tried testing index from 0 to Integer.MAX_VALUE with these divisors and reciprocal multiplication. It worked perfectly for all divisors except &lt;span class=&quot;error&quot;&gt;&amp;#91;5, 7, 9, 10, 21&amp;#93;&lt;/span&gt;. Unfortunately it already falls for divisor 21 at index 252645140, which makes it useless as a full replacement. If one were so inclined, it would be possible to select aligned implementation based on valueCount, with fallback to the &quot;slow&quot; version. The gain of using fast division seems quite substantial as it makes aligned 14-40% faster than packed (note: Just tested on a single machine). However, re-introducing aligned with four different implementations (Aligned32, Aligned32Fast, Aligned64, Aligned64Fast) is rather daunting and it would make the selection code really messy.&lt;/p&gt;

&lt;p&gt;I can see that there are well-known tricks to get around the rounding errors. Some are described at &lt;a href=&quot;http://www.cs.uiowa.edu/~jones/bcd/divide.html#fixed&quot; class=&quot;external-link&quot;&gt;http://www.cs.uiowa.edu/~jones/bcd/divide.html#fixed&lt;/a&gt; . I don&apos;t know if these extra tricks would negate the 14-40% speed gain though. Since I would like to get the patch out of the door, I vote for keeping aligned disabled and just note that more bit fiddling might make it attractive at some point.&lt;/p&gt;</comment>
                    <comment id="12842412" author="mikemccand" created="Sun, 7 Mar 2010 11:51:24 +0000"  >&lt;p&gt;Patch looks great Toke &amp;#8211; a few small things:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;I think we shouldn&apos;t add Aligned*.java to svn?  It&apos;ll just add&lt;br/&gt;
    unused bits to the JAR, and, we can always fallback to this issue&lt;br/&gt;
    to pull them in at a future time?&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Can you resolve the remaining nocommits?  EG (since we are&lt;br/&gt;
    unsigned) we can&apos;t get the 64 bit case working.  I don&apos;t think we&lt;br/&gt;
    should rename to UnsignedXXX, nor, support minValue at this&lt;br/&gt;
    point, and remove the ComparableBytesRef, and I&apos;ll merge BytesRef&lt;br/&gt;
    into flex&apos;s when I commit.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I can take these too &amp;#8211; I think it&apos;s ready to commit on flex after&lt;br/&gt;
this.  Thanks!&lt;/p&gt;</comment>
                    <comment id="12842414" author="thetaphi" created="Sun, 7 Mar 2010 11:59:05 +0000"  >&lt;p&gt;We should also add the @lucene.internal javadoc comments everywhere instead of the big NOTE. Why has one class a full-uppercase class name?&lt;/p&gt;</comment>
                    <comment id="12843027" author="toke" created="Tue, 9 Mar 2010 12:33:20 +0000"  >&lt;p&gt;Michael McCandless:&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
	&lt;li&gt;I think we shouldn&apos;t add Aligned*.java to svn? It&apos;ll just add&lt;br/&gt;
      unused bits to the JAR, and, we can always fallback to this issue&lt;br/&gt;
      to pull them in at a future time?&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;I agree. At the current state, Aligned is just dead weight.&lt;/p&gt;

&lt;p&gt;This also means that the performance tester won&apos;t be part of the commit though. I can quickly make a performance tester that does not use aligned, if it is preferable to keep performance testing.&lt;/p&gt;

&lt;blockquote&gt;
&lt;ul&gt;
	&lt;li&gt;Can you resolve the remaining nocommits? EG (since we are&lt;br/&gt;
      unsigned) we can&apos;t get the 64 bit case working. I don&apos;t think we&lt;br/&gt;
      should rename to UnsignedXXX, nor, support minValue at this&lt;br/&gt;
      point, and remove the ComparableBytesRef, and I&apos;ll merge BytesRef&lt;br/&gt;
      into flex&apos;s when I commit.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I can take these too - I think it&apos;s ready to commit on flex after this&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;It will help a lot if you take care of these issues, thanks.&lt;/p&gt;</comment>
                    <comment id="12843028" author="toke" created="Tue, 9 Mar 2010 12:34:52 +0000"  >&lt;p&gt;Uwe Schindler:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;We should also add the @lucene.internal javadoc comments everywhere instead of the big NOTE. Why has one class a full-uppercase class name? &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Are you looking at patch &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-1990&quot; title=&quot;Add unsigned packed int impls in oal.util&quot;&gt;&lt;del&gt;LUCENE-1990&lt;/del&gt;&lt;/a&gt;-te20100301.patch? I don&apos;t see any NOTE and no full-uppercase class name?&lt;/p&gt;</comment>
                    <comment id="12843215" author="mikemccand" created="Tue, 9 Mar 2010 18:19:06 +0000"  >&lt;blockquote&gt;&lt;p&gt;It will help a lot if you take care of these issues, thanks.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;OK will do.  I&apos;ll commit soon to flex... thanks Toke!&lt;/p&gt;</comment>
                    <comment id="12843218" author="mikemccand" created="Tue, 9 Mar 2010 18:23:35 +0000"  >&lt;blockquote&gt;&lt;p&gt;We should also add the @lucene.internal javadoc comments everywhere instead of the big NOTE.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I found one more NOTE in CodecUtil &amp;#8211; I&apos;ll fix before committing.&lt;/p&gt;</comment>
                    <comment id="12843220" author="mikemccand" created="Tue, 9 Mar 2010 18:26:12 +0000"  >&lt;blockquote&gt;&lt;p&gt;Why has one class a full-uppercase class name?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Uwe you mean eg STORAGE?  (And also BLOCK, IMPLEMENTATION... but they are package private).  These are enums &amp;#8211; seems OK to make them all caps?&lt;/p&gt;

&lt;p&gt;Though I do think we can simplify some of this now that we&apos;re removing the aligned case... eg PERSISTENCE is an enum with only one value.  I&apos;ll take a stab &amp;amp; post patch.&lt;/p&gt;</comment>
                    <comment id="12843325" author="mikemccand" created="Tue, 9 Mar 2010 22:14:17 +0000"  >&lt;p&gt;OK new patch attached:&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Ported to flex, and cutover to CodecUtil.  BytesRef required no&lt;br/&gt;
    changes...&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Simplified the API/impl to not use STORAGE, PERSISTENCE,&lt;br/&gt;
    IMPLEMENTATION, etc.  You just specify required bitsPerValue.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Removed Aligned*, and ConsumesRAM interface&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Fixed the nocommits.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I think it&apos;s ready!  I&apos;ll wait a day or two...&lt;/p&gt;</comment>
                    <comment id="12844914" author="mikemccand" created="Sat, 13 Mar 2010 16:34:52 +0000"  >&lt;p&gt;Thanks Toke!&lt;/p&gt;</comment>
                    <comment id="12845082" author="toke" created="Sun, 14 Mar 2010 13:56:18 +0000"  >&lt;p&gt;Thanks for rounding off, Michael. It&apos;s been a pleasure.&lt;/p&gt;</comment>
                    <comment id="12850550" author="rcmuir" created="Sat, 27 Mar 2010 17:04:45 +0000"  >&lt;p&gt;By the way Toke, we have lately been benchmarking automaton queries, which are pretty intensive on the terms dictionary.&lt;br/&gt;
I think we were expecting some acceptable slowdown once we switched to packed ints, but according to my benchmarks this is not the case.&lt;/p&gt;

&lt;p&gt;I think its pretty impressive to see no measurable performance impact on this stuff at all, great work.&lt;/p&gt;</comment>
                    <comment id="12851806" author="toke" created="Wed, 31 Mar 2010 11:28:39 +0100"  >&lt;p&gt;I am very happy to hear that, Robert. The benchmarks I made had the glaring flaw that they were ... well, benchmarks. With the CPU-cache being hammered in a real world scenario, your findings indicate that the slow round-trip to main memory dwarfs the extra logic for extracting the values from the packed structure. For a few scenarios, it might even be faster than plain arrays.&lt;/p&gt;

&lt;p&gt;Getting back to reality, my own findings indicates that using PackedInts for ord-based sorted search is not at all faster than plain arrays. The access pattern here is very sequential, so the chance that the needed value is already fetched from main memory is high for both plain and packed structures.&lt;/p&gt;</comment>
                    <comment id="12851809" author="mikemccand" created="Wed, 31 Mar 2010 11:39:04 +0100"  >&lt;p&gt;Toke, are sort ords that much slower than straight arrays for sorting?&lt;/p&gt;

&lt;p&gt;After flex lands I&apos;d really like to make a variant of FieldCache.StringIndex that uses BytesRef for the values and packed ints for the ords.... should save alot of memory in many cases (English text saves since utf8 is 1 byte per char; enumerated fields (eg country name) should save tons by using only a few bits instead of 32 we now always use) when sorting by string.&lt;/p&gt;</comment>
                    <comment id="12852074" author="toke" created="Wed, 31 Mar 2010 20:51:08 +0100"  >&lt;p&gt;In the original proof of concept for &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2335&quot; title=&quot;optimization: when sorting by field, if index has one segment and field values are not needed, do not load String[] into field cache&quot;&gt;LUCENE-2335&lt;/a&gt;, I measured the time for extracting top-20 for ... 10 million? documents and got something like 600ms when using PackedInts, which is fairly slow in my book and recall getting better performance with straight arrays for that. This is all wery fuzzy though and I&apos;d love to be proven wrong. If PackedInts are faster for sorting too, it&apos;s getting very hard to see the downside of that representation.&lt;/p&gt;

&lt;p&gt;Since &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2335&quot; title=&quot;optimization: when sorting by field, if index has one segment and field values are not needed, do not load String[] into field cache&quot;&gt;LUCENE-2335&lt;/a&gt; relies heavily on arays of sorted indexes into ordinal arrays, I&apos;ll make sure to performance test both PackedInts and straight arrays for sorted search.&lt;/p&gt;</comment>
                    <comment id="12852557" author="toke" created="Thu, 1 Apr 2010 22:24:39 +0100"  >&lt;p&gt;It seem like my unit-testing of PackedInts.Mutable wasn&apos;t good enough. There is a bug in Packed64 (and probably in Packed32 too) when using the set-method. In certain cases the secondary block is changed when it should be left alone. A simple unit-test is&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
    PackedInts.Mutable mutable = PackedInts.getMutable(26, 5);
    mutable.set(24, 31);
    mutable.set(4, 16);
    assertEquals(&lt;span class=&quot;code-quote&quot;&gt;&quot;The value #24 should remain unchanged&quot;&lt;/span&gt;, 31, mutable.get(24));
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The PackedWriter uses a different algorithm for generating the bit stream and is unaffected by this bug.&lt;/p&gt;

&lt;p&gt;I expect the write-masks for the set-method to be at fault and I am working on a fix. ETA: Within an hour or sometime during the weekend, depending on difficulty.&lt;/p&gt;</comment>
                    <comment id="12852562" author="mikemccand" created="Thu, 1 Apr 2010 22:38:20 +0100"  >&lt;p&gt;Good catch Toke!  Flex actually uses Mutable on loading the terms index when indexDivisor is not 1...&lt;/p&gt;</comment>
                    <comment id="12852563" author="mikemccand" created="Thu, 1 Apr 2010 22:38:39 +0100"  >&lt;p&gt;Reopen to fix issue Toke found with Mutable&lt;/p&gt;</comment>
                    <comment id="12852568" author="toke" created="Thu, 1 Apr 2010 22:51:46 +0100"  >&lt;p&gt;I&apos;ve located the bug and fixed it. As expected, it was in the write-masks. Unfortunately I&apos;m running out of time, so I cannot make a patch right now. The code for Packed64 is&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
  &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt;[][] WRITE_MASKS =
          &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt;[ENTRY_SIZE][ENTRY_SIZE * FAC_BITPOS];
  &lt;span class=&quot;code-keyword&quot;&gt;static&lt;/span&gt; {
    &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; (&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; elementBits = 1 ; elementBits &amp;lt;= BLOCK_SIZE ; elementBits++) {
        &lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt; elementPosMask = ~(~0L &amp;lt;&amp;lt; elementBits);
        &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt;[] currentShifts = SHIFTS[elementBits];
        &lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt;[] currentMasks = WRITE_MASKS[elementBits];
        &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; (&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; bitPos = 0 ; bitPos &amp;lt; BLOCK_SIZE ; bitPos++) {
            &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; base = bitPos * FAC_BITPOS;
            currentMasks[base  ] =~((elementPosMask
                               &amp;lt;&amp;lt; currentShifts[base + 1])
                              &amp;gt;&amp;gt;&amp;gt; currentShifts[base]);
            currentMasks[base+1] =
                ~(elementPosMask &amp;lt;&amp;lt; currentShifts[base + 2]);
            currentMasks[base+2] = currentShifts[base + 2] == 0 ? 0 : ~0;
          &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (bitPos &amp;lt;= BLOCK_SIZE - elementBits) { &lt;span class=&quot;code-comment&quot;&gt;// Second block not used
&lt;/span&gt;            currentMasks[base+1] = ~0; &lt;span class=&quot;code-comment&quot;&gt;// Keep all bits
&lt;/span&gt;            currentMasks[base+2] = 0;  &lt;span class=&quot;code-comment&quot;&gt;// Or with 0
&lt;/span&gt;          }
        }
    }
  }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The changed code is the addition of the last check for second block usage. Likewise the fix for Packed32 is&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
  &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt;[][] WRITE_MASKS =
          &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt;[ENTRY_SIZE][ENTRY_SIZE * FAC_BITPOS];
  &lt;span class=&quot;code-keyword&quot;&gt;static&lt;/span&gt; {
    &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; (&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; elementBits = 1 ; elementBits &amp;lt;= BLOCK_SIZE ; elementBits++) {
      &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; elementPosMask = ~(~0 &amp;lt;&amp;lt; elementBits);
      &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt;[] currentShifts = SHIFTS[elementBits];
      &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt;[] currentMasks = WRITE_MASKS[elementBits];
      &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; (&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; bitPos = 0 ; bitPos &amp;lt; BLOCK_SIZE ; bitPos++) {
        &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; base = bitPos * FAC_BITPOS;
        currentMasks[base  ] =~((elementPosMask
                &amp;lt;&amp;lt; currentShifts[base + 1])
                &amp;gt;&amp;gt;&amp;gt; currentShifts[base]);
        currentMasks[base+1] = ~(elementPosMask
                &amp;lt;&amp;lt; currentShifts[base + 2]);
        currentMasks[base+2] = currentShifts[base + 2] == 0 ? 0 : ~0;
        &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (bitPos &amp;lt;= BLOCK_SIZE - elementBits) { &lt;span class=&quot;code-comment&quot;&gt;// Second block not used
&lt;/span&gt;          currentMasks[base+1] = ~0; &lt;span class=&quot;code-comment&quot;&gt;// Keep all bits
&lt;/span&gt;          currentMasks[base+2] = 0;  &lt;span class=&quot;code-comment&quot;&gt;// Or with 0
&lt;/span&gt;        }
      }
    }
  }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Without checking thoroughly, I&apos;d expect the two pieces of code to be exactly the same, at the difference between Packed32 and Packed64 is just long vs. int and some constants. The unit-test from above can be used for Packed32 by explicitly creating a Packed32 instead of calling the factory.&lt;/p&gt;

&lt;p&gt;I&apos;ll be back behind the screen in a few days where I can make a patch, but you are more than welcome to roll the patch if it is more convenient to get it immediately.&lt;/p&gt;</comment>
                    <comment id="12852571" author="mikemccand" created="Thu, 1 Apr 2010 23:06:17 +0100"  >&lt;p&gt;I turned it into a patch (attached).&lt;/p&gt;

&lt;p&gt;But: without the fix, when I run the test, I don&apos;t see it failing (on current flex branch).  I&apos;ve fixed a few issues with packed ints on flex, but I don&apos;t think they would&apos;ve fixed this.  Confused....&lt;/p&gt;</comment>
                    <comment id="12854151" author="toke" created="Tue, 6 Apr 2010 21:08:48 +0100"  >&lt;p&gt;I did a checkout with&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
svn co https:&lt;span class=&quot;code-comment&quot;&gt;//svn.apache.org/repos/asf/lucene/dev/trunk/lucene lucene-flex&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;and added the following method to TestPackedInts&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; void testSecondaryBlockChange() &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; IOException {
    PackedInts.Mutable mutable = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; Packed64(26, 5);
    mutable.set(24, 31);
    assertEquals(&lt;span class=&quot;code-quote&quot;&gt;&quot;The value #24 should be correct&quot;&lt;/span&gt;, 31, mutable.get(24));
    mutable.set(4, 16);
    assertEquals(&lt;span class=&quot;code-quote&quot;&gt;&quot;The value #24 should remain unchanged&quot;&lt;/span&gt;, 31, mutable.get(24));
  }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;after which I ran&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
ant test-core
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;which gave me&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
    [junit] Testsuite: org.apache.lucene.util.packed.TestPackedInts
    [junit] Testcase: testSecondaryBlockChange(org.apache.lucene.util.packed.TestPackedInts):	FAILED
    [junit] The value #24 should remain unchanged expected:&amp;lt;31&amp;gt; but was:&amp;lt;28&amp;gt;
    [junit] junit.framework.AssertionFailedError: The value #24 should remain unchanged expected:&amp;lt;31&amp;gt; but was:&amp;lt;28&amp;gt;
    [junit] 	at org.apache.lucene.util.packed.TestPackedInts.testSecondaryBlockChange(TestPackedInts.java:106)
    [junit] 	at org.apache.lucene.util.LuceneTestCase.runBare(LuceneTestCase.java:276)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;then I added&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
              &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (bitPos &amp;lt;= BLOCK_SIZE - elementBits) { &lt;span class=&quot;code-comment&quot;&gt;// Second block not used
&lt;/span&gt;                currentMasks[base+1] = ~0; &lt;span class=&quot;code-comment&quot;&gt;// Keep all bits
&lt;/span&gt;                currentMasks[base+2] = 0;  &lt;span class=&quot;code-comment&quot;&gt;// Or with 0
&lt;/span&gt;              }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;to the relevant parts of Packed32 and Packed, as described above and ran&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
ant test-core
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;again, which gave me&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
    [junit] Testsuite: org.apache.lucene.util.packed.TestPackedInts
    [junit] Tests run: 7, Failures: 0, Errors: 0, Time elapsed: 5.463 sec
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;My initial unit-test contained an error, which I corrected after a minute or two (as far as I remember). Maybe you used the first version?&lt;/p&gt;

&lt;p&gt;It seems that the bug is indeed in trunk. It corrupts the value of the block after the current block in certain cases: Sequential assignment of values works fine, but out-of-order assignments corrupts the array.&lt;/p&gt;</comment>
                    <comment id="12854174" author="mikemccand" created="Tue, 6 Apr 2010 21:46:13 +0100"  >&lt;p&gt;OK indeed now I can see the failure when I add the test (I guess I did use the first version), and the changes fix it.  I&apos;ll commit shortly.  Thanks Toke!&lt;/p&gt;</comment>
                    <comment id="12854183" author="mikemccand" created="Tue, 6 Apr 2010 21:58:39 +0100"  >&lt;p&gt;Thanks Toke!&lt;/p&gt;</comment>
                    <comment id="12904560" author="toke" created="Tue, 31 Aug 2010 09:41:45 +0100"  >&lt;p&gt;I have discovered a bug in Packed32 and Packed64: When the number of bits exceed 2^31, the setters and getters fail. This is due to a missing cast in the calculation of the entry-point in the backing int/long-arrays.&lt;/p&gt;

&lt;p&gt;In both Packed32 and Packed64 the line&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
    &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt; majorBitPos = index * bitsPerValue;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;is used in get as well as set. This should be&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
    &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt; majorBitPos = (&lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt;)index * bitsPerValue;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;in all 4 cases.&lt;/p&gt;

&lt;p&gt;A unit test for this is&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
  /*
  Check &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; the structures properly handle the &lt;span class=&quot;code-keyword&quot;&gt;case&lt;/span&gt; where
  index * bitsPerValue &amp;gt; &lt;span class=&quot;code-object&quot;&gt;Integer&lt;/span&gt;.MAX_VALUE
   */
  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; void testIntOverflow() {
    &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; INDEX = (&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt;) &lt;span class=&quot;code-object&quot;&gt;Math&lt;/span&gt;.pow(2, 30);
    &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; BITS = 4;

    Packed32 p32 = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; Packed32(INDEX, BITS);
    p32.set(INDEX-1, 1);
    assertEquals(&lt;span class=&quot;code-quote&quot;&gt;&quot;The value at position &quot;&lt;/span&gt; + (INDEX-1)
        + &lt;span class=&quot;code-quote&quot;&gt;&quot; should be correct &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; Packed32&quot;&lt;/span&gt;, 1, p32.get(INDEX-1));
    p32 = &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;; &lt;span class=&quot;code-comment&quot;&gt;// To free 512MB
&lt;/span&gt;
    Packed64 p64 = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; Packed64(INDEX, BITS);
    p64.set(INDEX-1, 1);
    assertEquals(&lt;span class=&quot;code-quote&quot;&gt;&quot;The value at position &quot;&lt;/span&gt; + (INDEX-1)
        + &lt;span class=&quot;code-quote&quot;&gt;&quot; should be correct &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; Packed64&quot;&lt;/span&gt;, 1, p64.get(INDEX-1));
  }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;One big problem with the unit-test is that it requires 2^30*4/8 bytes = 512MB of heap. I am guessing that this makes it impossible to run in the standard test-suite.&lt;/p&gt;

&lt;p&gt;I am unsure as to how I should push this fix through. Should I create a new JIRA issue? Make a patch against trunk? Or maybe a committer could just try the test above and insert the fix in trunk?&lt;/p&gt;</comment>
                    <comment id="12904564" author="simonw" created="Tue, 31 Aug 2010 10:01:32 +0100"  >&lt;blockquote&gt;&lt;p&gt;One big problem with the unit-test is that it requires 2^30*4/8 bytes = 512MB of heap. I am guessing that this makes it impossible to run in the standard test-suite.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Seems to be a bit high for a unittest but you can&apos;t help it, right? &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I am unsure as to how I should push this fix through. Should I create a new JIRA issue? Make a patch against trunk? Or maybe a committer could just try the test above and insert the fix in trunk?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I would suggest to create a new issue and attach a patch with the fix including your unittest. Since the unittest might break hudson etc I would recommend to add an @Ignore on top of it (JUnit 4) until we decided how to include tests like that. Maybe we might introduce a special flag that enables tests like that with a JUnit Assume call but that needs to be further discussed.&lt;/p&gt;</comment>
                    <comment id="12904569" author="toke" created="Tue, 31 Aug 2010 10:21:14 +0100"  >&lt;p&gt;Remembering signed integer representation, a better test would be &lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
  /*
  Check &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; the structures properly handle the &lt;span class=&quot;code-keyword&quot;&gt;case&lt;/span&gt; where
  index * bitsPerValue &amp;gt; &lt;span class=&quot;code-object&quot;&gt;Integer&lt;/span&gt;.MAX_VALUE
   */
  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; void testIntOverflow() {
    &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; INDEX = (&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt;)&lt;span class=&quot;code-object&quot;&gt;Math&lt;/span&gt;.pow(2, 30)+1;
    &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; BITS = 2;

    Packed32 p32 = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; Packed32(INDEX, BITS);
    p32.set(INDEX-1, 1);
    assertEquals(&lt;span class=&quot;code-quote&quot;&gt;&quot;The value at position &quot;&lt;/span&gt; + (INDEX-1)
        + &lt;span class=&quot;code-quote&quot;&gt;&quot; should be correct &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; Packed32&quot;&lt;/span&gt;, 1, p32.get(INDEX-1));
    p32 = &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;; &lt;span class=&quot;code-comment&quot;&gt;// To free the 256MB used
&lt;/span&gt;
    Packed64 p64 = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; Packed64(INDEX, BITS);
    p64.set(INDEX-1, 1);
    assertEquals(&lt;span class=&quot;code-quote&quot;&gt;&quot;The value at position &quot;&lt;/span&gt; + (INDEX-1)
        + &lt;span class=&quot;code-quote&quot;&gt;&quot; should be correct &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; Packed64&quot;&lt;/span&gt;, 1, p64.get(INDEX-1));
  }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This still triggers the bug but requires &quot;only&quot; 256 MB. Is this acceptable in the Hudson environment?&lt;/p&gt;</comment>
                    <comment id="12904863" author="rcmuir" created="Wed, 1 Sep 2010 02:31:00 +0100"  >&lt;blockquote&gt;&lt;p&gt;This still triggers the bug but requires &quot;only&quot; 256 MB. Is this acceptable in the Hudson environment?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The default maxMemory size is wired at 512MB. i think this might be too large already: on my machine with 4 cpus&lt;br/&gt;
this means max 2GB for lucene and 4GB for solr tests.&lt;/p&gt;

&lt;p&gt;one way to do this would be to make this maxMemory configurable with a -D, set it accordingly in hudson, and&lt;br/&gt;
also setup a way to write &apos;hudson-only&apos; tests. then stuff like this could run in the nightly only.&lt;/p&gt;</comment>
                    <comment id="12905857" author="toke" created="Fri, 3 Sep 2010 12:26:54 +0100"  >&lt;p&gt;Correction: I have created &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2633&quot; title=&quot;PackedInts does not support structures above 256MB&quot;&gt;&lt;del&gt;LUCENE-2633&lt;/del&gt;&lt;/a&gt; and uploaded a patch for the issue above.&lt;/p&gt;</comment>
                    <comment id="13157798" author="thetaphi" created="Sun, 27 Nov 2011 12:29:33 +0000"  >&lt;p&gt;Bulk close after release of 3.5&lt;/p&gt;</comment>
                </comments>
                <issuelinks>
                        <issuelinktype id="10032">
                <name>Blocker</name>
                                <outwardlinks description="blocks">
                            <issuelink>
            <issuekey id="12444498">LUCENE-2186</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12442974">LUCENE-2141</issuekey>
        </issuelink>
                    </outwardlinks>
                                            </issuelinktype>
                        <issuelinktype id="10030">
                <name>Reference</name>
                                                <inwardlinks description="is related to">
                            <issuelink>
            <issuekey id="12473234">LUCENE-2633</issuekey>
        </issuelink>
                    </inwardlinks>
                            </issuelinktype>
                    </issuelinks>
                <attachments>
                    <attachment id="12437268" name="generated_performance-te20100226.txt" size="23136" author="toke" created="Fri, 26 Feb 2010 22:55:25 +0000" />
                    <attachment id="12440546" name="LUCENE-1990.patch" size="4122" author="mikemccand" created="Thu, 1 Apr 2010 23:06:17 +0100" />
                    <attachment id="12438331" name="LUCENE-1990.patch" size="74820" author="mikemccand" created="Tue, 9 Mar 2010 22:14:17 +0000" />
                    <attachment id="12430125" name="LUCENE-1990.patch" size="256575" author="mikemccand" created="Wed, 13 Jan 2010 12:22:15 +0000" />
                    <attachment id="12429343" name="LUCENE-1990_PerformanceMeasurements20100104.zip" size="18482" author="toke" created="Mon, 4 Jan 2010 14:25:02 +0000" />
                    <attachment id="12431107" name="LUCENE-1990-te20100122.patch" size="287474" author="toke" created="Fri, 22 Jan 2010 12:27:10 +0000" />
                    <attachment id="12435453" name="LUCENE-1990-te20100210.patch" size="73194" author="toke" created="Wed, 10 Feb 2010 14:49:44 +0000" />
                    <attachment id="12435654" name="LUCENE-1990-te20100212.patch" size="100880" author="toke" created="Fri, 12 Feb 2010 02:57:19 +0000" />
                    <attachment id="12436719" name="LUCENE-1990-te20100223.patch" size="105931" author="toke" created="Tue, 23 Feb 2010 15:33:30 +0000" />
                    <attachment id="12437199" name="LUCENE-1990-te20100226b.patch" size="136732" author="toke" created="Fri, 26 Feb 2010 15:49:46 +0000" />
                    <attachment id="12437267" name="LUCENE-1990-te20100226c.patch" size="364526" author="toke" created="Fri, 26 Feb 2010 22:55:25 +0000" />
                    <attachment id="12437187" name="LUCENE-1990-te20100226.patch" size="133496" author="toke" created="Fri, 26 Feb 2010 14:03:54 +0000" />
                    <attachment id="12437485" name="LUCENE-1990-te20100301.patch" size="372224" author="toke" created="Mon, 1 Mar 2010 14:11:43 +0000" />
                    <attachment id="12437338" name="perf-mkm-20100227.txt" size="7351" author="mikemccand" created="Sat, 27 Feb 2010 11:13:42 +0000" />
                    <attachment id="12437484" name="performance-20100301.txt" size="46484" author="toke" created="Mon, 1 Mar 2010 14:11:43 +0000" />
                    <attachment id="12437200" name="performance-te20100226.txt" size="33436" author="toke" created="Fri, 26 Feb 2010 15:49:46 +0000" />
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>16.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Tue, 10 Nov 2009 14:00:42 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>11781</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>25735</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                    <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                <customfieldname>Time in Status</customfieldname>
                <customfieldvalues>
                    
                </customfieldvalues>
            </customfield>
                            </customfields>
    </item>
</channel>
</rss>