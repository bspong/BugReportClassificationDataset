<!-- 
RSS generated by JIRA (5.2.8#851-sha1:3262fdc28b4bc8b23784e13eadc26a22399f5d88) at Tue Jul 16 13:24:52 UTC 2013

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/LUCENE-2680/LUCENE-2680.xml?field=key&field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>5.2.8</version>
        <build-number>851</build-number>
        <build-date>26-02-2013</build-date>
    </build-info>

<item>
            <title>[LUCENE-2680] Improve how IndexWriter flushes deletes against existing segments</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-2680</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;IndexWriter buffers up all deletes (by Term and Query) and only&lt;br/&gt;
applies them if 1) commit or NRT getReader() is called, or 2) a merge&lt;br/&gt;
is about to kickoff.&lt;/p&gt;

&lt;p&gt;We do this because, for a large index, it&apos;s very costly to open a&lt;br/&gt;
SegmentReader for every segment in the index.  So we defer as long as&lt;br/&gt;
we can.  We do it just before merge so that the merge can eliminate&lt;br/&gt;
the deleted docs.&lt;/p&gt;

&lt;p&gt;But, most merges are small, yet in a big index we apply deletes to all&lt;br/&gt;
of the segments, which is really very wasteful.&lt;/p&gt;

&lt;p&gt;Instead, we should only apply the buffered deletes to the segments&lt;br/&gt;
that are about to be merged, and keep the buffer around for the&lt;br/&gt;
remaining segments.&lt;/p&gt;

&lt;p&gt;I think it&apos;s not so hard to do; we&apos;d have to have generations of&lt;br/&gt;
pending deletions, because the newly merged segment doesn&apos;t need the&lt;br/&gt;
same buffered deletions applied again.  So every time a merge kicks&lt;br/&gt;
off, we pinch off the current set of buffered deletions, open a new&lt;br/&gt;
set (the next generation), and record which segment was created as of&lt;br/&gt;
which generation.&lt;/p&gt;

&lt;p&gt;This should be a very sizable gain for large indices that mix&lt;br/&gt;
deletes, though, less so in flex since opening the terms index is much&lt;br/&gt;
faster.&lt;/p&gt;</description>
                <environment></environment>
            <key id="12475670">LUCENE-2680</key>
            <summary>Improve how IndexWriter flushes deletes against existing segments</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.png">Major</priority>
                    <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png">Closed</status>
                    <resolution id="1">Fixed</resolution>
                                <assignee username="mikemccand">Michael McCandless</assignee>
                                <reporter username="mikemccand">Michael McCandless</reporter>
                        <labels>
                    </labels>
                <created>Fri, 1 Oct 2010 23:22:24 +0100</created>
                <updated>Thu, 2 May 2013 03:29:33 +0100</updated>
                    <resolved>Fri, 17 Dec 2010 10:11:07 +0000</resolved>
                                            <fixVersion>3.1</fixVersion>
                <fixVersion>4.0-ALPHA</fixVersion>
                                        <due></due>
                    <votes>0</votes>
                        <watches>2</watches>
                                                    <comments>
                    <comment id="12917174" author="mikemccand" created="Sat, 2 Oct 2010 10:47:21 +0100"  >&lt;p&gt;Hmm... I think there&apos;s another silliness going on inside IW: when applying deletes, we one-by-one open the SR, apply deletes, close it.&lt;/p&gt;

&lt;p&gt;But then immediately thereafter we open the N segments to be merged.&lt;/p&gt;

&lt;p&gt;We should somehow not do this double open, eg, use the pool temporarily, so that the reader is opened to apply deletes, and then kept open in order to do the merging.  Using the pool should be fine because the merge forcefully evicts the sub readers from the pool after completion.&lt;/p&gt;</comment>
                    <comment id="12919549" author="jasonrutherglen" created="Sun, 10 Oct 2010 01:57:48 +0100"  >&lt;p&gt;Maybe we should implement this as pending deletes per segment rather than using a generational system because with &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2655&quot; title=&quot;Get deletes working in the realtime branch&quot;&gt;&lt;del&gt;LUCENE-2655&lt;/del&gt;&lt;/a&gt;, we may need to maintain the per query/term docidupto per segment.  The downside is the extraneous memory consumed by the hash map, however, if we use BytesRefHash this&apos;ll be reduced, or would it?  Because we&apos;d be writing the term bytes to a unique byte pool per segment?  Hmm... Maybe there&apos;s a more efficient way.&lt;/p&gt;</comment>
                    <comment id="12919780" author="mikemccand" created="Mon, 11 Oct 2010 11:05:15 +0100"  >&lt;p&gt;Tracking per-segment would be easier but I worry about indices that have large numbers of segments... eg w/ a large mergeFactor and frequent flushing you can get very many segments.&lt;/p&gt;

&lt;p&gt;So if we track per-segment, suddenly the RAM required (as well as CPU cost of copying these deletions to the N segments) is multiplied by the number of segments.&lt;/p&gt;</comment>
                    <comment id="12927063" author="jasonrutherglen" created="Mon, 1 Nov 2010 17:58:06 +0000"  >&lt;p&gt;The general approach is to reuse BufferedDeletes though place them into a segment info keyed map for those segments generated post lastSegmentIndex as per what has been discussed here &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2655?focusedCommentId=12922894&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#action_12922894&quot; class=&quot;external-link&quot;&gt;https://issues.apache.org/jira/browse/LUCENE-2655?focusedCommentId=12922894&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#action_12922894&lt;/a&gt; and below.&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;lastSegmentIndex is added to IW&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;DW segmentDeletes is a map of segment info -&amp;gt; buffered deletes.  In the apply deletes method buffered deletes are pulled for a given segment info if they exist, otherwise they&apos;re taken from deletesFlushedLastSeg.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;I&apos;m not entirely sure what pushDeletes should do now, probably the same thing as currently, only the name should change slightly in that it&apos;s pushing deletes only for the RAM buffer docs.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;There needs to be tests to ensure the docid-upto logic is working correctly&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;I&apos;m not sure what to do with DW hasDeletes (it&apos;s usage is commented out)&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Does there need to be separate deletes for the ram buffer vis-&#224;-vis the (0 - lastSegmentIndex) deletes?&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;The memory accounting&apos;ll now get interesting as we&apos;ll need to track the RAM usage of terms/queries across multiple maps.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;In commitMerge, DW verifySegmentDeletes removes the unused info -&amp;gt; deletes&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;testDeletes deletes a doc in segment 1, then merges segments 1 and 2.  We then test to insure the deletes were in fact applied only to segment 1 and 2.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;testInitLastSegmentIndex insures that on IW init, the lastSegmentIndex is in fact set&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="12927488" author="jasonrutherglen" created="Tue, 2 Nov 2010 16:41:13 +0000"  >&lt;blockquote&gt;&lt;p&gt;The most recent one &quot;wins&quot;, and we should do only one delete (per segment) for that term.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;How should we define this recency and why does it matter?  Should it be per term/query or for the entire BD?&lt;/p&gt;

&lt;p&gt;I think there&apos;s an issue with keeping lastSegmentIndex in DW, while it&apos;s easy to maintain, Mike had mentioned keeping the lastSegmentIndex per BufferedDeletes object.  Coalescing the BDs should be easier to maintain after successful merge than maintaining a separate BD for them.  We&apos;ll see.&lt;/p&gt;

&lt;p&gt;I&apos;ll put together another patch with these changes.&lt;/p&gt;</comment>
                    <comment id="12927714" author="jasonrutherglen" created="Wed, 3 Nov 2010 01:22:17 +0000"  >&lt;p&gt;Here&apos;s a new patch with properly working last segment index.  &lt;/p&gt;

&lt;p&gt;The trunk version of apply deletes has become applyDeletesAll and is functionally unchanged.&lt;/p&gt;

&lt;p&gt;There&apos;s a new method, DW applyDeletesToSegments called by _mergeInit for segments that are about to be merged.  The deleted terms and queries for these segments are kept in hash sets because docid-uptos are not needed.  &lt;/p&gt;

&lt;p&gt;Like the last patch DW maintains the last segment index.  There&apos;s no need to maintain the last-segindex per BD, instead I think it&apos;s only useful per DW, and for trunk we only have one DW being used at a time.  &lt;/p&gt;

&lt;p&gt;On successful merge, the last segment index is set to the segment index previous to the start segment of the merge.  The merged segments deletes are coalesced into the startIndex-1&apos;s segment deletes.&lt;/p&gt;</comment>
                    <comment id="12927943" author="jasonrutherglen" created="Wed, 3 Nov 2010 18:38:58 +0000"  >&lt;p&gt;I&apos;m redoing things a bit to take into account the concurrency of merges.  For example, if a merge fails, we need to not have removed those segments&apos; deletes to be applied.  Also probably the most tricky part is that lastSegmentIndex could have changed since a merge started, which means we need to be careful about how and which deletes we coalesce.&lt;/p&gt;</comment>
                    <comment id="12927979" author="jasonrutherglen" created="Wed, 3 Nov 2010 20:07:31 +0000"  >&lt;p&gt;Another use case that can be wacky is if commit is called and a merge is finishing before or after, in that case all (point-in-time) deletes will have been applied by commit, however do we want to clear all per-segment deletes at the end of commit?  This would blank out deletes being applied by the merge, most of which should be cleared out, however if new deletes arrived during the commit (is this possible?), then we want these to be attached to segments and not lost.  I guess we want to DW sync&apos;d clear out deletes in the applyDeletesAll method.  ADA will apply those deletes, any incoming will queue up and be shuffled around.&lt;/p&gt;</comment>
                    <comment id="12928052" author="jasonrutherglen" created="Wed, 3 Nov 2010 22:48:53 +0000"  >&lt;p&gt;In my head at least I think the concurrency issues are worked&lt;br/&gt;
out in this patch. We&apos;re not taking into account recency of&lt;br/&gt;
deletes as I&apos;m not sure it matters. DW applyDeletesToSegments&lt;br/&gt;
takes care of the coalescing of segment deletes as this is a&lt;br/&gt;
synced DW method called by a synced IW method, meaning&lt;br/&gt;
nothing&apos;ll be changing anywhere, so we&apos;re good with the possible&lt;br/&gt;
concurrency issues. I&apos;m still a little worried about concurrent&lt;br/&gt;
incoming deleted terms/queries, however those can&apos;t be added&lt;br/&gt;
until after a successful ADTS call due to the DW sync. &lt;/p&gt;

&lt;p&gt;Guess it&apos;s time for the complete tests to run.&lt;/p&gt;</comment>
                    <comment id="12928075" author="jasonrutherglen" created="Thu, 4 Nov 2010 01:00:29 +0000"  >&lt;p&gt;There&apos;s an issue in that we&apos;re redundantly applying deletes in the applyDeletesAll case because the deletes may have already been applied to a segment when a merge happened, ie, by applyDeletesToSegments.  In the ADA case we need to use applyDeletesToSegments up to the segment point when the buffered deletes can be used.  &lt;/p&gt;</comment>
                    <comment id="12928078" author="jasonrutherglen" created="Thu, 4 Nov 2010 01:09:26 +0000"  >&lt;p&gt;This brings up another issue which is we&apos;re blindly iterating over docs in a segment reader to delete, even if we can know ahead of time that the reader&apos;s docs are going to exceed the term/query&apos;s docid-upto (from the max doc of the reader).  In applyDeletes we&apos;re opening a term docs iterator, though I think we&apos;re breaking at the first doc and moving on if the docid-upto is exceeded.  This term docs iterator opening can be skipped.&lt;/p&gt;</comment>
                    <comment id="12928417" author="jasonrutherglen" created="Thu, 4 Nov 2010 22:52:42 +0000"  >&lt;p&gt;Here&apos;s a nice little checkpoint with more tests passing.  &lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;A last known segment is recorded, which is the last segment seen when&lt;br/&gt;
adding a delete term/query per segment. This is for a applyDeletesAll&lt;br/&gt;
check to ensure a given query/term has not already been applied to a&lt;br/&gt;
segment. If a term/query exists in the per-segment deletes and is in&lt;br/&gt;
deletesFlushed, we delete, unless we&apos;re beyond the last known segment, at&lt;br/&gt;
which point we simply delete (adhering of course to the docid-upto).&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;In the interest of accuracy I nixed lastSegmentIndex in favor of&lt;br/&gt;
lastSegmentInfo which is easier for debugging and implementation when&lt;br/&gt;
segments are shuffled around and/or removed/added. There&apos;s not too much of&lt;br/&gt;
a penalty in terms of performance. &lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;org.apache.lucene.index tests pass&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;I need to address the applying deletes only on readers within the&lt;br/&gt;
docid-upto per term/query, perhaps that&apos;s best left to a different Jira&lt;br/&gt;
issue.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Still not committable as it needs cleaning up, complete unit tests, who&lt;br/&gt;
knows what else.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="12928923" author="jasonrutherglen" created="Sat, 6 Nov 2010 03:21:00 +0000"  >&lt;p&gt;All tests pass except org.apache.lucene.index.TestIndexWriterMergePolicy testMaxBufferedDocsChange.  Odd.  I&apos;m looking into this.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
[junit] junit.framework.AssertionFailedError: maxMergeDocs=2147483647; numSegments=11; upperBound=10; mergeFactor=10; 
segs=_65:c5950 _5t:c10-&amp;gt;_32 _5u:c10-&amp;gt;_32 _5v:c10-&amp;gt;_32 _5w:c10-&amp;gt;_32 _5x:c10-&amp;gt;_32 _5y:c10-&amp;gt;_32 _5z:c10-&amp;gt;_32 _60:c10-&amp;gt;_32 _61:c10-&amp;gt;_32 _62:c3-&amp;gt;_32 _64:c7-&amp;gt;_62
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Also, in IW deleteDocument&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/star_yellow.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; we&apos;re calling a new method, getSegmentInfos which is sync&apos;ed on IW.  Maybe we should use an atomic reference to a read only segment infos instead?&lt;/p&gt;</comment>
                    <comment id="12928933" author="jasonrutherglen" created="Sat, 6 Nov 2010 05:05:31 +0000"  >&lt;p&gt;Sorry, spoke too soon, I made a small change to not redundantly delete, in apply deletes all and TestStressIndexing2 is breaking.  I think we need to &quot;push&quot; segment infos changes to DW as they happen.  I&apos;m guessing that segment infos are being shuffled around and so the infos passed into DW in IW deleteDoc methods may be out of date by the time deletes are attached to segments.  Hopefully there aren&apos;t any lurking deadlock issues with this.&lt;/p&gt;</comment>
                    <comment id="12929229" author="jasonrutherglen" created="Sat, 6 Nov 2010 21:59:20 +0000"  >&lt;p&gt;Pushing the segment infos seems to have cleared up some of the tests failing, however intermittently (1/4 of the time) there&apos;s the one below.&lt;/p&gt;

&lt;p&gt;I&apos;m going to re-add lastSegmentInfo/Index, and assert that if we&apos;re not using it, that the deletes obtained from the segmentinfo -&amp;gt; deletes map is the same.  &lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
[junit] Testsuite: org.apache.lucene.index.TestStressIndexing2
    [junit] Testcase: testRandom(org.apache.lucene.index.TestStressIndexing2):	FAILED
    [junit] expected:&amp;lt;12&amp;gt; but was:&amp;lt;11&amp;gt;
    [junit] junit.framework.AssertionFailedError: expected:&amp;lt;12&amp;gt; but was:&amp;lt;11&amp;gt;
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:878)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:844)
    [junit] 	at org.apache.lucene.index.TestStressIndexing2.verifyEquals(TestStressIndexing2.java:278)
    [junit] 	at org.apache.lucene.index.TestStressIndexing2.verifyEquals(TestStressIndexing2.java:271)
    [junit] 	at org.apache.lucene.index.TestStressIndexing2.testRandom(TestStressIndexing2.java:89)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                    <comment id="12929247" author="jasonrutherglen" created="Sat, 6 Nov 2010 22:25:51 +0000"  >&lt;p&gt;I wasn&apos;t coalescing the merged segment&apos;s deletes, with that implemented, TestStressIndexing2 ran successfully 49 of 50 times.  Below is the error:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
[junit] Testsuite: org.apache.lucene.index.TestStressIndexing2
    [junit] Testcase: testMultiConfig(org.apache.lucene.index.TestStressIndexing2):	FAILED
    [junit] expected:&amp;lt;5&amp;gt; but was:&amp;lt;4&amp;gt;
    [junit] junit.framework.AssertionFailedError: expected:&amp;lt;5&amp;gt; but was:&amp;lt;4&amp;gt;
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:878)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:844)
    [junit] 	at org.apache.lucene.index.TestStressIndexing2.verifyEquals(TestStressIndexing2.java:278)
    [junit] 	at org.apache.lucene.index.TestStressIndexing2.verifyEquals(TestStressIndexing2.java:271)
    [junit] 	at org.apache.lucene.index.TestStressIndexing2.testMultiConfig(TestStressIndexing2.java:115)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                    <comment id="12929254" author="jasonrutherglen" created="Sat, 6 Nov 2010 22:37:26 +0000"  >&lt;p&gt;Putting a sync on DW block around the bulk of the segment alterations in IW commitMerge seems to have quelled the TestStressIndexing2 test failures.  Nice.&lt;/p&gt;</comment>
                    <comment id="12929272" author="jasonrutherglen" created="Sat, 6 Nov 2010 23:10:45 +0000"  >&lt;p&gt;Here&apos;s a check point patch before I re-add lastSegmentInfo/Index.  All tests pass except for what&apos;s below.  I&apos;m guessing segments with all docs deleted, are deleted before the test expects.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
[junit] Testcase: testCommitThreadSafety(org.apache.lucene.index.TestIndexWriter):	FAILED
    [junit] 
    [junit] junit.framework.AssertionFailedError: 
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:878)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:844)
    [junit] 	at org.apache.lucene.index.TestIndexWriter.testCommitThreadSafety(TestIndexWriter.java:4699)
    [junit] 
    [junit] 
    [junit] Testcase: testCommitThreadSafety(org.apache.lucene.index.TestIndexWriter):	FAILED
    [junit] Some threads threw uncaught exceptions!
    [junit] junit.framework.AssertionFailedError: Some threads threw uncaught exceptions!
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:878)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:844)
    [junit] 	at org.apache.lucene.util.LuceneTestCase.tearDown(LuceneTestCase.java:437)
    [junit] 
    [junit] 
    [junit] Tests run: 116, Failures: 2, Errors: 0, Time elapsed: 159.577 sec
    [junit] 
    [junit] ------------- Standard Output ---------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexWriter -Dtestmethod=testCommitThreadSafety -Dtests.seed=1826133140332330367:8102643307925777745
    [junit] NOTE: test params are: codec=MockFixedIntBlock(blockSize=564), locale=es_CR, timezone=Asia/Urumqi
    [junit] ------------- ---------------- ---------------
    [junit] ------------- Standard Error -----------------
    [junit] The following exceptions were thrown by threads:
    [junit] *** &lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;: &lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;-1106 ***
    [junit] java.lang.RuntimeException: java.lang.AssertionError: term=f:0_8; r=DirectoryReader(_0:c1  _1:c1  _2:c1  _3:c1  _4:c1  _5:c1  _6:c1  _7:c2  _8:c4 ) expected:&amp;lt;1&amp;gt; but was:&amp;lt;0&amp;gt;
    [junit] 	at org.apache.lucene.index.TestIndexWriter$9.run(TestIndexWriter.java:4690)
    [junit] Caused by: java.lang.AssertionError: term=f:0_8; r=DirectoryReader(_0:c1  _1:c1  _2:c1  _3:c1  _4:c1  _5:c1  _6:c1  _7:c2  _8:c4 ) expected:&amp;lt;1&amp;gt; but was:&amp;lt;0&amp;gt;
    [junit] 	at org.junit.Assert.fail(Assert.java:91)
    [junit] 	at org.junit.Assert.failNotEquals(Assert.java:645)
    [junit] 	at org.junit.Assert.assertEquals(Assert.java:126)
    [junit] 	at org.junit.Assert.assertEquals(Assert.java:470)
    [junit] 	at org.apache.lucene.index.TestIndexWriter$9.run(TestIndexWriter.java:4684)
    [junit] NOTE: all tests run in &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; JVM:
    [junit] [TestMockAnalyzer, TestByteSlices, TestFilterIndexReader, TestIndexFileDeleter, TestIndexReaderClone, TestIndexReaderReopen, TestIndexWriter]
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                    <comment id="12929404" author="jasonrutherglen" created="Sun, 7 Nov 2010 22:34:52 +0000"  >&lt;p&gt;I placed (for now) the segment deletes directly into the segment info object.  There&apos;s applied term/queries sets which are checked against when apply deletes all is called.  All tests pass except for TestTransactions and TestPersistentSnapshotDeletionPolicy only because of an assertion check I added, that the last segment info is in fact in the newly pushed segment infos.  I think in both cases segment infos is being altered in IW in a place where the segment infos isn&apos;t being pushed, yet.  I wanted to checkpoint this though as it&apos;s a fairly well working at this point, including the last segment info/index, which is can be turned on or off via a static variable.  &lt;/p&gt;</comment>
                    <comment id="12929422" author="jasonrutherglen" created="Mon, 8 Nov 2010 01:02:56 +0000"  >&lt;p&gt;Everything passes, except for tests that involve IW rollback.  We need to be able to rollback the last segment info/index in DW, however I&apos;m not sure how we want to do that quite yet.&lt;/p&gt;</comment>
                    <comment id="12929424" author="jasonrutherglen" created="Mon, 8 Nov 2010 01:31:08 +0000"  >&lt;p&gt;In DW abort (called by IW rollbackInternal) we should be able to simply clear all per segment pending deletes, however, I&apos;m not sure we can do that, in fact, if we have applied deletes for a merge, then we rollback, we can&apos;t undo those deletes thereby breaking our current rollback model?&lt;/p&gt;</comment>
                    <comment id="12929432" author="jasonrutherglen" created="Mon, 8 Nov 2010 01:55:25 +0000"  >&lt;p&gt;Here&apos;s an uncleaned up cut with all tests passing. I nulled out&lt;br/&gt;
the lastSegmentInfo on abort which fixes the my own assertion&lt;br/&gt;
that was causing the rollback tests to not pass. I don&apos;t know if&lt;br/&gt;
this is cheating or not yet just to get the tests to pass.&lt;/p&gt;</comment>
                    <comment id="12929629" author="jasonrutherglen" created="Mon, 8 Nov 2010 16:46:17 +0000"  >&lt;p&gt;I&apos;m running test-core multiple times and am seeing some lurking test&lt;br/&gt;
failures (thanks to the randomized tests that have been recently added).&lt;br/&gt;
I&apos;m guessing they&apos;re related to the syncs on IW and DW not being in &quot;sync&quot;&lt;br/&gt;
some of the time. &lt;/p&gt;

&lt;p&gt;I will clean up the patch so that others may properly review it and&lt;br/&gt;
hopefully we can figure out what&apos;s going on. &lt;/p&gt;</comment>
                    <comment id="12929743" author="jasonrutherglen" created="Mon, 8 Nov 2010 21:29:50 +0000"  >&lt;p&gt;Here&apos;s a cleaned up patch, please take a look.  I ran &apos;ant test-core&apos; 5 times with no failures, however running the below several times does eventually produce a failure.&lt;/p&gt;

&lt;p&gt;ant test-core -Dtestcase=TestThreadedOptimize -Dtestmethod=testThreadedOptimize -Dtests.seed=1547315783637080859:5267275843141383546&lt;/p&gt;

&lt;p&gt;ant test-core -Dtestcase=TestIndexWriterMergePolicy -Dtestmethod=testMaxBufferedDocsChange -Dtests.seed=7382971652679988823:-6672235304390823521&lt;/p&gt;</comment>
                    <comment id="12929810" author="jasonrutherglen" created="Mon, 8 Nov 2010 23:25:22 +0000"  >&lt;p&gt;The problem could be that IW deleteDocument is not synced on IW,&lt;br/&gt;
when I tried adding the sync, there was deadlock perhaps from DW&lt;br/&gt;
waitReady. We could be adding pending deletes to segments that&lt;br/&gt;
are not quite current because we&apos;re not adding them in an IW&lt;br/&gt;
sync block.&lt;/p&gt;</comment>
                    <comment id="12929927" author="jasonrutherglen" created="Tue, 9 Nov 2010 04:02:05 +0000"  >&lt;p&gt;Ok, TestThreadedOptimize works when the DW sync&apos;ed pushSegmentInfos method&lt;br/&gt;
isn&apos;t called anymore (no extra per-segment deleting is going on), and stops&lt;br/&gt;
working when pushSegmentInfos is turned back on. Something about the sync&lt;br/&gt;
on DW is causing a problem.  Hmm... We need another way to pass segment&lt;br/&gt;
infos around consistently. &lt;/p&gt;</comment>
                    <comment id="12930658" author="jasonrutherglen" created="Wed, 10 Nov 2010 17:23:23 +0000"  >&lt;p&gt;I think I&apos;ve taken &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2680&quot; title=&quot;Improve how IndexWriter flushes deletes against existing segments&quot;&gt;&lt;del&gt;LUCENE-2680&lt;/del&gt;&lt;/a&gt; as far as I can, though I&apos;ll&lt;br/&gt;
probably add some more assertions in there for good measure,&lt;br/&gt;
such as whether or not a delete has in fact been applied etc. It&lt;br/&gt;
seems to be working though again I should add more assertions to&lt;br/&gt;
that effect. I think there&apos;s a niggling sync issue in there as&lt;br/&gt;
TestThreadedOptimize only fails when I try to run it 100s of&lt;br/&gt;
times. I think the sync on DW is causing a wait notify to be&lt;br/&gt;
missed or skipped or something like that, as occasionally the&lt;br/&gt;
isOptimized call fails as well. This is likely related to the&lt;br/&gt;
appearance of deletes not being applied to segment(s) as&lt;br/&gt;
evidenced by the difference in the actual doc count and the&lt;br/&gt;
expected doc count.&lt;/p&gt;

&lt;p&gt;Below is the most common assertion failure. Maybe I should&lt;br/&gt;
upload my patch that includes a method that iterates 200 times&lt;br/&gt;
on testThreadedOptimize?&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
[junit] ------------- ---------------- ---------------
    [junit] Testsuite: org.apache.lucene.index.TestThreadedOptimize
    [junit] Testcase: testThreadedOptimize(org.apache.lucene.index.TestThreadedOptimize):	FAILED
    [junit] expected:&amp;lt;248&amp;gt; but was:&amp;lt;266&amp;gt;
    [junit] junit.framework.AssertionFailedError: expected:&amp;lt;248&amp;gt; but was:&amp;lt;266&amp;gt;
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:878)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:844)
    [junit] 	at org.apache.lucene.index.TestThreadedOptimize.runTest(TestThreadedOptimize.java:119)
    [junit] 	at org.apache.lucene.index.TestThreadedOptimize.testThreadedOptimize(TestThreadedOptimize.java:141)
    [junit] 
    [junit] 
    [junit] Tests run: 1, Failures: 1, Errors: 0, Time elapsed: 1.748 sec
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                    <comment id="12930845" author="jasonrutherglen" created="Wed, 10 Nov 2010 23:53:05 +0000"  >&lt;p&gt;I think I&apos;ve isolated this test failure to recording the applied deletes.&lt;br/&gt;
Because we&apos;re using last segment index/info, I was adding deletes that may&lt;br/&gt;
or may not have been applied to a particular segment to the last segment&lt;br/&gt;
info. I&apos;m not sure what to do in this case as if we record the applied&lt;br/&gt;
terms per segment, but keep the pending terms in last segment info, we&apos;re&lt;br/&gt;
effectively not gaining anything from using last segment info because&lt;br/&gt;
we&apos;re then recording all of the terms per-segment anyways. In fact, this&lt;br/&gt;
is how I&apos;ve isolated that this is the issue, I simply removed the usage of&lt;br/&gt;
last segment info, and instead went to maintaining pending deletes&lt;br/&gt;
per-segment. I&apos;ll give it some thought.&lt;/p&gt;

&lt;p&gt;In conclusion, when deletes are recorded per-segment with no last segment&lt;br/&gt;
info, the test passes after 200 times. &lt;/p&gt;</comment>
                    <comment id="12930949" author="jasonrutherglen" created="Thu, 11 Nov 2010 07:42:32 +0000"  >&lt;p&gt;Alright, we needed to clone the per-segment pending deletes in the&lt;br/&gt;
_mergeInit prior to the merge, like cloning the SRs. There were other&lt;br/&gt;
terms arriving after they were applied to a merge, then the coalescing of&lt;br/&gt;
applied deletes was incorrect. I believe that this was the remaining&lt;br/&gt;
lingering issue. The previous failures seem to have gone away, I ran the&lt;br/&gt;
test 400 times. I&apos;ll upload a new patch shortly.&lt;/p&gt;</comment>
                    <comment id="12931131" author="jasonrutherglen" created="Thu, 11 Nov 2010 18:54:00 +0000"  >&lt;p&gt;I&apos;m still seeing the error no matter what I do. Sometimes the index is not&lt;br/&gt;
optimized, and sometimes there are too many docs. It requires thousands of&lt;br/&gt;
iterations to provoke either test error. Perhaps it&apos;s simply related to&lt;br/&gt;
merges that are scheduled but IW close isn&apos;t waiting on properly.&lt;/p&gt;</comment>
                    <comment id="12931133" author="mikemccand" created="Thu, 11 Nov 2010 18:58:38 +0000"  >&lt;p&gt;TestThreadedOptimize is a known intermittent failure &amp;#8211; I&apos;m trying to track it down!!  (&lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2618&quot; title=&quot;Intermittent failure in TestThreadedOptimize&quot;&gt;&lt;del&gt;LUCENE-2618&lt;/del&gt;&lt;/a&gt;)&lt;/p&gt;</comment>
                    <comment id="12931143" author="jasonrutherglen" created="Thu, 11 Nov 2010 19:07:59 +0000"  >&lt;p&gt;Ah, nice, I should have looked for previous intermittent failures via Jira.  &lt;/p&gt;</comment>
                    <comment id="12932222" author="jasonrutherglen" created="Mon, 15 Nov 2010 22:09:05 +0000"  >&lt;p&gt;Now that the intermittent failures have been successfully dealt with, ie,&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2618&quot; title=&quot;Intermittent failure in TestThreadedOptimize&quot;&gt;&lt;del&gt;LUCENE-2618&lt;/del&gt;&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2576&quot; title=&quot;Intermittent failure in TestIndexWriter.testCommitThreadSafety&quot;&gt;&lt;del&gt;LUCENE-2576&lt;/del&gt;&lt;/a&gt;, and &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2118&quot; title=&quot;Intermittent failure in TestIndexWriterMergePolicy.testMaxBufferedDocsChange&quot;&gt;&lt;del&gt;LUCENE-2118&lt;/del&gt;&lt;/a&gt;, I&apos;ll merge this patch to trunk,&lt;br/&gt;
then it&apos;s probably time for benchmarking. That&apos;ll probably include&lt;br/&gt;
something like indexing, then updating many documents and comparing the&lt;br/&gt;
index time vs. trunk? &lt;/p&gt;</comment>
                    <comment id="12932508" author="jasonrutherglen" created="Tue, 16 Nov 2010 15:51:27 +0000"  >&lt;p&gt;Straight indexing and deleting will probably not show much of an&lt;br/&gt;
improvement from this patch. In trunk, apply deletes (all) is called on&lt;br/&gt;
all segments prior to a merge, so we need a synthetic way to measure the&lt;br/&gt;
improvement. One way is to monitor the merge time of small segments (of an&lt;br/&gt;
index with many deletes, and many existing large segments) with this patch&lt;br/&gt;
vs. trunk. This&apos;ll show that this patch in that case is faster (because&lt;br/&gt;
we&apos;re only applying deletes to the smaller segments). &lt;/p&gt;

&lt;p&gt;I think I&apos;ll add a merge start time variable to OneMerge that&apos;ll be set in&lt;br/&gt;
mergeinit. The var could also be useful for the info stream debug log. The&lt;br/&gt;
benchmark will simply print out the merge times (which&apos;ll be manufactured&lt;br/&gt;
synthetically). &lt;/p&gt;</comment>
                    <comment id="12932915" author="mikemccand" created="Wed, 17 Nov 2010 11:47:15 +0000"  >&lt;p&gt;Why do we still have deletesFlushed?  And why do we still need to&lt;br/&gt;
remap docIDs on merge?  I thought with this new approach the docIDUpto&lt;br/&gt;
for each buffered delete Term/Query would be a local docID to that&lt;br/&gt;
segment?&lt;/p&gt;

&lt;p&gt;On flush the deletesInRAM should be carried directly over to the&lt;br/&gt;
segmentDeletes, and there shouldn&apos;t be a deletesFlushed?&lt;/p&gt;

&lt;p&gt;A few other small things:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;You can use SegmentInfos.clone to copy the segment infos? (it&lt;br/&gt;
    makes a deep copy)&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;SegmentDeletes.clearAll() need not iterate through the&lt;br/&gt;
    terms/queries to subtract the RAM used?  Ie just multiply by&lt;br/&gt;
    .size() instead and make one call to deduct RAM used?&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;The SegmentDeletes use less than BYTES_PER_DEL_TERM because it&apos;s a&lt;br/&gt;
    simple HashSet not a HashMap?  Ie we are over-counting RAM used&lt;br/&gt;
    now?  (Same for by query)&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Can we store segment&apos;s deletes elsewhere?  The SegmentInfo should&lt;br/&gt;
    be a lightweight class... eg it&apos;s used by DirectoryReader to read&lt;br/&gt;
    the index, and if it&apos;s read only DirectoryReader there&apos;s no need&lt;br/&gt;
    for it to allocate the SegmentDeletes?  These data structures&lt;br/&gt;
    should only be held by IndexWriter/DocumentsWriter.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Do we really need to track appliedTerms/appliedQueries?  Ie is&lt;br/&gt;
    this just an optimization so that if the caller deletes by the&lt;br/&gt;
    Term/Query again we know to skip it?  Seems unnecessary if that&apos;s&lt;br/&gt;
    all...&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="12932945" author="mikemccand" created="Wed, 17 Nov 2010 13:32:55 +0000"  >&lt;p&gt;Also: why are we tracking the last segment info/index?  Ie, this should only be necessary on cutover to DWPT right?  Because effectively today we have only a single &quot;DWPT&quot;?&lt;/p&gt;</comment>
                    <comment id="12932988" author="jasonrutherglen" created="Wed, 17 Nov 2010 15:39:53 +0000"  >&lt;blockquote&gt;&lt;p&gt;Why do we still have deletesFlushed? And why do we still need to&lt;br/&gt;
remap docIDs on merge? I thought with this new approach the docIDUpto for&lt;br/&gt;
each buffered delete Term/Query would be a local docID to that&lt;br/&gt;
segment?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Deletes flushed can be removed if we store the docid-upto per segment.&lt;br/&gt;
Then we&apos;ll go back to having a hash map of deletes. &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;The SegmentDeletes use less than BYTES_PER_DEL_TERM because it&apos;s a&lt;br/&gt;
simple HashSet not a HashMap? Ie we are over-counting RAM used now? (Same&lt;br/&gt;
for by query)&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Intuitively, yes, however here&apos;s the constructor of hash set:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt; &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; HashSet() { map = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; HashMap&amp;lt;E,&lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt;&amp;gt;(); } &lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;blockquote&gt;&lt;p&gt;why are we tracking the last segment info/index?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I thought last segment was supposed to be used to mark the last segment of&lt;br/&gt;
a commit/flush. This way we save on the hash(set,map) space on the&lt;br/&gt;
segments upto the last segment when the commit occurred.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Can we store segment&apos;s deletes elsewhere?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;We can, however I had to minimize places in the code that were potentially&lt;br/&gt;
causing errors (trying to reduce the problem set, which helped locate the&lt;br/&gt;
intermittent exceptions), syncing segment infos with the per-segment&lt;br/&gt;
deletes was one was one of those places. That and I thought it&apos;d be worth&lt;br/&gt;
a try simplify (at the expense of breaking the unstated intention of the&lt;br/&gt;
SI class).&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Do we really need to track appliedTerms/appliedQueries? Ie is this&lt;br/&gt;
just an optimization so that if the caller deletes by the Term/Query again&lt;br/&gt;
we know to skip it? &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes to the 2nd question. Why would we want to try deleting multiple times?&lt;br/&gt;
The cost is the terms dictionary lookup which you&apos;re saying is in the&lt;br/&gt;
noise? I think potentially cracking open a query again could be costly in&lt;br/&gt;
cases where the query is indeed expensive.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;not iterate through the terms/queries to subtract the RAM&lt;br/&gt;
used?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Well, the RAM usage tracking can&apos;t be completely defined until we finish&lt;br/&gt;
how we&apos;re storing the terms/queries. &lt;/p&gt;</comment>
                    <comment id="12933067" author="mikemccand" created="Wed, 17 Nov 2010 18:25:38 +0000"  >
&lt;blockquote&gt;
&lt;p&gt;Deletes flushed can be removed if we store the docid-upto per segment.&lt;br/&gt;
Then we&apos;ll go back to having a hash map of deletes.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think we should do this?&lt;/p&gt;

&lt;p&gt;Ie, each flushed segment stores the map of del Term/Query to&lt;br/&gt;
docid-upto, where that docid-upto is private to the segment (no&lt;br/&gt;
remapping on merges needed).&lt;/p&gt;

&lt;p&gt;When it&apos;s time to apply deletes to about-to-be-merged segments, we&lt;br/&gt;
must apply all &quot;future&quot; segments deletions unconditionally to each&lt;br/&gt;
segment, and then conditionally (respecting the local docid-upto)&lt;br/&gt;
apply that segment&apos;s deletions.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Intuitively, yes, however here&apos;s the constructor of hash set:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;public HashSet() { map = new HashMap&amp;lt;E,Object&amp;gt;(); }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/blockquote&gt;

&lt;p&gt;Ugh I forgot about that.  Is that still true?  That&apos;s awful.&lt;/p&gt;

&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;why are we tracking the last segment info/index?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I thought last segment was supposed to be used to mark the last segment of&lt;br/&gt;
a commit/flush. This way we save on the hash(set,map) space on the&lt;br/&gt;
segments upto the last segment when the commit occurred.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Hmm... I think lastSegment was needed only for the multiple DWPT&lt;br/&gt;
case, to record the last segment already flushed in the index as of&lt;br/&gt;
when that DWPT was created.  This is so we know &quot;going back&quot; when we&lt;br/&gt;
can start unconditionally apply the buffered delete term.&lt;/p&gt;

&lt;p&gt;With the single DWPT we effectively have today isn&apos;t last segment&lt;br/&gt;
always going to be what we just flushed?  (Or null if we haven&apos;t yet&lt;br/&gt;
done a flush in the current session).&lt;/p&gt;

&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;Do we really need to track appliedTerms/appliedQueries? Ie is this just an optimization so that if the caller deletes by the Term/Query again we know to skip it?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes to the 2nd question. Why would we want to try deleting multiple times?&lt;br/&gt;
The cost is the terms dictionary lookup which you&apos;re saying is in the&lt;br/&gt;
noise? I think potentially cracking open a query again could be costly in&lt;br/&gt;
cases where the query is indeed expensive.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I&apos;m saying this is unlikely to be worthwhile way to spend RAM.&lt;/p&gt;

&lt;p&gt;EG most apps wouldn&apos;t delete by same term again, like they&apos;d&lt;br/&gt;
&quot;typically&quot; go and process a big batch of docs, deleting by an id&lt;br/&gt;
field and adding the new version of the doc, where a given id is seen&lt;br/&gt;
only once in this session, and then IW is committed/closed?&lt;/p&gt;</comment>
                    <comment id="12933082" author="jasonrutherglen" created="Wed, 17 Nov 2010 19:01:34 +0000"  >&lt;p&gt;DWPT deletes has perhaps confused this issue a little bit. &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Tracking per-segment would be easier but I worry about indices that&lt;br/&gt;
have large numbers of segments... eg w/ a large mergeFactor and frequent&lt;br/&gt;
flushing you can get very many segments.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think we may be back tracking here as I had earlier proposed we simply&lt;br/&gt;
store each term/query in a map per segment, however I think that was nixed&lt;br/&gt;
in favor of last segment + deletes per segment afterwards. We&apos;re not&lt;br/&gt;
worried about the cost of storing pending deletes in a map per segment&lt;br/&gt;
anymore?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;With the single DWPT we effectively have today isn&apos;t last segment&lt;br/&gt;
always going to be what we just flushed? (Or null if we haven&apos;t yet done a&lt;br/&gt;
flush in the current session).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Pretty much. &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;EG most apps wouldn&apos;t delete by same term again, like they&apos;d&lt;br/&gt;
&quot;typically&quot; go and process a big batch of docs, deleting by an id field&lt;br/&gt;
and adding the new version of the doc, where a given id is seen only once&lt;br/&gt;
in this session, and then IW is committed/closed?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;In an extreme RT app that uses Lucene like a database, it could in fact&lt;br/&gt;
update a doc many times, then we&apos;d start accumulating and deleting the&lt;br/&gt;
same ID over and over again. However in the straight batch indexing model&lt;br/&gt;
outlined, that is unlikely to happen. &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;When it&apos;s time to apply deletes to about-to-be-merged segments, we&lt;br/&gt;
must apply all &quot;future&quot; segments deletions unconditionally to each&lt;br/&gt;
segment, and then conditionally (respecting the local docid-upto) apply&lt;br/&gt;
that segment&apos;s deletions.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I&apos;ll use this as the go-ahead design then.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Is that still true?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;That&apos;s from Java 1.6.&lt;/p&gt;</comment>
                    <comment id="12933182" author="jasonrutherglen" created="Wed, 17 Nov 2010 21:54:30 +0000"  >&lt;p&gt;Additionally we need to decide how accounting&apos;ll work for&lt;br/&gt;
maxBufferedDeleteTerms. We won&apos;t have a centralized place to keep track of&lt;br/&gt;
the number of terms, and the unique term count in aggregate over many&lt;br/&gt;
segments could be a little too time consuming calculate in a method like&lt;br/&gt;
doApplyDeletes. An alternative is to maintain a global unique term count,&lt;br/&gt;
such that when a term is added, every other per-segment deletes is checked&lt;br/&gt;
for that term, and if it&apos;s not already been tallied, we increment the number&lt;br/&gt;
of buffered terms.&lt;/p&gt;</comment>
                    <comment id="12933227" author="mikemccand" created="Wed, 17 Nov 2010 23:12:04 +0000"  >&lt;blockquote&gt;
&lt;p&gt;I think we may be back tracking here as I had earlier proposed we simply&lt;br/&gt;
store each term/query in a map per segment, however I think that was nixed&lt;br/&gt;
in favor of last segment + deletes per segment afterwards. We&apos;re not&lt;br/&gt;
worried about the cost of storing pending deletes in a map per segment&lt;br/&gt;
anymore?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;OK sorry now I remember.&lt;/p&gt;

&lt;p&gt;Hmm but, my objection then was to carrying all deletes backward to all&lt;br/&gt;
segments?&lt;/p&gt;

&lt;p&gt;Whereas now I think what we can do is only record the deletions that&lt;br/&gt;
were added when that segment was a RAM buffer, in its pending deletes&lt;br/&gt;
map?  This should be fine, since we aren&apos;t storing a single deletion&lt;br/&gt;
in multiple places (well, until DWPTs anyway).  It&apos;s just that on&lt;br/&gt;
applying deletes to a segment because it&apos;s about to be merged we have&lt;br/&gt;
to do a merge sort of the buffered deletes all &quot;future&quot; segments.&lt;/p&gt;

&lt;p&gt;BTW it could also be possible to not necessarily apply deletes when a&lt;br/&gt;
segment is merged; eg if there are few enough deletes it may not be&lt;br/&gt;
worthwhile.  But we can leave that to another issue.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Additionally we need to decide how accounting&apos;ll work for&lt;br/&gt;
maxBufferedDeleteTerms. We won&apos;t have a centralized place to keep track of&lt;br/&gt;
the number of terms, and the unique term count in aggregate over many&lt;br/&gt;
segments could be a little too time consuming calculate in a method like&lt;br/&gt;
doApplyDeletes. An alternative is to maintain a global unique term count,&lt;br/&gt;
such that when a term is added, every other per-segment deletes is checked&lt;br/&gt;
for that term, and if it&apos;s not already been tallied, we increment the number&lt;br/&gt;
of buffered terms.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Maybe we should change the definition to be total number of pending&lt;br/&gt;
delete term/queries?  (Ie, not dedup&apos;d across segments).  This seems&lt;br/&gt;
reasonable since w/ this new approach the RAM consumed is in&lt;br/&gt;
proportion to that total number and not to dedup&apos;d count?&lt;/p&gt;</comment>
                    <comment id="12933299" author="jasonrutherglen" created="Thu, 18 Nov 2010 02:45:23 +0000"  >&lt;blockquote&gt;&lt;p&gt;Maybe we should change the definition to be total number of pending&lt;br/&gt;
delete term/queries? &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Lets go with this, as even though we could record the total unique term&lt;br/&gt;
count, the approach outlined is more conservative.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I think what we can do is only record the deletions that were added&lt;br/&gt;
when that segment was a RAM buffer, in its pending deletes map&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Ok, sounds like a design that&apos;ll work well.&lt;/p&gt;</comment>
                    <comment id="12933305" author="jasonrutherglen" created="Thu, 18 Nov 2010 03:29:04 +0000"  >&lt;p&gt;Flush deletes equals true means that all deletes are applied, however when it&apos;s false, that means we&apos;re moving the pending deletes into the newly flushed segment, as is, with no docId-upto remapping.  &lt;/p&gt;</comment>
                    <comment id="12933306" author="jasonrutherglen" created="Thu, 18 Nov 2010 03:42:21 +0000"  >&lt;p&gt;We can &quot;upgrade&quot; to an int[] from an ArrayList&amp;lt;Integer&amp;gt; for the aborted docs.&lt;/p&gt;</comment>
                    <comment id="12934058" author="jasonrutherglen" created="Fri, 19 Nov 2010 23:57:34 +0000"  >&lt;p&gt;I&apos;m seeing the following error which is probably triggered by the new per-segment deletes code, however also could be related to the recent CFS format changes?&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
MockDirectoryWrapper: cannot close: there are still open files: {_0.cfs=1, _1.cfs=1}
    [junit] java.lang.RuntimeException: MockDirectoryWrapper: cannot close: there are still open files: {_0.cfs=1, _1.cfs=1}
    [junit] 	at org.apache.lucene.store.MockDirectoryWrapper.close(MockDirectoryWrapper.java:395)
    [junit] 	at org.apache.lucene.index.TestIndexReader.testReopenChangeReadonly(TestIndexReader.java:1717)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:921)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:859)
    [junit] Caused by: java.lang.RuntimeException: unclosed IndexInput
    [junit] 	at org.apache.lucene.store.MockDirectoryWrapper.openInput(MockDirectoryWrapper.java:350)
    [junit] 	at org.apache.lucene.store.Directory.openInput(Directory.java:138)
    [junit] 	at org.apache.lucene.index.CompoundFileReader.&amp;lt;init&amp;gt;(CompoundFileReader.java:67)
    [junit] 	at org.apache.lucene.index.SegmentReader$CoreReaders.&amp;lt;init&amp;gt;(SegmentReader.java:121)
    [junit] 	at org.apache.lucene.index.SegmentReader.get(SegmentReader.java:527)
    [junit] 	at org.apache.lucene.index.IndexWriter$ReaderPool.get(IndexWriter.java:628)
    [junit] 	at org.apache.lucene.index.IndexWriter$ReaderPool.get(IndexWriter.java:603)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.applyDeletes(DocumentsWriter.java:1081)
    [junit] 	at org.apache.lucene.index.IndexWriter.applyDeletesAll(IndexWriter.java:4300)
    [junit] 	at org.apache.lucene.index.IndexWriter.doFlushInternal(IndexWriter.java:3440)
    [junit] 	at org.apache.lucene.index.IndexWriter.doFlush(IndexWriter.java:3276)
    [junit] 	at org.apache.lucene.index.IndexWriter.flush(IndexWriter.java:3266)
    [junit] 	at org.apache.lucene.index.IndexWriter.prepareCommit(IndexWriter.java:3131)
    [junit] 	at org.apache.lucene.index.IndexWriter.commit(IndexWriter.java:3206)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                    <comment id="12934372" author="jasonrutherglen" created="Mon, 22 Nov 2010 00:19:53 +0000"  >&lt;p&gt;In trying to implement the per-segment deletes I encountered this error&lt;br/&gt;
from TestStressIndexing2. So I started over with a new checkout of trunk,&lt;br/&gt;
and started slowly adding in the per-segment code, running&lt;br/&gt;
TestStressIndexing2 as each part was added. The attached patch breaks,&lt;br/&gt;
though the deletes are still using the old code. There&apos;s clearly some kind&lt;br/&gt;
of synchronization issue, though nothing esoteric has been added, yikes.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
[junit] Testcase: testMultiConfigMany(org.apache.lucene.index.TestStressIndexing2):	FAILED
    [junit] expected:&amp;lt;20&amp;gt; but was:&amp;lt;19&amp;gt;
    [junit] junit.framework.AssertionFailedError: expected:&amp;lt;20&amp;gt; but was:&amp;lt;19&amp;gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                    <comment id="12934381" author="jasonrutherglen" created="Mon, 22 Nov 2010 01:58:28 +0000"  >&lt;p&gt;Running TestStressIndexing2 500 times on trunk causes this error which is probably intermittent:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
[junit] Testsuite: org.apache.lucene.index.TestStressIndexing2
    [junit] Testcase: testMultiConfigMany(org.apache.lucene.index.TestStressIndexing2):	Caused an ERROR
    [junit] Array index out of range: 0
    [junit] java.lang.ArrayIndexOutOfBoundsException: Array index out of range: 0
    [junit] 	at java.util.Vector.get(Vector.java:721)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.applyDeletes(DocumentsWriter.java:1049)
    [junit] 	at org.apache.lucene.index.IndexWriter.applyDeletes(IndexWriter.java:4291)
    [junit] 	at org.apache.lucene.index.IndexWriter.doFlushInternal(IndexWriter.java:3444)
    [junit] 	at org.apache.lucene.index.IndexWriter.doFlush(IndexWriter.java:3279)
    [junit] 	at org.apache.lucene.index.IndexWriter.flush(IndexWriter.java:3269)
    [junit] 	at org.apache.lucene.index.IndexWriter.closeInternal(IndexWriter.java:1760)
    [junit] 	at org.apache.lucene.index.IndexWriter.close(IndexWriter.java:1723)
    [junit] 	at org.apache.lucene.index.IndexWriter.close(IndexWriter.java:1687)
    [junit] 	at org.apache.lucene.index.TestStressIndexing2.indexRandom(TestStressIndexing2.java:233)
    [junit] 	at org.apache.lucene.index.TestStressIndexing2.testMultiConfig(TestStressIndexing2.java:123)
    [junit] 	at org.apache.lucene.index.TestStressIndexing2.testMultiConfigMany(TestStressIndexing2.java:97)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:950)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:888)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                    <comment id="12934384" author="jasonrutherglen" created="Mon, 22 Nov 2010 02:10:33 +0000"  >&lt;p&gt;The above isn&apos;t on trunk, I misread the screen.&lt;/p&gt;</comment>
                    <comment id="12934388" author="jasonrutherglen" created="Mon, 22 Nov 2010 02:50:55 +0000"  >&lt;p&gt;I&apos;ve isolated the mismatch in num docs between the CMS vs. SMS generated&lt;br/&gt;
indexes to applying the deletes to the merging segments (whereas currently&lt;br/&gt;
we were/are not applying deletes to merging segments and&lt;br/&gt;
TestStressIndexing2 passes). Assuming the deletes are being applied&lt;br/&gt;
correctly to the merging segments, perhaps the logic of gathering up&lt;br/&gt;
forward segment deletes is incorrect somehow in the concurrent merge case.&lt;br/&gt;
When deletes were held in a map per segment, this test was passing. &lt;/p&gt;</comment>
                    <comment id="12934390" author="jasonrutherglen" created="Mon, 22 Nov 2010 03:25:55 +0000"  >&lt;p&gt;A test to see if the problem is the deletes per-segment go forward logic is to iterate over the deletes flushed map using the docid-upto to stay within the boundaries of the segment(s) being merged.&lt;/p&gt;</comment>
                    <comment id="12935441" author="mikemccand" created="Wed, 24 Nov 2010 18:53:20 +0000"  >&lt;p&gt;What a nice small patch &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.gif&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;I think the getDeletesSegmentsForward shouldn&apos;t be folding in the&lt;br/&gt;
deletesInRAM?  Ie, that newly flushed info will have carried over the&lt;br/&gt;
previous deletes in RAM?&lt;/p&gt;

&lt;p&gt;I think pushDeletes/pushSegmentDeletes should be merged, and we should&lt;br/&gt;
nuke DocumentsWriter.deletesFlushed?  Ie, we should push directly from&lt;br/&gt;
deletesInRAM to the new SegmentInfo?  EG you are now pushing all&lt;br/&gt;
deletesFlushed into the new SegmentInfo when actually you should only&lt;br/&gt;
push the deletes for that one segment.&lt;/p&gt;

&lt;p&gt;We shouldn&apos;t do the remap deletes anymore.  We can remove&lt;br/&gt;
DocumentsWriter.get/set/updateFlushedDocCount too.&lt;/p&gt;

&lt;p&gt;Hmm... so what are we supposed to do if someone opens IW, does a bunch&lt;br/&gt;
of deletes, then commits?  Ie flushDocs is false, so there&apos;s no new&lt;br/&gt;
SegmentInfo.  I think in this case we can stick the deletes against&lt;br/&gt;
the last segment in the index, with the docidUpto set to the maxDoc()&lt;br/&gt;
of that segment?&lt;/p&gt;</comment>
                    <comment id="12964432" author="jasonrutherglen" created="Sat, 27 Nov 2010 19:44:21 +0000"  >&lt;ul&gt;
	&lt;li&gt;I added pushDeletesLastSegment to doc writer&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Deletes flushed is gone, only deletesInRAM exists&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;In the apply merge deletes case, won&apos;t we want to add deletesInRAM in&lt;br/&gt;
the getForwardDeletes method?&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;The TestStressIndexing2 test still fails so there is still something&lt;br/&gt;
incorrect.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Though for the failing unit test it does not matter, we need to figure&lt;br/&gt;
out a solution for the pending doc ids deletions, eg, they can&apos;t simply&lt;br/&gt;
transferred around, they probably need to be applied as soon as possible.&lt;br/&gt;
Otherwise they require remapping. &lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="12964441" author="mikemccand" created="Sat, 27 Nov 2010 19:54:25 +0000"  >&lt;blockquote&gt;&lt;p&gt;In the apply merge deletes case, won&apos;t we want to add deletesInRAM in the getForwardDeletes method?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;No, we can&apos;t add those deletes until the current buffered segment is successfully flushed.&lt;/p&gt;

&lt;p&gt;Eg, say the segment hits a disk full on flush, and DocsWriter aborts (discards all buffered docs/deletions from that segment).  If we included these deletesInRAM when applying deletes then suddenly the app will see that some deletes were applied yet the added documents were not.  So on disk full during flush, calls to .updateDocument may wind up deleting the old doc but not adding the new one.&lt;/p&gt;

&lt;p&gt;So we need to keep them segregated for proper error case semantics.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Though for the failing unit test it does not matter, we need to figure&lt;br/&gt;
out a solution for the pending doc ids deletions, eg, they can&apos;t simply&lt;br/&gt;
transferred around, they probably need to be applied as soon as possible.&lt;br/&gt;
Otherwise they require remapping.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Hmm why must we remap?  Can&apos;t we carry these buffered deleteByDocIDs along with the segment?  The docIDs would be the segment&apos;s docIDs (ie no base added) so no shifting is needed?&lt;/p&gt;

&lt;p&gt;These deleted docIDs would only apply to the current segment, ie would not be included in getForwardDeletes?&lt;/p&gt;</comment>
                    <comment id="12964447" author="jasonrutherglen" created="Sat, 27 Nov 2010 20:08:01 +0000"  >&lt;p&gt;I made the changes listed above, ie, docids and deletesInRAM aren&apos;t included in getForwardDeletes.  However TestStressIndexing2 still fails, and the numbers are still off significantly which probably indicates it&apos;s not a synchronization issue.&lt;/p&gt;</comment>
                    <comment id="12964450" author="mikemccand" created="Sat, 27 Nov 2010 20:33:55 +0000"  >&lt;p&gt;So nice to see remapDeletes deleted!&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;Don&apos;t forget to remove DocumentsWriter.get/set/updateFlushedDocCount too.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Can you move the deletes out of SegmentInfo?  We can just use a&lt;br/&gt;
    Map&amp;lt;SegmentInfo,BufferedDeletes&amp;gt;?  But remember to delete segments&lt;br/&gt;
    from the map once we commit the merge...&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;I think DocsWriter shouldn&apos;t hold onto the SegmentInfos; we should&lt;br/&gt;
    pass it in to only those methods that need it.  That SegmentInfos&lt;br/&gt;
    is protected under IW&apos;s monitor so it makes me nervous if it&apos;s&lt;br/&gt;
    also a member on DW.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Hmm we&apos;re no longer accounting for RAM usage of per-segment&lt;br/&gt;
    deletes?  I think we need an AtomicInt, which we incr w/ RAM used&lt;br/&gt;
    on pushing deletes into a segment, and decr on clearing?&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;The change to the message(...) in DW.applyDeletes is wrong (ie&lt;br/&gt;
    switching to deletesInRAM); I think we should just remove the&lt;br/&gt;
    details, ie so it says &quot;applying deletes on N segments&quot;?  But then&lt;br/&gt;
    add a more detailed message per-segment with the aggregated&lt;br/&gt;
    (forward) deletes details?&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;I think we should move this delete handling out of DW as much as&lt;br/&gt;
    possible... that&apos;s really IW&apos;s role (DW is &quot;about&quot; flushing the&lt;br/&gt;
    next segment, not tracking details associated with all other&lt;br/&gt;
    segments in the index)&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Instead of adding pushDeletesLastSegment, can we just have IW call&lt;br/&gt;
    pushDeletes(lastSegmentInfo)?&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Calling .getForwardDeletes inside the for loop iterating over the&lt;br/&gt;
    infos is actually O(N^2) cost, and it could matter for&lt;br/&gt;
    delete-intensive many-segment indices.  Can you change this,&lt;br/&gt;
    instead, to walk the infos backwards, incrementally building up&lt;br/&gt;
    the forward deletes to apply to each segment by adding in that&lt;br/&gt;
    infos deletions?&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    <comment id="12964606" author="jasonrutherglen" created="Sun, 28 Nov 2010 22:49:22 +0000"  >&lt;p&gt;I guess you think the sync on doc writer is the cause of the&lt;br/&gt;
TestStressIndexing2 unit test failure?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I think we should move this delete handling out of DW&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I agree, I originally took this approach however unit tests were failing&lt;br/&gt;
when segment infos was passed directly into the apply deletes method(s).&lt;br/&gt;
This&apos;ll be the 2nd time however apparently the 3rd time&apos;s the charm.&lt;/p&gt;

&lt;p&gt;I&apos;ll make the changes and cross my fingers.&lt;/p&gt;</comment>
                    <comment id="12964610" author="jasonrutherglen" created="Mon, 29 Nov 2010 00:15:26 +0000"  >&lt;p&gt;I started on taking the approach of moving deletes to a SegmentDeletes class&lt;br/&gt;
that&apos;s a member of IW. Removing DW&apos;s addDeleteTerm is/was fairly trivial. &lt;/p&gt;

&lt;p&gt;In moving deletes out of DW, how should we handle the bufferDeleteTerms sync on&lt;br/&gt;
DW and the containing waitReady? The purpose of BDT is to check if RAM&lt;br/&gt;
consumption has reached it&apos;s peak, and if so, balance out the ram usage and/or&lt;br/&gt;
flush pending deletes that are ram consuming. This is probably why deletes are&lt;br/&gt;
intertwined with DW. We could change DW&apos;s BDT method though I&apos;m loathe to&lt;br/&gt;
change the wait logic of DW for fear of causing a ripple effect of inexplicable&lt;br/&gt;
unit test failures elsewhere.&lt;/p&gt;</comment>
                    <comment id="12964680" author="mikemccand" created="Mon, 29 Nov 2010 10:30:28 +0000"  >&lt;blockquote&gt;
&lt;p&gt;I guess you think the sync on doc writer is the cause of the&lt;br/&gt;
TestStressIndexing2 unit test failure?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I&apos;m not sure what&apos;s causing the failure, but, I think getting the net approach roughly right is the first goal, and then we see what&apos;s failing.&lt;/p&gt;

&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;I think we should move this delete handling out of DW&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I agree, I originally took this approach however unit tests were failing&lt;br/&gt;
when segment infos was passed directly into the apply deletes method(s).&lt;br/&gt;
This&apos;ll be the 2nd time however apparently the 3rd time&apos;s the charm.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Not only moving the SegmentInfos out of DW as a member, but also move all the applyDeletes logic out.  Ie it should be IW that pulls readers from the pool, walks the merged del term/queries/per-seg docIDs and actually does the deletion.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;In moving deletes out of DW, how should we handle the bufferDeleteTerms sync on DW and the containing waitReady?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think all the bufferDeleteX would move into IW, and timeToFlushDeletes. The RAM accounting can be done fully inside IW.&lt;/p&gt;

&lt;p&gt;The waitReady(null) is there so that DW.pauseAllThreads also pauses any threads doing deletions.  But, in moving these methods to IW, we&apos;d make them sync on IW (they are now sync&apos;d on DW), which takes care of pausing these threads.&lt;/p&gt;</comment>
                    <comment id="12964938" author="jasonrutherglen" created="Mon, 29 Nov 2010 20:58:35 +0000"  >&lt;blockquote&gt;&lt;p&gt;The waitReady(null) is there so that DW.pauseAllThreads also pauses any threads doing deletions&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;waitReady is used in getThreadState as well as bufferDeleteX, we may need to redundantly add it to SegmentDeletes?  Maybe not.  We&apos;ll be sync&apos;ing on IW when adding deletions, that seems like it&apos;ll be OK.  &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;in moving these methods to IW, we&apos;d make them sync on IW (they are now sync&apos;d on DW), which takes care of pausing these threads&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Because we&apos;re sync&apos;ing on IW we don&apos;t need to pause the indexing threads?  Ok this is because doFlushX is sync&apos;d on IW.  &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;The RAM accounting can be done fully inside IW.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Well, inside of SegmentDeletes.&lt;/p&gt;</comment>
                    <comment id="12965352" author="jasonrutherglen" created="Tue, 30 Nov 2010 19:12:33 +0000"  >&lt;p&gt;This patch separates out most of the deletes storage and processing into a&lt;br/&gt;
SegmentDeletes class. The segments are walked backwards to coalesce the pending&lt;br/&gt;
deletions. I think/hope this logic is correct. &lt;/p&gt;

&lt;p&gt;Update document is a bit tricky as we cannot sync on IW to insure correctness&lt;br/&gt;
of del term addition, nor can we really sync inside of DW without probably&lt;br/&gt;
causing deadlock. When I simply delete the doc after adding it in IW,&lt;br/&gt;
TestStressIndexing2 fails miserably with 0 num docs.&lt;/p&gt;</comment>
                    <comment id="12965360" author="jasonrutherglen" created="Tue, 30 Nov 2010 19:24:19 +0000"  >&lt;p&gt;Now we&apos;re returning the DocumentsWriterThreadState to IW to record the exact doc id as the del term limit.  TestStressIndexing2 fails but far less from the mark, eg, 1 when it does and actually passes some of the time.&lt;/p&gt;</comment>
                    <comment id="12965468" author="jasonrutherglen" created="Tue, 30 Nov 2010 22:48:19 +0000"  >&lt;p&gt;Here&apos;s a random guess, I think because with this patch we&apos;re applying deletes&lt;br/&gt;
sometimes multiple times, whereas before we were applying all of them and&lt;br/&gt;
clearing them out at once, there&apos;s a mismatch in terms of over/under-applying&lt;br/&gt;
deletes. Oddly when deletes are performed in _mergeInit on all segments vs.&lt;br/&gt;
only on the segments being merged, the former has a much higher success rate.&lt;br/&gt;
This is strange because all deletes will have been applied by the time&lt;br/&gt;
commit/getreader is called anyways. &lt;/p&gt;</comment>
                    <comment id="12969448" author="mikemccand" created="Wed, 8 Dec 2010 19:44:32 +0000"  >
&lt;p&gt;OK I started from the last patch and iterated quite a bit.&lt;/p&gt;

&lt;p&gt;The per-segment deletes are now working!  All tests pass.  It turned&lt;br/&gt;
out to be a little more hairy than we thought because you also must&lt;br/&gt;
account for deletes that are pushed backwards onto a segment being&lt;br/&gt;
merged (eg due to a flush, or a merge just ahead) while that merge is&lt;br/&gt;
still running.&lt;/p&gt;

&lt;p&gt;I swapped the two classes &amp;#8211; SegmentDeletes tracks deletes for one&lt;br/&gt;
segment, while BufferedDeletes tracks deletes for all segments.&lt;/p&gt;

&lt;p&gt;I also wound up doing a fair amount of cleanup/refactoring on how&lt;br/&gt;
DW/IW interact, I think a good step towards DWPT.  EG IW&apos;s flush is&lt;br/&gt;
now much smaller (could &lt;b&gt;almost&lt;/b&gt; become unsync&apos;d) since I moved&lt;br/&gt;
flushing doc stores, building CFS, etc. down into DW.&lt;/p&gt;

&lt;p&gt;I added a new FlushControl class to manage (external to DW) triggering&lt;br/&gt;
of flushing due to RAM, add doc count, buffered del count.  This way&lt;br/&gt;
DWPT can share the single FlushControl instance in IW.&lt;/p&gt;</comment>
                    <comment id="12969504" author="jasonrutherglen" created="Wed, 8 Dec 2010 21:41:53 +0000"  >&lt;p&gt;When patching there are errors on IndexWriter.&lt;/p&gt;</comment>
                    <comment id="12969693" author="mikemccand" created="Thu, 9 Dec 2010 10:58:42 +0000"  >&lt;p&gt;Woops, sorry about that Jason &amp;#8211; I wasn&apos;t fully updated.  Try this one?&lt;/p&gt;</comment>
                    <comment id="12969754" author="jasonrutherglen" created="Thu, 9 Dec 2010 15:02:58 +0000"  >&lt;p&gt;We&apos;re close, I think SegmentDeletes is missing?&lt;/p&gt;</comment>
                    <comment id="12969763" author="mikemccand" created="Thu, 9 Dec 2010 15:14:40 +0000"  >&lt;p&gt;Ugh, OK new patch.  3rd time&apos;s a charm?&lt;/p&gt;</comment>
                    <comment id="12969799" author="jasonrutherglen" created="Thu, 9 Dec 2010 16:38:25 +0000"  >&lt;p&gt;The patch applied.&lt;/p&gt;

&lt;p&gt;Ok, a likely cause of the TestStressIndexing2 failures was that when we&apos;re&lt;br/&gt;
flushing deletes to the last segment (because a segment isn&apos;t being flushed),&lt;br/&gt;
we needed to move deletes also to the newly merged segment?&lt;/p&gt;

&lt;p&gt;In the patch we&apos;ve gone away from sync&apos;ing on IW when deleting, which was a&lt;br/&gt;
challenge because we needed the sync on DW to properly wait on flushing threads&lt;br/&gt;
etc.&lt;/p&gt;</comment>
                    <comment id="12969822" author="jasonrutherglen" created="Thu, 9 Dec 2010 17:20:41 +0000"  >&lt;p&gt;All tests pass.&lt;/p&gt;</comment>
                    <comment id="12969914" author="mikemccand" created="Thu, 9 Dec 2010 20:08:00 +0000"  >&lt;blockquote&gt;
&lt;p&gt;Ok, a likely cause of the TestStressIndexing2 failures was that when we&apos;re&lt;br/&gt;
flushing deletes to the last segment (because a segment isn&apos;t being flushed),&lt;br/&gt;
we needed to move deletes also to the newly merged segment?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Right, and also the case where a merge just-ahead of you kicks off and dumps its merged deletes onto you.&lt;/p&gt;</comment>
                    <comment id="12970446" author="mikemccand" created="Sat, 11 Dec 2010 11:09:42 +0000"  >&lt;p&gt;OK I committed this to trunk.&lt;/p&gt;

&lt;p&gt;Since it&apos;s a biggish change I&apos;ll hold off on back-porting to 3.x for now... let&apos;s let hudson chew on it some first.&lt;/p&gt;</comment>
                    <comment id="13013326" author="gsingers" created="Wed, 30 Mar 2011 16:49:59 +0100"  >&lt;p&gt;Bulk close for 3.1&lt;/p&gt;</comment>
                    <comment id="13138649" author="ralekseenkov" created="Fri, 28 Oct 2011 20:03:42 +0100"  >&lt;p&gt;Hey, is it something that was ported to 3.x, or not really?&lt;/p&gt;</comment>
                    <comment id="13138661" author="rcmuir" created="Fri, 28 Oct 2011 20:14:51 +0100"  >&lt;p&gt;Hi, this was backported since lucene 3.1&lt;/p&gt;</comment>
                    <comment id="13138695" author="ralekseenkov" created="Fri, 28 Oct 2011 20:55:22 +0100"  >&lt;p&gt;thank you, Robert&lt;/p&gt;

&lt;p&gt;I was asking because we are having issues with 3.4.0 where applyDeletes() takes an large amount of time on commit for 150GB index, and this is stopping all indexing threads. it looks like applyDeletes() is re-scanning an entire index, even though it&apos;s unnecessary as we are only adding documents to the index but not deleting them&lt;/p&gt;

&lt;p&gt;if this optimization was backported, then I will probably have to find a solution for my problem elsewhere...&lt;/p&gt;</comment>
                    <comment id="13138700" author="rcmuir" created="Fri, 28 Oct 2011 20:59:29 +0100"  >&lt;blockquote&gt;
&lt;p&gt;even though it&apos;s unnecessary as we are only adding documents to the index but not deleting them&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Hi Roman, i saw your post.&lt;/p&gt;

&lt;p&gt;I think by default when you add a document with unique id X, Solr deletes-by-term of X.&lt;/p&gt;

&lt;p&gt;But I&apos;m pretty sure it has an option (sorry i dont know what it is), where you can tell it &lt;br/&gt;
that you are sure that the documents you are adding are new and it won&apos;t do this.&lt;/p&gt;</comment>
                </comments>
                <issuelinks>
                        <issuelinktype id="10001">
                <name>dependent</name>
                                                <inwardlinks description="is depended upon by">
                            <issuelink>
            <issuekey id="12474550">LUCENE-2655</issuekey>
        </issuelink>
                    </inwardlinks>
                            </issuelinktype>
                    </issuelinks>
                <attachments>
                    <attachment id="12465908" name="LUCENE-2680.patch" size="143982" author="mikemccand" created="Thu, 9 Dec 2010 15:14:40 +0000" />
                    <attachment id="12465894" name="LUCENE-2680.patch" size="126671" author="mikemccand" created="Thu, 9 Dec 2010 10:58:42 +0000" />
                    <attachment id="12465826" name="LUCENE-2680.patch" size="126535" author="mikemccand" created="Wed, 8 Dec 2010 19:44:32 +0000" />
                    <attachment id="12464980" name="LUCENE-2680.patch" size="58111" author="jasonrutherglen" created="Tue, 30 Nov 2010 19:24:19 +0000" />
                    <attachment id="12464979" name="LUCENE-2680.patch" size="57444" author="jasonrutherglen" created="Tue, 30 Nov 2010 19:12:33 +0000" />
                    <attachment id="12464787" name="LUCENE-2680.patch" size="19873" author="jasonrutherglen" created="Sat, 27 Nov 2010 20:08:01 +0000" />
                    <attachment id="12464786" name="LUCENE-2680.patch" size="19944" author="jasonrutherglen" created="Sat, 27 Nov 2010 19:44:21 +0000" />
                    <attachment id="12460150" name="LUCENE-2680.patch" size="9037" author="jasonrutherglen" created="Mon, 22 Nov 2010 00:19:53 +0000" />
                    <attachment id="12459087" name="LUCENE-2680.patch" size="43327" author="jasonrutherglen" created="Mon, 8 Nov 2010 21:29:50 +0000" />
                    <attachment id="12459030" name="LUCENE-2680.patch" size="45464" author="jasonrutherglen" created="Mon, 8 Nov 2010 01:55:25 +0000" />
                    <attachment id="12459028" name="LUCENE-2680.patch" size="44403" author="jasonrutherglen" created="Mon, 8 Nov 2010 01:02:56 +0000" />
                    <attachment id="12459025" name="LUCENE-2680.patch" size="41721" author="jasonrutherglen" created="Sun, 7 Nov 2010 22:34:51 +0000" />
                    <attachment id="12459011" name="LUCENE-2680.patch" size="42706" author="jasonrutherglen" created="Sat, 6 Nov 2010 23:10:45 +0000" />
                    <attachment id="12458859" name="LUCENE-2680.patch" size="45850" author="jasonrutherglen" created="Thu, 4 Nov 2010 22:52:42 +0000" />
                    <attachment id="12458770" name="LUCENE-2680.patch" size="38111" author="jasonrutherglen" created="Wed, 3 Nov 2010 22:48:51 +0000" />
                    <attachment id="12458700" name="LUCENE-2680.patch" size="31200" author="jasonrutherglen" created="Wed, 3 Nov 2010 01:22:17 +0000" />
                    <attachment id="12458565" name="LUCENE-2680.patch" size="33542" author="jasonrutherglen" created="Mon, 1 Nov 2010 17:58:06 +0000" />
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>17.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Sun, 10 Oct 2010 00:57:48 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>11163</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>25012</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                    <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                <customfieldname>Time in Status</customfieldname>
                <customfieldvalues>
                    
                </customfieldvalues>
            </customfield>
                            </customfields>
    </item>
</channel>
</rss>