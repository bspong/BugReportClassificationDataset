<!-- 
RSS generated by JIRA (5.2.8#851-sha1:3262fdc28b4bc8b23784e13eadc26a22399f5d88) at Tue Jul 16 13:32:59 UTC 2013

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/LUCENE-2098/LUCENE-2098.xml?field=key&field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>5.2.8</version>
        <build-number>851</build-number>
        <build-date>26-02-2013</build-date>
    </build-info>

<item>
            <title>[LUCENE-2098] make BaseCharFilter more efficient in performance</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-2098</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;Performance degradation in Solr 1.4 was reported. See:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.lucidimagination.com/search/document/43c4bdaf5c9ec98d/html_stripping_slower_in_solr_1_4&quot; class=&quot;external-link&quot;&gt;http://www.lucidimagination.com/search/document/43c4bdaf5c9ec98d/html_stripping_slower_in_solr_1_4&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The inefficiency has been pointed out in BaseCharFilter javadoc by Mike:&lt;/p&gt;

&lt;div class=&quot;panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;panelContent&quot;&gt;
&lt;p&gt;NOTE: This class is not particularly efficient. For example, a new class instance is created for every call to addOffCorrectMap(int, int), which is then appended to a private list. &lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;</description>
                <environment></environment>
            <key id="12442053">LUCENE-2098</key>
            <summary>make BaseCharFilter more efficient in performance</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png">Closed</status>
                    <resolution id="1">Fixed</resolution>
                                <assignee username="rcmuir">Robert Muir</assignee>
                                <reporter username="koji">Koji Sekiguchi</reporter>
                        <labels>
                    </labels>
                <created>Tue, 1 Dec 2009 06:13:28 +0000</created>
                <updated>Mon, 16 May 2011 19:15:53 +0100</updated>
                    <resolved>Sat, 30 Oct 2010 15:43:36 +0100</resolved>
                            <version>3.1</version>
                                <fixVersion>2.9.4</fixVersion>
                <fixVersion>3.0.3</fixVersion>
                <fixVersion>3.1</fixVersion>
                <fixVersion>4.0-ALPHA</fixVersion>
                                <component>modules/analysis</component>
                        <due></due>
                    <votes>0</votes>
                        <watches>3</watches>
                                                    <comments>
                    <comment id="12845558" author="rcmuir" created="Mon, 15 Mar 2010 22:07:31 +0000"  >&lt;p&gt;i haven&apos;t benchmarked to see if this is any faster, maybe even worse.&lt;/p&gt;

&lt;p&gt;but its no longer a linear algorithm&lt;/p&gt;</comment>
                    <comment id="12845774" author="mikemccand" created="Tue, 16 Mar 2010 09:35:07 +0000"  >&lt;p&gt;Why did this cause Solr to slowdown...?  Did Solr previously have a more efficient impl and then they cutover to Lucene&apos;s?&lt;/p&gt;</comment>
                    <comment id="12845776" author="mikemccand" created="Tue, 16 Mar 2010 09:38:42 +0000"  >&lt;p&gt;Patch looks like it should be a good net/net improvement &amp;#8211; lookups of the offset correction should now be fast (though insertion cost is probably higher &amp;#8211; we create likely 3 new objects (2 ints, one TreeMap$Entry) per insert) but I expect that&apos;s a good tradeoff.&lt;/p&gt;</comment>
                    <comment id="12845785" author="thetaphi" created="Tue, 16 Mar 2010 09:57:17 +0000"  >&lt;blockquote&gt;&lt;p&gt;Why did this cause Solr to slowdown...? Did Solr previously have a more efficient impl and then they cutover to Lucene&apos;s? &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Solr used another Filter in 1.3.&lt;/p&gt;</comment>
                    <comment id="12845788" author="mikemccand" created="Tue, 16 Mar 2010 10:11:35 +0000"  >&lt;p&gt;Ahh ok.&lt;/p&gt;

&lt;p&gt;Probably we should switch to parallel arrays here, to make it very fast... yes this will consume RAM (8 bytes per position, if we keep all of them).&lt;/p&gt;

&lt;p&gt;Really most apps do not need all positions stored, ie, they only need to see typically the current token.  So maybe we could make a filter that takes a &quot;lookbehind size&quot; and it&apos;d only keep that number of mappings cached?  That&apos;d have to be &amp;gt; the max size of any token you may analyze, so hard to bound perfectly, but eg setting this to the max allowed token in IndexWriter would guarantee that we&apos;d never have a miss?&lt;/p&gt;

&lt;p&gt;For analyzers that buffer tokens... they&apos;d have to set this max to infinity, or, ensure they remap the offsets before capturing the token&apos;s state?&lt;/p&gt;</comment>
                    <comment id="12845887" author="rcmuir" created="Tue, 16 Mar 2010 12:46:47 +0000"  >&lt;p&gt;Mark did some quick tests and this patch only seems to make things slower.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Really most apps do not need all positions stored, ie, they only need to see typically the current token.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think this is why it got slower with my patch, in practice it didn&apos;t matter that this thing did &apos;backwards linear lookup&apos; due to this reason?&lt;/p&gt;</comment>
                    <comment id="12845919" author="mikemccand" created="Tue, 16 Mar 2010 14:40:09 +0000"  >&lt;blockquote&gt;&lt;p&gt;I think this is why it got slower with my patch, in practice it didn&apos;t matter that this thing did &apos;backwards linear lookup&apos; due to this reason?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Ahh yes since presumably the test was simply looking up the offsets for the current token...&lt;/p&gt;</comment>
                    <comment id="12845934" author="rcmuir" created="Tue, 16 Mar 2010 15:20:41 +0000"  >&lt;p&gt;I think the best way to proceed would be to make it easy to benchmark &lt;br/&gt;
CharFilters in contrib/benchmark, especially this HTML stripping one.&lt;/p&gt;

&lt;p&gt;Honestly we don&apos;t even know for sure any performance degradation reported&lt;br/&gt;
in the original link is really due to BaseCharFilter yet, so I think we need&lt;br/&gt;
to benchmark and profile.&lt;/p&gt;</comment>
                    <comment id="12903266" author="rcmuir" created="Fri, 27 Aug 2010 07:50:59 +0100"  >&lt;p&gt;ok, i think this one is fixed.&lt;/p&gt;

&lt;p&gt;i ran a loop with the example doc in the tests and tested both removing the object creation and switching to binary search, both help.&lt;/p&gt;

&lt;p&gt;I&apos;d like to commit to trunk and 3x tomorrow.&lt;/p&gt;</comment>
                    <comment id="12903276" author="rcmuir" created="Fri, 27 Aug 2010 08:23:43 +0100"  >&lt;p&gt;here are the files i tested, htmlStripCharFilterTest.html (from the test, 12kb file) and &lt;a href=&quot;http://en.wikipedia.org/wiki/Benjamin_Franklin&quot; class=&quot;external-link&quot;&gt;http://en.wikipedia.org/wiki/Benjamin_Franklin&lt;/a&gt; (360kb file)&lt;/p&gt;

&lt;p&gt;i ran each 3 times:&lt;/p&gt;

&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;file&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;before&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;after&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;htmlStripCharFilterTest.html&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;9709ms,9560ms,9587ms&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;8755ms,8697ms,8708ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;benFranklin.html&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;26877ms,26963ms,26495ms&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;17593ms,17674ms,17694ms&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;


&lt;p&gt;here was the code (crude but i think it shows the point, the larger the files the worse the offset correction was):&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
    Charset charset = Charset.forName(&lt;span class=&quot;code-quote&quot;&gt;&quot;UTF-8&quot;&lt;/span&gt;);
    WhitespaceTokenizer tokenizer = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; WhitespaceTokenizer(Version.LUCENE_CURRENT, &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; StringReader(&quot;&quot;));
    &lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt; startMS = &lt;span class=&quot;code-object&quot;&gt;System&lt;/span&gt;.currentTimeMillis();
    &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; (&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; i = 0; i &amp;lt; 10000; i++) {
      InputStream stream = HTMLStripCharFilterTest.class.getResourceAsStream(&lt;span class=&quot;code-quote&quot;&gt;&quot;htmlStripReaderTest.html&quot;&lt;/span&gt;);
      HTMLStripCharFilter reader = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; HTMLStripCharFilter(CharReader.get(&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; InputStreamReader(stream, charset)));
      tokenizer.reset(reader);
      tokenizer.reset();
      &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; (tokenizer.incrementToken())
        ;
    }
    &lt;span class=&quot;code-object&quot;&gt;System&lt;/span&gt;.out.println(&lt;span class=&quot;code-quote&quot;&gt;&quot;time=&quot;&lt;/span&gt; + (&lt;span class=&quot;code-object&quot;&gt;System&lt;/span&gt;.currentTimeMillis() - startMS));
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                    <comment id="12903411" author="koji" created="Fri, 27 Aug 2010 14:45:32 +0100"  >&lt;p&gt;Patch looks good, Robert!&lt;/p&gt;

&lt;p&gt;I added a check code for currentOff before doing binary search.&lt;/p&gt;</comment>
                    <comment id="12903438" author="rcmuir" created="Fri, 27 Aug 2010 15:55:39 +0100"  >&lt;p&gt;Committed 990161 (trunk) 990167 (3x)&lt;/p&gt;</comment>
                    <comment id="12926558" author="rcmuir" created="Sat, 30 Oct 2010 11:35:30 +0100"  >&lt;p&gt;reopening for potential backport.&lt;/p&gt;

&lt;p&gt;I think this one (n^2) is really buggish territory, the original user reports &amp;gt; 2x slowdown with solr 1.4&lt;/p&gt;

&lt;p&gt;I think the fix is safe, its baked in trunk/3.x a while, and if we have perf bugs like this with no api breaks, &lt;br/&gt;
it makes sense... but if someone objects I won&apos;t backport.&lt;/p&gt;</comment>
                    <comment id="12926563" author="koji" created="Sat, 30 Oct 2010 12:10:22 +0100"  >&lt;p&gt;Robert, please backport.&lt;/p&gt;</comment>
                    <comment id="12926583" author="rcmuir" created="Sat, 30 Oct 2010 15:43:36 +0100"  >&lt;p&gt;Committed revision 1029077 to 3.0.x.&lt;br/&gt;
Committed revision 1029087 to 2.9.x.&lt;/p&gt;

&lt;p&gt;I also cleared up the svn:eol-style problems on these branches, &lt;br/&gt;
if you are merging on windows you shouldn&apos;t see any property conflicts anymore.&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12453239" name="LUCENE-2098.patch" size="3716" author="koji" created="Fri, 27 Aug 2010 14:45:32 +0100" />
                    <attachment id="12453217" name="LUCENE-2098.patch" size="3619" author="rcmuir" created="Fri, 27 Aug 2010 07:50:59 +0100" />
                    <attachment id="12438866" name="LUCENE-2098.patch" size="2207" author="rcmuir" created="Mon, 15 Mar 2010 22:07:31 +0000" />
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>3.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Mon, 15 Mar 2010 22:07:31 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>11681</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310120" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                <customfieldname>Lucene Fields</customfieldname>
                <customfieldvalues>
                        <customfieldvalue key="10121"><![CDATA[New]]></customfieldvalue>
    <customfieldvalue key="10120"><![CDATA[Patch Available]]></customfieldvalue>
    
                </customfieldvalues>
            </customfield>
                                            <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>25627</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                    <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                <customfieldname>Time in Status</customfieldname>
                <customfieldvalues>
                    
                </customfieldvalues>
            </customfield>
                            </customfields>
    </item>
</channel>
</rss>