<!-- 
RSS generated by JIRA (5.2.8#851-sha1:3262fdc28b4bc8b23784e13eadc26a22399f5d88) at Tue Jul 16 13:35:13 UTC 2013

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary add field=key&field=summary to the URL of your request.
For example:
https://issues.apache.org/jira/si/jira.issueviews:issue-xml/LUCENE-505/LUCENE-505.xml?field=key&field=summary
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>5.2.8</version>
        <build-number>851</build-number>
        <build-date>26-02-2013</build-date>
    </build-info>

<item>
            <title>[LUCENE-505] MultiReader.norm() takes up too much memory: norms byte[] should be made into an Object</title>
                <link>https://issues.apache.org/jira/browse/LUCENE-505</link>
                <project id="12310110" key="LUCENE">Lucene - Core</project>
                        <description>&lt;p&gt;MultiReader.norms() is very inefficient: it has to construct a byte array that&apos;s as long as all the documents in every segment.  This doubles the memory requirement for scoring MultiReaders vs. Segment Readers.  Although this is cached, it&apos;s still a baseline of memory that is unnecessary.&lt;/p&gt;

&lt;p&gt;The problem is that the Normalization Factors are passed around as a byte[].  If it were instead replaced with an Object, you could perform a whole host of optimizations&lt;br/&gt;
a.  When reading, you wouldn&apos;t have to construct a &quot;fakeNorms&quot; array of all 1.0fs.  You could instead return a singleton object that would just return 1.0f.&lt;br/&gt;
b.  MultiReader could use an object that could delegate to NormFactors of the subreaders&lt;br/&gt;
c.  You could write an implementation that could use mmap to access the norm factors.  Or if the index isn&apos;t long lived, you could use an implementation that reads directly from the disk.&lt;/p&gt;

&lt;p&gt;The patch provided here replaces the use of byte[] with a new abstract class called NormFactors.  &lt;br/&gt;
NormFactors has two methods on it&lt;br/&gt;
    public abstract byte getByte(int doc) throws IOException;  // Returns the byte&lt;span class=&quot;error&quot;&gt;&amp;#91;doc&amp;#93;&lt;/span&gt;&lt;br/&gt;
    public float getFactor(int doc) throws IOException;            // Calls Similarity.decodeNorm(getByte(doc))&lt;/p&gt;

&lt;p&gt;There are four implementations of this abstract class&lt;br/&gt;
1.  NormFactors.EmptyNormFactors - This replaces the fakeNorms with a singleton that only returns 1.0&lt;br/&gt;
2.  NormFactors.ByteNormFactors - Converts a byte[] to a NormFactors for backwards compatibility in constructors.&lt;br/&gt;
3.  MultiNormFactors - Multiplexes the NormFactors in MultiReader to prevent the need to construct the gigantic norms array.&lt;br/&gt;
4.  SegmentReader.Norm - Same class, but now extends NormFactors to provide the same access.&lt;/p&gt;

&lt;p&gt;In addition, Many of the Query and Scorer classes were changes to pass around NormFactors instead of byte[], and to call getFactor() instead of using the byte[].  I have kept around IndexReader.norms(String) for backwards compatibiltiy, but marked it as deprecated.  I believe that the use of ByteNormFactors in IndexReader.getNormFactors() will keep backward compatibility with other IndexReader implementations, but I don&apos;t know how to test that.&lt;/p&gt;</description>
                <environment>&lt;p&gt;Patch is against Lucene 1.9 trunk (as of Mar 1 06)&lt;/p&gt;</environment>
            <key id="12329630">LUCENE-505</key>
            <summary>MultiReader.norm() takes up too much memory: norms byte[] should be made into an Object</summary>
                <type id="4" iconUrl="https://issues.apache.org/jira/images/icons/issuetypes/improvement.png">Improvement</type>
                                <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.png">Minor</priority>
                    <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png">Closed</status>
                    <resolution id="1">Fixed</resolution>
                                <assignee username="-1">Unassigned</assignee>
                                <reporter username="tamm">Steven Tamm</reporter>
                        <labels>
                    </labels>
                <created>Thu, 2 Mar 2006 06:28:38 +0000</created>
                <updated>Tue, 25 Jan 2011 15:18:11 +0000</updated>
                    <resolved>Tue, 25 Jan 2011 15:18:11 +0000</resolved>
                            <version>2.0.0</version>
                                <fixVersion>2.9</fixVersion>
                                <component>core/index</component>
                        <due></due>
                    <votes>3</votes>
                        <watches>4</watches>
                                                    <comments>
                    <comment id="12368366" author="tamm" created="Thu, 2 Mar 2006 06:29:44 +0000"  >&lt;p&gt;This patch doesn&apos;t include my previous change to TermScorer.   It passes all of the lucene unit tests in addition to our set of tests.&lt;/p&gt;</comment>
                    <comment id="12368374" author="tamm" created="Thu, 2 Mar 2006 06:57:22 +0000"  >&lt;p&gt;Sorry, I didn&apos;t remove whitespace in the previous patch.  This one&apos;s easier to read.&lt;/p&gt;

&lt;p&gt;svn diff --diff-cmd diff -x &quot;-b -u&quot; works better than svn diff --diff-cmd diff -x -b -x -u&lt;/p&gt;</comment>
                    <comment id="12368378" author="yseeley@gmail.com" created="Thu, 2 Mar 2006 07:15:11 +0000"  >&lt;p&gt;&amp;gt; MultiReader.norms() is very inefficient: it has to construct a byte array that&apos;s as long as all the documents in every&lt;br/&gt;
&amp;gt; segment. This doubles the memory requirement for scoring MultiReaders vs. Segment Readers.&lt;/p&gt;

&lt;p&gt;Are you positive?  It shouldn&apos;t.  MultiReader.norms(field) does not call subReader.norms(field), it calls&lt;br/&gt;
norms(String field, byte[] result, int offset) that puts the results directly in the norm array without causing it to be cached in the subReader.&lt;/p&gt;

&lt;p&gt;Of course if you call norms() on both the MultiReader and subReaders yourself, then things will be doubly cached.&lt;/p&gt;

&lt;p&gt;What was the performance impact of your patches?&lt;/p&gt;</comment>
                    <comment id="12368381" author="tamm" created="Thu, 2 Mar 2006 07:24:18 +0000"  >&lt;p&gt;I made the change less for MultiReader, but to prevent the instantiation of the fakeNorms array (which is an extra 1MB of useless memory for us).  In addition, we don&apos;t have long lived indexes, so keeping the index loading memory consumption down is critical.  And being able to avoid all byte[] in the future is a necessity for us.&lt;/p&gt;

&lt;p&gt;You are correct that it won&apos;t help MultiReader.norms() that much, unless you are also calling doSetNorm (where upon you get the double instantiation, since the subreader will cache its norms as well). &lt;/p&gt;</comment>
                    <comment id="12368382" author="cutting" created="Thu, 2 Mar 2006 07:24:37 +0000"  >&lt;p&gt;I don&apos;t see how the memory requirements of MultiReader are twice that of SegmentReader.  MultiReader does not call norms(String) on each sub-reader, but rather norms(String, byte[], int), storing them in a previously allocated array, so the sub-reader normally never constructs an array for its norms.&lt;/p&gt;

&lt;p&gt;I also worry about performance with this change.  Have you benchmarked this while searching large indexes?  For example, in TermScorer.score(HitCollector, int), Lucene&apos;s innermost loop, you change two array accesses into a call to an interface.  That could make a substantial difference.  Small changes to that method can cause significant performance changes.&lt;/p&gt;

&lt;p&gt;The biggest advantage of this to my eye is the removal of fakeNorms, but I think those are only rarely used, and even those uses can be eliminated.  One can now omit norms when indexing, and, if such a field is searched with a normal query then fakeNorms will be used.  But a ConstantScoringQuery of the field should return the same results, and faster too!  So the bug to fix is that, when a query is run against a field with omitted norms it should automatically be rewritten as a ConstantScoringQuery, both for speed and to avoid allocating fakeNorms.&lt;/p&gt;

&lt;p&gt;Finally, a note for other committers: we should try not to deprecate anything in Lucene until we finish removing all of the methods that were deprecated in 1.9, to minimize confusion.  Ideally we can avoid having anything deprecated until after 2.0 is out the door.&lt;/p&gt;</comment>
                    <comment id="12368389" author="tamm" created="Thu, 2 Mar 2006 08:00:43 +0000"  >&lt;p&gt;&amp;gt; I also worry about performance with this change. Have you benchmarked this while searching large indexes?&lt;br/&gt;
yes.  see below.  &lt;/p&gt;

&lt;p&gt;&amp;gt; For example, in TermScorer.score(HitCollector, int), Lucene&apos;s innermost loop, you change two array accesses into a call to an interface. That could make a substantial difference. Small changes to that method can cause significant performance changes. &lt;/p&gt;

&lt;p&gt;Specifically &quot;you change two array accesses into a call to an interface.&quot;  I have changed two byte array references (one of which is static), to a method call on an abstract class.  I&apos;m using JDK 1.5.0_06.  Hotspot inlines both calls and performance was about the same with a 1M docs index (we have a low term/doc ratio, so we have about 8.5M terms).  HPROF doesn&apos;t even see the call to Similarity.decodeNorm.  If I was using JDK 1.3, I&apos;d probably agree with you, but HotSpot is very good at figuring this stuff out and autoinlining the calls.&lt;/p&gt;

&lt;p&gt;As for the numbers: an average request returning 5000 hits from our 0.5G index was at ~485ms average on my box before.  It&apos;s now at ~480ms.  (50 runs each).  Most of that is overhead, granted.  &lt;/p&gt;

&lt;p&gt;The increase in performance may be obscured by my other change in TermScorer (&lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-502&quot; title=&quot;TermScorer caches values unnecessarily&quot;&gt;&lt;del&gt;LUCENE-502&lt;/del&gt;&lt;/a&gt;).  I&apos;m not sure of the history of TermScorer, but it seems heavily optimized for a Large # Terms/Document.  We have a low # Terms/Document, so performance suffers greatly..  Performance was dramatically improved by not unnecessarily caching things.   TermScorer seems to be heavily optimized for a non-modern VM (like inlining next() into score(), caching result of Math.sqrt for each term being queried, having a doc/freq cache that provides no benefit unless iterating backwards, etc).  The total of the term scorer changes brought the average down from ~580ms. &lt;/p&gt;

&lt;p&gt;Since we use a lot of large indexes and don&apos;t keep them in memory all that often, our performance increases dramatically due to the reduction in GC overhead.  As we move to not actually storing the Norms array in memory but instead using the disk, this change will have an even higher benefit.  I&apos;m in the process of preparing a set of patches that will help people that don&apos;t have long-lived indexes, and this is just one part.&lt;/p&gt;</comment>
                    <comment id="12368436" author="tamm" created="Thu, 2 Mar 2006 12:23:52 +0000"  >&lt;p&gt;Here&apos;s a patch where if you turn LOAD_NORMS_INTO_MEM to false, it will instead load the norms from the disk all the time.  When combined with &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-508&quot; title=&quot;SegmentTermEnum.next() doesn&amp;#39;t maintain prevBuffer at end&quot;&gt;&lt;del&gt;LUCENE-508&lt;/del&gt;&lt;/a&gt; (the prefetching patch), you can dramatically reduce the amount of memory generated when you have 1 query/index.&lt;/p&gt;</comment>
                    <comment id="12368437" author="yseeley@gmail.com" created="Thu, 2 Mar 2006 12:38:09 +0000"  >&lt;p&gt;&amp;gt; I made the change less for MultiReader, but to prevent the instantiation of the fakeNorms array (which is an extra 1MB of useless memory for us).&lt;br/&gt;
Are you using the omitNorms feature?  If not, what is causing the fakeNorms to be allocated?&lt;/p&gt;</comment>
                    <comment id="12368440" author="yseeley@gmail.com" created="Thu, 2 Mar 2006 12:49:46 +0000"  >&lt;p&gt;&amp;gt; One can now omit norms when indexing, and, if such a field is searched with a normal query then fakeNorms will be used.&lt;br/&gt;
&amp;gt;  But a ConstantScoringQuery of the field should return the same results, and faster too!&lt;/p&gt;

&lt;p&gt;If you mean ConstantScoreQuery, that doesn&apos;t currently include tf or idf, so the scores won&apos;t match.&lt;/p&gt;

&lt;p&gt;There is a memory related optimization that can be made in SegmentReader.norms(field,array,offset) though.&lt;br/&gt;
System.arraycopy(fakeNorms(), 0, bytes, offset, maxDoc());&lt;br/&gt;
could be replaced with Arrays.fill&lt;/p&gt;</comment>
                    <comment id="12368443" author="tamm" created="Thu, 2 Mar 2006 13:03:04 +0000"  >&lt;p&gt;We&apos;re still using TermScorer, which generates the fakeNorms() regardless of omitNorms on or off.  ConstantTermScorer is a step in the right direction, but you already said what I was going to say about it .&lt;/p&gt;

&lt;p&gt;Specifically, we have one field where we want norms, and one field where we don&apos;t.  As you can see by the &quot;LazyNorms&quot; patch, the big bang for the buck for us (besides optimizing TermScorer.score()), was to lazily load the norms from the disk since we don&apos;t keep the indexes in memory.  If you only are scoring 3 docs (ever), why should it load a 1MB array (or in the case of TermScorer, a 1MB array from disk and then an empty 1MB array).  A better ConstantScoreQuery would also work, but only if the caller knew which fields had omitNorms on or off.&lt;/p&gt;</comment>
                    <comment id="12368447" author="yseeley@gmail.com" created="Thu, 2 Mar 2006 13:47:37 +0000"  >&lt;p&gt;&amp;gt;We&apos;re still using TermScorer, which generates the fakeNorms() regardless of omitNorms on or off.&lt;/p&gt;

&lt;p&gt;Let me focus on that point for the moment so we can see if there is a bug in fakeNorms or not.&lt;br/&gt;
If omitNorms is off (the normal behavior of all indexed fields having norms), then fakeNorms won&apos;t ever be allocated,&lt;br/&gt;
&lt;b&gt;except&lt;/b&gt; in the case of a MultiReader calling norms() on a subreader that doesn&apos;t know about that field (because no docs had that field indexed).  That allocation of fakeNorms() would also be eliminated by the Arrays.fill() fix I mentioned above.  &lt;br/&gt;
I&apos;ll look into that after Lucene 2.0 comes out.&lt;/p&gt;</comment>
                    <comment id="12368771" author="cutting" created="Sat, 4 Mar 2006 03:15:18 +0000"  >&lt;p&gt;It is not clear to me that your uses are typical uses.  These optimizations were added because they made big improvements.  They were not premature.  In some cases JVM&apos;s may have evolved so that some of them are no longer required.  But some of them may still make significant improvements for lots of users.&lt;/p&gt;

&lt;p&gt;I&apos;d like to see some benchmarks from other applications before we commit big changes to such inner loops.&lt;/p&gt;</comment>
                    <comment id="12374244" author="tamm" created="Thu, 13 Apr 2006 05:24:11 +0100"  >&lt;p&gt;There was a bug in MultiReader.java where I wasn&apos;t handling the caches correctly, specifically in getNormFactors and doSetNorm.&lt;/p&gt;

&lt;p&gt;This is also smaller and updated for 2.0&lt;/p&gt;</comment>
                    <comment id="12648054" author="markrmiller@gmail.com" created="Mon, 17 Nov 2008 00:28:08 +0000"  >&lt;p&gt;From my experience with &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-831&quot; title=&quot;Complete overhaul of FieldCache API/Implementation&quot;&gt;LUCENE-831&lt;/a&gt;, changing array access to method invocation does come with a good size penalty (5-15% was what I generally saw I believe). These are not identical things, but I think are close enough to take away that there will be a good measurable size hit to such a change. On the other hand, you could do cool norms reopen stuff with the method calls (eg have each IndexReader maintain its own norms and MultiReaders sub delegate norms calls)...&lt;/p&gt;</comment>
                    <comment id="12664255" author="svella" created="Thu, 15 Jan 2009 20:37:37 +0000"  >&lt;p&gt;Without this sort of change, searching a large index (think 100 million or more) uses an inordinate amount of heap.&lt;/p&gt;</comment>
                    <comment id="12664272" author="thetaphi" created="Thu, 15 Jan 2009 21:28:33 +0000"  >&lt;p&gt;In my opinion the problem with large indexes is more, that each SegmentReader has a cache of the last used norms. If you have many fields with norms enabled the cache grows and is never freed. In my opinion, the cache should be a LRU cache or a WeakHashMap or something like that.&lt;br/&gt;
You can see this problem, if you create an index with many fields with norms (I tested with about 4,000 fields) and many documents (half a million). If you then call CheckIndex, that calls norms() for each &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/warning.gif&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; field in the Segment and each of this calls creates a new cache entry, you get OutOfMemoryExceptions after short time (I tested with the above index: I was not able to do a CheckIndex even with &quot;-Xmx 16GB&quot; on 64bit Java).&lt;/p&gt;</comment>
                    <comment id="12664280" author="mikemccand" created="Thu, 15 Jan 2009 21:49:42 +0000"  >&lt;blockquote&gt;&lt;p&gt;In my opinion the problem with large indexes is more, that each SegmentReader has a cache of the last used norms.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I believe when MultiReader.norms is called (as Doug &amp;amp; Yonik said above), the underlying SegmentReaders do not in fact cache the norms (this is not readily obvious until you scrutinize the code).  Ie, it&apos;s only MultiReader that caches the full array.&lt;/p&gt;

&lt;p&gt;But I agree there would be good benefits (not creating fakeNorms) to moving away from byte[] for norms.  I think an iterator only API might be fine (giving us more freedom on the impl.), though I would worry about performance impact.&lt;/p&gt;

&lt;p&gt;Or we could make a new method to replace norms() that returns null when the field has no norms, and then Scorers that use this API would handle the null correctly.  We could fix all core/contribs to use the new API...&lt;/p&gt;

&lt;p&gt;Also note that with &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-1483&quot; title=&quot;Change IndexSearcher multisegment searches to search each individual segment using a single HitCollector&quot;&gt;&lt;del&gt;LUCENE-1483&lt;/del&gt;&lt;/a&gt;, we are moving to searching each segment at a time, so MultiReader.norms should not normally be called, unless it doesn&apos;t expose its underlying readers.&lt;/p&gt;</comment>
                    <comment id="12664290" author="thetaphi" created="Thu, 15 Jan 2009 22:15:20 +0000"  >&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;In my opinion the problem with large indexes is more, that each SegmentReader has a cache of the last used norms.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I believe when MultiReader.norms is called (as Doug &amp;amp; Yonik said above), the underlying SegmentReaders do not in fact cache the norms (this is not readily obvious until you scrutinize the code). Ie, it&apos;s only MultiReader that caches the full array.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;In my opinion, this is not correct. I did not use a MultiReader. CheckIndex opens and then tests each segment with a separate SegmentReader. The big index with the OutOfMemory problem was optimized, so consisting of one segment with about half a million docs and about 4,000 fields. Each byte[] array takes about a half MiB for this index. The CheckIndex funtion created the norm for 4000 fields and the SegmentReader cached them, which is about 2 GiB RAM. So OOMs are not unusal.&lt;/p&gt;

&lt;p&gt;The code taken from SegmentReader is here:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;  &lt;span class=&quot;code-keyword&quot;&gt;protected&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;synchronized&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;byte&lt;/span&gt;[] getNorms(&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt; field) &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; IOException {
    Norm norm = (Norm) norms.get(field);
    &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (norm == &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;) &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;;  &lt;span class=&quot;code-comment&quot;&gt;// not indexed, or norms not stored
&lt;/span&gt;    &lt;span class=&quot;code-keyword&quot;&gt;synchronized&lt;/span&gt;(norm) {
      &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (norm.bytes == &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;) {                     &lt;span class=&quot;code-comment&quot;&gt;// value not yet read
&lt;/span&gt;        &lt;span class=&quot;code-object&quot;&gt;byte&lt;/span&gt;[] bytes = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;byte&lt;/span&gt;[maxDoc()];
        norms(field, bytes, 0);
        norm.bytes = bytes;                         &lt;span class=&quot;code-comment&quot;&gt;// cache it
&lt;/span&gt;        &lt;span class=&quot;code-comment&quot;&gt;// it&apos;s OK to close the underlying IndexInput as we have cached the
&lt;/span&gt;        &lt;span class=&quot;code-comment&quot;&gt;// norms and will never read them again.
&lt;/span&gt;        norm.close();
      }
      &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; norm.bytes;
    }
  }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Each reader contains a Map of Norm entries for each field. When for the first time the norm for a specific field are read, norm.bytes==null and then it is cached inside this Norm object. And it is never freed.&lt;/p&gt;

&lt;p&gt;In my opinion, the best would be to use a Weak- or better a SoftReference so norms.bytes gets java.lang.ref.SoftReference&amp;lt;byte[]&amp;gt; and used for caching.&lt;/p&gt;

&lt;p&gt;I will prepare a patch, should I open a new issue for that? I found this problem yesterday when testing with very large indexes (you may have noticed my mail about removing norms from Trie fields).&lt;/p&gt;

&lt;p&gt;Uwe&lt;/p&gt;</comment>
                    <comment id="12664295" author="mikemccand" created="Thu, 15 Jan 2009 22:30:48 +0000"  >
&lt;blockquote&gt;&lt;p&gt;CheckIndex opens and then tests each segment with a separate SegmentReader.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;You&apos;re right: CheckIndex loads the norms of every field (even those&lt;br/&gt;
w/o norms), and then that memory is not released until that reader is&lt;br/&gt;
closed &amp;amp; unreferenced.&lt;/p&gt;

&lt;p&gt;But that&apos;s a different issue than this one (this one is about MultiReader).&lt;/p&gt;

&lt;p&gt;Can you open a new issue?  I don&apos;t think Soft/WeakReference is the right&lt;br/&gt;
solution (they give us little control on when the cache is evicted); we&lt;br/&gt;
could do something first specifically for CheckIndex (eg it could&lt;br/&gt;
simply use the 3-arg non-caching bytes method instead) to prevent OOM&lt;br/&gt;
errors when using it.&lt;/p&gt;</comment>
                    <comment id="12664306" author="thetaphi" created="Thu, 15 Jan 2009 22:44:58 +0000"  >&lt;p&gt;Mike: I created new issue &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-1520&quot; title=&quot;OOM erros with CheckIndex with indexes containg a lot of fields with norms&quot;&gt;&lt;del&gt;LUCENE-1520&lt;/del&gt;&lt;/a&gt; and added some remarks to your last message, too.&lt;/p&gt;</comment>
                    <comment id="12986426" author="thetaphi" created="Tue, 25 Jan 2011 15:18:11 +0000"  >&lt;p&gt;Since Lucene 2.9 we search on each segment separately, so MultiReader&apos;s norms cache would never be used, exept in custom code that calls norms() on the MultiReader/DirectoryReader. Since Lucene 4.0 this is also not allowed anymore, non-atomic readers don&apos;t support norms. If you still need to get global norms, you can use MultiNorms but that is discouraged.&lt;/p&gt;

&lt;p&gt;See also: &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-2771&quot; title=&quot;Remove norms() support from non-atomic IndexReaders&quot;&gt;&lt;del&gt;LUCENE-2771&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;</comment>
                </comments>
                    <attachments>
                    <attachment id="12323607" name="LazyNorms.patch" size="1507" author="tamm" created="Thu, 2 Mar 2006 12:23:52 +0000" />
                    <attachment id="12325256" name="NormFactors20.patch" size="21062" author="tamm" created="Thu, 13 Apr 2006 05:24:10 +0100" />
                    <attachment id="12323582" name="NormFactors.patch" size="26278" author="tamm" created="Thu, 2 Mar 2006 06:57:22 +0000" />
                    <attachment id="12323581" name="NormFactors.patch" size="48839" author="tamm" created="Thu, 2 Mar 2006 06:29:44 +0000" />
                </attachments>
            <subtasks>
        </subtasks>
                <customfields>
                                <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                <customfieldname>Attachment count</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>4.0</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310220" key="com.atlassian.jira.ext.charting:firstresponsedate">
                <customfieldname>Date of First Response</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>Thu, 2 Mar 2006 07:15:11 +0000</customfieldvalue>

                </customfieldvalues>
            </customfield>
                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Global Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>13245</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                <customfieldname>Rank</customfieldname>
                <customfieldvalues>
                    <customfieldvalue>27222</customfieldvalue>
                </customfieldvalues>
            </customfield>
                                                                                    <customfield id="customfield_12310222" key="com.atlassian.jira.ext.charting:timeinstatus">
                <customfieldname>Time in Status</customfieldname>
                <customfieldvalues>
                    
                </customfieldvalues>
            </customfield>
                            </customfields>
    </item>
</channel>
</rss>